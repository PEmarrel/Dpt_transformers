{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Pour torch si vous avez un GPU\n",
    "# device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "device = \"cpu\" # Pour forcer l'utilisation du CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environement potentielement testé\n",
    "from environnement.environnement import Environnement as env # mother class\n",
    "from environnement.environnement1 import Environnement1 as env1\n",
    "from environnement.environnement2Str import Environnement2 as env2Str\n",
    "from environnement.environnement3Str import Environnement3 as env3Str\n",
    "from environnement.environnement6Str import Environnement6 as env6Str\n",
    "from environnement.small_loop import small_loop\n",
    "\n",
    "# model machine learning\n",
    "from model.DeepNN import *\n",
    "from model.Tokenizer import *\n",
    "from outil import *\n",
    "from inter.interactions import Interaction\n",
    "from inter.simpleInteraction import simpleInteraction as inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'agent qui choisit son destin\n",
    "Dans se notebook nous reprennons la méthode de prédiciton de l'agent 1, mais cette fois l'action ne sera plus pris au hasard. Une fois que notre modèle est entrainer et arrive a correctement prédire le prochain feedback, l'agent poura prendre la meilleur action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 2\n",
    "Nous reprenons l'agent 1 et nous allons lui ajouter une fonction 'decide', qui permet de choisir la bonne action a faire en suivant une valence et les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2:\n",
    "    def __init__(self, model, all_outcomes, all_actions, valance, tokenizer, optimizer=None, loss_func=None):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self._otimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._tokenizer:SimpleTokenizerV1 = tokenizer\n",
    "        self._all_outcomes = all_outcomes\n",
    "        self._all_actions = all_actions\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "        self._valance=valance\n",
    "\n",
    "    def fit(self, actions:list, outcomes:list, validate_loader=None):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent\n",
    "        \"\"\"\n",
    "        actions = [[self._tokenizer.encode(act)] for act in actions]\n",
    "        outcomes = self._tokenizer.encode(outcomes)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.train()\n",
    "            actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
    "            outcomes = torch.tensor(outcomes, dtype=torch.long).to(device)\n",
    "            outcomes = torch.nn.functional.one_hot(outcomes, \n",
    "                num_classes=len(self._all_outcomes) # On précise le nombre d'ouctomes possible \n",
    "                ).to(torch.float)\n",
    "            \n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                torch.utils.data.TensorDataset(actions, outcomes),\n",
    "                batch_size=32, shuffle=True\n",
    "            )\n",
    "\n",
    "            train_with_batch(model=self._model, \n",
    "                    train_loader=data_loader,\n",
    "                    optimizer=self._otimizer,\n",
    "                    loss_func=self._loss_func,\n",
    "                    nb_epochs=10,\n",
    "                    validate_loader=validate_loader,\n",
    "                    print_=True)\n",
    "        else: # Si le model n'est pas un model pytorch\n",
    "            raise Exception('Not implemented')\n",
    "            self._model.fit(action, outcome)\n",
    "            pass\n",
    "    \n",
    "    # Ajout par raport a l'agent 1 bonus\n",
    "    def decide(self):\n",
    "        \"\"\"\n",
    "        Fonction qui va choisir la meilleur action a faire, dépandament des prédictions du modèles entrainné.\n",
    "        \"\"\"\n",
    "        best_act = self._all_actions[0]\n",
    "        best_expected_val = -np.inf\n",
    "        for act in self._all_actions:\n",
    "            predi = self.predict(act)\n",
    "            expected_val = self._valance[inter(act, predi)]\n",
    "            if expected_val > best_expected_val:\n",
    "                best_act = act\n",
    "                best_expected_val = expected_val\n",
    "        self._action = best_act\n",
    "        return best_act\n",
    "\n",
    "    def predict(self, action):\n",
    "        \"\"\"\n",
    "        Funciton de prédiction\n",
    "        \"\"\"\n",
    "        action = self._tokenizer.encode(action)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor([action], dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.argmax(x, dim=0).item()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return self._tokenizer.decode(x)\n",
    "\n",
    "    def action(self, outcome, fit=True, validate_loader=None):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \" \n",
    "                  f\"\\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\")\n",
    "            if self._predicted_outcome != outcome:\n",
    "                self.fit(self._history_act, self._history_fb, validate_loader)\n",
    "            # Maintenant nous choisissons la prochaine action en fonction de la valance\n",
    "            self._action = self.decide()\n",
    "            self._history_act.append(self._action)\n",
    "            self._predicted_outcome = self.predict(self._action)\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)\n",
    "            self._predicted_outcome = self.predict(self._action)\n",
    "            print(f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\")\n",
    "        \n",
    "        return self._action, self._predicted_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste hidden init [10, 5]\n",
      "=======================\u001b[0;32m iteration 0 \u001b[0m=======================\n",
      "Action de base : a Prediction: y\n",
      "action a predi y outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 1 \u001b[0m=======================\n",
      "Action: a, Prediction: y, Outcome: x, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Epoch 1/10, Loss: 0.7937\n",
      "Epoch 2/10, Loss: 0.6944\n",
      "Epoch 3/10, Loss: 0.5426\n",
      "Epoch 4/10, Loss: 0.3785\n",
      "Epoch 5/10, Loss: 0.2087\n",
      "Epoch 6/10, Loss: 0.0738\n",
      "Epoch 7/10, Loss: 0.0136\n",
      "Epoch 8/10, Loss: 0.0015\n",
      "Epoch 9/10, Loss: 0.0001\n",
      "Epoch 10/10, Loss: 0.0000\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 2 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 3 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 4 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 5 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 6 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 7 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 8 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 9 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m90 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 10 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 11 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 12 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 13 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 14 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 15 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 16 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 17 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 18 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 19 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_test2 = env1()\n",
    "\n",
    "model_ML = DeepNetwork(hidden_size=[10, 5], input_size=1, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "valence = {\n",
    "    inter('a', 'x') : -1,\n",
    "    inter('a', 'y') : 1,\n",
    "    inter('b', 'x') : -1,\n",
    "    inter('b', 'y') : 1\n",
    "}\n",
    "agent_test2 = Agent2(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    valance=valence,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "for i in range(20):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats\n",
    "Au début le modèl peut avoir de la chance et correctement prédire une action sur deux corectement. Dans ce cas nous risquons de ne pas tester toutes les actions possible. Si l'agent ne teste pas toutes les actions possible alors il risque de se contenté d'une seul action même si elle ne maximise pas la valence.\n",
    "\n",
    "## Exemple :\n",
    "Si le modèl prédit initialement (c'est à dire sans entrainement), a => x et b => x. Et que la valance est celle ci :\n",
    "\n",
    "```py\n",
    "valence = {\n",
    "    inter('a', 'x') : -1,\n",
    "    inter('a', 'y') : 1,\n",
    "    inter('b', 'x') : -10,\n",
    "    inter('b', 'y') : 10\n",
    "}\n",
    "```\n",
    "\n",
    "Alors le modèl va préférer l'action 'a' indépendament du vrai résultat de de l'action b.\n",
    "\n",
    "Si l'environement renvois :\n",
    "\n",
    "- 'a' : 'x'\n",
    "- 'b' : 'y'\n",
    "\n",
    "Nous préférions que l'agent chocise l'action 'b'.\n",
    "\n",
    "## Solution\n",
    "Nous avons plusieurs solutions possible a ce problème.\n",
    "\n",
    "### Tester toutes les actions possible\n",
    "Nous pouvons tester toutes les actions possible pour pouvoir prédire correctement.\n",
    "\n",
    "### Mécanisme d'ennui\n",
    "Nous pouvons imaginer que notre agent a 'envie' d'explorer son environement pour s'assurer que ses prédictions sont correct. Nous pouvons mettre en place un cycle qui tout les X temps choisir une autre actions. Nous pouvons pousser ce mécanisme en le lien avec les probabilité, et choisir de tenter des actions pour lequel le modèl n'est pas sûr du résulat.\n",
    "\n",
    "### Probabilités\n",
    "Nous pouvons regarder les probablités pour chaques actions. Si pour une action le modèl donne de faible probababilité, alors l'agent peut vouloir s'entrainner sur ces actions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution, probabilités\n",
    "\n",
    "Implémentation de l'agent avec cette notion de probabilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2Prob:\n",
    "    def __init__(self, model, all_outcomes, all_actions, valance, tokenizer, optimizer=None, loss_func=None):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self._otimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._tokenizer:SimpleTokenizerV1 = tokenizer\n",
    "        self._all_outcomes = all_outcomes\n",
    "        self._all_actions = all_actions\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "        self._valance=valance\n",
    "\n",
    "    def fit(self, actions:list, outcomes:list,nb_epoch:int= 5, validate_loader=None):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent\n",
    "        \"\"\"\n",
    "        actions = [[self._tokenizer.encode(act)] for act in actions]\n",
    "        outcomes = self._tokenizer.encode(outcomes)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.train()\n",
    "            actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
    "            outcomes = torch.tensor(outcomes, dtype=torch.long).to(device)\n",
    "            outcomes = torch.nn.functional.one_hot(outcomes, \n",
    "                num_classes=len(self._all_outcomes) # On précise le nombre d'ouctomes possible \n",
    "                ).to(torch.float)\n",
    "            \n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                torch.utils.data.TensorDataset(actions, outcomes),\n",
    "                batch_size=32, shuffle=True\n",
    "            )\n",
    "\n",
    "            train_with_batch(model=self._model, \n",
    "                    train_loader=data_loader,\n",
    "                    optimizer=self._otimizer,\n",
    "                    loss_func=self._loss_func,\n",
    "                    nb_epochs=nb_epoch,\n",
    "                    validate_loader=validate_loader,\n",
    "                    print_=True)\n",
    "        else: # Si le model n'est pas un model pytorch\n",
    "            raise Exception('Not implemented')\n",
    "            self._model.fit(action, outcome)\n",
    "            pass\n",
    "\n",
    "    def get_prediction(self, action):\n",
    "        action = self._tokenizer.encode(action)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor([action], dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.nn.functional.softmax(x, dim=0)\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Ajout par raport a l'agent 1 bonus\n",
    "    def decide(self):\n",
    "        \"\"\"\n",
    "        Fonction qui choisit l'action a faire en fonction des prédictions \\\n",
    "        du modèles entrainné. Nous renforçons choisisons les actions que \\\n",
    "        ou le modèle n'est pas sûr.\n",
    "        \"\"\"\n",
    "        best_act = self._all_actions[0]\n",
    "        best_expected_val = -np.inf\n",
    "        # Vérifie si le modèles est sur de sa prédiction\n",
    "        for act in self._all_actions:\n",
    "            probs:torch.Tensor = self.get_prediction(act)\n",
    "            max_prob = torch.max(probs).item()\n",
    "            # Formule utiliser : 1 / n + 0.5 / n\n",
    "            print(f'for action {act} probs {probs} max_prob {max_prob}')\n",
    "            if max_prob < 1 / probs.size(dim=0) + 0.5 / probs.size(dim=0):\n",
    "                return act\n",
    "            # Si le modèle as une prédiction sur, on regarde sa valance\n",
    "            predi = self._tokenizer.decode(torch.argmax(probs, dim=0).item())\n",
    "            expected_val = self._valance[inter(act, predi)]\n",
    "            if expected_val > best_expected_val:\n",
    "                best_act = act\n",
    "                best_expected_val = expected_val\n",
    "        self._action = best_act\n",
    "        return best_act\n",
    "\n",
    "    def predict(self, action):\n",
    "        \"\"\"\n",
    "        Funciton de prédiction\n",
    "        \"\"\"\n",
    "        action = self._tokenizer.encode(action)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor([action], dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.argmax(x, dim=0).item()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return self._tokenizer.decode(x)\n",
    "\n",
    "    def action(self, outcome, fit=True, validate_loader=None):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \" \n",
    "                  f\"\\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\")\n",
    "            if self._predicted_outcome != outcome:\n",
    "                self.fit(self._history_act, self._history_fb, validate_loader=validate_loader)\n",
    "            # Maintenant nous choisissons la prochaine action en fonction de la valance\n",
    "            self._action = self.decide()\n",
    "            self._history_act.append(self._action)\n",
    "            self._predicted_outcome = self.predict(self._action)\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)\n",
    "            self._predicted_outcome = self.predict(self._action)\n",
    "            print(f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\")\n",
    "        \n",
    "        return self._action, self._predicted_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste hidden init [10, 5]\n",
      "=======================\u001b[0;32m iteration 0 \u001b[0m=======================\n",
      "Action de base : a Prediction: y\n",
      "action a predi y outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 1 \u001b[0m=======================\n",
      "Action: a, Prediction: y, Outcome: x, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Epoch 1/5, Loss: 0.7538\n",
      "Epoch 2/5, Loss: 0.5847\n",
      "Epoch 3/5, Loss: 0.4935\n",
      "Epoch 4/5, Loss: 0.4183\n",
      "Epoch 5/5, Loss: 0.3414\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 2 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 3 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 4 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 5 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 6 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 7 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 8 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 9 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m90 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 10 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 11 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 12 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 13 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 14 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 15 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 16 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 17 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 18 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 19 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7727, 0.2273], grad_fn=<SoftmaxBackward0>) max_prob 0.772729218006134\n",
      "for action b probs tensor([0.7871, 0.2129], grad_fn=<SoftmaxBackward0>) max_prob 0.7870858311653137\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_test2 = env1()\n",
    "\n",
    "model_ML = DeepNetwork(hidden_size=[10, 5], input_size=1, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "valence = {\n",
    "    inter('a', 'x') : -1,\n",
    "    inter('a', 'y') : 1,\n",
    "    inter('b', 'x') : -1,\n",
    "    inter('b', 'y') : 1\n",
    "}\n",
    "agent_test2 = Agent2Prob(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    valance=valence,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "for i in range(20):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultat :\n",
    "L'agent renforce ca connaicance sur 'a', mais renforce aussi sa __mauvaisse__ connaissance sur 'b'. Il est sûr que 'b' donne 'x' sans avoir tester l'action. Pour palier a ce problème, nous pouvons obliger l'agent a tenter toutes les actions possible.\n",
    "\n",
    "# Nouvelle agent\n",
    "Nous allons implémenter deux solutions, celle des probabilité et celle de tester toutes les actions possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2ProbAllTest:\n",
    "    def __init__(self, model, all_outcomes, all_actions, valance, tokenizer, optimizer=None, loss_func=None):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self._otimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._tokenizer:SimpleTokenizerV1 = tokenizer\n",
    "        self._all_outcomes = all_outcomes\n",
    "        self._all_actions = all_actions\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "        self._valance=valance\n",
    "\n",
    "    def fit(self, actions:list, outcomes:list,nb_epoch:int= 5, validate_loader=None):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent\n",
    "        \"\"\"\n",
    "        actions = [[self._tokenizer.encode(act)] for act in actions]\n",
    "        outcomes = self._tokenizer.encode(outcomes)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.train()\n",
    "            actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
    "            outcomes = torch.tensor(outcomes, dtype=torch.long).to(device)\n",
    "            outcomes = torch.nn.functional.one_hot(outcomes, \n",
    "                num_classes=len(self._all_outcomes) # On précise le nombre d'ouctomes possible \n",
    "                ).to(torch.float)\n",
    "            \n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                torch.utils.data.TensorDataset(actions, outcomes),\n",
    "                batch_size=32, shuffle=True\n",
    "            )\n",
    "\n",
    "            train_with_batch(model=self._model, \n",
    "                    train_loader=data_loader,\n",
    "                    optimizer=self._otimizer,\n",
    "                    loss_func=self._loss_func,\n",
    "                    nb_epochs=nb_epoch,\n",
    "                    validate_loader=validate_loader,\n",
    "                    print_=True)\n",
    "        else: # Si le model n'est pas un model pytorch\n",
    "            raise Exception('Not implemented')\n",
    "            self._model.fit(action, outcome)\n",
    "            pass\n",
    "\n",
    "    def get_prediction(self, action):\n",
    "        action = self._tokenizer.encode(action)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor([action], dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.nn.functional.softmax(x, dim=0)\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Nouvelle fonction qui renvois None si le modèle a tester toutes les actions\n",
    "    def check_all_actions(self):\n",
    "        act_to_test = None\n",
    "        for act in self._all_actions:\n",
    "            if act not in self._history_act:\n",
    "                act_to_test = act\n",
    "                break\n",
    "        return act_to_test\n",
    "    \n",
    "    def decide(self):\n",
    "        \"\"\"\n",
    "        Fonction qui choisit l'action a faire en fonction des prédictions \\\n",
    "        du modèles entrainné. Nous renforçons choisisons les actions que \\\n",
    "        ou le modèle n'est pas sûr.\n",
    "        \"\"\"\n",
    "\n",
    "        act_test = self.check_all_actions()\n",
    "        if act_test:\n",
    "            print(\"i don't know\", act_test)\n",
    "            self._action = act_test\n",
    "            return act_test\n",
    "\n",
    "        best_act = self._all_actions[0]\n",
    "        best_expected_val = -np.inf\n",
    "        # Vérifie si le modèles est sur de sa prédiction\n",
    "        for act in self._all_actions:\n",
    "            probs:torch.Tensor = self.get_prediction(act)\n",
    "            max_prob = torch.max(probs).item()\n",
    "            # Formule utiliser : 1 / n + 0.5 / n\n",
    "            print(f'for action {act} probs {probs} max_prob {max_prob}')\n",
    "            if max_prob < 1 / probs.size(dim=0) + 0.5 / probs.size(dim=0):\n",
    "                print(\"je ne suis pas sur de \", act)\n",
    "                self.fit(self._history_act, self._history_fb, validate_loader=None)\n",
    "\n",
    "            probs:torch.Tensor = self.get_prediction(act)\n",
    "            if max_prob < 1 / probs.size(dim=0) + 0.5 / probs.size(dim=0):\n",
    "                print(\"je n'arrive pas à être sur de \", act)\n",
    "                return act\n",
    "            \n",
    "            # Si le modèle as une prédiction sur, on regarde sa valance\n",
    "            predi = self._tokenizer.decode(torch.argmax(probs, dim=0).item())\n",
    "            expected_val = self._valance[inter(act, predi)]\n",
    "            if expected_val > best_expected_val:\n",
    "                best_act = act\n",
    "                best_expected_val = expected_val\n",
    "        self._action = best_act\n",
    "        return best_act\n",
    "\n",
    "    def predict(self, action):\n",
    "        \"\"\"\n",
    "        Funciton de prédiction\n",
    "        \"\"\"\n",
    "        action = self._tokenizer.encode(action)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor([action], dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.argmax(x, dim=0).item()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return self._tokenizer.decode(x)\n",
    "\n",
    "    def action(self, outcome, fit=True, validate_loader=None):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \" \n",
    "                  f\"\\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\")\n",
    "            if self._predicted_outcome != outcome:\n",
    "                self.fit(self._history_act, self._history_fb, validate_loader=validate_loader)\n",
    "            # Maintenant nous choisissons la prochaine action en fonction de la valance\n",
    "            self._action = self.decide()\n",
    "            self._history_act.append(self._action)\n",
    "            self._predicted_outcome = self.predict(self._action)\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)\n",
    "            self._predicted_outcome = self.predict(self._action)\n",
    "            print(f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\")\n",
    "        \n",
    "        return self._action, self._predicted_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste hidden init [10, 5]\n",
      "=======================\u001b[0;32m iteration 0 \u001b[0m=======================\n",
      "Action de base : a Prediction: x\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 1 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "i don't know b\n",
      "action b predi x outcome y\n",
      "Action choisie : b \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 2 \u001b[0m=======================\n",
      "Action: b, Prediction: x, Outcome: y, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Epoch 1/5, Loss: 0.7508\n",
      "Epoch 2/5, Loss: 0.6654\n",
      "Epoch 3/5, Loss: 0.6310\n",
      "Epoch 4/5, Loss: 0.6550\n",
      "Epoch 5/5, Loss: 0.6118\n",
      "for action a probs tensor([0.5552, 0.4448], grad_fn=<SoftmaxBackward0>) max_prob 0.5551952123641968\n",
      "je ne suis pas sur de  a\n",
      "Epoch 1/5, Loss: 0.6128\n",
      "Epoch 2/5, Loss: 0.5934\n",
      "Epoch 3/5, Loss: 0.5524\n",
      "Epoch 4/5, Loss: 0.5158\n",
      "Epoch 5/5, Loss: 0.4858\n",
      "je n'arrive pas à être sur de  a\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 3 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.5636, 0.4364], grad_fn=<SoftmaxBackward0>) max_prob 0.5635899305343628\n",
      "je ne suis pas sur de  a\n",
      "Epoch 1/5, Loss: 0.4758\n",
      "Epoch 2/5, Loss: 0.3904\n",
      "Epoch 3/5, Loss: 0.3446\n",
      "Epoch 4/5, Loss: 0.2796\n",
      "Epoch 5/5, Loss: 0.2255\n",
      "je n'arrive pas à être sur de  a\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 4 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 5 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 6 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 7 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 8 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 9 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m90 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 10 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m90 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 11 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 12 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 13 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 14 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 15 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 16 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 17 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 18 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 19 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: y, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.8820, 0.1180], grad_fn=<SoftmaxBackward0>) max_prob 0.8819542527198792\n",
      "for action b probs tensor([0.2015, 0.7985], grad_fn=<SoftmaxBackward0>) max_prob 0.7985320687294006\n",
      "action b predi y outcome y\n",
      "Action choisie : b \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_test2 = env1()\n",
    "\n",
    "model_ML = DeepNetwork(hidden_size=[10, 5], input_size=1, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "valence = {\n",
    "    inter('a', 'x') : -1,\n",
    "    inter('a', 'y') : 1,\n",
    "    inter('b', 'x') : -1,\n",
    "    inter('b', 'y') : 1\n",
    "}\n",
    "agent_test2 = Agent2ProbAllTest(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    valance=valence,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "for i in range(20):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultat\n",
    "L'agent arrive a tester toutes les actions et renforce ses connaissances sur les actions dont il n'est sûr. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste hidden init [10, 5]\n",
      "=======================\u001b[0;32m iteration 0 \u001b[0m=======================\n",
      "Action de base : a Prediction: x\n",
      "action a predi x outcome y\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 1 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: y, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Epoch 1/5, Loss: 0.8688\n",
      "Epoch 2/5, Loss: 0.6334\n",
      "Epoch 3/5, Loss: 0.4280\n",
      "Epoch 4/5, Loss: 0.1810\n",
      "Epoch 5/5, Loss: 0.0330\n",
      "i don't know b\n",
      "action b predi y outcome x\n",
      "Action choisie : b \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 2 \u001b[0m=======================\n",
      "Action: b, Prediction: y, Outcome: x, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Epoch 1/5, Loss: 3.7932\n",
      "Epoch 2/5, Loss: 2.8725\n",
      "Epoch 3/5, Loss: 1.8562\n",
      "Epoch 4/5, Loss: 1.1559\n",
      "Epoch 5/5, Loss: 0.8428\n",
      "for action a probs tensor([0.3268, 0.6732], grad_fn=<SoftmaxBackward0>) max_prob 0.6732020378112793\n",
      "je ne suis pas sur de  a\n",
      "Epoch 1/5, Loss: 0.7571\n",
      "Epoch 2/5, Loss: 0.7443\n",
      "Epoch 3/5, Loss: 0.7324\n",
      "Epoch 4/5, Loss: 0.7217\n",
      "Epoch 5/5, Loss: 0.7127\n",
      "je n'arrive pas à être sur de  a\n",
      "action a predi y outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 3 \u001b[0m=======================\n",
      "Action: a, Prediction: y, Outcome: x, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Epoch 1/5, Loss: 0.7577\n",
      "Epoch 2/5, Loss: 0.7362\n",
      "Epoch 3/5, Loss: 0.7155\n",
      "Epoch 4/5, Loss: 0.6965\n",
      "Epoch 5/5, Loss: 0.6798\n",
      "for action a probs tensor([0.5479, 0.4521], grad_fn=<SoftmaxBackward0>) max_prob 0.5478665232658386\n",
      "je ne suis pas sur de  a\n",
      "Epoch 1/5, Loss: 0.6657\n",
      "Epoch 2/5, Loss: 0.6546\n",
      "Epoch 3/5, Loss: 0.6466\n",
      "Epoch 4/5, Loss: 0.6409\n",
      "Epoch 5/5, Loss: 0.6377\n",
      "je n'arrive pas à être sur de  a\n",
      "action a predi x outcome y\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 4 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: y, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Epoch 1/5, Loss: 0.7491\n",
      "Epoch 2/5, Loss: 0.7601\n",
      "Epoch 3/5, Loss: 0.7682\n",
      "Epoch 4/5, Loss: 0.7729\n",
      "Epoch 5/5, Loss: 0.7743\n",
      "for action a probs tensor([0.6916, 0.3084], grad_fn=<SoftmaxBackward0>) max_prob 0.6915950179100037\n",
      "je ne suis pas sur de  a\n",
      "Epoch 1/5, Loss: 0.7725\n",
      "Epoch 2/5, Loss: 0.7681\n",
      "Epoch 3/5, Loss: 0.7614\n",
      "Epoch 4/5, Loss: 0.7532\n",
      "Epoch 5/5, Loss: 0.7440\n",
      "je n'arrive pas à être sur de  a\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 5 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.6409, 0.3591], grad_fn=<SoftmaxBackward0>) max_prob 0.6408694386482239\n",
      "je ne suis pas sur de  a\n",
      "Epoch 1/5, Loss: 0.6766\n",
      "Epoch 2/5, Loss: 0.6745\n",
      "Epoch 3/5, Loss: 0.6733\n",
      "Epoch 4/5, Loss: 0.6730\n",
      "Epoch 5/5, Loss: 0.6733\n",
      "je n'arrive pas à être sur de  a\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 6 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.5766, 0.4234], grad_fn=<SoftmaxBackward0>) max_prob 0.5766417980194092\n",
      "je ne suis pas sur de  a\n",
      "Epoch 1/5, Loss: 0.6535\n",
      "Epoch 2/5, Loss: 0.6566\n",
      "Epoch 3/5, Loss: 0.6588\n",
      "Epoch 4/5, Loss: 0.6599\n",
      "Epoch 5/5, Loss: 0.6600\n",
      "je n'arrive pas à être sur de  a\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 7 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.5629, 0.4371], grad_fn=<SoftmaxBackward0>) max_prob 0.562940776348114\n",
      "je ne suis pas sur de  a\n",
      "Epoch 1/5, Loss: 0.6469\n",
      "Epoch 2/5, Loss: 0.6446\n",
      "Epoch 3/5, Loss: 0.6317\n",
      "Epoch 4/5, Loss: 0.6109\n",
      "Epoch 5/5, Loss: 0.5927\n",
      "je n'arrive pas à être sur de  a\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 8 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 9 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m60 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 10 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m70 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 11 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m80 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 12 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m90 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 13 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 14 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 15 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 16 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 17 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 18 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 19 \u001b[0m=======================\n",
      "Action: a, Prediction: x, Outcome: x, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "for action a probs tensor([0.7885, 0.2115], grad_fn=<SoftmaxBackward0>) max_prob 0.7885042428970337\n",
      "for action b probs tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward0>) max_prob 0.8312364220619202\n",
      "action a predi x outcome x\n",
      "Action choisie : a \u001b[0;34m100 \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_test2 = env6Str()\n",
    "\n",
    "model_ML = DeepNetwork(hidden_size=[10, 5], input_size=1, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "valence = {\n",
    "    inter('a', 'x') : -1,\n",
    "    inter('a', 'y') : 1,\n",
    "    inter('b', 'x') : -1,\n",
    "    inter('b', 'y') : 1\n",
    "}\n",
    "agent_test2 = Agent2ProbAllTest(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    valance=valence,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "for i in range(20):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
