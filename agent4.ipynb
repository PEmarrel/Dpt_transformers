{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Pour torch si vous avez un GPU\n",
    "# device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "device = \"cpu\" # Pour forcer l'utilisation du CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environement potentielement testé\n",
    "from environnement.environnement import Environnement as env # mother class\n",
    "from environnement.environnement1 import Environnement1 as env1\n",
    "from environnement.environnement2Str import Environnement2 as env2Str\n",
    "from environnement.environnement3Str import Environnement3 as env3Str\n",
    "from environnement.environnement6Str import Environnement6 as env6Str\n",
    "from environnement.small_loop import small_loop\n",
    "\n",
    "# model machine learning\n",
    "from model.DeepNN import *\n",
    "from model.Tokenizer import *\n",
    "from model.CustomLoader import CustomLoader\n",
    "from outil import *\n",
    "from inter.interactions import Interaction\n",
    "from inter.simpleInteraction import simpleInteraction as inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'agent qui voit les paterns\n",
    "Nos précedants agent fonctionnent pour des environements simple, dont le feedback ne dépand que de l'action choisit. Nous aimerions maintenant un agent capable de prédrie un feedback dans des environement dans lequel l'ouctome dépand des précédantes interactions. \n",
    "\n",
    "## Mécanisme de prédiction\n",
    "L'agent en lui même n'est pas différent de l'agent 2. Le changement a apporté se situe au niveau de l'entrainement du modèl. A la plade de lui passer seulement une action, nous allons lui donner une ou plusieurs interaction, suivit d'une action.\n",
    "\n",
    "### Exemples: \n",
    "Imaginons que notre agent a fais cette séquence d'interaction :\n",
    "\n",
    "- a, b : actions\n",
    "- x, y : feedback\n",
    "> a, x, b, y\n",
    "\n",
    "- Nous allons passer au modèls a, x, b\n",
    "- Et nous allons lui apprendre a prédire b\n",
    "\n",
    "L'idée derière est de reconnaitre des patterns, imaginons cette séquence d'interaction :\n",
    "> a, x, b, y, b, x, a, y, a, x\n",
    "\n",
    "Ici notre environement renvoie 'y' seulement quand l'action précédente est différente de celle actuelle. Nous allons passer comme jeu d'apprentissage au modèls :\n",
    "\n",
    "- a, x, b => y\n",
    "- b, y, b => x\n",
    "- b, x, a => y\n",
    "- a, y, a => x\n",
    "\n",
    "Le but est d'entrainer le modèls à reconnaitre des patterns pour prédire le bon feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent4:\n",
    "    def __init__(self, model, all_outcomes, all_actions, valance, tokenizer, optimizer=None, loss_func=None):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self._otimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._tokenizer:SimpleTokenizerV1 = tokenizer\n",
    "        self._all_outcomes = all_outcomes\n",
    "        self._all_actions = all_actions\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "        self._valance=valance\n",
    "\n",
    "    # Cette fonction est différente de l'agent 2\n",
    "    def fit(self, actions:list, outcomes:list, nb_epoch:int= 5, validate_loader=None):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent\n",
    "        \"\"\"\n",
    "        context_lenght = self._model.input_size\n",
    "        print(f'context_lenght {context_lenght}')\n",
    "        if len(actions) + len(outcomes) < context_lenght:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "\n",
    "        actions = [[self._tokenizer.encode(act)] for act in actions]\n",
    "        outcomes = self._tokenizer.encode(outcomes)\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.train()\n",
    "            actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
    "            outcomes = torch.tensor(outcomes, dtype=torch.long).to(device)\n",
    "            data_loarder = CustomLoader(actions=actions, outcomes=outcomes,\n",
    "                         context_lenght= context_lenght)\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                data_loarder,batch_size=32, shuffle=True)\n",
    "\n",
    "            train_with_batch(model=self._model, \n",
    "                    train_loader=data_loader,\n",
    "                    optimizer=self._otimizer,\n",
    "                    loss_func=self._loss_func,\n",
    "                    nb_epochs=nb_epoch,\n",
    "                    validate_loader=validate_loader,\n",
    "                    print_=True)\n",
    "        else: # Si le model n'est pas un model pytorch\n",
    "            raise Exception('Not implemented')\n",
    "            self._model.fit(action, outcome)\n",
    "            pass\n",
    "\n",
    "    def get_prediction(self, action):\n",
    "        gap = (self._model.input_size - 1) // 2\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            print(self._history_act)\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        action = self._tokenizer.encode(x)\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor(action, dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.nn.functional.softmax(x, dim=0)\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def check_all_actions(self):\n",
    "        act_to_test = None\n",
    "        for act in self._all_actions:\n",
    "            if act not in self._history_act:\n",
    "                act_to_test = act\n",
    "                break\n",
    "        return act_to_test\n",
    "    \n",
    "    def decide(self):\n",
    "        \"\"\"\n",
    "        Fonction qui choisit l'action a faire en fonction des prédictions \\\n",
    "        du modèles entrainné. Nous renforçons choisisons les actions que \\\n",
    "        ou le modèle n'est pas sûr.\n",
    "        \"\"\"\n",
    "\n",
    "        act_test = self.check_all_actions()\n",
    "        if act_test:\n",
    "            print(\"i don't know\", act_test)\n",
    "            self._action = act_test\n",
    "            return act_test\n",
    "\n",
    "        best_act = self._all_actions[0]\n",
    "        best_expected_val = -np.inf\n",
    "\n",
    "        # On vérifie que l'on a vue assez d'interaction pour faire des prédictions\n",
    "        if len(self._history_act) + len(self._history_fb) < self._model.input_size:\n",
    "            print(\"Not enough data to make a decision\")\n",
    "            return best_act\n",
    "\n",
    "        # Vérifie si le modèles est sur de sa prédiction\n",
    "        for act in self._all_actions:\n",
    "            probs:torch.Tensor = self.get_prediction(act)\n",
    "            max_prob = torch.max(probs).item()\n",
    "            # Formule utiliser : 1 / n + 0.5 / n\n",
    "            print(f'for action {act} probs {probs} max_prob {max_prob}')\n",
    "            if max_prob < 1 / probs.size(dim=0) + 0.5 / probs.size(dim=0):\n",
    "                print(\"je ne suis pas sur de \", act)\n",
    "                if len(self._all_actions) + len(self._all_outcomes) > self._model.input_size:\n",
    "                    self.fit(self._history_act, self._history_fb, validate_loader=None)\n",
    "\n",
    "            probs:torch.Tensor = self.get_prediction(act)\n",
    "            if max_prob < 1 / probs.size(dim=0) + 0.5 / probs.size(dim=0):\n",
    "                print(\"je n'arrive pas à être sur de \", act)\n",
    "                return act\n",
    "            \n",
    "            # Si le modèle as une prédiction sur, on regarde sa valance\n",
    "            predi = self._tokenizer.decode(torch.argmax(probs, dim=0).item())\n",
    "            expected_val = self._valance[inter(act, predi)]\n",
    "            if expected_val > best_expected_val:\n",
    "                best_act = act\n",
    "                best_expected_val = expected_val\n",
    "                print(f\"Action: {act}, Expected valance: {expected_val}\")\n",
    "        self._action = best_act\n",
    "        return best_act\n",
    "\n",
    "    # Modifier\n",
    "    def predict(self, action):\n",
    "        \"\"\"\n",
    "        Funciton de prédiction\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self._history_act) + len(self._history_fb) + 1 < self._model.input_size:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        \n",
    "        gap = (self._model.input_size - 1) // 2\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            print(self._history_act)\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        action = self._tokenizer.encode(x)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor(action, dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.argmax(x, dim=0).item()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return self._tokenizer.decode(x)\n",
    "\n",
    "    # Modofier\n",
    "    def action(self, outcome, fit=True, validate_loader=None):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \" \n",
    "                  f\"\\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\")\n",
    "            if self._predicted_outcome != outcome:\n",
    "                if len(self._history_act) + len(self._history_fb) > self._model.input_size:\n",
    "                    self.fit(self._history_act, self._history_fb, validate_loader=validate_loader)\n",
    "                else:\n",
    "                    self._action = self._all_actions[0]\n",
    "\n",
    "            # Maintenant nous choisissons la prochaine action en fonction de la valance\n",
    "            self._action = self.decide()\n",
    "            if len(self._history_act) + len(self._history_fb) + 1 > self._model.input_size:\n",
    "                self._predicted_outcome = self.predict(self._action)\n",
    "            self._predicted_outcome = self.predict(self._action)\n",
    "            self._history_act.append(self._action)\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)\n",
    "            print(f'len all interaction {len(self._history_act) + len(self._history_fb)}')\n",
    "            print(f'input size {self._model.input_size}')\n",
    "            \n",
    "            print(f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\")\n",
    "        \n",
    "        return self._action, self._predicted_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test2 = env3Str()\n",
    "\n",
    "model_ML = DeepNetwork(hidden_size=[10, 5], input_size=3, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "valence = {\n",
    "    inter('a', 'x') : -1,\n",
    "    inter('a', 'y') : 1,\n",
    "    inter('b', 'x') : 1,\n",
    "    inter('b', 'y') : 1\n",
    "}\n",
    "agent_test2 = Agent4(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    valance=valence,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "for i in range(25):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
