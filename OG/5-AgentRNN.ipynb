{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953e7771-f571-4331-aa8c-8408b0eb1e76",
   "metadata": {},
   "source": [
    "# AGENT RNN PAR OLIVIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9558e0ed-3996-4e38-968e-c566140d3034",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ogeorgeon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42d12b3-cc5c-4025-aaf5-82a8c517e76a",
   "metadata": {},
   "source": [
    "# Préparons l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a92d788-0cf7-4b64-8b38-47ad7a1f588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from ipywidgets import Output\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from IPython.display import display\n",
    "\n",
    "# Pour torch si vous avez un GPU\n",
    "# device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "device = \"cpu\" # Pour forcer l'utilisation du CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc269b25-58b8-4048-a751-888dfccf4eda",
   "metadata": {},
   "source": [
    "Les actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49edfed9-2601-40e9-affd-8b73500bba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORWARD = 2\n",
    "TURN_LEFT = 3\n",
    "TURN_RIGHT = 4\n",
    "FEEL_FRONT = 5\n",
    "FEEL_LEFT = 6  # Non utilisé\n",
    "FEEL_RIGHT = 7  # Non utilisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344a720-0c3c-4343-96d6-dc8201704059",
   "metadata": {},
   "source": [
    "Le Small Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d744460e-79da-48ce-9c90-ded24579c0f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "FEELING = 2\n",
    "BUMPING = 3\n",
    "\n",
    "class SmallLoop():\n",
    "    def __init__(self, poX, poY, direction):\n",
    "        self.grid = np.array([\n",
    "                [1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 1],\n",
    "                [1, 0, 1, 0, 1],\n",
    "                [1, 0, 0, 0, 1],\n",
    "                [1, 1, 1, 1, 1]\n",
    "        ])\n",
    "        self.maze = self.grid.copy()\n",
    "        self.poX = poX\n",
    "        self.poY = poY\n",
    "        self.direction = direction\n",
    "        self.cmap = ListedColormap(['white', 'green', 'yellow', 'red'])\n",
    "        self.norm = BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], self.cmap.N)\n",
    "\n",
    "    def outcome(self, action):\n",
    "        # print('before:', self.agent_position.strPosition(), action_dcit[action])\n",
    "        self.maze[:,:] = self.grid\n",
    "        result = 0\n",
    "        \n",
    "        if action == FORWARD:  # move forward\n",
    "            # print('the action is move forward')\n",
    "            # print(str(self.position.pointX)+': '+ str(self.position.pointY)+ ' ' +self.direction, action)\n",
    "        \n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX][self.poY - 1] == 0:\n",
    "                    self.poY -= 1\n",
    "                else:\n",
    "                    self.maze[self.poX][self.poY - 1] = BUMPING\n",
    "                    result = 1\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX + 1][self.poY] == 0:\n",
    "                    self.poX += 1\n",
    "                else:\n",
    "                    self.maze[self.poX + 1][self.poY] = BUMPING\n",
    "                    result = 1\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX][self.poY + 1] == 0:\n",
    "                    self.poY += 1\n",
    "                else:\n",
    "                    self.maze[self.poX][self.poY + 1] = BUMPING\n",
    "                    result = 1\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX - 1][self.poY] == 0:\n",
    "                    self.poX -= 1\n",
    "                else:\n",
    "                    self.maze[self.poX - 1][self.poY] = BUMPING\n",
    "                    result = 1\n",
    "            # print(str(self.position.pointX)+': '+ str(self.position.pointY)+ ' ' +self.direction, action)\n",
    "        elif action == TURN_RIGHT:\n",
    "            if self.direction == LEFT:\n",
    "                self.direction = UP\n",
    "            elif self.direction == DOWN:\n",
    "                self.direction = LEFT\n",
    "            elif self.direction == RIGHT:\n",
    "                self.direction = DOWN\n",
    "            elif self.direction == UP:\n",
    "                self.direction = RIGHT\n",
    "        elif action == TURN_LEFT:\n",
    "            if self.direction == LEFT:\n",
    "                self.direction = DOWN\n",
    "            elif self.direction == DOWN:\n",
    "                self.direction = RIGHT\n",
    "            elif self.direction == RIGHT:\n",
    "                self.direction = UP\n",
    "            elif self.direction == UP:\n",
    "                self.direction = LEFT\n",
    "        elif action == FEEL_FRONT:\n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX][self.poY - 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY - 1] = FEELING\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX + 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX + 1][self.poY] = FEELING\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX][self.poY + 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY + 1] = FEELING\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX - 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX - 1][self.poY] = FEELING\n",
    "        elif action == FEEL_LEFT:\n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX + 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX + 1][self.poY] = FEELING\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX][self.poY + 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY + 1] = FEELING\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX - 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX - 1][self.poY] = FEELING\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX][self.poY - 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY - 1] = FEELING\n",
    "        elif action == FEEL_RIGHT:\n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX - 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX - 1][self.poY] = FEELING\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX][self.poY - 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY - 1] = FEELING\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX + 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX + 1][self.poY] = FEELING\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX][self.poY + 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY + 1] = FEELING\n",
    "        # print(f\"Line: {self.poX}, Column: {self.poY}, direction: {self.direction}\")\n",
    "        return result  \n",
    "    \n",
    "    def display(self):\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            # ax.set_xticks([])\n",
    "            # ax.set_yticks([])\n",
    "            # ax.axis('off')\n",
    "            # ax.imshow(self.maze, cmap='Greens', vmin=0, vmax=2)\n",
    "            ax.imshow(self.maze, cmap=self.cmap, norm=self.norm)\n",
    "            if self.direction == LEFT:\n",
    "                # Y is column and X is line\n",
    "                plt.scatter(self.poY, self.poX, s=200, marker='<')\n",
    "            elif self.direction == DOWN:\n",
    "                plt.scatter(self.poY, self.poX, s=200, marker='v')\n",
    "            elif self.direction == RIGHT:\n",
    "                plt.scatter(self.poY, self.poX, s=200, marker='>')\n",
    "            else: # UP\n",
    "                plt.scatter(self.poY, self.poX, s=200, marker='^')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a1521-c899-4f18-80b1-20c155819c6e",
   "metadata": {},
   "source": [
    "# Le learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a6066e-7b4a-4694-bd56-6dd48f9dc840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1afce49bcd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f510fe0e-9466-4b36-b5a2-862a5fda699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Inspired by https://github.com/LukeDitria/pytorch_tutorials.git\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_emb, output_size, num_layers=1, hidden_size=128):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_emb = num_emb\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Create an embedding layer to convert token indices to dense vectors\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.5)\n",
    "        \n",
    "        # Define the output fully connected layer\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        # Convert token indices to dense vectors\n",
    "        input_embs = self.embedding(input_seq)\n",
    "\n",
    "        # Pass the embeddings through the LSTM layer\n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "                \n",
    "        # Pass the LSTM output through the fully connected layer to get the final output\n",
    "        return self.fc_out(output), hidden_out, mem_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ef611-e934-49f3-85f3-b4795c6396b3",
   "metadata": {},
   "source": [
    "# Le dataset pour l'entrainement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32558ab5-3c0a-4a2d-872a-fdcc66f2266e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataSetRNN(Dataset):\n",
    "    def __init__(self, actions:list, outcomes:list, dim_out:int, context_lenght:int, tokenizer=None):\n",
    "        \"\"\"\n",
    "        Creates a custom dataset\n",
    "\n",
    "        :param actions: list of actions\n",
    "        :param outcomes: list of outcomes\n",
    "        :param context_lenght: the length of the context\n",
    "        :param tokenizer: tokenizer to encode the actions and outcomes\n",
    "        \"\"\"\n",
    "        # Je ne suis pas sur d'y garder\n",
    "        assert context_lenght % 2 != 0, \"context_lenght must be odd\"\n",
    "        assert len(actions) == len(outcomes), \"actions and outcomes must have the same length\"\n",
    "        assert context_lenght <= len(actions) * 2, \"context_lenght must be less than or equal to the length of actions * 2\"\n",
    "        assert context_lenght > 0, \"context_lenght can't be negative or zero\"\n",
    "\n",
    "        self.actions = actions\n",
    "        self.outcomes = outcomes\n",
    "        self.context_lenght = context_lenght\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "    def create_x(self, idx):\n",
    "        gap = (self.context_lenght - 1) // 2\n",
    "        x = []\n",
    "        for i in range(idx, idx + gap):\n",
    "            x.append(self.actions[i])\n",
    "            x.append(self.outcomes[i])\n",
    "        x.append(self.actions[idx + gap])\n",
    "        y = self.outcomes[idx + gap]\n",
    "        if self.tokenizer is not None:\n",
    "            x = self.tokenizer.encode(x)\n",
    "            y = self.tokenizer.encode(y)\n",
    "        return x, y\n",
    "        \n",
    "                \n",
    "    def __len__(self):\n",
    "        gap = (self.context_lenght + 1) // 2\n",
    "        return len(self.actions) + 1 - gap\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the item at the index idx\n",
    "\n",
    "        :param idx: index\n",
    "        :return: x, y\n",
    "\n",
    "        Example\n",
    "        --------\n",
    "        actions = [\"a\", \"b\", \"c\", \"d\", \"e\"] \\n\n",
    "        outcomes = [\"1\", \"2\", \"3\", \"4\", \"5\"] \\n\n",
    "        context_lenght = 3 \\n\n",
    "        dataset = CustomDataSet(actions, outcomes, context_lenght) \\n\n",
    "        dataset[0] -> ([\"a\", \"1\", \"b\"], \"2\") \\n\n",
    "        dataset[1] -> ([\"b\", \"2\", \"c\"], \"3\") \\n\n",
    "        dataset[2] -> ([\"c\", \"3\", \"d\"], \"4\") \\n\n",
    "        dataset[3] -> ([\"d\", \"4\", \"e\"], \"5\") \\n\n",
    "        \"\"\"\n",
    "        x = []\n",
    "        x, label = self.create_x(idx)\n",
    "        x = torch.tensor(x, dtype=torch.int)\n",
    "        label = torch.tensor(label)\n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b51c4b-6005-4a72-87f2-5f17a76cafff",
   "metadata": {},
   "source": [
    "# L'agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab460674-9169-4eeb-bd84-ae0264afdc5f",
   "metadata": {},
   "source": [
    "La classe Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76add543-3b6c-46e9-be1f-92a5c2f3cd0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, _action, _outcome, _valence):\n",
    "        self._action = _action\n",
    "        self._outcome = _outcome\n",
    "        self._valence = _valence\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_decision(self):\n",
    "        \"\"\"Return the decision key\"\"\"\n",
    "        return f\"a{self._action}\"\n",
    "\n",
    "    def get_primitive_action(self):\n",
    "        \"\"\"Return the action for compatibility with CompositeInteraction\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_outcome(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._outcome\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self._action}{self._outcome}\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key. Used for compatibility with CompositeInteraction\"\"\"\n",
    "        return self.key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self._action}{self._outcome}:{self._valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.key() == other.key()\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_length(self):\n",
    "        \"\"\"The length of the sequence of this interaction\"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195cdcd4-4adb-4771-a889-0d344515c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentLSTM:\n",
    "    def __init__(self, interactions, model, optimizer, loss_func, context_length):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self.optimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._all_outcomes = list({i.get_outcome() for i in interactions})\n",
    "        self._all_actions = list({i.get_action() for i in interactions})\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "        # self._valence = valence\n",
    "        self._valence = {i.key():i.get_valence() for i in interactions}\n",
    "        self._gap_train = context_length\n",
    "        self._gap_test = context_length\n",
    "        self._context_length = context_length\n",
    "\n",
    "    def fit(self, actions:list, outcomes:list):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent \n",
    "        Avec data set custom, le model prends en inputs plusieurs données\n",
    "        \"\"\"\n",
    "\n",
    "        # The history must be longer than the context length parameter\n",
    "        if len(actions) + len(outcomes) < self._context_length:\n",
    "            return\n",
    "        \n",
    "        data_loarder = CustomDataSetRNN(actions=actions, outcomes=outcomes,context_lenght=self._context_length, dim_out=2)\n",
    "        data_loader = torch.utils.data.DataLoader(data_loarder, batch_size=32, shuffle=True)\n",
    "        \n",
    "        for e in range(50):\n",
    "            for x, t in data_loader:\n",
    "                bs = t.shape[0]\n",
    "                h = torch.zeros(self._model.num_layers, bs, self._model.hidden_size, device=device)\n",
    "                cell = torch.zeros(self._model.num_layers, bs, self._model.hidden_size, device=device)\n",
    "\n",
    "                pred, h, cell = self._model(x, h, cell)\n",
    "\n",
    "                loss = self._loss_func(pred[:, -1, :], t)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def predict(self, action):\n",
    "        if len(self._history_act) + len(self._history_fb) < self._context_length:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - self._context_length, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        # action = self._tokenizer.encode(x)\n",
    "\n",
    "        action = torch.tensor([x], dtype=torch.int).to(device)\n",
    "\n",
    "        h = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "        cell = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "        \n",
    "        with torch.no_grad():  # Pas de calcul de gradients en mode prédiction\n",
    "            pred, _, _ = self._model(action, h, cell)\n",
    "\n",
    "        pred_feedback = torch.argmax(pred[:, -1, :]).item()\n",
    "        \n",
    "        return pred_feedback\n",
    "    \n",
    "    def recursif_expective_valance(self, seq:list, max_depth:int, seuil:float=0.2, proba:float = 1,\n",
    "                                    seq_predi:list = []):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if max_depth == 0:\n",
    "            return {}\n",
    "        max_depth -= 1\n",
    "\n",
    "        if proba < seuil:\n",
    "            return {}\n",
    "        \n",
    "        self._model.eval()\n",
    "        exceptive_valance = {}\n",
    "        for act in self._all_actions:\n",
    "            new_seq = seq_predi + [act]\n",
    "            seq_to_predict = seq + [act]\n",
    "            seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.int).to(device)\n",
    "\n",
    "            hidden = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "            memory = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "\n",
    "            x, _, _ = self._model(seq_to_predict, hidden, memory)\n",
    "            x = x[0, -1, :]\n",
    "            # Transforme x into list proba\n",
    "            probs = torch.nn.functional.softmax(x, dim=0).tolist()\n",
    "            # for each outcomes we want proba with act\n",
    "            for i, out in enumerate(self._all_outcomes):\n",
    "                tmp_new_seq = new_seq + [out]\n",
    "                tmp_proba = probs[i] * proba\n",
    "                if tmp_proba < seuil:\n",
    "                    continue\n",
    "                tempo = float(np.round(self._valence[f\"{act}{out}\"] * tmp_proba, decimals=4))\n",
    "                # input(f'seq {seq_predi} act {act} out {out} proba {tmp_proba} valance {valance[(act, out)]} tempo {tempo}')\n",
    "                exceptive_valance.update(\n",
    "                    self.recursif_expective_valance(seq=seq[2:] + [act, out],\n",
    "                                                max_depth=max_depth, seuil=seuil, \n",
    "                                                proba=tmp_proba, seq_predi=tmp_new_seq.copy())\n",
    "                )\n",
    "                exceptive_valance[str(tmp_new_seq)] = tempo\n",
    "        return exceptive_valance\n",
    "        \n",
    "    def decide(self):\n",
    "        x = []\n",
    "        for i in range(-self._gap_test//2, 0, 1):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        # seq = self._tokenizer.encode(x)\n",
    "        res = self.recursif_expective_valance(seq=x, max_depth=4, seuil=0.2)\n",
    "        top_5 = sorted(res.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(f\"Top 5 of sequences with the best expected valance for {x}\")\n",
    "        for top in top_5:\n",
    "            print(f\"Sequence: {top[0]} Expected valance: {top[1]}\")\n",
    "        \n",
    "        print(f\"Action choisie : {eval(top_5[0][0])[0]}\")\n",
    "        return eval(top_5[0][0])[0]\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        description = None\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            description = f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\"\n",
    "            if len(self._history_act) + len(self._history_fb) > self._gap_train:\n",
    "                # and self._predicted_outcome != outcome\n",
    "                if self._predicted_outcome != outcome:\n",
    "                    self.fit(self._history_act, self._history_fb)\n",
    "                    self._boredom = 0\n",
    "                \n",
    "            if len(self._history_act) + len(self._history_fb) > self._gap_test:\n",
    "                self._action = self.decide()\n",
    "            else :\n",
    "                self._action = np.random.choice(self._all_actions)\n",
    "                \n",
    "            if len(self._history_act) + len(self._history_fb) > self._gap_test:\n",
    "                self._predicted_outcome = self.predict(self._action)\n",
    "            \n",
    "            self._history_act.append(self._action)\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)            \n",
    "            description = f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\"\n",
    "        \n",
    "        return self._action, self._predicted_outcome, description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35626980-5ef4-4616-a4d0-bb702c1069be",
   "metadata": {},
   "source": [
    "# On exécute l'agent dans le small loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b64219e-8c44-4eb5-83d5-829aa3ea72f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bff779c6264462a0baef7034718879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the seeds for replicability\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Reset the model\n",
    "model_ML = LSTM(hidden_size=128, num_emb=6, num_layers=2, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instanciate the environment\n",
    "small_loop = SmallLoop(1, 1, 0)\n",
    "\n",
    "# Instanciate the agent \n",
    "interactions = [\n",
    "    Interaction(FORWARD,0,5),\n",
    "    Interaction(FORWARD,1,-10),\n",
    "    Interaction(TURN_LEFT,0,-3),\n",
    "    Interaction(TURN_LEFT,1,-3),\n",
    "    Interaction(TURN_RIGHT,0,-6),\n",
    "    Interaction(TURN_RIGHT,1,-6),\n",
    "    Interaction(FEEL_FRONT,0,-1),\n",
    "    Interaction(FEEL_FRONT,1,-1)\n",
    "]\n",
    "\n",
    "agent = AgentLSTM(\n",
    "    interactions=interactions,\n",
    "    model=model_ML,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func,\n",
    "    context_length=3)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "\n",
    "out = Output()\n",
    "small_loop.display()\n",
    "display(out)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3676217d-5634-4b7e-bc28-bc354f65e192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 28\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 4, 0]\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.0375\n",
      "Sequence: [2, 0] Expected valance: 2.9125\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.8552\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.6837\n",
      "Sequence: [4, 0, 2, 0, 2, 0] Expected valance: 1.749\n",
      "Action choisie : 4\n"
     ]
    }
   ],
   "source": [
    "print(\"step:\", step)\n",
    "step += 1\n",
    "action, predi, description = agent.action(outcome)\n",
    "# print(description)\n",
    "small_loop.display()\n",
    "outcome = small_loop.outcome(action)\n",
    "history_good.append(outcome == predi)\n",
    "pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317df090-f4b9-46ec-938b-f57507bcc752",
   "metadata": {},
   "source": [
    "# On exécute 50 interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8e8549bb-5bf2-42c5-9209-c33dbe63f814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35368c9661a5485db0e2da032a3b6d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 of sequences with the best expected valance for [2, 0, 2, 1]\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.3963\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.1776\n",
      "Sequence: [2, 1, 4, 0, 2, 0] Expected valance: 2.108\n",
      "Sequence: [2, 1, 2, 1, 4, 0, 2, 0] Expected valance: 2.0696\n",
      "Sequence: [2, 1, 3, 1, 4, 0, 2, 0] Expected valance: 1.9226\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [2, 1, 4, 0]\n",
      "Sequence: [2, 0] Expected valance: 2.4851\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.2583\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 1.9535\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 1.6899\n",
      "Sequence: [3, 0, 2, 0] Expected valance: 1.2845\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 2, 0]\n",
      "Sequence: [2, 0] Expected valance: 2.9683\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.6655\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.6393\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.449\n",
      "Sequence: [2, 0, 2, 0] Expected valance: 1.6132\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 2, 0]\n",
      "Sequence: [2, 0] Expected valance: 2.7174\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.5394\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.5144\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.3331\n",
      "Sequence: [5, 1, 4, 0, 2, 0] Expected valance: 1.6173\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 2, 1]\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.6247\n",
      "Sequence: [2, 1, 4, 0, 2, 0] Expected valance: 3.3099\n",
      "Sequence: [2, 1, 2, 1, 4, 0, 2, 0] Expected valance: 3.2873\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 3.2141\n",
      "Sequence: [2, 1, 3, 1, 4, 0, 2, 0] Expected valance: 3.1534\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [2, 1, 4, 0]\n",
      "Sequence: [2, 0] Expected valance: 3.6706\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.2548\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 3.0594\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.8757\n",
      "Sequence: [2, 0, 2, 0] Expected valance: 2.1135\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 2, 0]\n",
      "Sequence: [2, 0] Expected valance: 2.8789\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.7154\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.6036\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.5523\n",
      "Sequence: [4, 0, 4, 0, 2, 0, 2, 0] Expected valance: 1.5635\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 2, 0]\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.5521\n",
      "Sequence: [2, 0] Expected valance: 2.5469\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.447\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.3989\n",
      "Sequence: [2, 1, 4, 0, 2, 0] Expected valance: 1.7784\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 4, 0]\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.0375\n",
      "Sequence: [2, 0] Expected valance: 2.9125\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.8552\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.6837\n",
      "Sequence: [4, 0, 2, 0, 2, 0] Expected valance: 1.749\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 4, 0]\n",
      "Sequence: [2, 0] Expected valance: 3.3647\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.1627\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.9728\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.7943\n",
      "Sequence: [2, 0, 2, 0] Expected valance: 1.9373\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 2, 0]\n",
      "Sequence: [2, 0] Expected valance: 2.8789\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.7154\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.6036\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.5523\n",
      "Sequence: [4, 0, 4, 0, 2, 0, 2, 0] Expected valance: 1.5635\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 2, 0]\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.5521\n",
      "Sequence: [2, 0] Expected valance: 2.5469\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.447\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.3989\n",
      "Sequence: [2, 1, 4, 0, 2, 0] Expected valance: 1.7784\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 4, 0]\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.0375\n",
      "Sequence: [2, 0] Expected valance: 2.9125\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.8552\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.6837\n",
      "Sequence: [4, 0, 2, 0, 2, 0] Expected valance: 1.749\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 4, 0]\n",
      "Sequence: [2, 0] Expected valance: 3.3647\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.1627\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.9728\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.7943\n",
      "Sequence: [2, 0, 2, 0] Expected valance: 1.9373\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 2, 0]\n",
      "Sequence: [2, 0] Expected valance: 2.8789\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.7154\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.6036\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.5523\n",
      "Sequence: [4, 0, 4, 0, 2, 0, 2, 0] Expected valance: 1.5635\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 2, 0]\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.5521\n",
      "Sequence: [2, 0] Expected valance: 2.5469\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.447\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.3989\n",
      "Sequence: [2, 1, 4, 0, 2, 0] Expected valance: 1.7784\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 4, 0]\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.0375\n",
      "Sequence: [2, 0] Expected valance: 2.9125\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.8552\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.6837\n",
      "Sequence: [4, 0, 2, 0, 2, 0] Expected valance: 1.749\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 4, 0]\n",
      "Sequence: [2, 0] Expected valance: 3.3647\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.1627\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.9728\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.7943\n",
      "Sequence: [2, 0, 2, 0] Expected valance: 1.9373\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 2, 0]\n",
      "Sequence: [2, 0] Expected valance: 2.8789\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.7154\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.6036\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.5523\n",
      "Sequence: [4, 0, 4, 0, 2, 0, 2, 0] Expected valance: 1.5635\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 2, 0]\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.5521\n",
      "Sequence: [2, 0] Expected valance: 2.5469\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.447\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.3989\n",
      "Sequence: [2, 1, 4, 0, 2, 0] Expected valance: 1.7784\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 4, 0]\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.0375\n",
      "Sequence: [2, 0] Expected valance: 2.9125\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.8552\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.6837\n",
      "Sequence: [4, 0, 2, 0, 2, 0] Expected valance: 1.749\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 4, 0]\n",
      "Sequence: [2, 0] Expected valance: 3.3647\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.1627\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.9728\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.7943\n",
      "Sequence: [2, 0, 2, 0] Expected valance: 1.9373\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 2, 0]\n",
      "Sequence: [2, 0] Expected valance: 2.8789\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.7154\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.6036\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.5523\n",
      "Sequence: [4, 0, 4, 0, 2, 0, 2, 0] Expected valance: 1.5635\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 2, 0]\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.5521\n",
      "Sequence: [2, 0] Expected valance: 2.5469\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.447\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.3989\n",
      "Sequence: [2, 1, 4, 0, 2, 0] Expected valance: 1.7784\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 4, 0]\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.0375\n",
      "Sequence: [2, 0] Expected valance: 2.9125\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.8552\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.6837\n",
      "Sequence: [4, 0, 2, 0, 2, 0] Expected valance: 1.749\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 4, 0]\n",
      "Sequence: [2, 0] Expected valance: 3.3647\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.1627\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.9728\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.7943\n",
      "Sequence: [2, 0, 2, 0] Expected valance: 1.9373\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 2, 0]\n",
      "Sequence: [2, 0] Expected valance: 2.8789\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.7154\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.6036\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.5523\n",
      "Sequence: [4, 0, 4, 0, 2, 0, 2, 0] Expected valance: 1.5635\n",
      "Action choisie : 2\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 2, 0]\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.5521\n",
      "Sequence: [2, 0] Expected valance: 2.5469\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 2.447\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.3989\n",
      "Sequence: [2, 1, 4, 0, 2, 0] Expected valance: 1.7784\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [2, 0, 4, 0]\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.0375\n",
      "Sequence: [2, 0] Expected valance: 2.9125\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.8552\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.6837\n",
      "Sequence: [4, 0, 2, 0, 2, 0] Expected valance: 1.749\n",
      "Action choisie : 4\n",
      "Top 5 of sequences with the best expected valance for [4, 0, 4, 0]\n",
      "Sequence: [2, 0] Expected valance: 3.3647\n",
      "Sequence: [4, 0, 2, 0] Expected valance: 3.1627\n",
      "Sequence: [4, 0, 4, 0, 2, 0] Expected valance: 2.9728\n",
      "Sequence: [4, 0, 4, 0, 4, 0, 2, 0] Expected valance: 2.7943\n",
      "Sequence: [2, 0, 2, 0] Expected valance: 1.9373\n",
      "Action choisie : 2\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(30)):\n",
    "    # start_time = time.time()\n",
    "    action, predi, description = agent.action(outcome)\n",
    "    # print(description)\n",
    "    outcome = small_loop.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    small_loop.display()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b0e08-b383-45b5-a798-91c202807dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762bcc1b-80d2-415d-a0bf-2d6107d21b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
