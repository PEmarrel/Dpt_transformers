{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953e7771-f571-4331-aa8c-8408b0eb1e76",
   "metadata": {},
   "source": [
    "# AGENT RNN PAR OLIVIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9558e0ed-3996-4e38-968e-c566140d3034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ogeorgeon\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42d12b3-cc5c-4025-aaf5-82a8c517e76a",
   "metadata": {},
   "source": [
    "# Préparons l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a92d788-0cf7-4b64-8b38-47ad7a1f588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from ipywidgets import Output\n",
    "from tqdm.notebook import trange, tqdm\n",
    "# Import necessary libraries\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from IPython.display import display\n",
    "\n",
    "# Pour torch si vous avez un GPU\n",
    "# device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "device = \"cpu\" # Pour forcer l'utilisation du CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc269b25-58b8-4048-a751-888dfccf4eda",
   "metadata": {},
   "source": [
    "Les actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49edfed9-2601-40e9-affd-8b73500bba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORWARD = 2\n",
    "TURN_LEFT = 3\n",
    "TURN_RIGHT = 4\n",
    "FEEL_FRONT = 5\n",
    "FEEL_LEFT = 6  # Non utilisé\n",
    "FEEL_RIGHT = 7  # Non utilisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344a720-0c3c-4343-96d6-dc8201704059",
   "metadata": {},
   "source": [
    "Le Small Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d744460e-79da-48ce-9c90-ded24579c0f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "FEELING = 2\n",
    "BUMPING = 3\n",
    "\n",
    "class SmallLoop():\n",
    "    def __init__(self, poX, poY, direction):\n",
    "        self.grid = np.array([\n",
    "                [1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 1],\n",
    "                [1, 0, 1, 0, 1],\n",
    "                [1, 0, 0, 0, 1],\n",
    "                [1, 1, 1, 1, 1]\n",
    "        ])\n",
    "        self.maze = self.grid.copy()\n",
    "        self.poX = poX\n",
    "        self.poY = poY\n",
    "        self.direction = direction\n",
    "        self.cmap = ListedColormap(['white', 'green', 'yellow', 'red'])\n",
    "        self.norm = BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], self.cmap.N)\n",
    "\n",
    "    def outcome(self, action):\n",
    "        # print('before:', self.agent_position.strPosition(), action_dcit[action])\n",
    "        self.maze[:,:] = self.grid\n",
    "        result = 0\n",
    "        \n",
    "        if action == FORWARD:  # move forward\n",
    "            # print('the action is move forward')\n",
    "            # print(str(self.position.pointX)+': '+ str(self.position.pointY)+ ' ' +self.direction, action)\n",
    "        \n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX][self.poY - 1] == 0:\n",
    "                    self.poY -= 1\n",
    "                else:\n",
    "                    self.maze[self.poX][self.poY - 1] = BUMPING\n",
    "                    result = 1\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX + 1][self.poY] == 0:\n",
    "                    self.poX += 1\n",
    "                else:\n",
    "                    self.maze[self.poX + 1][self.poY] = BUMPING\n",
    "                    result = 1\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX][self.poY + 1] == 0:\n",
    "                    self.poY += 1\n",
    "                else:\n",
    "                    self.maze[self.poX][self.poY + 1] = BUMPING\n",
    "                    result = 1\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX - 1][self.poY] == 0:\n",
    "                    self.poX -= 1\n",
    "                else:\n",
    "                    self.maze[self.poX - 1][self.poY] = BUMPING\n",
    "                    result = 1\n",
    "            # print(str(self.position.pointX)+': '+ str(self.position.pointY)+ ' ' +self.direction, action)\n",
    "        elif action == TURN_RIGHT:\n",
    "            if self.direction == LEFT:\n",
    "                self.direction = UP\n",
    "            elif self.direction == DOWN:\n",
    "                self.direction = LEFT\n",
    "            elif self.direction == RIGHT:\n",
    "                self.direction = DOWN\n",
    "            elif self.direction == UP:\n",
    "                self.direction = RIGHT\n",
    "        elif action == TURN_LEFT:\n",
    "            if self.direction == LEFT:\n",
    "                self.direction = DOWN\n",
    "            elif self.direction == DOWN:\n",
    "                self.direction = RIGHT\n",
    "            elif self.direction == RIGHT:\n",
    "                self.direction = UP\n",
    "            elif self.direction == UP:\n",
    "                self.direction = LEFT\n",
    "        elif action == FEEL_FRONT:\n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX][self.poY - 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY - 1] = FEELING\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX + 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX + 1][self.poY] = FEELING\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX][self.poY + 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY + 1] = FEELING\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX - 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX - 1][self.poY] = FEELING\n",
    "        elif action == FEEL_LEFT:\n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX + 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX + 1][self.poY] = FEELING\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX][self.poY + 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY + 1] = FEELING\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX - 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX - 1][self.poY] = FEELING\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX][self.poY - 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY - 1] = FEELING\n",
    "        elif action == FEEL_RIGHT:\n",
    "            if self.direction == LEFT:\n",
    "                if self.maze[self.poX - 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX - 1][self.poY] = FEELING\n",
    "            elif self.direction == DOWN:\n",
    "                if self.maze[self.poX][self.poY - 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY - 1] = FEELING\n",
    "            elif self.direction == RIGHT:\n",
    "                if self.maze[self.poX + 1][self.poY] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX + 1][self.poY] = FEELING\n",
    "            elif self.direction == UP:\n",
    "                if self.maze[self.poX][self.poY + 1] != 0:\n",
    "                    result = 1\n",
    "                self.maze[self.poX][self.poY + 1] = FEELING\n",
    "        # print(f\"Line: {self.poX}, Column: {self.poY}, direction: {self.direction}\")\n",
    "        return result  \n",
    "    \n",
    "    def display(self):\n",
    "        out.clear_output(wait=True)\n",
    "        with out:\n",
    "            fig, ax = plt.subplots()\n",
    "            # ax.set_xticks([])\n",
    "            # ax.set_yticks([])\n",
    "            # ax.axis('off')\n",
    "            # ax.imshow(self.maze, cmap='Greens', vmin=0, vmax=2)\n",
    "            ax.imshow(self.maze, cmap=self.cmap, norm=self.norm)\n",
    "            if self.direction == LEFT:\n",
    "                # Y is column and X is line\n",
    "                plt.scatter(self.poY, self.poX, s=200, marker='<')\n",
    "            elif self.direction == DOWN:\n",
    "                plt.scatter(self.poY, self.poX, s=200, marker='v')\n",
    "            elif self.direction == RIGHT:\n",
    "                plt.scatter(self.poY, self.poX, s=200, marker='>')\n",
    "            else: # UP\n",
    "                plt.scatter(self.poY, self.poX, s=200, marker='^')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5503528-b6dd-4407-8ee3-bcb9af3cb149",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_loop = SmallLoop(1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a1521-c899-4f18-80b1-20c155819c6e",
   "metadata": {},
   "source": [
    "# Le learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a6066e-7b4a-4694-bd56-6dd48f9dc840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23040541c30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f510fe0e-9466-4b36-b5a2-862a5fda699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedbackPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, h, _):\n",
    "        if h is None:\n",
    "            h = torch.zeros(1, x.size(0), 16)\n",
    "        out, h = self.rnn(x, h)\n",
    "        return self.fc(out[:, -1, :]), h, None\n",
    "\n",
    "# Inspired by https://github.com/LukeDitria/pytorch_tutorials.git\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_emb, output_size, num_layers=1, hidden_size=128):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_emb = num_emb\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Create an embedding layer to convert token indices to dense vectors\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.5)\n",
    "        \n",
    "        # Define the output fully connected layer\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        # Convert token indices to dense vectors\n",
    "        input_embs = self.embedding(input_seq)\n",
    "\n",
    "        # Pass the embeddings through the LSTM layer\n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "                \n",
    "        # Pass the LSTM output through the fully connected layer to get the final output\n",
    "        return self.fc_out(output), hidden_out, mem_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b51c4b-6005-4a72-87f2-5f17a76cafff",
   "metadata": {},
   "source": [
    "# L'agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab460674-9169-4eeb-bd84-ae0264afdc5f",
   "metadata": {},
   "source": [
    "La classe Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76add543-3b6c-46e9-be1f-92a5c2f3cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction:\n",
    "    \"\"\"An interaction is a tuple (action, outcome) with a valence\"\"\"\n",
    "    def __init__(self, _action, _outcome, _valence):\n",
    "        self._action = _action\n",
    "        self._outcome = _outcome\n",
    "        self._valence = _valence\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_decision(self):\n",
    "        \"\"\"Return the decision key\"\"\"\n",
    "        return f\"a{self._action}\"\n",
    "\n",
    "    def get_primitive_action(self):\n",
    "        \"\"\"Return the action for compatibility with CompositeInteraction\"\"\"\n",
    "        return self._action\n",
    "\n",
    "    def get_outcome(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._outcome\n",
    "\n",
    "    def get_valence(self):\n",
    "        \"\"\"Return the action\"\"\"\n",
    "        return self._valence\n",
    "\n",
    "    def key(self):\n",
    "        \"\"\" The key to find this interaction in the dictinary is the string '<action><outcome>'. \"\"\"\n",
    "        return f\"{self._action}{self._outcome}\"\n",
    "\n",
    "    def pre_key(self):\n",
    "        \"\"\"Return the key. Used for compatibility with CompositeInteraction\"\"\"\n",
    "        return self.key()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Print interaction in the form '<action><outcome:<valence>' for debug.\"\"\"\n",
    "        return f\"{self._action}{self._outcome}:{self._valence}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Interactions are equal if they have the same key \"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.key() == other.key()\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_length(self):\n",
    "        \"\"\"The length of the sequence of this interaction\"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195cdcd4-4adb-4771-a889-0d344515c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentLSTM:\n",
    "    def __init__(self, interactions, model, optimizer, gap_train=11, gap_test=11, loss_func=None):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self.optimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._all_outcomes = [i.get_outcome() for i in interactions]\n",
    "        self._all_actions = [i.get_action() for i in interactions]\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "        # self._valence = valence\n",
    "        self._interactions = interactions\n",
    "        self._gap_train = gap_train\n",
    "        self._gap_test = gap_test\n",
    "        self._boredom = 0\n",
    "\n",
    "    def fit(self, actions:list, outcomes:list, nb_epoch:int=25, context_lenght=5, validate_loader=None):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent \n",
    "        Avec data set custom, le model prends en inputs plusieurs données\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(actions) + len(outcomes) < context_lenght:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            data_loarder = CustomDataSetRNN(actions=actions, outcomes=outcomes,\n",
    "                    context_lenght=context_lenght, dim_out=2, tokenizer=None)\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                data_loarder, batch_size=32, shuffle=True)\n",
    "            for e in range(nb_epoch):\n",
    "                for x,t in data_loader:\n",
    "                    bs = t.shape[0]\n",
    "                    h = torch.zeros(self._model.num_layers, bs, self._model.hidden_size, device=device)\n",
    "                    cell = torch.zeros(self._model.num_layers, bs, self._model.hidden_size, device=device)\n",
    "\n",
    "                    pred, h, cell = self._model(x, h, cell)\n",
    "\n",
    "                    loss = self._loss_func(pred[:, -1, :], t)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "        else: # Si le model n'est pas un model pytorch\n",
    "            raise Exception('Not implemented')\n",
    "            self._model.fit(action, outcome)\n",
    "            pass\n",
    "\n",
    "    def predict(self, action, gap=5):\n",
    "        if len(self._history_act) + len(self._history_fb) < gap:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        # action = self._tokenizer.encode(x)\n",
    "\n",
    "        action = torch.tensor([x], dtype=torch.int).to(device)\n",
    "\n",
    "        h = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "        cell = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "        \n",
    "        with torch.no_grad():  # Pas de calcul de gradients en mode prédiction\n",
    "            pred, _, _ = self._model(action, h, cell)\n",
    "\n",
    "        pred_feedback = torch.argmax(pred[:, -1, :]).item()\n",
    "        # pred_feedback = self._tokenizer.decode(pred_feedback)\n",
    "        \n",
    "        return pred_feedback\n",
    "    \n",
    "    def recursif_expective_valance(self, seq:list, max_depth:int, seuil:float=0.2, proba:float = 1,\n",
    "                                    seq_predi:list = []):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if max_depth == 0:\n",
    "            return {}\n",
    "        max_depth -= 1\n",
    "\n",
    "        if proba < seuil:\n",
    "            return {}\n",
    "        \n",
    "        self._model.eval()\n",
    "        exceptive_valance = {}\n",
    "        for act in self._all_actions:\n",
    "            new_seq = seq_predi + [act]\n",
    "            # seq_to_predict = seq + [self._tokenizer.encode(act)]\n",
    "            seq_to_predict = seq + [act]\n",
    "            seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.int).to(device)\n",
    "\n",
    "            hidden = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "            memory = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "\n",
    "            x, _, _ = self._model(seq_to_predict, hidden, memory)\n",
    "            x = x[0, -1, :]\n",
    "            # Transforme x into list proba\n",
    "            probs = torch.nn.functional.softmax(x, dim=0).tolist()\n",
    "            # for each outcomes we want proba with act\n",
    "            for i, out in enumerate(self._all_outcomes):\n",
    "                tmp_new_seq = new_seq + [out]\n",
    "                tmp_proba = probs[i] * proba\n",
    "                if tmp_proba < seuil:\n",
    "                    continue\n",
    "                tempo = float(np.round(self._valence[inter(act, out)] * tmp_proba, decimals=4))\n",
    "                # input(f'seq {seq_predi} act {act} out {out} proba {tmp_proba} valance {valance[(act, out)]} tempo {tempo}')\n",
    "                exceptive_valance.update(\n",
    "                    self.recursif_expective_valance(seq=seq[2:] + [act, out],\n",
    "                                                max_depth=max_depth, seuil=seuil, \n",
    "                                                proba=tmp_proba, seq_predi=tmp_new_seq.copy())\n",
    "                )\n",
    "                exceptive_valance[str(tmp_new_seq)] = tempo\n",
    "        return exceptive_valance\n",
    "        \n",
    "    def decide(self):\n",
    "        x = []\n",
    "        for i in range(-self._gap_test//2, 0, 1):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        # seq = self._tokenizer.encode(x) MODIFIE\n",
    "        res = self.recursif_expective_valance(seq=x, max_depth=3, seuil=0.3)\n",
    "        top_5 = sorted(res.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        # print(f\"Top 5 of sequences with the best expected valance for {x}\")\n",
    "        # for top in top_5:\n",
    "        #     print(f\"Sequence: {top[0]} Expected valance: {top[1]}\")\n",
    "        \n",
    "        # print(f\"Action choisie : {eval(top_5[0][0])[0]}\")\n",
    "        return eval(top_5[0][0])[0]\n",
    "\n",
    "    # Modifier\n",
    "    def action(self, outcome, fit=True, decide=True, validate_loader=None, force_fit=False):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        self._boredom += 1\n",
    "        description = None\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            description = f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\"\n",
    "            if len(self._history_act) + len(self._history_fb) > self._gap_train:\n",
    "                # and self._predicted_outcome != outcome\n",
    "                if fit and self._predicted_outcome != outcome:\n",
    "                    self.fit(self._history_act, self._history_fb, validate_loader=validate_loader,\n",
    "                            nb_epoch=50, context_lenght=self._gap_train)\n",
    "                    self._boredom = 0\n",
    "                \n",
    "            if decide and len(self._history_act) + len(self._history_fb) > self._gap_test:\n",
    "                self._action = self.decide()\n",
    "            else :\n",
    "                self._action = np.random.choice(self._all_actions) #  MODIFIE\n",
    "                \n",
    "            if len(self._history_act) + len(self._history_fb) > self._gap_test:\n",
    "                self._predicted_outcome = self.predict(self._action, gap=self._gap_test)\n",
    "            \n",
    "            self._history_act.append(self._action)\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)            \n",
    "            description = f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\"\n",
    "        \n",
    "        return self._action, self._predicted_outcome, description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35626980-5ef4-4616-a4d0-bb702c1069be",
   "metadata": {},
   "source": [
    "# On exécute l'agent dans le small loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b64219e-8c44-4eb5-83d5-829aa3ea72f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b431b40f1d4c6492d9c792709ccbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95835dbd299474292afc8dd5fb15443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action de base : 2 Prediction: None\n",
      "Action: 2, Prediction: None, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 3, Prediction: None, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 5, Prediction: None, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 5, Prediction: None, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: None, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: None, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 3, Prediction: None, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: None, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 5, Prediction: 1, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 1, Outcome: 1, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 3, Prediction: 1, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 3, Prediction: 1, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 5, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 4, Prediction: 1, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 5, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 2, Prediction: 1, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: 1, Outcome: 1, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 4, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 4, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 1, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 4, Prediction: 1, Outcome: 0, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 5, Prediction: 1, Outcome: 1, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 1, Outcome: 1, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 3, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 4, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 3, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 1, Outcome: 1, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 3, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 4, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: 5, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 2, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 5, Prediction: 0, Outcome: 1, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "Action: 4, Prediction: 0, Outcome: 0, \u001b[0;31m Satisfaction: True \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_ML = LSTM(hidden_size=128, num_emb=6, num_layers=2, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instanciate the agent \n",
    "interactions = [\n",
    "    Interaction(FORWARD,0,5),\n",
    "    Interaction(FORWARD,1,-10),\n",
    "    Interaction(TURN_LEFT,0,-3),\n",
    "    Interaction(TURN_LEFT,1,-3),\n",
    "    Interaction(TURN_RIGHT,0,-6),\n",
    "    Interaction(TURN_RIGHT,1,-6),\n",
    "    Interaction(FEEL_FRONT,0,-1),\n",
    "    Interaction(FEEL_FRONT,1,-1)\n",
    "]\n",
    "\n",
    "agent_test2 = AgentLSTM(\n",
    "    interactions=interactions,\n",
    "    model=model_ML,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func,\n",
    "    gap_train=15,\n",
    "    gap_test=15)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "\n",
    "out = Output()\n",
    "display(out)\n",
    "\n",
    "for i in tqdm(range(50)):\n",
    "    # start_time = time.time()\n",
    "    action, predi, description = agent_test2.action(outcome, fit=False, decide=False)\n",
    "    print(description)\n",
    "    outcome = small_loop.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    small_loop.display()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b0e08-b383-45b5-a798-91c202807dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762bcc1b-80d2-415d-a0bf-2d6107d21b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
