{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc23876",
   "metadata": {},
   "source": [
    "# Notebook : Entraînement d'un Transformer simple pour la prédiction du prochain token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f7f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af7cbf-54b7-4c6f-b069-67b65207858d",
   "metadata": {},
   "source": [
    "Les séquences générées par Agent13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a6be2a-2d7b-41ef-8fc4-0bb9ac568ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_sequences = [\n",
    "[[0, 1], [1, 1], [1, 3], [3, 1], [1, 4], [4, 1], [1, 7], [7, 1], [1, 8], [8, 0], [0, 2], [2, 0], [3, 3], [3, 4], [4, 3], [3, 7], [7, 3], [3, 8], [3, 10], [10, 1], [1, 10], [10, 2], [5, 1], [3, 5], [5, 3], [3, 6], [6, 1], [1, 5], [5, 5], [5, 6], [6, 3], [5, 8], [8, 1], [7, 4], [4, 4], [4, 7], [7, 7], [7, 8], [0, 3], [6, 5], [5, 10]],\n",
    "[[0, 1, 1], [0, 1, 1], [1, 1, 3], [1, 1, 3], [1, 3, 1], [1, 3, 1], [3, 1, 4], [3, 1, 4], [1, 4, 1], [1, 4, 1], [4, 1, 7], [4, 1, 7], [1, 7, 1], [1, 7, 1], [7, 1, 8], [7, 1, 8], [1, 8, 0], [1, 8, 0], [8, 0, 2], [8, 0, 2], [0, 2, 0], [0, 2, 0], [2, 0, 3], [3, 3, 4], [3, 3, 4], [3, 4, 3], [3, 4, 3], [4, 3, 7], [4, 3, 7], [3, 7, 3], [3, 7, 3], [7, 3, 8], [7, 3, 8], [3, 10, 1], [3, 10, 1], [10, 1, 10], [10, 1, 10], [1, 10, 2], [1, 10, 2], [5, 1, 3], [5, 1, 3], [1, 3, 3], [1, 3, 3], [3, 3, 5], [3, 3, 5], [3, 5, 3], [3, 5, 3], [5, 3, 6], [5, 3, 6], [3, 6, 1], [3, 6, 1], [6, 1, 5], [6, 1, 5], [1, 5, 5], [1, 5, 5], [5, 5, 6], [5, 5, 6], [5, 6, 3], [5, 6, 3], [6, 3, 3], [6, 3, 3], [3, 5, 8], [3, 5, 8], [5, 8, 1], [5, 8, 1], [8, 1, 7], [8, 1, 7], [1, 7, 4], [1, 7, 4], [7, 4, 4], [7, 4, 4], [4, 4, 7], [4, 4, 7], [4, 7, 7], [4, 7, 7], [7, 7, 8], [7, 7, 8], [8, 0, 3], [6, 5, 10], [6, 5, 10], [2, 0, 3]],\n",
    "[[0, 1, 1, 3], [1, 1, 3, 1], [1, 3, 1, 4], [3, 1, 4, 1], [1, 4, 1, 7], [4, 1, 7, 1], [1, 7, 1, 8], [7, 1, 8, 0], [1, 8, 0, 2], [8, 0, 2, 0], [2, 0, 2, 0], [2, 0, 2, 0], [0, 2, 0, 3], [0, 2, 0, 3], [2, 0, 3, 3], [2, 0, 3, 3], [3, 3, 4, 3], [3, 4, 3, 7], [4, 3, 7, 3], [3, 7, 3, 8], [3, 8, 0, 2], [3, 8, 0, 2], [3, 10, 1, 10], [10, 1, 10, 2], [2, 0, 3, 5], [5, 1, 3, 3], [1, 3, 3, 5], [3, 3, 5, 3], [3, 5, 3, 6], [5, 3, 6, 1], [3, 6, 1, 5], [6, 1, 5, 5], [1, 5, 5, 6], [5, 5, 6, 3], [5, 6, 3, 3], [6, 3, 3, 5], [3, 3, 5, 8], [3, 5, 8, 1], [5, 8, 1, 7], [8, 1, 7, 4], [1, 7, 4, 4], [7, 4, 4, 7], [4, 4, 7, 7], [4, 7, 7, 8], [0, 2, 0, 3], [7, 8, 0, 3], [10, 2, 0, 3], [0, 2, 0, 2]],\n",
    "[[0, 2, 0, 2, 0], [2, 0, 2, 0, 3], [0, 2, 0, 3, 3], [2, 0, 3, 3, 4], [7, 3, 8, 0, 2], [0, 2, 0, 3, 10], [0, 2, 0, 3, 10], [10, 2, 0, 2, 0], [10, 2, 0, 2, 0], [2, 0, 3, 5, 1], [2, 0, 3, 5, 1], [8, 0, 2, 0, 3], [7, 8, 0, 3, 6], [5, 10, 2, 0, 3], [5, 10, 2, 0, 3], [8, 0, 2, 0, 2]],\n",
    "[[8, 0, 2, 0, 2, 0], [8, 0, 2, 0, 2, 0], [0, 2, 0, 2, 0, 3], [0, 2, 0, 2, 0, 3], [0, 2, 0, 3, 10, 1], [1, 10, 2, 0, 2, 0], [0, 2, 0, 2, 0, 3], [2, 0, 3, 5, 1, 3], [7, 8, 0, 2, 0, 3], [7, 8, 0, 3, 6, 5], [7, 8, 0, 3, 6, 5], [6, 5, 10, 2, 0, 3], [7, 8, 0, 2, 0, 2]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6ec28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, sequences, max_length=10):\n",
    "        self.sequences = sequences\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "        input_seq = input_seq + [0] * (self.max_length - len(input_seq))\n",
    "        target_seq = target_seq + [0] * (self.max_length - len(target_seq))\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "def create_dataloader(sequences, batch_size=2, max_length=10):\n",
    "    dataset = TokenDataset(sequences, max_length=max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6c9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=12, d_model=32, nhead=4, num_layers=2, max_length=10):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_length, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = self.token_emb(x) + self.pos_emb(positions)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        out = self.transformer_encoder(x)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        logits = self.fc_out(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b651767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_next_token_probs(model, input_sequence, max_length=10, device='cpu'):\n",
    "    model.eval()\n",
    "    if len(input_sequence) > max_length:\n",
    "        raise ValueError(f\"Input sequence length {len(input_sequence)} exceeds maximum {max_length}\")\n",
    "    input_seq = input_sequence + [0] * (max_length - len(input_sequence))\n",
    "    input_tensor = torch.tensor(input_seq, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    last_index = len(input_sequence) - 1\n",
    "    logits_for_next_token = logits[0, last_index, :]\n",
    "    # Pairwise probabilities\n",
    "    pairwise_logits = logits[0, -1, :].reshape(-1, 2)\n",
    "    probabilities = nn.functional.softmax(pairwise_logits, dim=1).flatten().tolist()\n",
    "    return probabilities\n",
    "    # probs = F.softmax(logits_for_next_token, dim=0)\n",
    "    # return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "998cb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, dataloader, num_epochs=10, learning_rate=1e-3, device='cpu'):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            targets = targets.view(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Époque [{epoch+1}/{num_epochs}], Perte moyenne : {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48639edc-f12b-4ea0-88fb-78065dc91860",
   "metadata": {},
   "source": [
    "On instancie le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72f8a307-2254-4372-85ec-6729fe34c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleTransformer(vocab_size=12, max_length=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ac0f8c2-67a1-4a92-9f14-f7ce7a761b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [1, 1], [1, 3], [3, 1], [1, 4], [4, 1], [1, 7], [7, 1], [1, 8], [8, 0], [0, 2], [2, 0], [3, 3], [3, 4], [4, 3], [3, 7], [7, 3], [3, 8], [3, 10], [10, 1], [1, 10], [10, 2], [5, 1], [3, 5], [5, 3], [3, 6], [6, 1], [1, 5], [5, 5], [5, 6], [6, 3], [5, 8], [8, 1], [7, 4], [4, 4], [4, 7], [7, 7], [7, 8], [0, 3], [6, 5], [5, 10]]\n",
      "Époque [1/20], Perte moyenne : 4.5009\n",
      "Époque [2/20], Perte moyenne : 2.8230\n",
      "Époque [3/20], Perte moyenne : 2.5124\n",
      "Époque [4/20], Perte moyenne : 2.3704\n",
      "Époque [5/20], Perte moyenne : 2.3127\n",
      "Époque [6/20], Perte moyenne : 2.1738\n",
      "Époque [7/20], Perte moyenne : 2.0714\n",
      "Époque [8/20], Perte moyenne : 1.8600\n",
      "Époque [9/20], Perte moyenne : 1.8573\n",
      "Époque [10/20], Perte moyenne : 1.8303\n",
      "Époque [11/20], Perte moyenne : 1.8040\n",
      "Époque [12/20], Perte moyenne : 1.6494\n",
      "Époque [13/20], Perte moyenne : 1.8957\n",
      "Époque [14/20], Perte moyenne : 1.8013\n",
      "Époque [15/20], Perte moyenne : 1.8641\n",
      "Époque [16/20], Perte moyenne : 1.8372\n",
      "Époque [17/20], Perte moyenne : 1.8302\n",
      "Époque [18/20], Perte moyenne : 1.7575\n",
      "Époque [19/20], Perte moyenne : 1.6817\n",
      "Époque [20/20], Perte moyenne : 1.7365\n",
      "[[0, 1, 1], [0, 1, 1], [1, 1, 3], [1, 1, 3], [1, 3, 1], [1, 3, 1], [3, 1, 4], [3, 1, 4], [1, 4, 1], [1, 4, 1], [4, 1, 7], [4, 1, 7], [1, 7, 1], [1, 7, 1], [7, 1, 8], [7, 1, 8], [1, 8, 0], [1, 8, 0], [8, 0, 2], [8, 0, 2], [0, 2, 0], [0, 2, 0], [2, 0, 3], [3, 3, 4], [3, 3, 4], [3, 4, 3], [3, 4, 3], [4, 3, 7], [4, 3, 7], [3, 7, 3], [3, 7, 3], [7, 3, 8], [7, 3, 8], [3, 10, 1], [3, 10, 1], [10, 1, 10], [10, 1, 10], [1, 10, 2], [1, 10, 2], [5, 1, 3], [5, 1, 3], [1, 3, 3], [1, 3, 3], [3, 3, 5], [3, 3, 5], [3, 5, 3], [3, 5, 3], [5, 3, 6], [5, 3, 6], [3, 6, 1], [3, 6, 1], [6, 1, 5], [6, 1, 5], [1, 5, 5], [1, 5, 5], [5, 5, 6], [5, 5, 6], [5, 6, 3], [5, 6, 3], [6, 3, 3], [6, 3, 3], [3, 5, 8], [3, 5, 8], [5, 8, 1], [5, 8, 1], [8, 1, 7], [8, 1, 7], [1, 7, 4], [1, 7, 4], [7, 4, 4], [7, 4, 4], [4, 4, 7], [4, 4, 7], [4, 7, 7], [4, 7, 7], [7, 7, 8], [7, 7, 8], [8, 0, 3], [6, 5, 10], [6, 5, 10], [2, 0, 3]]\n",
      "Époque [1/20], Perte moyenne : 0.5210\n",
      "Époque [2/20], Perte moyenne : 0.2215\n",
      "Époque [3/20], Perte moyenne : 0.2104\n",
      "Époque [4/20], Perte moyenne : 0.1738\n",
      "Époque [5/20], Perte moyenne : 0.1730\n",
      "Époque [6/20], Perte moyenne : 0.2165\n",
      "Époque [7/20], Perte moyenne : 0.2185\n",
      "Époque [8/20], Perte moyenne : 0.2258\n",
      "Époque [9/20], Perte moyenne : 0.2126\n",
      "Époque [10/20], Perte moyenne : 0.2071\n",
      "Époque [11/20], Perte moyenne : 0.1577\n",
      "Époque [12/20], Perte moyenne : 0.1770\n",
      "Époque [13/20], Perte moyenne : 0.1462\n",
      "Époque [14/20], Perte moyenne : 0.1084\n",
      "Époque [15/20], Perte moyenne : 0.1597\n",
      "Époque [16/20], Perte moyenne : 0.1355\n",
      "Époque [17/20], Perte moyenne : 0.1704\n",
      "Époque [18/20], Perte moyenne : 0.1206\n",
      "Époque [19/20], Perte moyenne : 0.1086\n",
      "Époque [20/20], Perte moyenne : 0.1506\n",
      "[[0, 1, 1, 3], [1, 1, 3, 1], [1, 3, 1, 4], [3, 1, 4, 1], [1, 4, 1, 7], [4, 1, 7, 1], [1, 7, 1, 8], [7, 1, 8, 0], [1, 8, 0, 2], [8, 0, 2, 0], [2, 0, 2, 0], [2, 0, 2, 0], [0, 2, 0, 3], [0, 2, 0, 3], [2, 0, 3, 3], [2, 0, 3, 3], [3, 3, 4, 3], [3, 4, 3, 7], [4, 3, 7, 3], [3, 7, 3, 8], [3, 8, 0, 2], [3, 8, 0, 2], [3, 10, 1, 10], [10, 1, 10, 2], [2, 0, 3, 5], [5, 1, 3, 3], [1, 3, 3, 5], [3, 3, 5, 3], [3, 5, 3, 6], [5, 3, 6, 1], [3, 6, 1, 5], [6, 1, 5, 5], [1, 5, 5, 6], [5, 5, 6, 3], [5, 6, 3, 3], [6, 3, 3, 5], [3, 3, 5, 8], [3, 5, 8, 1], [5, 8, 1, 7], [8, 1, 7, 4], [1, 7, 4, 4], [7, 4, 4, 7], [4, 4, 7, 7], [4, 7, 7, 8], [0, 2, 0, 3], [7, 8, 0, 3], [10, 2, 0, 3], [0, 2, 0, 2]]\n",
      "Époque [1/20], Perte moyenne : 0.4010\n",
      "Époque [2/20], Perte moyenne : 0.2109\n",
      "Époque [3/20], Perte moyenne : 0.1760\n",
      "Époque [4/20], Perte moyenne : 0.1742\n",
      "Époque [5/20], Perte moyenne : 0.1290\n",
      "Époque [6/20], Perte moyenne : 0.1224\n",
      "Époque [7/20], Perte moyenne : 0.1319\n",
      "Époque [8/20], Perte moyenne : 0.0969\n",
      "Époque [9/20], Perte moyenne : 0.1068\n",
      "Époque [10/20], Perte moyenne : 0.1489\n",
      "Époque [11/20], Perte moyenne : 0.1648\n",
      "Époque [12/20], Perte moyenne : 0.0892\n",
      "Époque [13/20], Perte moyenne : 0.0996\n",
      "Époque [14/20], Perte moyenne : 0.0843\n",
      "Époque [15/20], Perte moyenne : 0.1319\n",
      "Époque [16/20], Perte moyenne : 0.1089\n",
      "Époque [17/20], Perte moyenne : 0.0969\n",
      "Époque [18/20], Perte moyenne : 0.0635\n",
      "Époque [19/20], Perte moyenne : 0.1529\n",
      "Époque [20/20], Perte moyenne : 0.0698\n",
      "[[0, 2, 0, 2, 0], [2, 0, 2, 0, 3], [0, 2, 0, 3, 3], [2, 0, 3, 3, 4], [7, 3, 8, 0, 2], [0, 2, 0, 3, 10], [0, 2, 0, 3, 10], [10, 2, 0, 2, 0], [10, 2, 0, 2, 0], [2, 0, 3, 5, 1], [2, 0, 3, 5, 1], [8, 0, 2, 0, 3], [7, 8, 0, 3, 6], [5, 10, 2, 0, 3], [5, 10, 2, 0, 3], [8, 0, 2, 0, 2]]\n",
      "Époque [1/20], Perte moyenne : 0.4212\n",
      "Époque [2/20], Perte moyenne : 0.3220\n",
      "Époque [3/20], Perte moyenne : 0.2440\n",
      "Époque [4/20], Perte moyenne : 0.1836\n",
      "Époque [5/20], Perte moyenne : 0.2095\n",
      "Époque [6/20], Perte moyenne : 0.1197\n",
      "Époque [7/20], Perte moyenne : 0.1373\n",
      "Époque [8/20], Perte moyenne : 0.1539\n",
      "Époque [9/20], Perte moyenne : 0.1468\n",
      "Époque [10/20], Perte moyenne : 0.1287\n",
      "Époque [11/20], Perte moyenne : 0.1243\n",
      "Époque [12/20], Perte moyenne : 0.1209\n",
      "Époque [13/20], Perte moyenne : 0.1542\n",
      "Époque [14/20], Perte moyenne : 0.1466\n",
      "Époque [15/20], Perte moyenne : 0.1080\n",
      "Époque [16/20], Perte moyenne : 0.1409\n",
      "Époque [17/20], Perte moyenne : 0.1255\n",
      "Époque [18/20], Perte moyenne : 0.1141\n",
      "Époque [19/20], Perte moyenne : 0.1038\n",
      "Époque [20/20], Perte moyenne : 0.1078\n",
      "[[8, 0, 2, 0, 2, 0], [8, 0, 2, 0, 2, 0], [0, 2, 0, 2, 0, 3], [0, 2, 0, 2, 0, 3], [0, 2, 0, 3, 10, 1], [1, 10, 2, 0, 2, 0], [0, 2, 0, 2, 0, 3], [2, 0, 3, 5, 1, 3], [7, 8, 0, 2, 0, 3], [7, 8, 0, 3, 6, 5], [7, 8, 0, 3, 6, 5], [6, 5, 10, 2, 0, 3], [7, 8, 0, 2, 0, 2]]\n",
      "Époque [1/20], Perte moyenne : 0.2123\n",
      "Époque [2/20], Perte moyenne : 0.0904\n",
      "Époque [3/20], Perte moyenne : 0.1110\n",
      "Époque [4/20], Perte moyenne : 0.0712\n",
      "Époque [5/20], Perte moyenne : 0.0581\n",
      "Époque [6/20], Perte moyenne : 0.0467\n",
      "Époque [7/20], Perte moyenne : 0.0518\n",
      "Époque [8/20], Perte moyenne : 0.0570\n",
      "Époque [9/20], Perte moyenne : 0.0571\n",
      "Époque [10/20], Perte moyenne : 0.0572\n",
      "Époque [11/20], Perte moyenne : 0.0819\n",
      "Époque [12/20], Perte moyenne : 0.0404\n",
      "Époque [13/20], Perte moyenne : 0.0425\n",
      "Époque [14/20], Perte moyenne : 0.0441\n",
      "Époque [15/20], Perte moyenne : 0.0301\n",
      "Époque [16/20], Perte moyenne : 0.0656\n",
      "Époque [17/20], Perte moyenne : 0.0310\n",
      "Époque [18/20], Perte moyenne : 0.0568\n",
      "Époque [19/20], Perte moyenne : 0.0338\n",
      "Époque [20/20], Perte moyenne : 0.0287\n"
     ]
    }
   ],
   "source": [
    "for example_sequences in Training_sequences:\n",
    "    print(example_sequences)\n",
    "    dataloader = create_dataloader(example_sequences, batch_size=16, max_length=10)\n",
    "    train_model(model, dataloader, num_epochs=20, learning_rate=1e-3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22c457d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = [2]  # Expect next token 0\n",
    "probs = predict_next_token_probs(model, example_input, max_length=10, device=device)\n",
    "# print(\"\\nProbabilités pour chaque token (index 0-11) :\")\n",
    "# print(probs.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84f7e95c-3f3d-41eb-bd3c-0aedecb58e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>action</th>\n",
       "      <th>outcome</th>\n",
       "      <th>valence</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.018175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.981825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.039293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.960707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.514183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.485817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.375161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.624839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.964635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.035365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.945083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.054917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token  action  outcome  valence  probability\n",
       "0       0       0        0        5     0.018175\n",
       "1       1       0        1      -10     0.981825\n",
       "2       2       1        0       -1     0.039293\n",
       "3       3       1        1       -1     0.960707\n",
       "4       4       2        0       -1     0.514183\n",
       "5       5       2        1       -1     0.485817\n",
       "6       6       3        0       -1     0.375161\n",
       "7       7       3        1       -1     0.624839\n",
       "8       8       4        0       -3     0.964635\n",
       "9       9       4        1       -3     0.035365\n",
       "10     10       5        0       -3     0.945083\n",
       "11     11       5        1       -3     0.054917"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "expected_df = pd.DataFrame({\n",
    "    'token':  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "    'action': [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5],\n",
    "    'outcome':[0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "    'valence':[5, -10, -1, -1, -1, -1, -1, -1, -3, -3, -3, -3],\n",
    "    'probability': probs})\n",
    "# expected_df['expected_valence'] = expected_df['valence'] * expected_df['probability']\n",
    "# action_expectation_df = expected_df.groupby('action').agg({'expected_valence': 'sum'}).reset_index()\n",
    "# action_expectation_df = action_expectation_df.sort_values(by=['expected_valence'], ascending=[False]).reset_index(drop=True)\n",
    "expected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bfbffe-b499-4ea2-8fc1-9d14b4bc68dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyton 3.12",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
