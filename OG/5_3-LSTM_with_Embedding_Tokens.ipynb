{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f09cd53",
   "metadata": {},
   "source": [
    "\n",
    "# üî§ Utilisation d'un Embedding avec un LSTM pour traiter des s√©quences de tokens\n",
    "\n",
    "Ce notebook montre comment utiliser un `nn.Embedding` pour transformer une s√©quence de **tokens discrets** (entiers repr√©sentant des mots ou des cat√©gories) en vecteurs denses, avant de les faire passer dans un LSTM.\n",
    "\n",
    "Ce cas est tr√®s courant en traitement automatique du langage (NLP), mais s‚Äôapplique aussi √† toute s√©quence cat√©gorielle.\n",
    "\n",
    "## üîß Objectif :\n",
    "1. Cr√©er des s√©quences de tokens al√©atoires.\n",
    "2. Appliquer un embedding.\n",
    "3. Entra√Æner un LSTM pour classifier la s√©quence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ec7d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc57d4",
   "metadata": {},
   "source": [
    "## üì¶ G√©n√©ration d‚Äôun jeu de donn√©es avec des tokens al√©atoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d087f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Param√®tres\n",
    "VOCAB_SIZE = 50\n",
    "SEQ_LEN = 8\n",
    "\n",
    "def generate_token_batch(batch_size=50):\n",
    "    sequences = torch.randint(0, VOCAB_SIZE, (batch_size, SEQ_LEN))\n",
    "    labels = torch.tensor([\n",
    "        1 if (seq[0] + seq[-1]) % 2 == 0 else 0  # r√®gle arbitraire : parit√© de la somme d√©but+fin\n",
    "        for seq in sequences\n",
    "    ])\n",
    "    return sequences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2731e71a-1966-4884-85ee-7aa687195652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21,  6, 37, 33, 18, 48,  1, 25],\n",
      "        [27, 24, 29, 20, 37, 44, 45,  1],\n",
      "        [ 9,  6, 29, 37, 31, 45, 40,  1],\n",
      "        [10, 34, 41,  4, 47, 26, 24, 38],\n",
      "        [49, 25,  9, 40,  4, 27, 22, 44],\n",
      "        [ 6,  9,  5, 35,  4, 44, 29, 39],\n",
      "        [ 2, 44, 43, 34, 12, 16,  3,  2],\n",
      "        [44,  8, 42,  1, 23, 49, 46, 34],\n",
      "        [36, 47, 44, 43, 25, 17, 40, 18],\n",
      "        [ 2, 12, 42, 41, 25, 28, 10, 19],\n",
      "        [22, 10, 17,  7, 25,  0, 28, 39],\n",
      "        [ 4, 20, 11, 39, 12, 47, 23,  2],\n",
      "        [41, 21, 49, 27, 22, 24, 43, 35],\n",
      "        [15,  5, 42, 32, 28, 19, 14, 38],\n",
      "        [44, 44,  9, 14,  7, 34, 23,  4],\n",
      "        [42, 36, 31, 30, 23, 14,  5, 30],\n",
      "        [ 7, 37,  5, 18, 18, 29, 38,  8],\n",
      "        [24, 43, 12, 40, 35, 30, 16, 43],\n",
      "        [ 2, 34, 37, 20, 23,  9, 38, 27],\n",
      "        [12,  7,  9, 35,  8, 43, 11, 39],\n",
      "        [37, 48, 33, 41, 42, 45, 27, 25],\n",
      "        [18, 11, 46, 39, 21, 19, 29, 25],\n",
      "        [48, 39,  3, 40, 40,  4, 39, 13],\n",
      "        [ 8, 33, 14, 40, 43, 16, 10, 38],\n",
      "        [40, 29, 23, 44, 16, 36, 48, 48],\n",
      "        [22, 43, 24, 18, 10, 35, 24, 10],\n",
      "        [42, 17, 48,  2, 13, 25, 19, 30],\n",
      "        [13,  8, 17, 33,  3, 32, 16, 29],\n",
      "        [10, 36,  3,  4,  0, 38, 21, 43],\n",
      "        [16, 37,  3,  3, 25, 31,  9,  3],\n",
      "        [30, 25,  6, 41,  9, 48, 14, 20],\n",
      "        [43, 39, 32, 35,  0,  5,  4,  8],\n",
      "        [21, 30, 37, 42, 23,  2, 36, 17],\n",
      "        [13, 47,  1, 21, 11, 13, 29, 49],\n",
      "        [ 7, 35, 15, 47, 10, 38, 21, 38],\n",
      "        [32, 45, 18, 28, 44, 11, 42, 12],\n",
      "        [38, 44, 42,  0, 17, 49, 42,  8],\n",
      "        [36,  6,  4,  5, 41, 26, 38, 36],\n",
      "        [41, 15, 16,  1, 41,  8, 10, 14],\n",
      "        [27,  6, 49,  9,  5, 26, 12, 45],\n",
      "        [19,  6, 35, 35, 28, 21, 30,  7],\n",
      "        [39, 49, 11, 31,  8,  0, 27,  6],\n",
      "        [24,  5, 19, 36, 13, 26, 39, 17],\n",
      "        [49, 36, 40,  5, 20, 26, 37, 29],\n",
      "        [12, 22, 47, 33, 44, 11, 23, 27],\n",
      "        [ 1, 15, 20, 20,  6,  0,  0, 20],\n",
      "        [26, 35,  2,  9,  7, 28, 17, 20],\n",
      "        [21, 34,  9, 45, 47,  4, 30, 18],\n",
      "        [32, 13, 35, 10,  0, 20, 47, 15],\n",
      "        [43,  1, 19,  3, 18, 47,  4, 25]])\n",
      "tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1])\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_token_batch()\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fb50f",
   "metadata": {},
   "source": [
    "## üß† Mod√®le : Embedding + LSTM + Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0fd4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=32, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # [B, T] -> [B, T, E]\n",
    "        _, (hn, _) = self.lstm(x)  # hn: [1, B, H]\n",
    "        return self.fc(hn.squeeze(0))  # [B, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab03fb",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Fonction d'entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eefddb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, epochs=50):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        X, y = generate_token_batch()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (outputs.argmax(1) == y).float().mean().item()\n",
    "        print(f\"Epoch {epoch+1:02d} - Loss: {loss.item():.4f} - Accuracy: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611a351",
   "metadata": {},
   "source": [
    "## üöÄ Entra√Ænons le mod√®le sur des tokens !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90566c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 - Loss: 0.7001 - Accuracy: 52.00%\n",
      "Epoch 02 - Loss: 0.7429 - Accuracy: 38.00%\n",
      "Epoch 03 - Loss: 0.6728 - Accuracy: 60.00%\n",
      "Epoch 04 - Loss: 0.6890 - Accuracy: 52.00%\n",
      "Epoch 05 - Loss: 0.6883 - Accuracy: 54.00%\n",
      "Epoch 06 - Loss: 0.7296 - Accuracy: 42.00%\n",
      "Epoch 07 - Loss: 0.7131 - Accuracy: 52.00%\n",
      "Epoch 08 - Loss: 0.6996 - Accuracy: 44.00%\n",
      "Epoch 09 - Loss: 0.7290 - Accuracy: 44.00%\n",
      "Epoch 10 - Loss: 0.6974 - Accuracy: 52.00%\n",
      "Epoch 11 - Loss: 0.7083 - Accuracy: 48.00%\n",
      "Epoch 12 - Loss: 0.6897 - Accuracy: 56.00%\n",
      "Epoch 13 - Loss: 0.7043 - Accuracy: 50.00%\n",
      "Epoch 14 - Loss: 0.7078 - Accuracy: 40.00%\n",
      "Epoch 15 - Loss: 0.7021 - Accuracy: 44.00%\n",
      "Epoch 16 - Loss: 0.6965 - Accuracy: 46.00%\n",
      "Epoch 17 - Loss: 0.6953 - Accuracy: 52.00%\n",
      "Epoch 18 - Loss: 0.6777 - Accuracy: 58.00%\n",
      "Epoch 19 - Loss: 0.6806 - Accuracy: 54.00%\n",
      "Epoch 20 - Loss: 0.6999 - Accuracy: 48.00%\n",
      "Epoch 21 - Loss: 0.6865 - Accuracy: 60.00%\n",
      "Epoch 22 - Loss: 0.7032 - Accuracy: 44.00%\n",
      "Epoch 23 - Loss: 0.6958 - Accuracy: 52.00%\n",
      "Epoch 24 - Loss: 0.7172 - Accuracy: 36.00%\n",
      "Epoch 25 - Loss: 0.6942 - Accuracy: 48.00%\n",
      "Epoch 26 - Loss: 0.7100 - Accuracy: 46.00%\n",
      "Epoch 27 - Loss: 0.7101 - Accuracy: 48.00%\n",
      "Epoch 28 - Loss: 0.7019 - Accuracy: 40.00%\n",
      "Epoch 29 - Loss: 0.7097 - Accuracy: 38.00%\n",
      "Epoch 30 - Loss: 0.6892 - Accuracy: 56.00%\n",
      "Epoch 31 - Loss: 0.6978 - Accuracy: 50.00%\n",
      "Epoch 32 - Loss: 0.7073 - Accuracy: 42.00%\n",
      "Epoch 33 - Loss: 0.7052 - Accuracy: 50.00%\n",
      "Epoch 34 - Loss: 0.7177 - Accuracy: 44.00%\n",
      "Epoch 35 - Loss: 0.6905 - Accuracy: 52.00%\n",
      "Epoch 36 - Loss: 0.7009 - Accuracy: 52.00%\n",
      "Epoch 37 - Loss: 0.6945 - Accuracy: 50.00%\n",
      "Epoch 38 - Loss: 0.6963 - Accuracy: 44.00%\n",
      "Epoch 39 - Loss: 0.7094 - Accuracy: 44.00%\n",
      "Epoch 40 - Loss: 0.7076 - Accuracy: 40.00%\n",
      "Epoch 41 - Loss: 0.6942 - Accuracy: 48.00%\n",
      "Epoch 42 - Loss: 0.7049 - Accuracy: 52.00%\n",
      "Epoch 43 - Loss: 0.6959 - Accuracy: 52.00%\n",
      "Epoch 44 - Loss: 0.6856 - Accuracy: 62.00%\n",
      "Epoch 45 - Loss: 0.6876 - Accuracy: 54.00%\n",
      "Epoch 46 - Loss: 0.6867 - Accuracy: 58.00%\n",
      "Epoch 47 - Loss: 0.6973 - Accuracy: 46.00%\n",
      "Epoch 48 - Loss: 0.7113 - Accuracy: 44.00%\n",
      "Epoch 49 - Loss: 0.6922 - Accuracy: 46.00%\n",
      "Epoch 50 - Loss: 0.6965 - Accuracy: 52.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TokenLSTMClassifier(vocab_size=VOCAB_SIZE)\n",
    "train(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c54d6c3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ R√©sum√©\n",
    "\n",
    "- Les tokens sont encod√©s par un `nn.Embedding`, ce qui permet au LSTM de travailler dans un espace vectoriel dense.\n",
    "- Le LSTM extrait les d√©pendances s√©quentielles de ces vecteurs.\n",
    "- Le mod√®le apprend une t√¢che arbitraire sur la base des valeurs de d√©but et de fin de s√©quence.\n",
    "\n",
    "Vous pouvez facilement adapter ce code √† des s√©quences de texte r√©elles en utilisant un tokenizer (comme ceux de HuggingFace).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1d73e64-cc8c-45f3-96a9-936310c82630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S√©quence d'entr√©e : [21, 6, 37, 33, 18, 48, 1, 25]\n",
      "Token pr√©dit      : 0\n"
     ]
    }
   ],
   "source": [
    "# üîÆ Fonction de pr√©diction\n",
    "def predict_next_token(model, sequence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(sequence).unsqueeze(0)  # [1, seq_len]\n",
    "        logits = model(input_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        predicted_token = torch.argmax(probs, dim=1).item()\n",
    "    return predicted_token, probs.squeeze().tolist()\n",
    "\n",
    "# Exemple de pr√©diction\n",
    "sample_sequence = [21,  6, 37, 33, 18, 48,  1, 25]  # Longueur doit √™tre √©gale √† SEQ_LEN\n",
    "predicted_token, probs = predict_next_token(model, sample_sequence)\n",
    "\n",
    "print(f\"S√©quence d'entr√©e : {sample_sequence}\")\n",
    "print(f\"Token pr√©dit      : {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dfea12-b50d-4c94-a6d5-1778b6945dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
