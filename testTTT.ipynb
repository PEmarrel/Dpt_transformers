{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.CustomDataSet import CustomDataSetRNN, CustomDataSet\n",
    "from model.Tokenizer import SimpleTokenizerV1\n",
    "from model.TTT import *\n",
    "from environnement.environnement1Str import Environnement1\n",
    "from environnement.environnement3Str import Environnement3\n",
    "from environnement.environnement6Str import Environnement6\n",
    "from environnement.small_loop import small_loop\n",
    "from outil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['turn_right', 'turn_right', 'feel_front', 'turn_left', 'turn_right', 'forward', 'turn_right', 'turn_left', 'turn_right', 'turn_right', 'turn_right', 'turn_left', 'forward', 'turn_left', 'feel_front', 'turn_left', 'turn_right', 'forward', 'turn_right', 'turn_left', 'turn_left', 'turn_right', 'forward', 'feel_front', 'forward', 'forward', 'turn_left', 'forward', 'feel_front', 'turn_right', 'turn_left', 'forward', 'feel_front', 'forward', 'feel_front', 'feel_front', 'feel_front', 'turn_left', 'forward', 'turn_right', 'feel_front', 'turn_left', 'turn_left', 'turn_left', 'turn_right', 'feel_front', 'feel_front', 'feel_front', 'feel_front', 'turn_left']\n",
      "['empty', 'empty', 'wall', 'empty', 'empty', 'wall', 'empty', 'empty', 'empty', 'empty', 'empty', 'empty', 'wall', 'empty', 'wall', 'empty', 'empty', 'wall', 'empty', 'empty', 'empty', 'empty', 'wall', 'wall', 'wall', 'wall', 'empty', 'wall', 'wall', 'empty', 'empty', 'wall', 'wall', 'wall', 'wall', 'wall', 'wall', 'empty', 'empty', 'empty', 'wall', 'empty', 'empty', 'empty', 'empty', 'wall', 'wall', 'wall', 'wall', 'empty']\n"
     ]
    }
   ],
   "source": [
    "# init data\n",
    "env_test = small_loop(x= 1, y=1, theta=0, world= np.array([\n",
    "                [1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 1],\n",
    "                # [1, 0, 1, 0, 1],\n",
    "                # [1, 0, 0, 0, 1],\n",
    "                [1, 1, 1, 1, 1],\n",
    "            ]))\n",
    "actions, outcomes = [], []\n",
    "\n",
    "for i in range(50):\n",
    "    action = str(np.random.choice(env_test.get_actions()))\n",
    "    outcome = env_test.outcome(action)\n",
    "    actions.append(action)\n",
    "    outcomes.append(outcome)\n",
    "\n",
    "print(actions)\n",
    "print(outcomes)\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab=\n",
    "        create_dico_numerate_word(env_test.get_outcomes() + env_test.get_actions()))\n",
    "\n",
    "data_set = CustomDataSet(actions=actions, outcomes=outcomes,\n",
    "                    context_lenght=21, dim_out=, tokenizer=tokenizer)\n",
    "\n",
    "data_loader_train = DataLoader(data_set, batch_size=32, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "# text, target = next(iter(data_loader_train))\n",
    "# print(text)\n",
    "# print(target)\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    action = str(np.random.choice(env_test.get_actions()))\n",
    "    outcome = env_test.outcome(action)\n",
    "    actions.append(action)\n",
    "    outcomes.append(outcome)\n",
    "\n",
    "data_set_test = CustomDataSet(actions=actions, outcomes=outcomes,\n",
    "                    context_lenght=15, dim_out=2, tokenizer=tokenizer)\n",
    "\n",
    "data_loader_test = DataLoader(data_set, batch_size=128, shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de configuration\n",
    "config = TTTConfig(\n",
    "    vocab_size=6,  # Exemple de taille de vocabulaire\n",
    "    hidden_size=32,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    pretraining_tp=0.15,\n",
    ")\n",
    "\n",
    "model = TTTModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define the loss function as CrossEntropyLoss for classification\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "tensor([[2, 1, 2, 0, 2, 0, 3, 1, 3, 1, 3, 1, 2, 0, 4, 1, 4, 1, 2, 0, 4],\n",
      "        [3, 1, 2, 1, 4, 1, 5, 0, 2, 0, 4, 1, 3, 1, 4, 1, 5, 1, 3, 1, 3],\n",
      "        [5, 1, 5, 1, 5, 1, 2, 1, 3, 1, 3, 1, 4, 1, 2, 0, 5, 0, 4, 1, 5],\n",
      "        [4, 1, 3, 1, 5, 0, 5, 0, 5, 0, 3, 1, 2, 0, 2, 0, 4, 1, 3, 1, 5],\n",
      "        [3, 1, 2, 1, 3, 1, 3, 1, 3, 1, 2, 0, 3, 1, 5, 1, 4, 1, 5, 0, 5],\n",
      "        [4, 1, 4, 1, 5, 0, 2, 0, 4, 1, 5, 1, 5, 1, 5, 1, 2, 1, 3, 1, 4],\n",
      "        [2, 0, 3, 1, 3, 1, 4, 1, 5, 1, 2, 1, 3, 1, 3, 1, 3, 1, 5, 0, 5],\n",
      "        [3, 1, 3, 1, 3, 1, 4, 1, 4, 1, 3, 1, 4, 1, 5, 1, 2, 1, 5, 1, 4],\n",
      "        [5, 0, 3, 1, 3, 1, 3, 1, 2, 1, 5, 1, 4, 1, 2, 0, 4, 1, 4, 1, 3],\n",
      "        [5, 0, 2, 0, 5, 0, 5, 0, 4, 1, 4, 1, 4, 1, 2, 0, 5, 0, 3, 1, 2],\n",
      "        [3, 1, 4, 1, 3, 1, 2, 0, 5, 0, 4, 1, 5, 0, 3, 1, 3, 1, 3, 1, 4],\n",
      "        [5, 0, 2, 0, 2, 0, 4, 1, 5, 1, 2, 1, 3, 1, 3, 1, 2, 1, 4, 1, 3],\n",
      "        [3, 1, 5, 0, 2, 0, 3, 1, 2, 0, 3, 1, 2, 1, 4, 1, 2, 0, 2, 0, 3],\n",
      "        [3, 1, 4, 1, 2, 0, 4, 1, 4, 1, 4, 1, 5, 0, 3, 1, 5, 1, 5, 1, 5],\n",
      "        [2, 0, 3, 1, 3, 1, 2, 0, 2, 0, 2, 0, 4, 1, 4, 1, 5, 0, 3, 1, 4],\n",
      "        [4, 1, 2, 0, 4, 1, 3, 1, 4, 1, 4, 1, 4, 1, 3, 1, 2, 0, 3, 1, 5],\n",
      "        [5, 0, 3, 1, 2, 0, 2, 0, 4, 1, 3, 1, 5, 0, 5, 0, 5, 0, 4, 1, 5],\n",
      "        [3, 1, 2, 0, 2, 0, 2, 0, 3, 1, 3, 1, 2, 1, 2, 1, 5, 0, 4, 1, 4],\n",
      "        [3, 1, 5, 0, 5, 0, 5, 0, 2, 0, 3, 1, 2, 1, 3, 1, 4, 1, 4, 1, 4],\n",
      "        [3, 1, 5, 0, 5, 0, 4, 1, 2, 0, 5, 0, 3, 1, 3, 1, 5, 1, 4, 1, 2],\n",
      "        [3, 1, 3, 1, 2, 1, 2, 0, 3, 1, 2, 0, 5, 0, 4, 1, 2, 0, 4, 1, 5],\n",
      "        [2, 0, 5, 0, 2, 0, 2, 0, 4, 1, 4, 1, 3, 1, 5, 0, 2, 0, 2, 0, 3],\n",
      "        [5, 0, 3, 1, 5, 1, 3, 1, 4, 1, 5, 1, 2, 1, 2, 1, 3, 1, 2, 0, 2],\n",
      "        [4, 1, 3, 1, 5, 0, 3, 1, 5, 1, 3, 1, 4, 1, 5, 1, 2, 1, 2, 1, 3],\n",
      "        [2, 0, 4, 1, 3, 1, 4, 1, 5, 1, 4, 1, 5, 0, 2, 0, 2, 0, 3, 1, 3],\n",
      "        [2, 0, 3, 1, 2, 0, 3, 1, 2, 1, 3, 1, 3, 1, 3, 1, 2, 0, 3, 1, 5],\n",
      "        [5, 0, 2, 0, 3, 1, 2, 1, 3, 1, 5, 0, 3, 1, 2, 1, 4, 1, 5, 0, 2],\n",
      "        [3, 1, 2, 1, 2, 0, 3, 1, 2, 0, 5, 0, 4, 1, 2, 0, 4, 1, 5, 0, 2],\n",
      "        [4, 1, 2, 0, 2, 0, 3, 1, 3, 1, 5, 0, 3, 1, 2, 0, 4, 1, 3, 1, 2],\n",
      "        [3, 1, 2, 0, 3, 1, 3, 1, 3, 1, 5, 1, 3, 1, 5, 0, 3, 1, 4, 1, 3],\n",
      "        [5, 0, 4, 1, 3, 1, 3, 1, 5, 0, 4, 1, 3, 1, 5, 0, 5, 0, 2, 0, 2],\n",
      "        [2, 0, 5, 0, 4, 1, 2, 0, 5, 0, 2, 0, 3, 1, 5, 0, 5, 0, 2, 0, 5]],\n",
      "       dtype=torch.int32)\n",
      "labels\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0])\n",
      "outputs\n",
      "None\n",
      "outputs last_hidden_state\n",
      "tensor([[[ 8.3689e-01, -4.8121e-01,  2.3053e+00,  ...,  1.2536e+00,\n",
      "          -8.0286e-01,  6.0460e-01],\n",
      "         [ 9.7227e-01, -3.4766e-01,  1.5288e+00,  ...,  1.7385e+00,\n",
      "          -1.0405e+00,  1.1233e+00],\n",
      "         [ 5.2039e-01, -2.0703e-01,  1.5951e+00,  ...,  1.8928e+00,\n",
      "          -1.2832e+00,  1.2501e+00],\n",
      "         ...,\n",
      "         [ 3.6746e-01,  8.9124e-01,  2.2749e+00,  ...,  1.9713e+00,\n",
      "          -1.8089e-01,  1.5338e+00],\n",
      "         [ 4.2213e-01,  1.0188e+00,  2.0339e+00,  ...,  1.8735e+00,\n",
      "          -1.2064e-01,  1.3600e+00],\n",
      "         [ 5.6451e-01,  7.6533e-01,  2.2519e+00,  ...,  1.9718e+00,\n",
      "           1.8554e-02,  1.4221e+00]],\n",
      "\n",
      "        [[ 3.5221e-01,  6.2699e-01,  8.2273e-01,  ..., -1.1344e-01,\n",
      "           1.1175e+00, -7.9697e-01],\n",
      "         [ 4.2969e-01,  3.5521e-01,  5.1718e-01,  ...,  1.2915e+00,\n",
      "           7.8567e-01, -1.3708e-01],\n",
      "         [-6.5689e-01,  5.1942e-01,  8.7851e-01,  ...,  1.6261e+00,\n",
      "           2.8100e-01,  2.1826e-01],\n",
      "         ...,\n",
      "         [-7.7861e-01,  6.2009e-01,  1.8177e+00,  ...,  1.9853e+00,\n",
      "          -4.5700e-01, -2.1233e-01],\n",
      "         [-8.8455e-01,  5.9907e-01,  1.7237e+00,  ...,  2.1083e+00,\n",
      "          -3.6015e-01,  1.2817e-01],\n",
      "         [-5.6965e-01,  5.7713e-01,  1.9000e+00,  ...,  1.9969e+00,\n",
      "          -6.4005e-01, -2.1437e-01]],\n",
      "\n",
      "        [[-1.7712e+00, -7.6187e-02, -1.0527e+00,  ...,  2.4046e-01,\n",
      "           8.6719e-01, -1.4231e+00],\n",
      "         [-1.0369e+00, -1.9092e-01, -1.3856e+00,  ...,  1.2843e+00,\n",
      "           5.7174e-01, -5.8883e-01],\n",
      "         [-1.6718e+00, -2.1675e-03, -1.1632e+00,  ...,  1.4976e+00,\n",
      "           9.1482e-02, -1.2440e+00],\n",
      "         ...,\n",
      "         [-1.4192e+00, -7.1794e-01,  2.8088e-01,  ...,  2.3609e+00,\n",
      "           3.3906e-01, -1.3507e+00],\n",
      "         [-1.2608e+00, -6.2208e-01, -4.9769e-02,  ...,  2.2620e+00,\n",
      "           3.0281e-01, -1.1771e+00],\n",
      "         [-1.2191e+00, -5.5725e-01,  6.8054e-02,  ...,  2.3254e+00,\n",
      "           1.1166e-01, -1.6024e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.5221e-01,  6.2699e-01,  8.2273e-01,  ..., -1.1344e-01,\n",
      "           1.1175e+00, -7.9697e-01],\n",
      "         [ 4.2969e-01,  3.5521e-01,  5.1718e-01,  ...,  1.2915e+00,\n",
      "           7.8567e-01, -1.3708e-01],\n",
      "         [-6.5689e-01,  5.1942e-01,  8.7851e-01,  ...,  1.6261e+00,\n",
      "           2.8100e-01,  2.1826e-01],\n",
      "         ...,\n",
      "         [-1.2855e-01,  7.0522e-01,  2.3950e+00,  ...,  2.2812e+00,\n",
      "           5.3398e-01,  4.1594e-01],\n",
      "         [-2.1254e-02,  6.1242e-01,  2.1678e+00,  ...,  2.2646e+00,\n",
      "           5.2394e-01,  5.8217e-01],\n",
      "         [ 1.7960e-01,  6.7043e-01,  2.3110e+00,  ...,  2.0502e+00,\n",
      "           2.6876e-01,  2.6633e-01]],\n",
      "\n",
      "        [[-1.7712e+00, -7.6187e-02, -1.0527e+00,  ...,  2.4046e-01,\n",
      "           8.6719e-01, -1.4231e+00],\n",
      "         [-2.0256e+00,  6.8189e-01, -1.9930e+00,  ...,  1.9635e-01,\n",
      "           1.5683e+00, -1.2363e+00],\n",
      "         [-2.0750e+00,  3.7981e-01, -1.3998e+00,  ...,  5.7127e-01,\n",
      "           2.2183e+00, -9.5177e-01],\n",
      "         ...,\n",
      "         [-5.5825e-01,  5.3339e-01,  1.6720e-01,  ...,  2.1347e+00,\n",
      "           9.6294e-01, -1.0647e+00],\n",
      "         [-5.2309e-01,  7.2286e-01,  7.7690e-02,  ...,  2.2156e+00,\n",
      "           9.8676e-01, -1.0387e+00],\n",
      "         [-5.1670e-01,  6.1277e-01,  3.2778e-01,  ...,  2.2953e+00,\n",
      "           9.1551e-01, -8.9206e-01]],\n",
      "\n",
      "        [[ 8.3689e-01, -4.8121e-01,  2.3053e+00,  ...,  1.2536e+00,\n",
      "          -8.0286e-01,  6.0460e-01],\n",
      "         [ 2.4472e-01,  5.1791e-01,  1.4953e+00,  ...,  1.8482e+00,\n",
      "          -1.1374e+00,  4.0812e-01],\n",
      "         [-1.9530e-02,  6.9708e-01,  1.6406e+00,  ...,  2.1057e+00,\n",
      "          -1.1157e+00,  1.5452e-01],\n",
      "         ...,\n",
      "         [ 8.3229e-02,  1.5045e+00,  1.1283e+00,  ...,  1.6722e+00,\n",
      "          -3.5812e-03,  5.8685e-01],\n",
      "         [-2.5660e-02,  1.8967e+00,  8.5525e-01,  ...,  1.6695e+00,\n",
      "           8.5513e-02,  6.7744e-01],\n",
      "         [ 1.8606e-01,  1.7456e+00,  1.0409e+00,  ...,  1.9155e+00,\n",
      "           1.1828e-02,  2.6655e-01]]], grad_fn=<MulBackward0>)\n",
      "torch.Size([32, 21, 128])\n",
      "outputs cache_params\n",
      "None\n",
      "predi\n",
      "tensor([[10, 19,  0,  ..., 11, 15, 15],\n",
      "        [ 1, 16, 14,  ..., 11,  0,  3],\n",
      "        [10,  2, 18,  ..., 11,  0,  1],\n",
      "        ...,\n",
      "        [ 1, 15, 18,  ..., 13,  0, 15],\n",
      "        [12,  9, 20,  ..., 11,  2,  3],\n",
      "        [ 0, 17,  0,  ...,  8, 15,  3]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [32, 128], got [32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(predi)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Pass the raw logits to the loss function\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:1295\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [32, 128], got [32]"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in data_loader_train:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.int64)\n",
    "        print(\"inputs\")\n",
    "        print(inputs)\n",
    "        print(\"labels\")\n",
    "        print(labels)\n",
    "        outputs = model(inputs)\n",
    "        print(\"outputs\")\n",
    "        print(outputs.hidden_states)\n",
    "        print(\"outputs last_hidden_state\")\n",
    "        print(outputs.last_hidden_state)\n",
    "        print(outputs.last_hidden_state.shape)\n",
    "        print(\"outputs cache_params\")\n",
    "        print(outputs.cache_params)\n",
    "        \n",
    "        predi = torch.argmax(outputs.last_hidden_state, dim=1).to(torch.int64)\n",
    "        print(\"predi\")\n",
    "        print(predi)\n",
    "        # Pass the raw logits to the loss function\n",
    "        loss = loss_fn(outputs.last_hidden_state, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
