{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from model.CustomDataSet import CustomDataSet\n",
    "from model.Tokenizer import SimpleTokenizerV1\n",
    "from environnement.environnement1Str import Environnement1\n",
    "from environnement.environnement3Str import Environnement3\n",
    "from environnement.environnement6Str import Environnement6\n",
    "from environnement.small_loop import small_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.rnn(x, h)\n",
    "        return self.fc(out[:, -1, :]), h\n",
    "    \n",
    "class InteractionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(InteractionLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        lstm_out, h = self.lstm(x, hidden)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_training(model, test_seq_act, test_seq_out, action, n_steps=5):\n",
    "    model.train()\n",
    "    \n",
    "    data_set = CustomDataSet(actions=test_seq_act, \n",
    "                            outcomes=test_seq_out, \n",
    "                            context_lenght=model.input_size,)\n",
    "\n",
    "    for _ in range(n_steps):  # Ajustement sur le test\n",
    "        optimizer.zero_grad()\n",
    "        output = model(test_seq)\n",
    "        loss = criterion(output, action_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(test_seq)\n",
    "    \n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
