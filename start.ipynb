{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following file was inspired by the following tutorial:\n",
    "# https://colab.research.google.com/drive/1SiF0KZJp75rUeetKOWqpsA8clmHP6jMg?usp=sharing#scrollTo=d7utFz27cO9q\n",
    "\n",
    "# And use this cource for explanation:\n",
    "# https://bruno-yun.notion.site/The-Transformer-Model-for-NLG-c4413bd5a8044325a7658cb8ff5535f2\n",
    "# https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "from graphviz import Digraph\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environnement.environnement1 import Environnement1 as env1\n",
    "from environnement.environnement2Str import Environnement2 as env2Str\n",
    "from environnement.environnement3Str import Environnement3 as env3Str\n",
    "from environnement.environnement6Str import Environnement6 as env6Str\n",
    "\n",
    "from environnement.small_loop import small_loop\n",
    "\n",
    "from inter.interactions import Interaction as inter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro, bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporaire\n",
    "VOCAB_SIZE = 2 # For now we are only using binary data (0, 1) later can be modify by 12\n",
    "CONTEXT_LENGHT = 3 # And we are using a context of 3 bits (may be changed later)\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads,\n",
    "                 qkv_bias = False, device = 'cpu'):\n",
    "        \n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0), \"d_out should be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        # self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        # self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        # self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias) \n",
    "        # Don't use this because we want to have only one projection to optimize\n",
    "        # To have only one projection we use the following line\n",
    "        self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        self.out_proj = nn.Linear(d_out, d_out).to(device)\n",
    "        # If we want to see past\n",
    "        mask = torch.triu(torch.ones(context_length,context_length), diagonal=1).to(device)\n",
    "        \n",
    "        # if we want to see future\n",
    "        # mask = torch.tril(torch.ones(context_length,context_length), diagonal=-1).to(device)\n",
    "        \n",
    "        # If we want to see all the context\n",
    "        # mask = torch.zeros(context_length, context_length, device=device)\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "    def forward(self,x: torch.Tensor):\n",
    "        queries: torch.Tensor\n",
    "        keys: torch.Tensor\n",
    "        values: torch.Tensor\n",
    "        b, num_tokens, d_in = x.shape # b, num_token, d_in\n",
    "\n",
    "        # self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Use one projection to get queries, keys and values\n",
    "        # self.W_qkv(x) -> b, num_token, 3*d_out (is a tensor)\n",
    "        # chunk(3, dim=-1) -> b, num_token, d_out we split the tensor in 3 parts\n",
    "        queries, keys, values= self.W_qkv(x).chunk(3, dim=-1)\n",
    "        \n",
    "        # b, num_token, numheads, head_dim\n",
    "        queries = queries.reshape(b,\n",
    "                                num_tokens,\n",
    "                                self.num_heads,\n",
    "                                self.head_dim\n",
    "                            ).transpose(1, 2)\n",
    "        keys = keys.reshape(b,\n",
    "                            num_tokens,\n",
    "                            self.num_heads,\n",
    "                            self.head_dim\n",
    "                        ).transpose(1, 2)\n",
    "        values = values.reshape(b,\n",
    "                                num_tokens,\n",
    "                                self.num_heads,\n",
    "                                self.head_dim\n",
    "                            ).transpose(1, 2)\n",
    "        \n",
    "        # b, num_heads, num_token, num_token\n",
    "        attn_scores = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "        \n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask[:num_tokens, :num_tokens].unsqueeze(0).unsqueeze(0).bool() == 1, \n",
    "            float('-inf')\n",
    "        )\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.einsum('bhqk, bhkd -> bhqd', \n",
    "                               attn_weights, \n",
    "                               values\n",
    "                            ).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        \n",
    "        \n",
    "        return self.out_proj(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain of class MultiHeadAttention\n",
    "In transformers model, the **MultiHeadAttention** class is used to apply attention mechanisms to input sequences. Understanding how this class works requires first exploring **single-head** attention before generalizing to **multi-head attention**.\n",
    "\n",
    "## Single-Head Attention\n",
    "Attention in Transformers relies on measuring the similarity between tokens in a sequence. This measurement is performed using three distinct transformations:\n",
    "\n",
    "- Query (Q): Represents the current token’s focus when comparing it to all other tokens.\n",
    "- Key (K): Represents a token being compared to the query.\n",
    "- Value (V): Represents the actual information to be aggregated based on attention scores.\n",
    "\n",
    "These three representations are obtained by applying linear transformations to the input sequence. The transformed matrices allow us to compute the attention scores using the scaled dot-product attention formula:\n",
    "\n",
    "$ \n",
    "score(x_i, x_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}} \n",
    "$\n",
    "\n",
    "Where $q_i$ is the query representation of token $i$, $k_j$ is the key representation of token $j$, and $d_k$ is the dimension of the key representation. \n",
    "\n",
    "In forward function is :\n",
    "```python\n",
    "attn_scores = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "```\n",
    "\n",
    "To prevent a token from attending to future tokens (as in autoregressive models like GPT), we apply a mask to the attention scores. This ensures that each token only attends to itself and previous tokens:\n",
    "\n",
    "$\n",
    "a_i = \\sum_{j \\leq i} \\alpha_{ij}v_j\n",
    "$\n",
    "\n",
    "In forward function is :\n",
    "```python\n",
    "attn_scores = attn_scores.masked_fill(self.mask[:num_tokens, :num_tokens] == 0,\n",
    "                                float('-inf'))\n",
    "```\n",
    "\n",
    "Once the attention scores are computed and masked, they are normalized using the softmax function to produce attention weights:\n",
    "\n",
    "$\n",
    "\\alpha_{ij} = \\frac{\\exp(score(x_i,x_j))}{\\sum_j \\exp(score(x_i,x_j))}\n",
    "$\n",
    "\n",
    "In forward function is :\n",
    "```python\n",
    "attn_scores = nn.fonctional.softmax(attn_scores, dim=-1)\n",
    "```\n",
    "\n",
    "To prevent overfitting, a dropout layer is applied to the attention weights (self.dropout) before computing the context vector.\n",
    "\n",
    "To compute the context vector, we multiply the attention weights by the Value (V) representations to obtain the context vector.\n",
    "\n",
    "In forward function is :\n",
    "\n",
    "```python\n",
    "context = torch.einsum('bhqk, bhkd -> bhqd', attn_weights, values)\n",
    "```\n",
    "\n",
    "To resume we have done this :\n",
    "\n",
    "![explain one head](./img/ExplainOneHeadAttention.png)\n",
    "*Source: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/9.pdf)*\n",
    "\n",
    "## Multi-Head Attention\n",
    "Single-head attention allows each token to focus on others, but it has limitations: it can only capture one type of relationship at a time. Multi-head attention improves this by using multiple attention heads in parallel.\n",
    "Instead of computing attention once, we split the input into multiple heads, each with a different representation of the sequence. This allows the model to capture diverse patterns.\n",
    "All heads are concatenated and linearly transformed to produce the final output. This process is implemented in the **MultiHeadAttention**, is why we use self.head_dim or 'd' in code.\n",
    "\n",
    "This figure resume compute with matrix :\n",
    "\n",
    "![Compute](./img/ExplainCompute.png)\n",
    "*Source: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/9.pdf)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        # TODO fct FFN we can change the number of layers\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"] // 2, bias=cfg[\"qkv_bias\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(cfg[\"emb_dim\"] // 2, cfg[\"emb_dim\"], bias=cfg[\"qkv_bias\"])\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The FeedForward in transformer\n",
    "The **FeedForward** class is used to apply a feedforward neural network to the output of the multi-head attention layer. The feedforward network consists of two linear transformations with a GELU (or RELU) activation function in between.\n",
    "\n",
    "## GELU and RELU\n",
    "GELU and RELU are activation functions used in neural networks after linear transformations. GELU is a smoother version of RELU that has been shown to improve performance in transformer models. \n",
    "\n",
    "The GELU function is defined as:\n",
    "\n",
    "$\n",
    "GELU(x) = 0.5x(1 + tanh(\\sqrt{2/\\pi}(x + 0.044715x^3)))\n",
    "$\n",
    "\n",
    "RELU is a simpler activation function that sets all negative values to zero:\n",
    "\n",
    "$\n",
    "RELU(x) = max(0, x)\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(cfg[\"emb_dim\"]).to(cfg[\"device\"])\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"],\n",
    "            device=cfg[\"device\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg).to(cfg[\"device\"])\n",
    "        self.norm2 = nn.LayerNorm(cfg[\"emb_dim\"]).to(cfg[\"device\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"]).to(cfg[\"device\"])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # print(\"we are in one transformer block\")\n",
    "        # print(x.shape)\n",
    "        # print(x)\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        # print(\"after att\", x.shape)\n",
    "        # print(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x+ shortcut\n",
    "        # print(x.shape)\n",
    "        # print(x)\n",
    "        # print('end trs block')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain of class TransformerBlock\n",
    "The **TransformerBlock** class is the core building block of the transformer model. It consists of three main components: the **MultiHeadAttention**, the **FeedForward** layers and **LayerNorm**. The **MultiHeadAttention** layer is used to capture the relationships between tokens, while the **FeedForward** layer is used to apply non-linear transformations to the output of the attention layer. We use **LayerNorm** to normalize X and for normalize the output of the **FeedForward** layer.\n",
    "\n",
    "![image.png](./img/ExplainArchiTrans.png)\n",
    "*Source: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/9.pdf)*\n",
    "\n",
    "In this figure we can see the Residual Stream. In the forward funciton it's name shortcut. It's a way to avoid the vanishing gradient problem. The output of the **FeedForward** layer is added to the input of the **MultiHeadAttention** layer. This allows the model to learn the difference between the input and output of the block, which helps to improve performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Embeddings\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]).to(cfg[\"device\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]).to(cfg[\"device\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"]).to(cfg[\"device\"])\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.trf_blocks = nn.ModuleList(\n",
    "                [TransformerBlock(cfg) for _ in range(cfg[\"n_leayers\"])]\n",
    "            )\n",
    "        \n",
    "        # Normalization & Output layer\n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"]).to(cfg[\"device\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"out_vocab_size\"], bias=False).to(cfg[\"device\"])\n",
    "        \n",
    "        # Weight tying: Share weights between embedding and output projection\n",
    "        self.tok_emb.weight = self.out_head.weight\n",
    "        \n",
    "        # Initialize weights (If we need)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # Special initialization for transformer projection layers\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p,\n",
    "                                    mean=0.0,\n",
    "                                    std=0.02 / math.sqrt(2 * self.cfg[\"n_layers\"]))\n",
    "                \n",
    "    def forward(self, x: torch.Tensor, return_full_sequence=False):\n",
    "        b, seq_len = x.shape\n",
    "        device = x.device\n",
    "        # assert device != self.cfg['device'], \"Input tensor device does not match model device\"\n",
    "\n",
    "        # print(\"in GPT, forward \",x)\n",
    "        \n",
    "        # Token & Positional Embeddings\n",
    "        # (b, seq_len, emb_dim)\n",
    "        tok_embeds = self.tok_emb(x)\n",
    "        # print(\"tok_embeds \",tok_embeds)\n",
    "        # (seq_len, emb_dim)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=device))\n",
    "        # print(\"pos_embeds \",pos_embeds)\n",
    "        \n",
    "        x = tok_embeds + pos_embeds\n",
    "        # print(\"tok_embeds + pos_embeds \",x)\n",
    "        x = self.drop_emb(x)\n",
    "        # print(\"self.drop_emb(x) \",x)\n",
    "        \n",
    "        # Pass through Transformer Blocks\n",
    "        deleteme = 0\n",
    "        for block_ in self.trf_blocks:\n",
    "            deleteme += 1\n",
    "            x = block_(x)\n",
    "            # print(f\"block(x) n°{deleteme} : {x}\")\n",
    "        \n",
    "        # Final normalization & output projection\n",
    "        x = self.final_norm(x)\n",
    "        # print(\"self.final_norm(x) \",x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        # print(\"logits \",logits)\n",
    "        \n",
    "        if not return_full_sequence:\n",
    "            logits = logits[:, -1, :]  # (b, vocab_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_val, y_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_val, False)\n",
    "        loss = nn.functional.cross_entropy(logits, y_val)\n",
    "    return loss.item()\n",
    "\n",
    "def train_simple(model, optimizer, inputs, targets, n_iter, x_val=None, y_val=None, verbose=True):\n",
    "    all_train_loss, all_val_loss = [], []\n",
    "    for i in range(n_iter):\n",
    "        logits = model(inputs, False)\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            all_train_loss.append(loss.item())\n",
    "            if x_val is not None:\n",
    "                val_loss = evaluate_model(model, x_val, y_val)\n",
    "                all_val_loss.append(val_loss)\n",
    "                if verbose:\n",
    "                    print(f'for {i} epochs, loss is {loss.item()} and val_loss is {val_loss}')\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f'for {i} epochs, loss is {loss.item()}')\n",
    "                \n",
    "\n",
    "    return all_train_loss, all_val_loss\n",
    "\n",
    "def train_for_one_seq(model, optimizer, inputs, targets, n_iter, x_val=None, y_val=None, verbose=True):\n",
    "    \"\"\" \n",
    "    Train the model for one sequence\n",
    "    \"\"\"\n",
    "    all_train_loss, all_val_loss = [], []\n",
    "    model.train()\n",
    "    for i in range(n_iter):\n",
    "        logits = model(inputs, False)\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        for g in optimizer.param_groups:\n",
    "            tmp = g['lr']\n",
    "            break\n",
    "        if predictions != targets:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] /= 10\n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = tmp\n",
    "\n",
    "        # if i % 10 == 0:\n",
    "        all_train_loss.append(loss.item())\n",
    "        if x_val is not None:\n",
    "            val_loss = evaluate_model(model, x_val, y_val)\n",
    "            all_val_loss.append(val_loss)\n",
    "            if verbose:\n",
    "                print(f'for {i} epochs, loss is {loss.item()} and val_loss is {val_loss}')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f'for {i} epochs, loss is {loss.item()}')\n",
    "                \n",
    "    if len(all_val_loss) == 0:\n",
    "        all_val_loss = [20]\n",
    "\n",
    "    if len(all_train_loss) == 0:\n",
    "        all_train_loss = [20]\n",
    "    return all_train_loss, all_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sequence(model, context, n_tokens):\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_tokens):\n",
    "            logits = model(context, False)\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.argmax(probas, dim=-1).unsqueeze(0)\n",
    "            context = torch.cat([context, next_token], dim=-1).to(device)\n",
    "    return context\n",
    "\n",
    "CONFIG_TEST = {\n",
    "    \"vocab_size\": 2,\n",
    "    \"context_length\": 15,\n",
    "    \"emb_dim\": 16,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_leayers\": 4,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device,\n",
    "    \"out_vocab_size\": 2\n",
    "}\n",
    "\n",
    "model_test = GPTModel(CONFIG_TEST)\n",
    "generate_sequence(model_test, torch.randint(0, CONFIG_TEST[\"vocab_size\"], (1, 3)).to(CONFIG_TEST[\"device\"]), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M_small = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_leayers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "OUR_CONFIG = {\n",
    "    \"vocab_size\": VOCAB_SIZE,\n",
    "    \"context_length\": CONTEXT_LENGHT,\n",
    "    \"emb_dim\": 16,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_leayers\": 4,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_possible(n, k):\n",
    "    \"\"\"\n",
    "    Generate all possible combination of n elements taken k by k\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        yield []\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            for c in all_possible(n, k - 1):\n",
    "                yield [i] + c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(gpt, token_to = None, graph = True):\n",
    "    \"\"\"\n",
    "    Plot the transition graph of the GPT model\n",
    "    \"\"\"\n",
    "    dot = Digraph(comment='Baby GPT', engine='circo')\n",
    "\n",
    "    for xi in all_possible(gpt.cfg[\"vocab_size\"], gpt.cfg[\"context_length\"]):\n",
    "        \n",
    "        # forward the GPT and get probabilities for next token\n",
    "        x = torch.tensor(xi, dtype=torch.long)[None, ...]\n",
    "        x = x.to(gpt.cfg[\"device\"])\n",
    "        # turn the list into a torch tensor and add a batch dimension\n",
    "        logits = gpt(x, False) # forward the gpt neural net\n",
    "\n",
    "        # print('logits :', logits)\n",
    "        probs = nn.functional.softmax(logits, dim=-1) # get the probabilities\n",
    "        y = probs[0].tolist() # remove the batch dimension and unpack the tensor into simple list\n",
    "        if token_to:\n",
    "            print(f\"input {token_to(xi)} ---> {y}\")\n",
    "        else:\n",
    "            print(f\"input {xi} ---> {y}\")\n",
    "\n",
    "        if graph:\n",
    "            # also build up the transition graph for plotting later\n",
    "            current_node_signature = \"\".join(str(d) for d in xi)\n",
    "            dot.node(current_node_signature)\n",
    "            # input(\"Press Enter to continue...\")\n",
    "\n",
    "            for t in range(gpt.cfg[\"vocab_size\"]):\n",
    "                next_node = xi[1:] + [t] # crop the context and append the next character\n",
    "                next_node_signature = \"\".join(str(d) for d in next_node)\n",
    "                p = y[t]\n",
    "\n",
    "                label=f\"{t}({p*100:.0f}%)\"\n",
    "                dot.edge(current_node_signature, next_node_signature, label=label)\n",
    "    \n",
    "    return dot\n",
    "\n",
    "# gpt = GPTModel(OUR_CONFIG)\n",
    "# plot_model(gpt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_TRAIN = {\n",
    "    \"vocab_size\": 2,\n",
    "    \"context_length\": 3,\n",
    "    \"emb_dim\": 16,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_leayers\": 4,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "# inputs = torch.tensor([[0, 1, 1],\n",
    "#                        [1, 0, 0],\n",
    "#                        [1, 1, 0],\n",
    "#                        [0, 0, 1]]).to(CONFIG_TRAIN[\"device\"])\n",
    "\n",
    "# targets = torch.tensor([[1, 1, 1],\n",
    "#                         [0, 0, 1],\n",
    "#                         [1, 0, 1],\n",
    "#                         [0, 1, 0]]).to(CONFIG_TRAIN[\"device\"])\n",
    "\n",
    "\n",
    "# probas = torch.softmax(model_test(inputs), dim=-1)\n",
    "\n",
    "# print(probas)\n",
    "\n",
    "# output = torch.argmax(probas, dim=-1)\n",
    "\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# text_prob1= probas[0, targets[0]]\n",
    "# print(\"proba text1\", text_prob1)\n",
    "# print(\"proba text1 - what we obtained\", probas[0, output[0].flatten()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# let's train our baby GPT on this sequence\n",
    "seq = list(map(int, \"111101111011110\"))\n",
    "print('seq', seq)\n",
    "\n",
    "# convert the sequence to a tensor holding all the individual examples in that sequence\n",
    "X, Y = [], []\n",
    "# iterate over the sequence and grab every consecutive 3 bits\n",
    "# the correct label for what's next is the next bit at each position\n",
    "# for i in range(len(seq) - 3):\n",
    "#     X.append(seq[i:i+3])\n",
    "#     Y.append(seq[i+3])\n",
    "#     print(f\"example {i+1:2d}: {X[-1]} --> {Y[-1]}\")\n",
    "# X = torch.tensor(X, dtype=torch.long).to(CONFIG_TRAIN[\"device\"])\n",
    "# Y = torch.tensor(Y, dtype=torch.long).to(CONFIG_TRAIN[\"device\"])\n",
    "# print(X.shape, Y.shape)\n",
    "# torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "# config = GPTConfig(\n",
    "#     block_size = 3,\n",
    "#     vocab_size = 2,\n",
    "#     n_layer = 4,\n",
    "#     n_head = 4,\n",
    "#     n_embd = 16,\n",
    "#     bias = False,\n",
    "# )\n",
    "# gpt = GPT(config)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "\n",
    "# for i in range(50):\n",
    "#     logits = gpt(X)\n",
    "#     loss = nn.functional.cross_entropy(logits, Y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(i, loss.item())\n",
    "\n",
    "\n",
    "# print(\"Training data sequence, as a reminder:\", seq)\n",
    "# plot_model()\n",
    "\n",
    "\n",
    "\n",
    "CONFIG_TRAIN = {\n",
    "    \"vocab_size\": 2,\n",
    "    \"context_length\": 3,\n",
    "    \"emb_dim\": 16,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_leayers\": 4,\n",
    "    \"drop_rate\": 0.000,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device\n",
    "}\n",
    "# model = GPTModel(CONFIG_TRAIN)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "\n",
    "# for i in range(10):\n",
    "#     logits = model(X, False)\n",
    "#     print(\"logits\", logits)\n",
    "#     loss = nn.functional.cross_entropy(logits, Y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(i, loss.item())\n",
    "\n",
    "\n",
    "# print(\"Training data sequence, as a reminder:\", seq)\n",
    "# plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPTModel(CONFIG_TRAIN)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "\n",
    "# inputs = torch.tensor([[0, 1, 1, 1],\n",
    "#                        [1, 0, 0, 1],\n",
    "#                        [1, 1, 0, 0]])\n",
    "\n",
    "# targets = torch.tensor([1, 1, 1])\n",
    "\n",
    "# # train the GPT for some number of iterations\n",
    "# for i in range(3):\n",
    "\n",
    "#     logits = model(inputs, False)\n",
    "#     print(logits)\n",
    "#     loss = nn.functional.cross_entropy(logits, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment 1\n",
    "# Action 0 => 0\n",
    "# Action 1 => 1\n",
    "\n",
    "# history_action = []\n",
    "# hystory_fb = []\n",
    "# history_pred = []\n",
    "\n",
    "# env = env1()\n",
    "# model = GPTModel(\n",
    "#     {\n",
    "#         \"vocab_size\": 2,\n",
    "#         \"context_length\": 1,\n",
    "#         \"emb_dim\": 16,\n",
    "#         \"n_heads\": 4,\n",
    "#         \"n_leayers\": 4,\n",
    "#         \"drop_rate\": 0.1,\n",
    "#         \"qkv_bias\": False,\n",
    "#         \"device\": device\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# action = 0\n",
    "# inputs = torch.tensor([[action]]).t\n",
    "# targets = torch.tensor([0])\n",
    "\n",
    "# for i in range(100):\n",
    "#     action = i%2\n",
    "#     history_action.append(action)\n",
    "#     feedback = env.outcome(action)\n",
    "#     hystory_fb.append(feedback)\n",
    "\n",
    "#     inputs = torch.tensor([[action]]).to(device)\n",
    "#     targets = torch.tensor([feedback]).to(device)\n",
    "#     logits = model(inputs, False)\n",
    "#     print(logits)\n",
    "#     history_pred.append(torch.argmax(logits).item())\n",
    "#     loss = nn.functional.cross_entropy(logits, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(i, loss.item())\n",
    "    \n",
    "# print(history_action)\n",
    "# print(hystory_fb)\n",
    "# print(history_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addapt_seq(seq, context_length):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(seq) - context_length):\n",
    "        X.append(seq[i:i+context_length])\n",
    "        Y.append(seq[i+context_length])\n",
    "    return torch.tensor(X, dtype=torch.long), torch.tensor(Y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Environment 2, goal is predict outcome with action.\n",
    "# # X are all action\n",
    "# # For each X we associate Y, the outcome of the action\n",
    "# # X is context lenght action\n",
    "\n",
    "# history_action = []\n",
    "# hystory_fb = []\n",
    "# history_pred = []\n",
    "\n",
    "# env = env2Str()\n",
    "\n",
    "# action = 0\n",
    "# inputs = torch.tensor([[action]])\n",
    "# targets = torch.tensor([0])\n",
    "\n",
    "# for i in range(20):\n",
    "#     action = torch.randint(0, 2, (1,)).item()\n",
    "#     history_action.append(action)\n",
    "#     feedback = env.outcome(action)\n",
    "#     hystory_fb.append(feedback)\n",
    "\n",
    "# CONTEXT_LENGHT = 4\n",
    "# model = GPTModel(\n",
    "#     {\n",
    "#         \"vocab_size\": 2,\n",
    "#         \"context_length\": CONTEXT_LENGHT,\n",
    "#         \"emb_dim\": 16,\n",
    "#         \"n_heads\": 4,\n",
    "#         \"n_leayers\": 4,\n",
    "#         \"drop_rate\": 0.1,\n",
    "#         \"qkv_bias\": False,\n",
    "#         \"device\": device\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "\n",
    "# inputs, targets = addapt_seq(history_action, CONTEXT_LENGHT)\n",
    "# # inputs, targets = addapt_seq(list(map(int, \"111101111011110\")), CONTEXT_LENGHT)\n",
    "# inputs = inputs.to(device)\n",
    "# targets = torch.tensor(hystory_fb[CONTEXT_LENGHT-1:-1]).to(device)\n",
    "\n",
    "# print(\"inputs\")\n",
    "# print(inputs)\n",
    "# print(targets)\n",
    "\n",
    "# for j in range(100):\n",
    "#     logits = model(inputs, False)\n",
    "#     loss = nn.functional.cross_entropy(logits, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(j, loss.item())\n",
    "\n",
    "# history_pred.append(torch.argmax(logits).item())\n",
    "    \n",
    "# print(history_action)\n",
    "# print(hystory_fb)\n",
    "# print(history_pred)\n",
    "\n",
    "# plot_model(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 0]\n",
      "[(0, 1), (1, 0), (1, 1), (0, 0)]\n",
      "[3, 12, 9, 8]\n",
      "[(0, 3), (3, 0), (2, 1), (2, 0)]\n"
     ]
    }
   ],
   "source": [
    "def interaction_to_token(interaction, base=2):\n",
    "    return [int(f\"{a}{b}\", base) for a, b in interaction]\n",
    "    \n",
    "def token_to_interaction(token, base=2):\n",
    "    return [(i // base, i % base) for i in token]\n",
    "\n",
    "x = interaction_to_token([(0, 1), (1, 0), (1, 1), (0, 0)])\n",
    "print(x)\n",
    "y = token_to_interaction(x)\n",
    "print(y)\n",
    "\n",
    "x = interaction_to_token([(0, 3), (3, 0), (2, 1), (2, 0)], 4)\n",
    "print(x)\n",
    "y = token_to_interaction(x, 4)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# history = []\n",
    "# history_action = []\n",
    "# hystory_fb = []\n",
    "# history_pred = []\n",
    "# context_lenght = 2\n",
    "\n",
    "# env = env3()\n",
    "\n",
    "# action = 0\n",
    "# inputs = torch.tensor([[action]])\n",
    "# targets = torch.tensor([0])\n",
    "\n",
    "# for i in range(1000):\n",
    "#     action = torch.randint(0, 2, (1,)).item()\n",
    "#     feedback = env.outcome(action)\n",
    "\n",
    "#     history_action.append(action)\n",
    "#     hystory_fb.append(feedback)\n",
    "\n",
    "#     history.append((action, feedback))\n",
    "\n",
    "# history_token = interaction_to_token(history)\n",
    "# voc = 4\n",
    "# inputs, targets = addapt_seq(history_token, context_lenght)\n",
    "\n",
    "# inputs = inputs.to(device)\n",
    "# targets = targets.to(device)\n",
    "\n",
    "# mymodel = GPTModel(\n",
    "#     {\n",
    "#         \"vocab_size\": voc,\n",
    "#         \"context_length\": context_lenght,\n",
    "#         \"emb_dim\": 16,\n",
    "#         \"n_heads\": 4,\n",
    "#         \"n_leayers\": 4,\n",
    "#         \"drop_rate\": 0.1,\n",
    "#         \"qkv_bias\": False,\n",
    "#         \"device\" : device\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# for j in range(100):\n",
    "#     logits = mymodel(inputs, False)\n",
    "#     loss = nn.functional.cross_entropy(logits, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(j, loss.item())\n",
    "\n",
    "# plot_model(mymodel, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = mymodel(torch.tensor([[1, 1]]).to(device), False)\n",
    "# probs = nn.functional.softmax(x, dim=-1)\n",
    "# print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'000': 0, '001': 1, '002': 2, '003': 3, '004': 4, '005': 5, '006': 6, '007': 7, '008': 8, '009': 9, '0010': 10, '0011': 11, '0012': 12, '0013': 13, '0014': 14, '0015': 15, '0016': 16, '0017': 17, '0018': 18, '0019': 19, '0020': 20, '0021': 21, '0022': 22, '0023': 23, '0024': 24, '0025': 25, '0026': 26, '0027': 27, '0028': 28, '0029': 29, '0030': 30, '0031': 31, '0032': 32, '0033': 33, '0034': 34, '0035': 35, '0036': 36, '0037': 37, '0038': 38, '0039': 39, '0040': 40, '0041': 41, '0042': 42, '0043': 43, '0044': 44, '0045': 45, '0046': 46, '0047': 47, '0048': 48, '0049': 49, '0050': 50, '0051': 51, '0052': 52, '0053': 53, '0054': 54, '0055': 55, '0056': 56, '0057': 57, '0058': 58, '0059': 59, '0060': 60, '0061': 61, '0062': 62, '0063': 63, '0064': 64, '0065': 65, '0066': 66, '0067': 67, '0068': 68, '0069': 69, '0070': 70, '0071': 71, '0072': 72, '0073': 73, '0074': 74, '0075': 75, '0076': 76, '0077': 77, '0078': 78, '0079': 79, '0080': 80, '010': 81, '011': 82, '012': 83, '013': 84, '014': 85, '015': 86, '016': 87, '017': 88, '018': 89, '019': 90, '0110': 91, '0111': 92, '0112': 93, '0113': 94, '0114': 95, '0115': 96, '0116': 97, '0117': 98, '0118': 99, '0119': 100, '0120': 101, '0121': 102, '0122': 103, '0123': 104, '0124': 105, '0125': 106, '0126': 107, '0127': 108, '0128': 109, '0129': 110, '0130': 111, '0131': 112, '0132': 113, '0133': 114, '0134': 115, '0135': 116, '0136': 117, '0137': 118, '0138': 119, '0139': 120, '0140': 121, '0141': 122, '0142': 123, '0143': 124, '0144': 125, '0145': 126, '0146': 127, '0147': 128, '0148': 129, '0149': 130, '0150': 131, '0151': 132, '0152': 133, '0153': 134, '0154': 135, '0155': 136, '0156': 137, '0157': 138, '0158': 139, '0159': 140, '0160': 141, '0161': 142, '0162': 143, '0163': 144, '0164': 145, '0165': 146, '0166': 147, '0167': 148, '0168': 149, '0169': 150, '0170': 151, '0171': 152, '0172': 153, '0173': 154, '0174': 155, '0175': 156, '0176': 157, '0177': 158, '0178': 159, '0179': 160, '0180': 161, '100': 162, '101': 163, '102': 164, '103': 165, '104': 166, '105': 167, '106': 168, '107': 169, '108': 170, '109': 171, '1010': 172, '1011': 173, '1012': 174, '1013': 175, '1014': 176, '1015': 177, '1016': 178, '1017': 179, '1018': 180, '1019': 181, '1020': 182, '1021': 183, '1022': 184, '1023': 185, '1024': 186, '1025': 187, '1026': 188, '1027': 189, '1028': 190, '1029': 191, '1030': 192, '1031': 193, '1032': 194, '1033': 195, '1034': 196, '1035': 197, '1036': 198, '1037': 199, '1038': 200, '1039': 201, '1040': 202, '1041': 203, '1042': 204, '1043': 205, '1044': 206, '1045': 207, '1046': 208, '1047': 209, '1048': 210, '1049': 211, '1050': 212, '1051': 213, '1052': 214, '1053': 215, '1054': 216, '1055': 217, '1056': 218, '1057': 219, '1058': 220, '1059': 221, '1060': 222, '1061': 223, '1062': 224, '1063': 225, '1064': 226, '1065': 227, '1066': 228, '1067': 229, '1068': 230, '1069': 231, '1070': 232, '1071': 233, '1072': 234, '1073': 235, '1074': 236, '1075': 237, '1076': 238, '1077': 239, '1078': 240, '1079': 241, '1080': 242, '110': 243, '111': 244, '112': 245, '113': 246, '114': 247, '115': 248, '116': 249, '117': 250, '118': 251, '119': 252, '1110': 253, '1111': 254, '1112': 255, '1113': 256, '1114': 257, '1115': 258, '1116': 259, '1117': 260, '1118': 261, '1119': 262, '1120': 263, '1121': 264, '1122': 265, '1123': 266, '1124': 267, '1125': 268, '1126': 269, '1127': 270, '1128': 271, '1129': 272, '1130': 273, '1131': 274, '1132': 275, '1133': 276, '1134': 277, '1135': 278, '1136': 279, '1137': 280, '1138': 281, '1139': 282, '1140': 283, '1141': 284, '1142': 285, '1143': 286, '1144': 287, '1145': 288, '1146': 289, '1147': 290, '1148': 291, '1149': 292, '1150': 293, '1151': 294, '1152': 295, '1153': 296, '1154': 297, '1155': 298, '1156': 299, '1157': 300, '1158': 301, '1159': 302, '1160': 303, '1161': 304, '1162': 305, '1163': 306, '1164': 307, '1165': 308, '1166': 309, '1167': 310, '1168': 311, '1169': 312, '1170': 313, '1171': 314, '1172': 315, '1173': 316, '1174': 317, '1175': 318, '1176': 319, '1177': 320, '1178': 321, '1179': 322, '1180': 323}\n"
     ]
    }
   ],
   "source": [
    "def create_all_words(valence:dict, nb_diff:int):\n",
    "    vocab = []\n",
    "    for k in valence.keys():\n",
    "        for i in range(0, nb_diff + 1):\n",
    "            l, r = k\n",
    "            vocab.append(str(str(l) + str(r) + str(i)))\n",
    "    return vocab\n",
    "\n",
    "valence = {\n",
    "    (0, 0) : 0,\n",
    "    (0, 1) : -10,\n",
    "    (1, 0) : 10,\n",
    "    (1, 1) : 0\n",
    "}\n",
    "\n",
    "all_words = create_all_words(valence, 80)\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "def create_all_words_enumerate(valence:dict, nb_valance_possible:int):\n",
    "    all_words = create_all_words(valence, nb_valance_possible)\n",
    "    return {token:integer for integer, token in enumerate(all_words)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        if type(text) == list:\n",
    "            return [self.str_to_int[word] for word in text]\n",
    "        else:\n",
    "            return self.str_to_int[text]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        if type(ids) == list:\n",
    "            return [self.int_to_str[id] for id in ids]\n",
    "        else:\n",
    "            return self.int_to_str[ids]\n",
    "    \n",
    "# tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "# print(tokenizer.encode([\"011\", \"102\", \"113\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "list_tempo = [1, 2, 3, 4, 5, 6]\n",
    "print(list_tempo[2: 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valance_in_seq(seq:list, valence:dict):\n",
    "    \"\"\"\n",
    "    Sum all valences in a sequence\n",
    "    Exemple:\n",
    "    seq = [(1, 0), (0, 1), (0, 1)]\n",
    "    valence = {(0, 0) : 0, (0, 1) : -10, (1, 0) : 10, (1, 1) : 0}\n",
    "    get_valance_in_seq(seq, valence) => -10\n",
    "    \"\"\"\n",
    "    return sum(valence.get(tuple(pair), None) for pair in seq)\n",
    "\n",
    "def valance_seq(valence, seq, len_seq):\n",
    "    \"\"\"\n",
    "    Create a list of valences for all sequences of size len_seq in seq\n",
    "    Exemple:\n",
    "    seq = [(1, 0), (0, 1), (0, 1), (0, 1)]\n",
    "    valence = {(0, 0) : 0, (0, 1) : -10, (1, 0) : 10, (1, 1) : 0}\n",
    "    len_seq = 2\n",
    "    valance_seq(valence, seq, len_seq) => [0, -10, -10]\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    for i in range(0, len(seq) - len_seq + 1):\n",
    "        X.append(get_valance_in_seq(seq[i: i+len_seq], valence))\n",
    "    return X\n",
    "\n",
    "def inter_valance_by_nb_seq(valence:dict, seq:list, n_seq:int):\n",
    "    \"\"\"\n",
    "    Create \"word\" for each interaction. A \"word\" is interaction + valence of the next n_seq interactions\n",
    "    Exemple:\n",
    "    valence = {(0, 0) : 0, (0, 1) : -10, (1, 0) : 10, (1, 1) : 0}\n",
    "    sed = [(1, 0), (1, 0), (0, 1), (0, 1), (1, 0), (1, 0), (1, 0), (0, 1), (0, 1)]\n",
    "    n_seq = 2\n",
    "    inter_valance_by_nb_seq(valence, seq, n_seq) => ['1020', '100', '0120', '0140', '1040', '1020', '100']\n",
    "\n",
    "    This word can be tokenize\n",
    "    \"\"\"\n",
    "    seq_valance = []\n",
    "    valance_by_seq = valance_seq(valence, seq[1:], n_seq)\n",
    "    ajout  = n_seq * abs(min(valence.values()))\n",
    "    for i, val in enumerate(valance_by_seq):\n",
    "        l, r = seq[i]\n",
    "        seq_valance.append(str(l) + str(r) + str(val + ajout))\n",
    "    # seq_valance.pop()\n",
    "    return seq_valance\n",
    "\n",
    "    \n",
    "\n",
    "valence = {\n",
    "    (0, 0) : 0,\n",
    "    (0, 1) : -10,\n",
    "    (1, 0) : 10,\n",
    "    (1, 1) : 0\n",
    "}\n",
    "\n",
    "# print(\"teste :\", valance_seq(valence, [(1, 0), (1, 0), (0, 1), (0, 1), (1, 0), (1, 0), (1, 0), (0, 1), (0, 1)], 2))\n",
    "\n",
    "# inputs = inter_valance_by_nb_seq(valence, \n",
    "#                     [(1, 0), (1, 0), (0, 1), (0, 1), (1, 0), (1, 0), (1, 0), (0, 1), (0, 1)], 2)\n",
    "# print(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(valence.values())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "#     input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "#     logits = model(input_batch)\n",
    "#     return torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "\n",
    "# def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         train_loss = calc_loss_batch(train_loader, model, device)\n",
    "#         val_loss = calc_loss_batch(val_loader, model, device, num_batches=eval_iter)\n",
    "#     model.train()\n",
    "#     return train_loss, val_loss\n",
    "\n",
    "# def train_batch(model,\n",
    "#                train_loader,\n",
    "#                val_lodaer,\n",
    "#                optimizer, \n",
    "#                device,\n",
    "#                num_epochs,\n",
    "#                eval_freq = 10):\n",
    "\n",
    "#     train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "#     token_seen, global_step = 0, -1\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "\n",
    "#         for input_batch, target_batch in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             token_seen += input_batch.numel()\n",
    "#             global_step +=1 #nombre de batch vu\n",
    "\n",
    "#             if global_step % 10 == 0:\n",
    "#                 train_loss, val_loss = evaluate_model(model, train_loader, val_lodaer, device, eval_freq)            \n",
    "#                 train_losses.append(train_loss)\n",
    "#                 val_losses.append(val_loss)\n",
    "#                 track_tokens_seen.append(token_seen)\n",
    "#                 print(\"Epoch\", epoch+1, \"global step\", global_step, \"Train loss\", train_loss,\n",
    "#                       \"Val loss\", val_loss)\n",
    "\n",
    "#     return train_losses, val_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# history = []\n",
    "# history_action = []\n",
    "# hystory_fb = []\n",
    "# history_pred = []\n",
    "# context_lenght = 30\n",
    "# nb_seq_valance = 3\n",
    "\n",
    "# env = env2()\n",
    "\n",
    "# action = 0\n",
    "# inputs = torch.tensor([[action]])\n",
    "# targets = torch.tensor([0])\n",
    "\n",
    "# valence = {\n",
    "#     (0, 0) : 0,\n",
    "#     (0, 1) : -1,\n",
    "#     (1, 0) : 1,\n",
    "#     (1, 1) : 0\n",
    "# }\n",
    "\n",
    "\n",
    "# for i in range(100):\n",
    "#     action = torch.randint(0, 2, (1,)).item()\n",
    "#     feedback = env.outcome(action)\n",
    "#     history_action.append(action)\n",
    "#     hystory_fb.append(feedback)\n",
    "#     history.append((action, feedback))\n",
    "\n",
    "\n",
    "# nb_valence_possible = nb_seq_valance * (abs(min(valence.values())) + abs(max(valence.values())))\n",
    "# print('nb_valence_possible')\n",
    "# print(nb_valence_possible)\n",
    "                                                                         \n",
    "\n",
    "# all_world_enum = create_all_words_enumerate(valence=valence,\n",
    "#             nb_valance_possible=nb_valence_possible)\n",
    "\n",
    "# tokenizer = SimpleTokenizerV1(all_world_enum)\n",
    "# voc = len(all_world_enum)\n",
    "# print('voc size')\n",
    "# print(voc)\n",
    "\n",
    "# inputs = inter_valance_by_nb_seq(valence, history, nb_seq_valance)\n",
    "# inputs = tokenizer.encode(inputs)\n",
    "\n",
    "# inputs, targets = addapt_seq(inputs, context_lenght)\n",
    "# inputs = inputs.to(device)\n",
    "# targets = targets.to(device)\n",
    "\n",
    "# print('inputs done :')\n",
    "# print(inputs)\n",
    "# print(\"target done :\")\n",
    "# print(targets)\n",
    "\n",
    "\n",
    "# mymodel = GPTModel({\n",
    "#         \"vocab_size\": voc,\n",
    "#         \"context_length\": context_lenght,\n",
    "#         \"emb_dim\": 320,\n",
    "#         \"n_heads\": 4,\n",
    "#         \"n_leayers\": 4,\n",
    "#         \"drop_rate\": 0.1,\n",
    "#         \"qkv_bias\": False,\n",
    "#         \"device\": device\n",
    "#     })\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.AdamW(mymodel.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "# mymodel.train()\n",
    "# # train_simple(mymodel, optimizer, inputs, targets, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_proba(probs, tokenizer = None):\n",
    "    plt.bar(range(len(probs)), probs)\n",
    "    plt.text(probs.index(max(probs)), max(probs), str(max(probs)), ha='center')\n",
    "    if tokenizer:\n",
    "        plt.text(probs.index(max(probs)), max(probs) + 0.5, tokenizer.decode(max(probs)), ha='center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq = ['014']\n",
    "# mymodel.eval()\n",
    "# x = mymodel(torch.tensor([tokenizer.encode(seq)]).to(device), False)\n",
    "# probs = nn.functional.softmax(x, dim=-1)\n",
    "# predi = torch.argmax(probs)\n",
    "\n",
    "# see_proba(probs[0].tolist(), None)\n",
    "# print(tokenizer.decode([predi.item()]))\n",
    "# print(f'for seq {str(seq)} the next token is {tokenizer.decode([predi.item()])}')\n",
    "\n",
    "# seq_gen = generate_sequence(mymodel, torch.tensor([tokenizer.encode(seq)]).to(device), context_lenght)\n",
    "\n",
    "# print(tokenizer.decode(seq_gen[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input = ActionOutcomeAction\n",
    "# Output = ActionOutcomeAction**Outcome**\n",
    "# Word is Action and Outcome\n",
    "\n",
    "# All words\n",
    "def create_all_words_action_outcome(action:list, feedback:list):\n",
    "    return action + feedback\n",
    "\n",
    "def create_all_words_action_outcome_enumerate(action:list, feedback:list):\n",
    "    all_words = create_all_words_action_outcome(action, feedback)\n",
    "    return {word:integer for integer, word in enumerate(all_words)}\n",
    "\n",
    "# create_all_words_action_outcome([0, 1, 2, 3], ['A', 'B'])\n",
    "# create_all_words_action_outcome_enumerate([0, 1, 2, 3], ['A', 'B'])\n",
    "# tokenizer_act_ff = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(['0', '1', '2', '3'], ['A', 'B']))\n",
    "# tempo = tokenizer_act_ff.encode(['0', 'A', '1', 'B'])\n",
    "# print(tempo)\n",
    "# print(tokenizer_act_ff.decode(tempo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3, 2, 3], ['A', 'B']]\n"
     ]
    }
   ],
   "source": [
    "My_list = [[0, 1, 2, 3], ['A', 'B']]\n",
    "My_list[0]\n",
    "\n",
    "My_list[0] = [0, 3, 2, 3]\n",
    "print(My_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# [(0, 'x'), (1, 'x'), (1, 'x'), (1, 'x'), (1, 'x'), (1, 'x'), (1, 'x'), (1, 'x')]\n",
    "# [[0], [0, x, 1], []\n",
    "def inter_action_and_feedback_size(history:list, size:int):\n",
    "    \"\"\"\n",
    "    Transform history into input and target.\n",
    "    history is a sequence of action and feedback.\n",
    "    We want to have all sequence of size size, associate with the feedback of the last action (targets).\n",
    "    Exemple:\n",
    "    history = [('0', 'x'), ('1', 'y'), ('0', 'x'), ('1', 'y'), ('0', 'x'), ('1', 'y'), ('0', 'x')]\n",
    "    size = 5\n",
    "    inter_action_and_feedback_size(history, size) => \n",
    "    inputs = [['0', 'x', '1', 'y', '0'], \n",
    "    ['1', 'y', '0', 'x', '1'],\n",
    "    ['0', 'x', '1', 'y', '0'],\n",
    "    ['1', 'y', '0', 'x', '1'],\n",
    "    ['0', 'x', '1', 'y', '0']]\n",
    "\n",
    "    targets = ['x', 'y', 'x', 'y', 'x']\n",
    "\n",
    "    \"\"\"\n",
    "    inputs, targets = [], []\n",
    "    for act, ff in history:\n",
    "        if inputs:\n",
    "            temp = inputs[-1][-int(size - 2):] + [targets[-1], act]\n",
    "            inputs.append(temp)\n",
    "        else:\n",
    "            inputs.append([act])\n",
    "        targets.append(ff)\n",
    "    return inputs[size - 1:], targets[size- 1:]\n",
    "\n",
    "inputs, targets = inter_action_and_feedback_size([('0', 'x'), ('1', 'y'), ('0', 'x')], 5)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# print(create_all_words_action_outcome_enumerate(['0', '1'], ['x', 'y']))\n",
    "# tokenizer = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(['0', '1'], ['x', 'y']))\n",
    "# print(targets)\n",
    "# print(tokenizer.encode(targets))\n",
    "\n",
    "# print(inputs)\n",
    "# for i, one_input in enumerate(inputs):\n",
    "#     inputs[i] = tokenizer.encode(inputs[i])\n",
    "\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ax': 0, 'ay': 1, 'bx': 2, 'by': 3}\n"
     ]
    }
   ],
   "source": [
    "def create_all_words_by_env(env):\n",
    "    vocab = []\n",
    "    action = env.get_actions()\n",
    "    feedback = env.get_outcomes()\n",
    "    for act in action:\n",
    "        for ff in feedback:\n",
    "            vocab.append(str(act) + str(ff))\n",
    "    return vocab\n",
    "\n",
    "def create_all_words_by_env_enumerate(env):\n",
    "    all_words = create_all_words_by_env(env)\n",
    "    return {word:integer for integer, word in enumerate(all_words)}\n",
    "\n",
    "env = env2Str()\n",
    "tokenizer = create_all_words_by_env_enumerate(env)\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 0 epochs, loss is 1.4193642139434814 and val_loss is 1.2963099479675293\n",
      "for 10 epochs, loss is 0.7694078683853149 and val_loss is 0.7486584782600403\n",
      "for 20 epochs, loss is 0.7001091241836548 and val_loss is 0.6969918012619019\n",
      "for 30 epochs, loss is 0.6986018419265747 and val_loss is 0.6972209215164185\n",
      "for 40 epochs, loss is 0.6944737434387207 and val_loss is 0.6913231015205383\n",
      "for 50 epochs, loss is 0.6927670240402222 and val_loss is 0.6907631158828735\n",
      "for 60 epochs, loss is 0.6917691230773926 and val_loss is 0.6895061135292053\n",
      "for 70 epochs, loss is 0.6910179257392883 and val_loss is 0.6888019442558289\n",
      "for 80 epochs, loss is 0.6903334856033325 and val_loss is 0.6884320974349976\n",
      "for 90 epochs, loss is 0.6895778775215149 and val_loss is 0.6877660751342773\n",
      "for 100 epochs, loss is 0.688504159450531 and val_loss is 0.6869984269142151\n",
      "for 110 epochs, loss is 0.685599148273468 and val_loss is 0.6850634217262268\n",
      "for 120 epochs, loss is 0.6914135217666626 and val_loss is 0.6903521418571472\n",
      "for 130 epochs, loss is 0.6877455115318298 and val_loss is 0.6858111023902893\n",
      "for 140 epochs, loss is 0.6848558187484741 and val_loss is 0.6745480298995972\n",
      "for 150 epochs, loss is 0.6841987371444702 and val_loss is 0.6768968105316162\n",
      "for 160 epochs, loss is 0.6811395883560181 and val_loss is 0.6684461236000061\n",
      "for 170 epochs, loss is 0.6877589821815491 and val_loss is 0.6881780624389648\n",
      "for 180 epochs, loss is 0.6906845569610596 and val_loss is 0.687319815158844\n",
      "for 190 epochs, loss is 0.690029501914978 and val_loss is 0.6874346137046814\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = []\n",
    "historyTest = []\n",
    "context_lenght = 21\n",
    "nb_seq_valance = 3\n",
    "\n",
    "env = env3Str()\n",
    "\n",
    "action = 0\n",
    "inputs = torch.tensor([[action]])\n",
    "targets = torch.tensor([0])\n",
    "\n",
    "valence = {\n",
    "    ('a', 'x') : 0,\n",
    "    ('a', 'y') : -1,\n",
    "    ('b', 'x') : 1,\n",
    "    ('b', 'y') : 0\n",
    "}\n",
    "\n",
    "# train\n",
    "for i in range(100):\n",
    "    action = np.random.choice(['a', 'b'])\n",
    "    feedback = env.outcome(action)\n",
    "    history.append((str(action), str(feedback)))\n",
    "\n",
    "# test\n",
    "for i in range(100):\n",
    "    action = np.random.choice(['a', 'b'])\n",
    "    feedback = env.outcome(action)\n",
    "    historyTest.append((str(action), str(feedback)))\n",
    "\n",
    "print(history)\n",
    "tmpInput, tmpTarget = inter_action_and_feedback_size(history, context_lenght)\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(['a', 'b', 'x', 'y'], []))\n",
    "\n",
    "inputs = []\n",
    "for i, one_input in enumerate(tmpInput):\n",
    "    inputs.append(tokenizer.encode(one_input))\n",
    "targets = tokenizer.encode(tmpTarget)\n",
    "\n",
    "inputs= torch.tensor(inputs, dtype=torch.long).to(device)\n",
    "targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "tmpXtest, tmpYtest = inter_action_and_feedback_size(historyTest, context_lenght)\n",
    "\n",
    "x_test = []\n",
    "for i, one_input in enumerate(tmpXtest):\n",
    "    x_test.append(tokenizer.encode(one_input))\n",
    "y_test = tokenizer.encode(tmpYtest)\n",
    "\n",
    "x_test = torch.tensor(x_test, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "mymodel = GPTModel({\n",
    "        \"vocab_size\": len(['a', 'b', 'x', 'y']),\n",
    "        \"context_length\": context_lenght,\n",
    "        \"emb_dim\": 16,\n",
    "        \"n_heads\": 4,\n",
    "        \"n_leayers\": 4,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": device,\n",
    "        \"out_vocab_size\": len(['a', 'b', 'x', 'y'])\n",
    "    })\n",
    "\n",
    "optimizer = torch.optim.AdamW(mymodel.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "\n",
    "train_loss, val_loss = train_simple(mymodel, optimizer, inputs, targets, 200, x_test, y_test)\n",
    "\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for seq tensor([[1, 2, 1, 2, 1, 2, 0, 3, 0, 2, 0, 2, 1, 3, 0, 2, 0, 2, 1, 2, 0]]) the next token is ['x']\n"
     ]
    }
   ],
   "source": [
    "# test model \n",
    "\n",
    "seq =torch.tensor([[1, 2, 1, 2, 1, 2, 0, 3, 0, 2, 0, 2, 1, 3, 0, 2, 0, 2, 1, 2, 0]]).to(device)\n",
    "mymodel.eval()\n",
    "x = mymodel(seq, False)\n",
    "probs = nn.functional.softmax(x, dim=-1)\n",
    "predi = torch.argmax(probs)\n",
    "see_proba(probs[0].tolist(), None)\n",
    "print(f'for seq {str(seq)} the next token is {tokenizer.decode([predi.item()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, x, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, False)\n",
    "        # loss = nn.functional.cross_entropy(logits, y)\n",
    "        pred = torch.argmax(logits, dim=-1)\n",
    "        acc = (pred == y).float().mean()\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('a', 'y')]\n",
      "for 0 epochs, loss is 1.4575928449630737 and val_loss is 1.1933810710906982\n",
      "for 10 epochs, loss is 0.7026928663253784 and val_loss is 0.7077137231826782\n",
      "for 20 epochs, loss is 0.6923390030860901 and val_loss is 0.7031181454658508\n",
      "for 30 epochs, loss is 0.6872144341468811 and val_loss is 0.6979638338088989\n",
      "for 40 epochs, loss is 0.6800611019134521 and val_loss is 0.7193068861961365\n",
      "for 50 epochs, loss is 0.6919413208961487 and val_loss is 0.7030155658721924\n",
      "for 60 epochs, loss is 0.6228407621383667 and val_loss is 0.8649433255195618\n",
      "for 70 epochs, loss is 0.6883114576339722 and val_loss is 0.6940068602561951\n",
      "for 80 epochs, loss is 0.6857410669326782 and val_loss is 0.6898558735847473\n",
      "for 90 epochs, loss is 0.664732813835144 and val_loss is 0.6719443798065186\n",
      "0.8816326260566711\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = []\n",
    "historyTest = []\n",
    "context_lenght = 21\n",
    "nb_seq_valance = 3\n",
    "\n",
    "env = env3Str()\n",
    "\n",
    "action = 0\n",
    "inputs = torch.tensor([[action]])\n",
    "targets = torch.tensor([0])\n",
    "\n",
    "valence = {\n",
    "    ('a', 'x') : 0,\n",
    "    ('a', 'y') : -1,\n",
    "    ('b', 'x') : 1,\n",
    "    ('b', 'y') : 0\n",
    "}\n",
    "\n",
    "# test\n",
    "for i in range(1000):\n",
    "    action = np.random.choice(['a', 'b'])\n",
    "    feedback = env.outcome(action)\n",
    "    historyTest.append((str(action), str(feedback)))\n",
    "\n",
    "# TODO lunch test to see evolued accuracy and loss in function of the number of training\n",
    "# train\n",
    "for i in range(100):\n",
    "    action = np.random.choice(['a', 'b'])\n",
    "    feedback = env.outcome(action)\n",
    "    history.append((str(action), str(feedback)))\n",
    "\n",
    "\n",
    "print(history)\n",
    "tmpInput, tmpTarget = inter_action_and_feedback_size(history, context_lenght)\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(['a', 'b', 'x', 'y'], []))\n",
    "\n",
    "inputs = []\n",
    "for i, one_input in enumerate(tmpInput):\n",
    "    inputs.append(tokenizer.encode(one_input))\n",
    "targets = tokenizer.encode(tmpTarget)\n",
    "\n",
    "inputs= torch.tensor(inputs, dtype=torch.long).to(device)\n",
    "targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "tmpXtest, tmpYtest = inter_action_and_feedback_size(historyTest, context_lenght)\n",
    "\n",
    "x_test = []\n",
    "for i, one_input in enumerate(tmpXtest):\n",
    "    x_test.append(tokenizer.encode(one_input))\n",
    "y_test = tokenizer.encode(tmpYtest)\n",
    "\n",
    "x_test = torch.tensor(x_test, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "mymodel = GPTModel({\n",
    "        \"vocab_size\": len(['a', 'b', 'x', 'y']),\n",
    "        \"context_length\": context_lenght,\n",
    "        \"emb_dim\": 16 * 2,\n",
    "        \"n_heads\": 4,\n",
    "        \"n_leayers\": 4,\n",
    "        \"drop_rate\": 0.2,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": device,\n",
    "        \"out_vocab_size\": len(['a', 'b', 'x', 'y'])\n",
    "    })\n",
    "\n",
    "optimizer = torch.optim.AdamW(mymodel.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "\n",
    "train_loss, val_loss = train_simple(mymodel, optimizer, inputs, targets, 100, x_test, y_test)\n",
    "\n",
    "print(accuracy(mymodel, x_test, y_test))\n",
    "\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot_loss(train_loss, val_loss, path:str=\"img_loss\", title:str=\"\"):\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    # if title == \"\" title = 'loss' + nb img save\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    if title == \"\":\n",
    "        title = 'loss' + str(len(os.listdir(path)))\n",
    "    plt.savefig(path + '/' + title + '.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]]) the next token is ['x']\n"
     ]
    }
   ],
   "source": [
    "# test model \n",
    "# seq =torch.tensor([tokenizer.encode(['b', 'x', 'a', 'x', 'b', 'x', 'b', 'y', 'a', 'x', 'b', 'x', 'a', 'x', 'b', 'x', 'b', 'x', 'a', 'y', 'a'])]).to(device)\n",
    "# Context = 21\n",
    "seq =torch.tensor([tokenizer.encode(['b', 'x', 'a' , 'y', 'a', 'y', 'a', 'x', 'a', 'x', 'a', 'x', 'a', 'x', 'a', 'x', 'a', 'x', 'b', 'x', 'b'])]).to(device)\n",
    "#=============================================================^noise=================================================================^noise\n",
    "# Context = 5\n",
    "# seq =torch.tensor([tokenizer.encode(['b', 'x', 'a' , 'y', 'b'])]).to(device)\n",
    "\n",
    "# Second noise have an impact on the prediction\n",
    "# Not the first one\n",
    "mymodel.eval()\n",
    "x = mymodel(seq, False)\n",
    "probs = nn.functional.softmax(x, dim=-1)\n",
    "predi = torch.argmax(probs)\n",
    "see_proba(probs[0].tolist(), None)\n",
    "print(f'for seq {str(seq)} the next token is {tokenizer.decode([predi.item()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_evolued_train_loss(train_loss):\n",
    "    for i, loss_list in enumerate(train_loss):\n",
    "        plt.plot(loss_list, label=f'Iteration {i}', color=plt.cm.viridis(i / len(train_loss)))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b']\n"
     ]
    }
   ],
   "source": [
    "# def make_tree_prediction(model, env, max_deep:int, last_sequence:list, seq_predi:list=[]):\n",
    "#     if max_deep == 0:\n",
    "#         return []\n",
    "#     # input(f'{last_sequence} | {seq_predi}')\n",
    "#     max_deep -= 1\n",
    "    \n",
    "#     fake_tree = []\n",
    "#     model.eval()\n",
    "#     for act in env.get_actions():\n",
    "#         seq_to_predict = tokenizer.encode(last_sequence + [act])\n",
    "#         seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.long).to(device)\n",
    "#         # input(f'seq to pass in model {seq_to_predict}')\n",
    "#         x = model(seq_to_predict, False)\n",
    "#         probs = nn.functional.softmax(x, dim=-1)\n",
    "#         predi = tokenizer.decode([torch.argmax(probs).item()])\n",
    "#         new__last_seq = last_sequence[2:] + [act, predi]\n",
    "#         new_seq_predi = seq_predi + [act, predi]\n",
    "#         # input(f'new last seq {new__last_seq} | new seq pred {new_seq_predi}')\n",
    "#         fake_tree.append(new_seq_predi) \n",
    "#         fake_tree += (make_tree_prediction(model, env, max_deep, new__last_seq, new_seq_predi))\n",
    "\n",
    "#     return fake_tree\n",
    "\n",
    "\n",
    "def make_tree_prediction(model, env, valence:dict, max_deep: int, last_sequence: list):\n",
    "    model.eval()\n",
    "    stack = [(last_sequence, [], 1)]\n",
    "    fake_tree = {}\n",
    "\n",
    "    while stack:\n",
    "        last_sequence, seq_predi, value = stack.pop()\n",
    "        # input(f'start wihle {stack} | \\n last seq {last_sequence} \\n seq predi {seq_predi}')\n",
    "        \n",
    "        if len(seq_predi) // 2 >= max_deep:\n",
    "            continue\n",
    "\n",
    "        for act in env.get_actions():\n",
    "            seq_to_predict = tokenizer.encode(last_sequence + [act])\n",
    "            seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.long).to(device)\n",
    "            \n",
    "            x = model(seq_to_predict, False)\n",
    "            probs = nn.functional.softmax(x, dim=-1)\n",
    "            predi = tokenizer.decode([torch.argmax(probs).item()])\n",
    "            best_proba = probs[0][torch.argmax(probs)].item()\n",
    "            # print(f'best proba {best_proba}')\n",
    "            \n",
    "            new_last_seq = last_sequence[2:] + [act, predi]\n",
    "            new_seq_predi = seq_predi + [act, predi]\n",
    "            new_value = value + best_proba * valence[(act, predi)]\n",
    "            fake_tree[str(new_seq_predi)] = new_value\n",
    "            stack.append((new_last_seq, new_seq_predi, new_value))\n",
    "\n",
    "    return fake_tree\n",
    "\n",
    "\n",
    "def model_in_env(model, tokenizer:SimpleTokenizerV1, env, valance:dict, iter:int, rand_iter:int = 10, path_save = \"loss_\", _lr=1e-2, weight_decay=1e-2, max_depth=3):\n",
    "    history = []\n",
    "    context_lenght = model.cfg[\"context_length\"]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=_lr, weight_decay=weight_decay)\n",
    "    evolued_train_loss = []\n",
    "    evolued_val_loss = []\n",
    "    all_predit = None\n",
    "    good_predicts:list[bool] = []\n",
    "    historyTest = []\n",
    "\n",
    "    for i in range(1000):\n",
    "        action = np.random.choice(env.get_actions())\n",
    "        feedback = env.outcome(action)\n",
    "        historyTest.append((str(action), str(feedback)))\n",
    "\n",
    "    tmpXtest, tmpYtest = inter_action_and_feedback_size(historyTest, context_lenght)\n",
    "\n",
    "    x_test = []\n",
    "    for i, one_input in enumerate(tmpXtest):\n",
    "        x_test.append(tokenizer.encode(one_input))\n",
    "    y_test = tokenizer.encode(tmpYtest)\n",
    "\n",
    "    x_val = torch.tensor(x_test, dtype=torch.long).to(device)\n",
    "    y_val = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(0, iter):\n",
    "        if i < rand_iter:\n",
    "            action = np.random.choice(env.get_actions())\n",
    "        else:\n",
    "            # train model\n",
    "            if i % 10 == 0:\n",
    "                model.apply(model._init_weights)\n",
    "                tmpInput, tmpTarget = inter_action_and_feedback_size(history, context_lenght)\n",
    "                inputs = []\n",
    "                for one_input in tmpInput:\n",
    "                    inputs.append(tokenizer.encode(one_input))\n",
    "                targets = tokenizer.encode(tmpTarget)\n",
    "\n",
    "                inputs= torch.tensor(inputs, dtype=torch.long).to(device)\n",
    "                targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "                time_start = time.time()\n",
    "                train_loss, val_loss = train_simple(model, optimizer, inputs, targets, 1000, verbose=False, x_val=x_val, y_val=y_val)\n",
    "                save_plot_loss(train_loss, val_loss, path=path_save, title=f'loss_{i}')\n",
    "                time_end = time.time()\n",
    "                print(f'time to train {time_end - time_start}')\n",
    "                evolued_train_loss.append(train_loss)\n",
    "                evolued_val_loss.append(val_loss)\n",
    "\n",
    "            # predict next action\n",
    "            tmp = tmpInput[-1][2:] + [tmpTarget[-1]]\n",
    "            time_start = time.time()\n",
    "            all_predit:dict = make_tree_prediction(\n",
    "                    model=model, env=env, max_deep=max_depth, valence=valance,\n",
    "                    last_sequence= tmp)\n",
    "            print(f'for this sequence {tmp} all prediction {all_predit}')\n",
    "            interact_max_val = max(all_predit.items())[0]\n",
    "            interact_max_val = eval(interact_max_val)\n",
    "            # input(f'max is {interact_max_val}')\n",
    "            action = interact_max_val[0]\n",
    "            predict = interact_max_val[1]\n",
    "            # input(f'action {action}')\n",
    "            # input(f'predict {predict}')\n",
    "            # input(f'for this sequence {tmp} all prediction {test}')\n",
    "            time_end = time.time()\n",
    "            print(f'time to make tree {time_end - time_start}')\n",
    "            # print(f'for this sequence {tmp}')\n",
    "            # print(f'all prediction {test}')\n",
    "            # max_potentiel = -np.inf\n",
    "            # for act in env.get_actions():\n",
    "            #     seq_to_predict = tokenizer.encode(tmpInput[-1][2:] + [tmpTarget[-1], act])\n",
    "            #     seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.long).to(device)\n",
    "            #     x = model(seq_to_predict, False)\n",
    "            #     probs = nn.functional.softmax(x, dim=-1)\n",
    "            #     predi = torch.argmax(probs)\n",
    "            #     val_predi = valance[(act, tokenizer.decode([predi.item()]))]\n",
    "            #     if val_predi > max_potentiel:\n",
    "            #         max_potentiel = val_predi\n",
    "            #         action = act\n",
    "            #     elif val_predi == max_potentiel:\n",
    "            #         action = np.random.choice([action, act])\n",
    "\n",
    "            # action = np.random.choice(env.get_actions()) # TODO delete this\n",
    "            \n",
    "\n",
    "        feedback = env.outcome(action)\n",
    "        if all_predit is not None:\n",
    "            list_keys = all_predit.keys()\n",
    "            # list_keys = [eval(key) for key in list_keys]\n",
    "            # input(f'I try to find if {str([str(action), feedback])} in {list_keys}')\n",
    "            good_predicts.append(str([str(action), feedback]) in list_keys)\n",
    "            print(f'% good predict : {sum(good_predicts[-10:]) * 10 if len(good_predicts) >= 10 else 0}')\n",
    "        print(f'iteration {i} action {action} feedback {feedback} predict {all_predit.keys() if all_predit else \"None\"}')\n",
    "        history.append((str(action), str(feedback)))\n",
    "\n",
    "    return evolued_train_loss, evolued_val_loss, good_predicts\n",
    "\n",
    "\n",
    "mymodel = GPTModel({\n",
    "        \"vocab_size\": len(['a', 'b', 'x', 'y']),\n",
    "        \"context_length\": 21,\n",
    "        \"emb_dim\": 16 *2,\n",
    "        \"n_heads\": 1, # 4\n",
    "        \"n_leayers\": 1, # 4\n",
    "        \"drop_rate\": 0.2,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": device,\n",
    "        \"out_vocab_size\": len(['a', 'b', 'x', 'y'])\n",
    "    })\n",
    "\n",
    "\n",
    "valence = {\n",
    "    ('a', 'x') : -10,\n",
    "    ('a', 'y') : 10,\n",
    "    ('b', 'x') : -10,\n",
    "    ('b', 'y') : 10\n",
    "}\n",
    "\n",
    "env = env6Str()\n",
    "\n",
    "print(env.get_actions())\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(['a', 'b', 'x', 'y'], []))\n",
    "\n",
    "# evolued_train_loss, evolued_val_loss = model_in_env(mymodel, tokenizer, env, valence, 500, 100)\n",
    "\n",
    "# see_evolued_train_loss(evolued_train_loss)\n",
    "# see_evolued_train_loss(evolued_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicRt = {\n",
    "    str(['a',' b']): 1111\n",
    "}\n",
    "\n",
    "if str(['a',' c']) in dicRt:\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see_evolued_train_loss(evolued_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forward', 'turn_left', 'turn_right', 'feel_front']\n"
     ]
    }
   ],
   "source": [
    "env = small_loop(1, 1, 0)\n",
    "\n",
    "mymodel = GPTModel({\n",
    "        \"vocab_size\": len(env.get_actions()) + len(env.get_outcomes()),\n",
    "        \"context_length\": 51,\n",
    "        \"emb_dim\": 16 * 4,\n",
    "        \"n_heads\": 1, # 4\n",
    "        \"n_leayers\": 1, # 4\n",
    "        \"drop_rate\": 0.2,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": device,\n",
    "        \"out_vocab_size\": len(env.get_actions()) + len(env.get_outcomes())\n",
    "    })\n",
    "\n",
    "\n",
    "valence = {\n",
    "    ('forward', 'wall') : -10,\n",
    "    ('forward', 'empty') : 10,\n",
    "    ('turn_left', 'wall') : -1000000, # Can not produce\n",
    "    ('turn_left', 'empty') : -3,\n",
    "    ('turn_right', 'wall') : -1000000, # Can not produce\n",
    "    ('turn_right', 'empty') : -3,\n",
    "    ('feel_front', 'wall') : -3,\n",
    "    ('feel_front', 'empty') : -2,\n",
    "    ('feel_left', 'wall') : -3,\n",
    "    ('feel_left', 'empty') : -2,\n",
    "    ('feel_right', 'wall') : -3,\n",
    "    ('feel_right', 'empty') : -2\n",
    "\n",
    "}\n",
    "\n",
    "print(env.get_actions())\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(env.get_actions(), env.get_outcomes()))\n",
    "\n",
    "# evolued_train_loss, evolued_val_loss, _ = model_in_env(mymodel, tokenizer, env, valence, 500, 100, \"loss_SL_samll_embeding\", _lr=1e-3, weight_decay=1e-2, max_depth=4)\n",
    "\n",
    "# see_evolued_train_loss(evolued_train_loss)\n",
    "# see_evolued_train_loss(evolued_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage superviser\n",
    "## Decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "context_lenght = None\n",
    "\n",
    "def make_data_set(tokenizer, env, context_lenght:int, rand_iter:int = 100):\n",
    "    history = []\n",
    "\n",
    "    # Create data val\n",
    "    historyTest = []\n",
    "    for i in range(1000):\n",
    "        action = np.random.choice(env.get_actions())\n",
    "        feedback = env.outcome(action)\n",
    "        historyTest.append((str(action), str(feedback)))\n",
    "\n",
    "    tmpXtest, tmpYtest = inter_action_and_feedback_size(historyTest, context_lenght)\n",
    "    x_test = []\n",
    "    for i, one_input in enumerate(tmpXtest):\n",
    "        x_test.append(tokenizer.encode(one_input))\n",
    "    y_test = tokenizer.encode(tmpYtest)\n",
    "\n",
    "    # Create first rand sequence\n",
    "    for i in range(rand_iter):\n",
    "        action = np.random.choice(env.get_actions())\n",
    "        feedback = env.outcome(action)\n",
    "        history.append((str(action), str(feedback)))\n",
    "\n",
    "    tmpXfit, tmpYfit = inter_action_and_feedback_size(history, context_lenght)\n",
    "    x_fit = []\n",
    "    for i, one_input in enumerate(tmpXfit):\n",
    "        x_fit.append(tokenizer.encode(one_input))\n",
    "    y_fit = tokenizer.encode(tmpYfit)\n",
    "\n",
    "    return x_fit, y_fit, x_test, y_test\n",
    "\n",
    "\n",
    "# env = small_loop(1, 1, 0)\n",
    "env = env6Str()\n",
    "tokenizer = SimpleTokenizerV1(\n",
    "    create_all_words_action_outcome_enumerate(\n",
    "        env.get_actions(), env.get_outcomes()))\n",
    "\n",
    "valence = {\n",
    "    ('forward', 'wall') : -10,\n",
    "    ('forward', 'empty') : 10,\n",
    "    ('turn_left', 'wall') : -1000000, # Can not produce\n",
    "    ('turn_left', 'empty') : -3,\n",
    "    ('turn_right', 'wall') : -1000000, # Can not produce\n",
    "    ('turn_right', 'empty') : -3,\n",
    "    ('feel_front', 'wall') : -3,\n",
    "    ('feel_front', 'empty') : -2,\n",
    "    ('feel_left', 'wall') : -3,\n",
    "    ('feel_left', 'empty') : -2,\n",
    "    ('feel_right', 'wall') : -3,\n",
    "    ('feel_right', 'empty') : -2\n",
    "}\n",
    "\n",
    "# x_fit, y_fit, x_val, y_val = make_data_set(env=env,\n",
    "#               rand_iter=200,\n",
    "#               tokenizer=tokenizer,\n",
    "#               )\n",
    "\n",
    "# # analyse data\n",
    "# # Correlation Matrix\n",
    "# df = pd.DataFrame(x_fit)\n",
    "# df['target'] = y_fit\n",
    "# corr = df.corr()\n",
    "# print(corr)\n",
    "\n",
    "# # Count y_fit\n",
    "# print(\"Count y_fit \")\n",
    "# print(pd.Series(y_fit).value_counts())\n",
    "\n",
    "\n",
    "# # Test with decision tree\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf = clf.fit(x_fit, y_fit)\n",
    "# y_pred = clf.predict(x_val)\n",
    "\n",
    "# print(f'Tree : accuracy {sk.metrics.accuracy_score(y_val, y_pred)}')\n",
    "# # Confusion matrix\n",
    "# print(sk.metrics.confusion_matrix(y_val, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe utilise plusieurs couche cachées\n",
    "    Pour définir un réseau de neurones profonds\n",
    "    \n",
    "    - Il y a une couche d'entrée\n",
    "    - Plusieurs couches cachées dépendant de la liste hidden_size\n",
    "    - Une couche de sortie\n",
    "\n",
    "    les poids sont initialise de manière aléatoire\n",
    "    \"\"\"\n",
    "    \n",
    "    name = \"DeepNetwork\"\n",
    "    \n",
    "    def __init__(self, input_size:int, hidden_size:list[int], output_size:int):\n",
    "        \"\"\"\n",
    "        Constructeur de la classe, applique le constructeur de la classe parent\n",
    "        Et crée les couches du réseau :\n",
    "        - 1 couche d'entrée\n",
    "        - Plusieurs couches cachées\n",
    "        - 1 couche de sortie\n",
    "        \n",
    "        Les couches cachées sont définies mis en place avec nn.ModuleList, pour permettre\n",
    "        l'ajout dynamique de couches cachées. (Concretement nous pouvons passer le \n",
    "        modele.to(deviece) et la sauvgarde des poids par torch)\n",
    "\n",
    "        :param input_size: la taille des données d'entrée\n",
    "        :param output_size: la taille des données de sortie\n",
    "        :param hidden_size: la taille des couches cachées, \n",
    "        le nombre d'éléments dans la liste correspond au nombre de couches cachées       \n",
    "        \"\"\"\n",
    "        super(DeepNetwork, self).__init__()\n",
    "        # nn.Linear initialise les poids de manière aléatoire\n",
    "        print(\"liste hidden init\",hidden_size)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for size in range(len(hidden_size) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size[size], hidden_size[size + 1]))\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = nn.functional.relu(layer(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, test_loader: torch.utils.data.DataLoader, loss_funct) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Méthode d'évaluation du modèle\n",
    "\n",
    "    :param model: le modèle à évaluer\n",
    "    :param test_loader: le lecteur de données de test\n",
    "    :param loss_funct: la fonction de perte à utiliser pour l'évaluation\n",
    "    :param device: le device sur lequel effectuer les calculs\n",
    "    :return: une tuple contenant (le taux de réussite, la perte moyenne)\n",
    "    \"\"\"\n",
    "    acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    model.to(device)\n",
    "    model.eval()  # Pour désactiver les couches dropout ou batchnorm\n",
    "    with torch.no_grad():  # Pas besoin de calculer les gradients en mode évaluation\n",
    "        for x, t in test_loader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            \n",
    "            # Prédictions\n",
    "            y = model(x)\n",
    "            \n",
    "            # Calcul de la loss\n",
    "            loss = loss_funct(y, t)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calcul de la précision\n",
    "            if len(t.shape) > 1 and t.shape[1] > 1:  # Si les labels sont one-hot encodés\n",
    "                t = torch.argmax(t, dim=1)  # On convertit en indices de classes\n",
    "            acc += (torch.argmax(y, 1) == t).sum().item()\n",
    "            \n",
    "            total_samples += t.size(0)\n",
    "\n",
    "    avg_acc = acc / total_samples\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "\n",
    "    return avg_acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module,\n",
    "        train_loader:torch.utils.data.DataLoader,\n",
    "        validate_loader:torch.utils.data.DataLoader,\n",
    "        optimizer:torch.optim.SGD,\n",
    "        loss_func:torch.nn.MSELoss,\n",
    "        nb_epochs:int,\n",
    "        print_:bool=False\n",
    "        ) -> Tuple[float, nn.Module]:\n",
    "    \"\"\"\n",
    "    Méthode d'entraînement du modèle\n",
    "\n",
    "    Pour chaque époque:\n",
    "    - on entraîne le modèle sur les données d'apprentissage\n",
    "    - Puis on évalue le modèle sur les données de validation\n",
    "\n",
    "    Au final nous renvoyons le modèle avec le meilleur taux de réussite\n",
    "    (Nous évitons le sur-apprentissage tout en testant l'entièreté des époques)\n",
    "\n",
    "    :param model: le modèle à entraîner\n",
    "    :param train_loader: le lecteur de données d'apprentissage\n",
    "    :param validate_loader: le lecteur de données de validation\n",
    "    :param optimizer: l'optimiseur lié au modèle\n",
    "    :param loss_func: la fonction de loss\n",
    "    :param nb_epochs: le nombre d'époques\n",
    "    :param print_: afficher ou non les résultats\n",
    "    :return: le taux de réussite final et le meilleur modèle\n",
    "    \"\"\"\n",
    "    meilleur_acc:int = 0\n",
    "    meilleur_model = None\n",
    "    model.to(device)\n",
    "    loss_func.to(device)\n",
    "    for epoch in range(nb_epochs):\n",
    "        for x,t in train_loader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            # on calcule la sortie du modèle\n",
    "            y = model(x)\n",
    "            # on met à jour les poids\n",
    "            if t.dim() > 1 and t.shape[1] > 1:  # Vérifie si t est en one-hot encoding\n",
    "                t = torch.argmax(t, dim=1)  # Convertit en indices de classes\n",
    "\n",
    "            loss:torch.Tensor = loss_func(y,t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        acc, _ = evaluate(model=model, test_loader=validate_loader, loss_funct=loss_func)\n",
    "        if acc > meilleur_acc:\n",
    "            meilleur_acc = acc\n",
    "            meilleur_model = model\n",
    "\n",
    "        if print_:\n",
    "            print(f'Epoch {epoch+1}/{nb_epochs}, Accuracy: {acc}')\n",
    "    return meilleur_acc, meilleur_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste hidden init [10, 5]\n",
      "Epoch 1/100, Accuracy: 0.0\n",
      "Epoch 2/100, Accuracy: 0.0\n",
      "Epoch 3/100, Accuracy: 0.17356205852674067\n",
      "Epoch 4/100, Accuracy: 0.7547931382441978\n",
      "Epoch 5/100, Accuracy: 0.7547931382441978\n",
      "Epoch 6/100, Accuracy: 0.7547931382441978\n",
      "Epoch 7/100, Accuracy: 0.7547931382441978\n",
      "Epoch 8/100, Accuracy: 0.7547931382441978\n",
      "Epoch 9/100, Accuracy: 0.7547931382441978\n",
      "Epoch 10/100, Accuracy: 0.7547931382441978\n",
      "Epoch 11/100, Accuracy: 0.7547931382441978\n",
      "Epoch 12/100, Accuracy: 0.7547931382441978\n",
      "Epoch 13/100, Accuracy: 0.7547931382441978\n",
      "Epoch 14/100, Accuracy: 0.7547931382441978\n",
      "Epoch 15/100, Accuracy: 0.7547931382441978\n",
      "Epoch 16/100, Accuracy: 0.7547931382441978\n",
      "Epoch 17/100, Accuracy: 0.7547931382441978\n",
      "Epoch 18/100, Accuracy: 0.7547931382441978\n",
      "Epoch 19/100, Accuracy: 0.7547931382441978\n",
      "Epoch 20/100, Accuracy: 0.7547931382441978\n",
      "Epoch 21/100, Accuracy: 0.7547931382441978\n",
      "Epoch 22/100, Accuracy: 0.7547931382441978\n",
      "Epoch 23/100, Accuracy: 0.7547931382441978\n",
      "Epoch 24/100, Accuracy: 0.7547931382441978\n",
      "Epoch 25/100, Accuracy: 0.7547931382441978\n",
      "Epoch 26/100, Accuracy: 0.7547931382441978\n",
      "Epoch 27/100, Accuracy: 0.7547931382441978\n",
      "Epoch 28/100, Accuracy: 0.7547931382441978\n",
      "Epoch 29/100, Accuracy: 0.7547931382441978\n",
      "Epoch 30/100, Accuracy: 0.7547931382441978\n",
      "Epoch 31/100, Accuracy: 0.7547931382441978\n",
      "Epoch 32/100, Accuracy: 0.7547931382441978\n",
      "Epoch 33/100, Accuracy: 0.7547931382441978\n",
      "Epoch 34/100, Accuracy: 0.7547931382441978\n",
      "Epoch 35/100, Accuracy: 0.7547931382441978\n",
      "Epoch 36/100, Accuracy: 0.7547931382441978\n",
      "Epoch 37/100, Accuracy: 0.7547931382441978\n",
      "Epoch 38/100, Accuracy: 0.7547931382441978\n",
      "Epoch 39/100, Accuracy: 0.7547931382441978\n",
      "Epoch 40/100, Accuracy: 0.7547931382441978\n",
      "Epoch 41/100, Accuracy: 0.7547931382441978\n",
      "Epoch 42/100, Accuracy: 0.7547931382441978\n",
      "Epoch 43/100, Accuracy: 0.7547931382441978\n",
      "Epoch 44/100, Accuracy: 0.7547931382441978\n",
      "Epoch 45/100, Accuracy: 0.7547931382441978\n",
      "Epoch 46/100, Accuracy: 0.7547931382441978\n",
      "Epoch 47/100, Accuracy: 0.7547931382441978\n",
      "Epoch 48/100, Accuracy: 0.7547931382441978\n",
      "Epoch 49/100, Accuracy: 0.7547931382441978\n",
      "Epoch 50/100, Accuracy: 0.7547931382441978\n",
      "Epoch 51/100, Accuracy: 0.7547931382441978\n",
      "Epoch 52/100, Accuracy: 0.7547931382441978\n",
      "Epoch 53/100, Accuracy: 0.7547931382441978\n",
      "Epoch 54/100, Accuracy: 0.7547931382441978\n",
      "Epoch 55/100, Accuracy: 0.7547931382441978\n",
      "Epoch 56/100, Accuracy: 0.7547931382441978\n",
      "Epoch 57/100, Accuracy: 0.7547931382441978\n",
      "Epoch 58/100, Accuracy: 0.7547931382441978\n",
      "Epoch 59/100, Accuracy: 0.7547931382441978\n",
      "Epoch 60/100, Accuracy: 0.7547931382441978\n",
      "Epoch 61/100, Accuracy: 0.7547931382441978\n",
      "Epoch 62/100, Accuracy: 0.7547931382441978\n",
      "Epoch 63/100, Accuracy: 0.7547931382441978\n",
      "Epoch 64/100, Accuracy: 0.7547931382441978\n",
      "Epoch 65/100, Accuracy: 0.7547931382441978\n",
      "Epoch 66/100, Accuracy: 0.7547931382441978\n",
      "Epoch 67/100, Accuracy: 0.7547931382441978\n",
      "Epoch 68/100, Accuracy: 0.7547931382441978\n",
      "Epoch 69/100, Accuracy: 0.7547931382441978\n",
      "Epoch 70/100, Accuracy: 0.7547931382441978\n",
      "Epoch 71/100, Accuracy: 0.7547931382441978\n",
      "Epoch 72/100, Accuracy: 0.7547931382441978\n",
      "Epoch 73/100, Accuracy: 0.7547931382441978\n",
      "Epoch 74/100, Accuracy: 0.7547931382441978\n",
      "Epoch 75/100, Accuracy: 0.7547931382441978\n",
      "Epoch 76/100, Accuracy: 0.7547931382441978\n",
      "Epoch 77/100, Accuracy: 0.7547931382441978\n",
      "Epoch 78/100, Accuracy: 0.7547931382441978\n",
      "Epoch 79/100, Accuracy: 0.7547931382441978\n",
      "Epoch 80/100, Accuracy: 0.7547931382441978\n",
      "Epoch 81/100, Accuracy: 0.7547931382441978\n",
      "Epoch 82/100, Accuracy: 0.7547931382441978\n",
      "Epoch 83/100, Accuracy: 0.7547931382441978\n",
      "Epoch 84/100, Accuracy: 0.7547931382441978\n",
      "Epoch 85/100, Accuracy: 0.7547931382441978\n",
      "Epoch 86/100, Accuracy: 0.7547931382441978\n",
      "Epoch 87/100, Accuracy: 0.7547931382441978\n",
      "Epoch 88/100, Accuracy: 0.7547931382441978\n",
      "Epoch 89/100, Accuracy: 0.7547931382441978\n",
      "Epoch 90/100, Accuracy: 0.7547931382441978\n",
      "Epoch 91/100, Accuracy: 0.7547931382441978\n",
      "Epoch 92/100, Accuracy: 0.7547931382441978\n",
      "Epoch 93/100, Accuracy: 0.7547931382441978\n",
      "Epoch 94/100, Accuracy: 0.7547931382441978\n",
      "Epoch 95/100, Accuracy: 0.7547931382441978\n",
      "Epoch 96/100, Accuracy: 0.7547931382441978\n",
      "Epoch 97/100, Accuracy: 0.7547931382441978\n",
      "Epoch 98/100, Accuracy: 0.7547931382441978\n",
      "Epoch 99/100, Accuracy: 0.7547931382441978\n",
      "Epoch 100/100, Accuracy: 0.7547931382441978\n",
      "Accuracy: 0.7547931382441978, Loss: 0.5523771605184001\n"
     ]
    }
   ],
   "source": [
    "env = env6Str()\n",
    "tokenizer = SimpleTokenizerV1(\n",
    "    create_all_words_action_outcome_enumerate(\n",
    "        env.get_actions(), env.get_outcomes()))\n",
    "\n",
    "\n",
    "# Création des données\n",
    "x_fit, y_fit, x_val, y_val = make_data_set(env=env,\n",
    "              rand_iter=200,\n",
    "              tokenizer=tokenizer,context_lenght=10\n",
    "              )\n",
    "\n",
    "x_fit = torch.tensor(x_fit, dtype=torch.float).to(device)\n",
    "y_fit = torch.tensor(y_fit, dtype=torch.long).to(device)\n",
    "\n",
    "# Transforme y_fit en one hot\n",
    "y_fit = torch.nn.functional.one_hot(y_fit.to(torch.int64), len(env.get_actions()) + len(env.get_outcomes())).to(torch.float)\n",
    "\n",
    "x_val = torch.tensor(x_val, dtype=torch.float).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "# Création des DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(x_fit, y_fit), batch_size=32, shuffle=True\n",
    ")\n",
    "validate_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(x_val, y_val), batch_size=32, shuffle=False\n",
    ")\n",
    "\n",
    "# Création du modèle\n",
    "model = DeepNetwork(\n",
    "    input_size=len(x_fit[0]),\n",
    "    hidden_size=[10, 5],\n",
    "    output_size=len(env.get_actions()) + len(env.get_outcomes())\n",
    ")\n",
    "\n",
    "# Création de l'optimiseur\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Création de la fonction de perte\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Entraînement du modèle\n",
    "best_acc, best_model = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    validate_loader=validate_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func,\n",
    "    nb_epochs=100,\n",
    "    print_=True\n",
    ")\n",
    "\n",
    "# Évaluation du modèle\n",
    "acc, loss = evaluate(model=best_model, test_loader=validate_loader, loss_funct=loss_func)\n",
    "print(f'Accuracy: {acc}, Loss: {loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin de la biffurcasion on reprend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 170\u001b[0m\n\u001b[1;32m    155\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SimpleTokenizerV1(\n\u001b[1;32m    156\u001b[0m     create_all_words_action_outcome_enumerate(env\u001b[38;5;241m.\u001b[39mget_actions(), env\u001b[38;5;241m.\u001b[39mget_outcomes()))\n\u001b[1;32m    158\u001b[0m mymodel \u001b[38;5;241m=\u001b[39m GPTModel({\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39mget_actions()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39mget_outcomes()),\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m51\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_vocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39mget_outcomes())\n\u001b[1;32m    168\u001b[0m     })\n\u001b[0;32m--> 170\u001b[0m evolued_train_loss, evolued_val_loss, good_predicts, finish \u001b[38;5;241m=\u001b[39m \u001b[43mact_in_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmymodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss_act_in_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinish : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinish\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m see_evolued_train_loss([finish])\n",
      "Cell \u001b[0;32mIn[53], line 109\u001b[0m, in \u001b[0;36mact_in_env\u001b[0;34m(model, tokenizer, env, valance, iter, path_save, _lr, weight_decay, max_depth)\u001b[0m\n\u001b[1;32m    106\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# train_loss, val_loss = train_simple(model, optimizer, \u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#                         inputs, targets, 50, verbose=True, x_val=x_val, y_val=y_val)\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_for_one_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m save_plot_loss(train_loss, val_loss, path\u001b[38;5;241m=\u001b[39mpath_save, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# We remove 2 word and add 1 word. Beacause compute_expective_valance give us the last word (each action)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36mtrain_for_one_seq\u001b[0;34m(model, optimizer, inputs, targets, n_iter, x_val, y_val, verbose)\u001b[0m\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[0;32m---> 39\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(logits, targets)\n\u001b[1;32m     41\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m, in \u001b[0;36mGPTModel.forward\u001b[0;34m(self, x, return_full_sequence)\u001b[0m\n\u001b[1;32m     43\u001b[0m device \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# assert device != self.cfg['device'], \"Input tensor device does not match model device\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# print(\"in GPT, forward \",x)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Token & Positional Embeddings\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# (b, seq_len, emb_dim)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m tok_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# print(\"tok_embeds \",tok_embeds)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# (seq_len, emb_dim)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m pos_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb(torch\u001b[38;5;241m.\u001b[39marange(seq_len, device\u001b[38;5;241m=\u001b[39mdevice))\n",
      "File \u001b[0;32m~/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "def make_data_set(tokenizer, env, rand_iter:int, context_lenght:int):\n",
    "    \"\"\"\n",
    "    Create a data set from the environment, make action randomly \\\n",
    "    and store (action, feedback) in list of list of tuple for the model\n",
    "    :param tokenizer: the tokenizer to encode the data\n",
    "    :param env: the environment to interact with\n",
    "    :param rand_iter: the number of random iteration\n",
    "    :param context_lenght: the context lenght to use\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    for i in range(rand_iter):\n",
    "        action = np.random.choice(env.get_actions())\n",
    "        feedback = env.outcome(action)\n",
    "        history.append((str(action), str(feedback)))\n",
    "\n",
    "    tmpXfit, tmpYfit = inter_action_and_feedback_size(history, context_lenght)\n",
    "    x_fit = []\n",
    "    for i, one_input in enumerate(tmpXfit):\n",
    "        x_fit.append(tokenizer.encode(one_input))\n",
    "    y_fit = tokenizer.encode(tmpYfit)\n",
    "\n",
    "    return x_fit, y_fit\n",
    "\n",
    "def tempo_recursif_expective_valance(model:nn.Module, env, seq:list,\n",
    "                                    max_depth:int, valance:dict, \n",
    "                                    seuil:float=0.2, proba:float = 1,\n",
    "                                    seq_predi:list = []):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if max_depth == 0:\n",
    "        return {}\n",
    "    max_depth -= 1\n",
    "\n",
    "    if proba < seuil:\n",
    "        return {}\n",
    "    \n",
    "    model.eval()\n",
    "    exceptive_valance = {}\n",
    "    for act in env.get_actions():\n",
    "        new_seq = seq_predi + [act]\n",
    "        seq_to_predict = seq + [tokenizer.encode(act)]\n",
    "        seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.long).to(device)\n",
    "        x = model(seq_to_predict, False)\n",
    "        probs = nn.functional.softmax(x, dim=-1)\n",
    "        # for each outcomes we want proba with act\n",
    "        for out in env.get_outcomes():\n",
    "            tmp_new_seq = new_seq + [out]\n",
    "            tmp_proba = probs[0][tokenizer.encode(out)].item() * proba\n",
    "            if tmp_proba < seuil:\n",
    "                continue\n",
    "            tempo =np.round(valance[(act, out)] * tmp_proba, decimals=4)\n",
    "            # input(f'seq {seq_predi} act {act} out {out} proba {tmp_proba} valance {valance[(act, out)]} tempo {tempo}')\n",
    "\n",
    "            exceptive_valance.update(\n",
    "                tempo_recursif_expective_valance(model, env, \n",
    "                                            seq[2:] + [tokenizer.encode(act), tokenizer.encode(out)],\n",
    "                                            max_depth, valance, seuil, tmp_proba, tmp_new_seq.copy())\n",
    "            )\n",
    "            exceptive_valance[str(tmp_new_seq)] = tempo\n",
    "    return exceptive_valance\n",
    "\n",
    "def compute_expective_valance(model:nn.Module, env,\n",
    "                            seq:list, depth:int, valance:dict):\n",
    "    \"\"\" \n",
    "    Not implemented\n",
    "    \"\"\"\n",
    "    raise Exception(\"Not implemented\")\n",
    "\n",
    "def act_in_env(model: nn.Module, \n",
    "            tokenizer:SimpleTokenizerV1,\n",
    "            env, valance:dict, iter:int,\n",
    "            path_save = \"env\", _lr=1e-2, weight_decay=1e-2, max_depth=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    context_lenght = model.cfg[\"context_length\"]\n",
    "    device = model.cfg[\"device\"]\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=_lr, weight_decay=weight_decay)\n",
    "    evolued_train_loss = []\n",
    "    finish_evolued_train_loss = []\n",
    "    evolued_val_loss = []\n",
    "    good_predicts:list[bool] = []\n",
    "\n",
    "    # TODO delete validation\n",
    "    x_val, y_val = make_data_set(env=env,\n",
    "              rand_iter=1000,\n",
    "              tokenizer=tokenizer,\n",
    "              context_lenght=context_lenght\n",
    "              )\n",
    "\n",
    "    x_val = torch.tensor(x_val, dtype=torch.long).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "    x_train, y_train = make_data_set(env=env,\n",
    "                rand_iter=context_lenght,\n",
    "                tokenizer=tokenizer,\n",
    "                context_lenght=context_lenght\n",
    "                )\n",
    "    assert len(x_train) == 1 and len(y_train) == 1, \"We need to have only one sequence to first train the model\"\n",
    "    assert len(x_train[0]) == context_lenght, \"The context lenght is not correct\"\n",
    "\n",
    "    inputs= torch.tensor(x_train, dtype=torch.long).to(device)\n",
    "    targets = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(0, iter):\n",
    "        # train model\n",
    "        model.train()\n",
    "        # train_loss, val_loss = train_simple(model, optimizer, \n",
    "        #                         inputs, targets, 50, verbose=True, x_val=x_val, y_val=y_val)\n",
    "        train_loss, val_loss = train_for_one_seq(model, optimizer, \n",
    "                                inputs, targets, 2, verbose=True, x_val=x_val, y_val=y_val)\n",
    "        save_plot_loss(train_loss, val_loss, path=path_save, title=f'loss_{i}')\n",
    "        # We remove 2 word and add 1 word. Beacause compute_expective_valance give us the last word (each action)\n",
    "        seq_to_predict = x_train[0][2:] + [targets[-1]]\n",
    "        exceptive_valance = tempo_recursif_expective_valance(model, env, seq_to_predict, max_depth, valance, seuil=0.0001)\n",
    "        print(f'for seq {seq} all exceptive valance {exceptive_valance}')\n",
    "        try:\n",
    "            max_val_pred = eval(max(exceptive_valance, key=exceptive_valance.get))\n",
    "            act = max_val_pred[0]\n",
    "            predi = max_val_pred[1]\n",
    "        except Exception as e:\n",
    "            print(f'\\033[0;31m iterationerror dont find max val pred {e}')\n",
    "            print(\"act chose randomly\\033[0m iteration\")\n",
    "            act = np.random.choice(env.get_actions())\n",
    "\n",
    "        # input(f'for seq {seq} the next action is {act} and the next predi is {predi} in max val pred {max_val_pred}')\n",
    "\n",
    "        # act, predi = compute_expective_valance(model, seq=seq_to_predict, env=env,\n",
    "        #                                        depth=max_depth, valance=valance)\n",
    "        fb = env.outcome(act)\n",
    "        good_predicts.append(predi == fb)\n",
    "        x_train = [x_train[0][2:] + [tokenizer.encode(act)]]\n",
    "        y_train = [tokenizer.encode(fb)]\n",
    "        inputs= torch.tensor(x_train, dtype=torch.long).to(device)\n",
    "        targets= torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        evolued_train_loss.append(train_loss)\n",
    "        evolued_val_loss.append(val_loss)\n",
    "        print(f'It s train loss {train_loss}')\n",
    "        finish_evolued_train_loss.append(train_loss[-1])\n",
    "\n",
    "        print(f'% good predict : {sum(good_predicts[-10:]) * 10 if len(good_predicts) >= 10 else -1}')\n",
    "        print(f'\\033[0;34miteration {i} action \\033[0;31m {act} \\033[0;35m feedback {fb} \\033[0;31m predict {predi} \\033[0m')\n",
    "\n",
    "\n",
    "    return evolued_train_loss, evolued_val_loss, good_predicts, finish_evolued_train_loss\n",
    "\n",
    "\n",
    "valence = {\n",
    "    ('a', 'x') : -10,\n",
    "    ('a', 'y') : 10,\n",
    "    ('b', 'x') : -10,\n",
    "    ('b', 'y') : 10\n",
    "}\n",
    "env = env3Str()\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(\n",
    "    create_all_words_action_outcome_enumerate(env.get_actions(), env.get_outcomes()))\n",
    "\n",
    "mymodel = GPTModel({\n",
    "        \"vocab_size\": len(env.get_actions()) + len(env.get_outcomes()),\n",
    "        \"context_length\": 51,\n",
    "        \"emb_dim\": 16 * 2,\n",
    "        \"n_heads\": 1, # 4\n",
    "        \"n_leayers\": 1, # 4\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": device,\n",
    "        \"out_vocab_size\": len(env.get_outcomes())\n",
    "    })\n",
    "\n",
    "evolued_train_loss, evolued_val_loss, good_predicts, finish = act_in_env(\n",
    "    mymodel, tokenizer, env, valence, 100, \"loss_act_in_env\", \n",
    "    _lr=1e-1, weight_decay=1e-2, max_depth=3)\n",
    "print(f'finish : {finish}')\n",
    "see_evolued_train_loss([finish])\n",
    "\n",
    "see_evolued_train_loss(evolued_train_loss)\n",
    "\n",
    "see_evolued_train_loss(evolued_val_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
