{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following file was inspired by the following tutorial:\n",
    "# https://colab.research.google.com/drive/1SiF0KZJp75rUeetKOWqpsA8clmHP6jMg?usp=sharing#scrollTo=d7utFz27cO9q\n",
    "\n",
    "# And use this cource for explanation:\n",
    "# https://bruno-yun.notion.site/The-Transformer-Model-for-NLG-c4413bd5a8044325a7658cb8ff5535f2\n",
    "# https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "from graphviz import Digraph\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environnement.environnement1 import Environment1 as env1\n",
    "from environnement.environnement2Str import Environment2 as env2Str\n",
    "from environnement.environnement3Str import Environment3 as env3Str\n",
    "from environnement.environnement6Str import Environment6 as env6Str\n",
    "\n",
    "from environnement.small_loop import small_loop\n",
    "\n",
    "from inter.interactions import Interaction as inter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro, bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporaire\n",
    "VOCAB_SIZE = 2 # For now we are only using binary data (0, 1) later can be modify by 12\n",
    "CONTEXT_LENGHT = 3 # And we are using a context of 3 bits (may be changed later)\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads,\n",
    "                 qkv_bias = False, device = 'cpu'):\n",
    "        \n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0), \"d_out should be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        # self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        # self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        # self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias) \n",
    "        # Don't use this because we want to have only one projection to optimize\n",
    "        # To have only one projection we use the following line\n",
    "        self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        self.out_proj = nn.Linear(d_out, d_out).to(device)\n",
    "        # If we want to see past\n",
    "        mask = torch.triu(torch.ones(context_length,context_length), diagonal=1).to(device)\n",
    "        \n",
    "        # if we want to see future\n",
    "        # mask = torch.tril(torch.ones(context_length,context_length), diagonal=-1).to(device)\n",
    "        \n",
    "        # If we want to see all the context\n",
    "        # mask = torch.zeros(context_length, context_length, device=device)\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "    def forward(self,x: torch.Tensor):\n",
    "        queries: torch.Tensor\n",
    "        keys: torch.Tensor\n",
    "        values: torch.Tensor\n",
    "        b, num_tokens, d_in = x.shape # b, num_token, d_in\n",
    "\n",
    "        # self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Use one projection to get queries, keys and values\n",
    "        # self.W_qkv(x) -> b, num_token, 3*d_out (is a tensor)\n",
    "        # chunk(3, dim=-1) -> b, num_token, d_out we split the tensor in 3 parts\n",
    "        queries, keys, values= self.W_qkv(x).chunk(3, dim=-1)\n",
    "        \n",
    "        # b, num_token, numheads, head_dim\n",
    "        queries = queries.reshape(b,\n",
    "                                num_tokens,\n",
    "                                self.num_heads,\n",
    "                                self.head_dim\n",
    "                            ).transpose(1, 2)\n",
    "        keys = keys.reshape(b,\n",
    "                            num_tokens,\n",
    "                            self.num_heads,\n",
    "                            self.head_dim\n",
    "                        ).transpose(1, 2)\n",
    "        values = values.reshape(b,\n",
    "                                num_tokens,\n",
    "                                self.num_heads,\n",
    "                                self.head_dim\n",
    "                            ).transpose(1, 2)\n",
    "        \n",
    "        # b, num_heads, num_token, num_token\n",
    "        attn_scores = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "        \n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask[:num_tokens, :num_tokens].unsqueeze(0).unsqueeze(0).bool() == 1, \n",
    "            float('-inf')\n",
    "        )\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.einsum('bhqk, bhkd -> bhqd', \n",
    "                               attn_weights, \n",
    "                               values\n",
    "                            ).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        \n",
    "        \n",
    "        return self.out_proj(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain of class MultiHeadAttention\n",
    "In transformers model, the **MultiHeadAttention** class is used to apply attention mechanisms to input sequences. Understanding how this class works requires first exploring **single-head** attention before generalizing to **multi-head attention**.\n",
    "\n",
    "## Single-Head Attention\n",
    "Attention in Transformers relies on measuring the similarity between tokens in a sequence. This measurement is performed using three distinct transformations:\n",
    "\n",
    "- Query (Q): Represents the current tokenâ€™s focus when comparing it to all other tokens.\n",
    "- Key (K): Represents a token being compared to the query.\n",
    "- Value (V): Represents the actual information to be aggregated based on attention scores.\n",
    "\n",
    "These three representations are obtained by applying linear transformations to the input sequence. The transformed matrices allow us to compute the attention scores using the scaled dot-product attention formula:\n",
    "\n",
    "$ \n",
    "score(x_i, x_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}} \n",
    "$\n",
    "\n",
    "Where $q_i$ is the query representation of token $i$, $k_j$ is the key representation of token $j$, and $d_k$ is the dimension of the key representation. \n",
    "\n",
    "In forward function is :\n",
    "```python\n",
    "attn_scores = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "```\n",
    "\n",
    "To prevent a token from attending to future tokens (as in autoregressive models like GPT), we apply a mask to the attention scores. This ensures that each token only attends to itself and previous tokens:\n",
    "\n",
    "$\n",
    "a_i = \\sum_{j \\leq i} \\alpha_{ij}v_j\n",
    "$\n",
    "\n",
    "In forward function is :\n",
    "```python\n",
    "attn_scores = attn_scores.masked_fill(self.mask[:num_tokens, :num_tokens] == 0,\n",
    "                                float('-inf'))\n",
    "```\n",
    "\n",
    "Once the attention scores are computed and masked, they are normalized using the softmax function to produce attention weights:\n",
    "\n",
    "$\n",
    "\\alpha_{ij} = \\frac{\\exp(score(x_i,x_j))}{\\sum_j \\exp(score(x_i,x_j))}\n",
    "$\n",
    "\n",
    "In forward function is :\n",
    "```python\n",
    "attn_scores = nn.fonctional.softmax(attn_scores, dim=-1)\n",
    "```\n",
    "\n",
    "To prevent overfitting, a dropout layer is applied to the attention weights (self.dropout) before computing the context vector.\n",
    "\n",
    "To compute the context vector, we multiply the attention weights by the Value (V) representations to obtain the context vector.\n",
    "\n",
    "In forward function is :\n",
    "\n",
    "```python\n",
    "context = torch.einsum('bhqk, bhkd -> bhqd', attn_weights, values)\n",
    "```\n",
    "\n",
    "To resume we have done this :\n",
    "\n",
    "![explain one head](./img/ExplainOneHeadAttention.png)\n",
    "*Source: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/9.pdf)*\n",
    "\n",
    "## Multi-Head Attention\n",
    "Single-head attention allows each token to focus on others, but it has limitations: it can only capture one type of relationship at a time. Multi-head attention improves this by using multiple attention heads in parallel.\n",
    "Instead of computing attention once, we split the input into multiple heads, each with a different representation of the sequence. This allows the model to capture diverse patterns.\n",
    "All heads are concatenated and linearly transformed to produce the final output. This process is implemented in the **MultiHeadAttention**, is why we use self.head_dim or 'd' in code.\n",
    "\n",
    "This figure resume compute with matrix :\n",
    "\n",
    "![Compute](./img/ExplainCompute.png)\n",
    "*Source: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/9.pdf)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        # TODO fct FFN we can change the number of layers\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"] // 2, bias=cfg[\"qkv_bias\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(cfg[\"emb_dim\"] // 2, cfg[\"emb_dim\"], bias=cfg[\"qkv_bias\"])\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The FeedForward in transformer\n",
    "The **FeedForward** class is used to apply a feedforward neural network to the output of the multi-head attention layer. The feedforward network consists of two linear transformations with a GELU (or RELU) activation function in between.\n",
    "\n",
    "## GELU and RELU\n",
    "GELU and RELU are activation functions used in neural networks after linear transformations. GELU is a smoother version of RELU that has been shown to improve performance in transformer models. \n",
    "\n",
    "The GELU function is defined as:\n",
    "\n",
    "$\n",
    "GELU(x) = 0.5x(1 + tanh(\\sqrt{2/\\pi}(x + 0.044715x^3)))\n",
    "$\n",
    "\n",
    "RELU is a simpler activation function that sets all negative values to zero:\n",
    "\n",
    "$\n",
    "RELU(x) = max(0, x)\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(cfg[\"emb_dim\"]).to(cfg[\"device\"])\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"],\n",
    "            device=cfg[\"device\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg).to(cfg[\"device\"])\n",
    "        self.norm2 = nn.LayerNorm(cfg[\"emb_dim\"]).to(cfg[\"device\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"]).to(cfg[\"device\"])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # print(\"we are in one transformer block\")\n",
    "        # print(x.shape)\n",
    "        # print(x)\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        # print(\"after att\", x.shape)\n",
    "        # print(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x+ shortcut\n",
    "        # print(x.shape)\n",
    "        # print(x)\n",
    "        # print('end trs block')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain of class TransformerBlock\n",
    "The **TransformerBlock** class is the core building block of the transformer model. It consists of three main components: the **MultiHeadAttention**, the **FeedForward** layers and **LayerNorm**. The **MultiHeadAttention** layer is used to capture the relationships between tokens, while the **FeedForward** layer is used to apply non-linear transformations to the output of the attention layer. We use **LayerNorm** to normalize X and for normalize the output of the **FeedForward** layer.\n",
    "\n",
    "![image.png](./img/ExplainArchiTrans.png)\n",
    "*Source: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/9.pdf)*\n",
    "\n",
    "In this figure we can see the Residual Stream. In the forward funciton it's name shortcut. It's a way to avoid the vanishing gradient problem. The output of the **FeedForward** layer is added to the input of the **MultiHeadAttention** layer. This allows the model to learn the difference between the input and output of the block, which helps to improve performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Embeddings\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]).to(cfg[\"device\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]).to(cfg[\"device\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"]).to(cfg[\"device\"])\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.trf_blocks = nn.ModuleList(\n",
    "                [TransformerBlock(cfg) for _ in range(cfg[\"n_leayers\"])]\n",
    "            )\n",
    "        \n",
    "        # Normalization & Output layer\n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"]).to(cfg[\"device\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False).to(cfg[\"device\"])\n",
    "        \n",
    "        # Weight tying: Share weights between embedding and output projection\n",
    "        self.tok_emb.weight = self.out_head.weight\n",
    "        \n",
    "        # Initialize weights (If we need)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # Special initialization for transformer projection layers\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p,\n",
    "                                    mean=0.0,\n",
    "                                    std=0.02 / math.sqrt(2 * self.cfg[\"n_layers\"]))\n",
    "                \n",
    "    def forward(self, x: torch.Tensor, return_full_sequence=False):\n",
    "        b, seq_len = x.shape\n",
    "        device = x.device\n",
    "        # assert device != self.cfg['device'], \"Input tensor device does not match model device\"\n",
    "\n",
    "        # print(\"in GPT, forward \",x)\n",
    "        \n",
    "        # Token & Positional Embeddings\n",
    "        # (b, seq_len, emb_dim)\n",
    "        tok_embeds = self.tok_emb(x)\n",
    "        # print(\"tok_embeds \",tok_embeds)\n",
    "        # (seq_len, emb_dim)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=device))\n",
    "        # print(\"pos_embeds \",pos_embeds)\n",
    "        \n",
    "        x = tok_embeds + pos_embeds\n",
    "        # print(\"tok_embeds + pos_embeds \",x)\n",
    "        x = self.drop_emb(x)\n",
    "        # print(\"self.drop_emb(x) \",x)\n",
    "        \n",
    "        # Pass through Transformer Blocks\n",
    "        deleteme = 0\n",
    "        for block_ in self.trf_blocks:\n",
    "            deleteme += 1\n",
    "            x = block_(x)\n",
    "            # print(f\"block(x) nÂ°{deleteme} : {x}\")\n",
    "        \n",
    "        # Final normalization & output projection\n",
    "        x = self.final_norm(x)\n",
    "        # print(\"self.final_norm(x) \",x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        # print(\"logits \",logits)\n",
    "        \n",
    "        if not return_full_sequence:\n",
    "            logits = logits[:, -1, :]  # (b, vocab_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_val, y_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_val, False)\n",
    "        loss = nn.functional.cross_entropy(logits, y_val)\n",
    "    return loss.item()\n",
    "\n",
    "def train_simple(model, optimizer, inputs, targets, n_iter, x_val=None, y_val=None, verbose=True):\n",
    "    all_train_loss, all_val_loss = [], []\n",
    "    for i in range(n_iter):\n",
    "        logits = model(inputs, False)\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            all_train_loss.append(loss.item())\n",
    "            if x_val is not None:\n",
    "                val_loss = evaluate_model(model, x_val, y_val)\n",
    "                all_val_loss.append(val_loss)\n",
    "                if verbose:\n",
    "                    print(f'for {i} epochs, loss is {loss.item()} and val_loss is {val_loss}')\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f'for {i} epochs, loss is {loss.item()}')\n",
    "                \n",
    "\n",
    "    return all_train_loss, all_val_loss\n",
    "\n",
    "def train_for_one_seq(model, optimizer, inputs, targets, n_iter, x_val=None, y_val=None, verbose=True):\n",
    "    \"\"\" \n",
    "    Train the model for one sequence\n",
    "    \"\"\"\n",
    "    all_train_loss, all_val_loss = [], []\n",
    "    for i in range(n_iter):\n",
    "        logits = model(inputs, False)\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        for g in optimizer.param_groups:\n",
    "            tmp = g['lr']\n",
    "            break\n",
    "        if predictions != targets:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] /= 10\n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = tmp\n",
    "\n",
    "        # if i % 10 == 0:\n",
    "        all_train_loss.append(loss.item())\n",
    "        if x_val is not None:\n",
    "            val_loss = evaluate_model(model, x_val, y_val)\n",
    "            all_val_loss.append(val_loss)\n",
    "            if verbose:\n",
    "                print(f'for {i} epochs, loss is {loss.item()} and val_loss is {val_loss}')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f'for {i} epochs, loss is {loss.item()}')\n",
    "                \n",
    "    if len(all_val_loss) == 0:\n",
    "        all_val_loss = [20]\n",
    "\n",
    "    if len(all_train_loss) == 0:\n",
    "        all_train_loss = [20]\n",
    "    return all_train_loss, all_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sequence(model, context, n_tokens):\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_tokens):\n",
    "            logits = model(context, False)\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.argmax(probas, dim=-1).unsqueeze(0)\n",
    "            context = torch.cat([context, next_token], dim=-1).to(device)\n",
    "    return context\n",
    "\n",
    "CONFIG_TEST = {\n",
    "    \"vocab_size\": 2,\n",
    "    \"context_length\": 15,\n",
    "    \"emb_dim\": 16,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_leayers\": 4,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "model_test = GPTModel(CONFIG_TEST)\n",
    "generate_sequence(model_test, torch.randint(0, CONFIG_TEST[\"vocab_size\"], (1, 3)).to(CONFIG_TEST[\"device\"]), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M_small = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_leayers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "OUR_CONFIG = {\n",
    "    \"vocab_size\": VOCAB_SIZE,\n",
    "    \"context_length\": CONTEXT_LENGHT,\n",
    "    \"emb_dim\": 16,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_leayers\": 4,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_possible(n, k):\n",
    "    \"\"\"\n",
    "    Generate all possible combination of n elements taken k by k\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        yield []\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            for c in all_possible(n, k - 1):\n",
    "                yield [i] + c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(gpt, token_to = None, graph = True):\n",
    "    \"\"\"\n",
    "    Plot the transition graph of the GPT model\n",
    "    \"\"\"\n",
    "    dot = Digraph(comment='Baby GPT', engine='circo')\n",
    "\n",
    "    for xi in all_possible(gpt.cfg[\"vocab_size\"], gpt.cfg[\"context_length\"]):\n",
    "        \n",
    "        # forward the GPT and get probabilities for next token\n",
    "        x = torch.tensor(xi, dtype=torch.long)[None, ...]\n",
    "        x = x.to(gpt.cfg[\"device\"])\n",
    "        # turn the list into a torch tensor and add a batch dimension\n",
    "        logits = gpt(x, False) # forward the gpt neural net\n",
    "\n",
    "        # print('logits :', logits)\n",
    "        probs = nn.functional.softmax(logits, dim=-1) # get the probabilities\n",
    "        y = probs[0].tolist() # remove the batch dimension and unpack the tensor into simple list\n",
    "        if token_to:\n",
    "            print(f\"input {token_to(xi)} ---> {y}\")\n",
    "        else:\n",
    "            print(f\"input {xi} ---> {y}\")\n",
    "\n",
    "        if graph:\n",
    "            # also build up the transition graph for plotting later\n",
    "            current_node_signature = \"\".join(str(d) for d in xi)\n",
    "            dot.node(current_node_signature)\n",
    "            # input(\"Press Enter to continue...\")\n",
    "\n",
    "            for t in range(gpt.cfg[\"vocab_size\"]):\n",
    "                next_node = xi[1:] + [t] # crop the context and append the next character\n",
    "                next_node_signature = \"\".join(str(d) for d in next_node)\n",
    "                p = y[t]\n",
    "\n",
    "                label=f\"{t}({p*100:.0f}%)\"\n",
    "                dot.edge(current_node_signature, next_node_signature, label=label)\n",
    "    \n",
    "    return dot\n",
    "\n",
    "# gpt = GPTModel(OUR_CONFIG)\n",
    "# plot_model(gpt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_TRAIN = {\n",
    "    \"vocab_size\": 2,\n",
    "    \"context_length\": 3,\n",
    "    \"emb_dim\": 16,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_leayers\": 4,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "# inputs = torch.tensor([[0, 1, 1],\n",
    "#                        [1, 0, 0],\n",
    "#                        [1, 1, 0],\n",
    "#                        [0, 0, 1]]).to(CONFIG_TRAIN[\"device\"])\n",
    "\n",
    "# targets = torch.tensor([[1, 1, 1],\n",
    "#                         [0, 0, 1],\n",
    "#                         [1, 0, 1],\n",
    "#                         [0, 1, 0]]).to(CONFIG_TRAIN[\"device\"])\n",
    "\n",
    "\n",
    "# probas = torch.softmax(model_test(inputs), dim=-1)\n",
    "\n",
    "# print(probas)\n",
    "\n",
    "# output = torch.argmax(probas, dim=-1)\n",
    "\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# text_prob1= probas[0, targets[0]]\n",
    "# print(\"proba text1\", text_prob1)\n",
    "# print(\"proba text1 - what we obtained\", probas[0, output[0].flatten()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# let's train our baby GPT on this sequence\n",
    "seq = list(map(int, \"111101111011110\"))\n",
    "print('seq', seq)\n",
    "\n",
    "# convert the sequence to a tensor holding all the individual examples in that sequence\n",
    "X, Y = [], []\n",
    "# iterate over the sequence and grab every consecutive 3 bits\n",
    "# the correct label for what's next is the next bit at each position\n",
    "# for i in range(len(seq) - 3):\n",
    "#     X.append(seq[i:i+3])\n",
    "#     Y.append(seq[i+3])\n",
    "#     print(f\"example {i+1:2d}: {X[-1]} --> {Y[-1]}\")\n",
    "# X = torch.tensor(X, dtype=torch.long).to(CONFIG_TRAIN[\"device\"])\n",
    "# Y = torch.tensor(Y, dtype=torch.long).to(CONFIG_TRAIN[\"device\"])\n",
    "# print(X.shape, Y.shape)\n",
    "# torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "# config = GPTConfig(\n",
    "#     block_size = 3,\n",
    "#     vocab_size = 2,\n",
    "#     n_layer = 4,\n",
    "#     n_head = 4,\n",
    "#     n_embd = 16,\n",
    "#     bias = False,\n",
    "# )\n",
    "# gpt = GPT(config)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "\n",
    "# for i in range(50):\n",
    "#     logits = gpt(X)\n",
    "#     loss = nn.functional.cross_entropy(logits, Y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(i, loss.item())\n",
    "\n",
    "\n",
    "# print(\"Training data sequence, as a reminder:\", seq)\n",
    "# plot_model()\n",
    "\n",
    "\n",
    "\n",
    "CONFIG_TRAIN = {\n",
    "    \"vocab_size\": 2,\n",
    "    \"context_length\": 3,\n",
    "    \"emb_dim\": 16,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_leayers\": 4,\n",
    "    \"drop_rate\": 0.000,\n",
    "    \"qkv_bias\": False,\n",
    "    \"device\": device\n",
    "}\n",
    "# model = GPTModel(CONFIG_TRAIN)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "\n",
    "# for i in range(10):\n",
    "#     logits = model(X, False)\n",
    "#     print(\"logits\", logits)\n",
    "#     loss = nn.functional.cross_entropy(logits, Y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(i, loss.item())\n",
    "\n",
    "\n",
    "# print(\"Training data sequence, as a reminder:\", seq)\n",
    "# plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPTModel(CONFIG_TRAIN)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "\n",
    "# inputs = torch.tensor([[0, 1, 1, 1],\n",
    "#                        [1, 0, 0, 1],\n",
    "#                        [1, 1, 0, 0]])\n",
    "\n",
    "# targets = torch.tensor([1, 1, 1])\n",
    "\n",
    "# # train the GPT for some number of iterations\n",
    "# for i in range(3):\n",
    "\n",
    "#     logits = model(inputs, False)\n",
    "#     print(logits)\n",
    "#     loss = nn.functional.cross_entropy(logits, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment 1\n",
    "# Action 0 => 0\n",
    "# Action 1 => 1\n",
    "\n",
    "# history_action = []\n",
    "# hystory_fb = []\n",
    "# history_pred = []\n",
    "\n",
    "# env = env1()\n",
    "# model = GPTModel(\n",
    "#     {\n",
    "#         \"vocab_size\": 2,\n",
    "#         \"context_length\": 1,\n",
    "#         \"emb_dim\": 16,\n",
    "#         \"n_heads\": 4,\n",
    "#         \"n_leayers\": 4,\n",
    "#         \"drop_rate\": 0.1,\n",
    "#         \"qkv_bias\": False,\n",
    "#         \"device\": device\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# action = 0\n",
    "# inputs = torch.tensor([[action]]).t\n",
    "# targets = torch.tensor([0])\n",
    "\n",
    "# for i in range(100):\n",
    "#     action = i%2\n",
    "#     history_action.append(action)\n",
    "#     feedback = env.outcome(action)\n",
    "#     hystory_fb.append(feedback)\n",
    "\n",
    "#     inputs = torch.tensor([[action]]).to(device)\n",
    "#     targets = torch.tensor([feedback]).to(device)\n",
    "#     logits = model(inputs, False)\n",
    "#     print(logits)\n",
    "#     history_pred.append(torch.argmax(logits).item())\n",
    "#     loss = nn.functional.cross_entropy(logits, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(i, loss.item())\n",
    "    \n",
    "# print(history_action)\n",
    "# print(hystory_fb)\n",
    "# print(history_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addapt_seq(seq, context_length):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(seq) - context_length):\n",
    "        X.append(seq[i:i+context_length])\n",
    "        Y.append(seq[i+context_length])\n",
    "    return torch.tensor(X, dtype=torch.long), torch.tensor(Y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Environment 2, goal is predict outcome with action.\n",
    "# # X are all action\n",
    "# # For each X we associate Y, the outcome of the action\n",
    "# # X is context lenght action\n",
    "\n",
    "# history_action = []\n",
    "# hystory_fb = []\n",
    "# history_pred = []\n",
    "\n",
    "# env = env2Str()\n",
    "\n",
    "# action = 0\n",
    "# inputs = torch.tensor([[action]])\n",
    "# targets = torch.tensor([0])\n",
    "\n",
    "# for i in range(20):\n",
    "#     action = torch.randint(0, 2, (1,)).item()\n",
    "#     history_action.append(action)\n",
    "#     feedback = env.outcome(action)\n",
    "#     hystory_fb.append(feedback)\n",
    "\n",
    "# CONTEXT_LENGHT = 4\n",
    "# model = GPTModel(\n",
    "#     {\n",
    "#         \"vocab_size\": 2,\n",
    "#         \"context_length\": CONTEXT_LENGHT,\n",
    "#         \"emb_dim\": 16,\n",
    "#         \"n_heads\": 4,\n",
    "#         \"n_leayers\": 4,\n",
    "#         \"drop_rate\": 0.1,\n",
    "#         \"qkv_bias\": False,\n",
    "#         \"device\": device\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "\n",
    "# inputs, targets = addapt_seq(history_action, CONTEXT_LENGHT)\n",
    "# # inputs, targets = addapt_seq(list(map(int, \"111101111011110\")), CONTEXT_LENGHT)\n",
    "# inputs = inputs.to(device)\n",
    "# targets = torch.tensor(hystory_fb[CONTEXT_LENGHT-1:-1]).to(device)\n",
    "\n",
    "# print(\"inputs\")\n",
    "# print(inputs)\n",
    "# print(targets)\n",
    "\n",
    "# for j in range(100):\n",
    "#     logits = model(inputs, False)\n",
    "#     loss = nn.functional.cross_entropy(logits, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(j, loss.item())\n",
    "\n",
    "# history_pred.append(torch.argmax(logits).item())\n",
    "    \n",
    "# print(history_action)\n",
    "# print(hystory_fb)\n",
    "# print(history_pred)\n",
    "\n",
    "# plot_model(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 0]\n",
      "[(0, 1), (1, 0), (1, 1), (0, 0)]\n",
      "[3, 12, 9, 8]\n",
      "[(0, 3), (3, 0), (2, 1), (2, 0)]\n"
     ]
    }
   ],
   "source": [
    "def interaction_to_token(interaction, base=2):\n",
    "    return [int(f\"{a}{b}\", base) for a, b in interaction]\n",
    "    \n",
    "def token_to_interaction(token, base=2):\n",
    "    return [(i // base, i % base) for i in token]\n",
    "\n",
    "x = interaction_to_token([(0, 1), (1, 0), (1, 1), (0, 0)])\n",
    "print(x)\n",
    "y = token_to_interaction(x)\n",
    "print(y)\n",
    "\n",
    "x = interaction_to_token([(0, 3), (3, 0), (2, 1), (2, 0)], 4)\n",
    "print(x)\n",
    "y = token_to_interaction(x, 4)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# history = []\n",
    "# history_action = []\n",
    "# hystory_fb = []\n",
    "# history_pred = []\n",
    "# context_lenght = 2\n",
    "\n",
    "# env = env3()\n",
    "\n",
    "# action = 0\n",
    "# inputs = torch.tensor([[action]])\n",
    "# targets = torch.tensor([0])\n",
    "\n",
    "# for i in range(1000):\n",
    "#     action = torch.randint(0, 2, (1,)).item()\n",
    "#     feedback = env.outcome(action)\n",
    "\n",
    "#     history_action.append(action)\n",
    "#     hystory_fb.append(feedback)\n",
    "\n",
    "#     history.append((action, feedback))\n",
    "\n",
    "# history_token = interaction_to_token(history)\n",
    "# voc = 4\n",
    "# inputs, targets = addapt_seq(history_token, context_lenght)\n",
    "\n",
    "# inputs = inputs.to(device)\n",
    "# targets = targets.to(device)\n",
    "\n",
    "# mymodel = GPTModel(\n",
    "#     {\n",
    "#         \"vocab_size\": voc,\n",
    "#         \"context_length\": context_lenght,\n",
    "#         \"emb_dim\": 16,\n",
    "#         \"n_heads\": 4,\n",
    "#         \"n_leayers\": 4,\n",
    "#         \"drop_rate\": 0.1,\n",
    "#         \"qkv_bias\": False,\n",
    "#         \"device\" : device\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# for j in range(100):\n",
    "#     logits = mymodel(inputs, False)\n",
    "#     loss = nn.functional.cross_entropy(logits, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     print(j, loss.item())\n",
    "\n",
    "# plot_model(mymodel, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = mymodel(torch.tensor([[1, 1]]).to(device), False)\n",
    "# probs = nn.functional.softmax(x, dim=-1)\n",
    "# print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'000': 0, '001': 1, '002': 2, '003': 3, '004': 4, '005': 5, '006': 6, '007': 7, '008': 8, '009': 9, '0010': 10, '0011': 11, '0012': 12, '0013': 13, '0014': 14, '0015': 15, '0016': 16, '0017': 17, '0018': 18, '0019': 19, '0020': 20, '0021': 21, '0022': 22, '0023': 23, '0024': 24, '0025': 25, '0026': 26, '0027': 27, '0028': 28, '0029': 29, '0030': 30, '0031': 31, '0032': 32, '0033': 33, '0034': 34, '0035': 35, '0036': 36, '0037': 37, '0038': 38, '0039': 39, '0040': 40, '0041': 41, '0042': 42, '0043': 43, '0044': 44, '0045': 45, '0046': 46, '0047': 47, '0048': 48, '0049': 49, '0050': 50, '0051': 51, '0052': 52, '0053': 53, '0054': 54, '0055': 55, '0056': 56, '0057': 57, '0058': 58, '0059': 59, '0060': 60, '0061': 61, '0062': 62, '0063': 63, '0064': 64, '0065': 65, '0066': 66, '0067': 67, '0068': 68, '0069': 69, '0070': 70, '0071': 71, '0072': 72, '0073': 73, '0074': 74, '0075': 75, '0076': 76, '0077': 77, '0078': 78, '0079': 79, '0080': 80, '010': 81, '011': 82, '012': 83, '013': 84, '014': 85, '015': 86, '016': 87, '017': 88, '018': 89, '019': 90, '0110': 91, '0111': 92, '0112': 93, '0113': 94, '0114': 95, '0115': 96, '0116': 97, '0117': 98, '0118': 99, '0119': 100, '0120': 101, '0121': 102, '0122': 103, '0123': 104, '0124': 105, '0125': 106, '0126': 107, '0127': 108, '0128': 109, '0129': 110, '0130': 111, '0131': 112, '0132': 113, '0133': 114, '0134': 115, '0135': 116, '0136': 117, '0137': 118, '0138': 119, '0139': 120, '0140': 121, '0141': 122, '0142': 123, '0143': 124, '0144': 125, '0145': 126, '0146': 127, '0147': 128, '0148': 129, '0149': 130, '0150': 131, '0151': 132, '0152': 133, '0153': 134, '0154': 135, '0155': 136, '0156': 137, '0157': 138, '0158': 139, '0159': 140, '0160': 141, '0161': 142, '0162': 143, '0163': 144, '0164': 145, '0165': 146, '0166': 147, '0167': 148, '0168': 149, '0169': 150, '0170': 151, '0171': 152, '0172': 153, '0173': 154, '0174': 155, '0175': 156, '0176': 157, '0177': 158, '0178': 159, '0179': 160, '0180': 161, '100': 162, '101': 163, '102': 164, '103': 165, '104': 166, '105': 167, '106': 168, '107': 169, '108': 170, '109': 171, '1010': 172, '1011': 173, '1012': 174, '1013': 175, '1014': 176, '1015': 177, '1016': 178, '1017': 179, '1018': 180, '1019': 181, '1020': 182, '1021': 183, '1022': 184, '1023': 185, '1024': 186, '1025': 187, '1026': 188, '1027': 189, '1028': 190, '1029': 191, '1030': 192, '1031': 193, '1032': 194, '1033': 195, '1034': 196, '1035': 197, '1036': 198, '1037': 199, '1038': 200, '1039': 201, '1040': 202, '1041': 203, '1042': 204, '1043': 205, '1044': 206, '1045': 207, '1046': 208, '1047': 209, '1048': 210, '1049': 211, '1050': 212, '1051': 213, '1052': 214, '1053': 215, '1054': 216, '1055': 217, '1056': 218, '1057': 219, '1058': 220, '1059': 221, '1060': 222, '1061': 223, '1062': 224, '1063': 225, '1064': 226, '1065': 227, '1066': 228, '1067': 229, '1068': 230, '1069': 231, '1070': 232, '1071': 233, '1072': 234, '1073': 235, '1074': 236, '1075': 237, '1076': 238, '1077': 239, '1078': 240, '1079': 241, '1080': 242, '110': 243, '111': 244, '112': 245, '113': 246, '114': 247, '115': 248, '116': 249, '117': 250, '118': 251, '119': 252, '1110': 253, '1111': 254, '1112': 255, '1113': 256, '1114': 257, '1115': 258, '1116': 259, '1117': 260, '1118': 261, '1119': 262, '1120': 263, '1121': 264, '1122': 265, '1123': 266, '1124': 267, '1125': 268, '1126': 269, '1127': 270, '1128': 271, '1129': 272, '1130': 273, '1131': 274, '1132': 275, '1133': 276, '1134': 277, '1135': 278, '1136': 279, '1137': 280, '1138': 281, '1139': 282, '1140': 283, '1141': 284, '1142': 285, '1143': 286, '1144': 287, '1145': 288, '1146': 289, '1147': 290, '1148': 291, '1149': 292, '1150': 293, '1151': 294, '1152': 295, '1153': 296, '1154': 297, '1155': 298, '1156': 299, '1157': 300, '1158': 301, '1159': 302, '1160': 303, '1161': 304, '1162': 305, '1163': 306, '1164': 307, '1165': 308, '1166': 309, '1167': 310, '1168': 311, '1169': 312, '1170': 313, '1171': 314, '1172': 315, '1173': 316, '1174': 317, '1175': 318, '1176': 319, '1177': 320, '1178': 321, '1179': 322, '1180': 323}\n"
     ]
    }
   ],
   "source": [
    "def create_all_words(valence:dict, nb_diff:int):\n",
    "    vocab = []\n",
    "    for k in valence.keys():\n",
    "        for i in range(0, nb_diff + 1):\n",
    "            l, r = k\n",
    "            vocab.append(str(str(l) + str(r) + str(i)))\n",
    "    return vocab\n",
    "\n",
    "valence = {\n",
    "    (0, 0) : 0,\n",
    "    (0, 1) : -10,\n",
    "    (1, 0) : 10,\n",
    "    (1, 1) : 0\n",
    "}\n",
    "\n",
    "all_words = create_all_words(valence, 80)\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "def create_all_words_enumerate(valence:dict, nb_valance_possible:int):\n",
    "    all_words = create_all_words(valence, nb_valance_possible)\n",
    "    return {token:integer for integer, token in enumerate(all_words)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        if type(text) == list:\n",
    "            return [self.str_to_int[word] for word in text]\n",
    "        else:\n",
    "            return self.str_to_int[text]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        if type(ids) == list:\n",
    "            return [self.int_to_str[id] for id in ids]\n",
    "        else:\n",
    "            return self.int_to_str[ids]\n",
    "    \n",
    "# tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "# print(tokenizer.encode([\"011\", \"102\", \"113\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "list_tempo = [1, 2, 3, 4, 5, 6]\n",
    "print(list_tempo[2: 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valance_in_seq(seq:list, valence:dict):\n",
    "    \"\"\"\n",
    "    Sum all valences in a sequence\n",
    "    Exemple:\n",
    "    seq = [(1, 0), (0, 1), (0, 1)]\n",
    "    valence = {(0, 0) : 0, (0, 1) : -10, (1, 0) : 10, (1, 1) : 0}\n",
    "    get_valance_in_seq(seq, valence) => -10\n",
    "    \"\"\"\n",
    "    return sum(valence.get(tuple(pair), None) for pair in seq)\n",
    "\n",
    "def valance_seq(valence, seq, len_seq):\n",
    "    \"\"\"\n",
    "    Create a list of valences for all sequences of size len_seq in seq\n",
    "    Exemple:\n",
    "    seq = [(1, 0), (0, 1), (0, 1), (0, 1)]\n",
    "    valence = {(0, 0) : 0, (0, 1) : -10, (1, 0) : 10, (1, 1) : 0}\n",
    "    len_seq = 2\n",
    "    valance_seq(valence, seq, len_seq) => [0, -10, -10]\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    for i in range(0, len(seq) - len_seq + 1):\n",
    "        X.append(get_valance_in_seq(seq[i: i+len_seq], valence))\n",
    "    return X\n",
    "\n",
    "def inter_valance_by_nb_seq(valence:dict, seq:list, n_seq:int):\n",
    "    \"\"\"\n",
    "    Create \"word\" for each interaction. A \"word\" is interaction + valence of the next n_seq interactions\n",
    "    Exemple:\n",
    "    valence = {(0, 0) : 0, (0, 1) : -10, (1, 0) : 10, (1, 1) : 0}\n",
    "    sed = [(1, 0), (1, 0), (0, 1), (0, 1), (1, 0), (1, 0), (1, 0), (0, 1), (0, 1)]\n",
    "    n_seq = 2\n",
    "    inter_valance_by_nb_seq(valence, seq, n_seq) => ['1020', '100', '0120', '0140', '1040', '1020', '100']\n",
    "\n",
    "    This word can be tokenize\n",
    "    \"\"\"\n",
    "    seq_valance = []\n",
    "    valance_by_seq = valance_seq(valence, seq[1:], n_seq)\n",
    "    ajout  = n_seq * abs(min(valence.values()))\n",
    "    for i, val in enumerate(valance_by_seq):\n",
    "        l, r = seq[i]\n",
    "        seq_valance.append(str(l) + str(r) + str(val + ajout))\n",
    "    # seq_valance.pop()\n",
    "    return seq_valance\n",
    "\n",
    "    \n",
    "\n",
    "valence = {\n",
    "    (0, 0) : 0,\n",
    "    (0, 1) : -10,\n",
    "    (1, 0) : 10,\n",
    "    (1, 1) : 0\n",
    "}\n",
    "\n",
    "# print(\"teste :\", valance_seq(valence, [(1, 0), (1, 0), (0, 1), (0, 1), (1, 0), (1, 0), (1, 0), (0, 1), (0, 1)], 2))\n",
    "\n",
    "# inputs = inter_valance_by_nb_seq(valence, \n",
    "#                     [(1, 0), (1, 0), (0, 1), (0, 1), (1, 0), (1, 0), (1, 0), (0, 1), (0, 1)], 2)\n",
    "# print(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(valence.values())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "#     input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "#     logits = model(input_batch)\n",
    "#     return torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "\n",
    "# def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         train_loss = calc_loss_batch(train_loader, model, device)\n",
    "#         val_loss = calc_loss_batch(val_loader, model, device, num_batches=eval_iter)\n",
    "#     model.train()\n",
    "#     return train_loss, val_loss\n",
    "\n",
    "# def train_batch(model,\n",
    "#                train_loader,\n",
    "#                val_lodaer,\n",
    "#                optimizer, \n",
    "#                device,\n",
    "#                num_epochs,\n",
    "#                eval_freq = 10):\n",
    "\n",
    "#     train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "#     token_seen, global_step = 0, -1\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "\n",
    "#         for input_batch, target_batch in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             token_seen += input_batch.numel()\n",
    "#             global_step +=1 #nombre de batch vu\n",
    "\n",
    "#             if global_step % 10 == 0:\n",
    "#                 train_loss, val_loss = evaluate_model(model, train_loader, val_lodaer, device, eval_freq)            \n",
    "#                 train_losses.append(train_loss)\n",
    "#                 val_losses.append(val_loss)\n",
    "#                 track_tokens_seen.append(token_seen)\n",
    "#                 print(\"Epoch\", epoch+1, \"global step\", global_step, \"Train loss\", train_loss,\n",
    "#                       \"Val loss\", val_loss)\n",
    "\n",
    "#     return train_losses, val_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# history = []\n",
    "# history_action = []\n",
    "# hystory_fb = []\n",
    "# history_pred = []\n",
    "# context_lenght = 30\n",
    "# nb_seq_valance = 3\n",
    "\n",
    "# env = env2()\n",
    "\n",
    "# action = 0\n",
    "# inputs = torch.tensor([[action]])\n",
    "# targets = torch.tensor([0])\n",
    "\n",
    "# valence = {\n",
    "#     (0, 0) : 0,\n",
    "#     (0, 1) : -1,\n",
    "#     (1, 0) : 1,\n",
    "#     (1, 1) : 0\n",
    "# }\n",
    "\n",
    "\n",
    "# for i in range(100):\n",
    "#     action = torch.randint(0, 2, (1,)).item()\n",
    "#     feedback = env.outcome(action)\n",
    "#     history_action.append(action)\n",
    "#     hystory_fb.append(feedback)\n",
    "#     history.append((action, feedback))\n",
    "\n",
    "\n",
    "# nb_valence_possible = nb_seq_valance * (abs(min(valence.values())) + abs(max(valence.values())))\n",
    "# print('nb_valence_possible')\n",
    "# print(nb_valence_possible)\n",
    "                                                                         \n",
    "\n",
    "# all_world_enum = create_all_words_enumerate(valence=valence,\n",
    "#             nb_valance_possible=nb_valence_possible)\n",
    "\n",
    "# tokenizer = SimpleTokenizerV1(all_world_enum)\n",
    "# voc = len(all_world_enum)\n",
    "# print('voc size')\n",
    "# print(voc)\n",
    "\n",
    "# inputs = inter_valance_by_nb_seq(valence, history, nb_seq_valance)\n",
    "# inputs = tokenizer.encode(inputs)\n",
    "\n",
    "# inputs, targets = addapt_seq(inputs, context_lenght)\n",
    "# inputs = inputs.to(device)\n",
    "# targets = targets.to(device)\n",
    "\n",
    "# print('inputs done :')\n",
    "# print(inputs)\n",
    "# print(\"target done :\")\n",
    "# print(targets)\n",
    "\n",
    "\n",
    "# mymodel = GPTModel({\n",
    "#         \"vocab_size\": voc,\n",
    "#         \"context_length\": context_lenght,\n",
    "#         \"emb_dim\": 320,\n",
    "#         \"n_heads\": 4,\n",
    "#         \"n_leayers\": 4,\n",
    "#         \"drop_rate\": 0.1,\n",
    "#         \"qkv_bias\": False,\n",
    "#         \"device\": device\n",
    "#     })\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.AdamW(mymodel.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "# mymodel.train()\n",
    "# # train_simple(mymodel, optimizer, inputs, targets, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_proba(probs, tokenizer = None):\n",
    "    plt.bar(range(len(probs)), probs)\n",
    "    plt.text(probs.index(max(probs)), max(probs), str(max(probs)), ha='center')\n",
    "    if tokenizer:\n",
    "        plt.text(probs.index(max(probs)), max(probs) + 0.5, tokenizer.decode(max(probs)), ha='center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq = ['014']\n",
    "# mymodel.eval()\n",
    "# x = mymodel(torch.tensor([tokenizer.encode(seq)]).to(device), False)\n",
    "# probs = nn.functional.softmax(x, dim=-1)\n",
    "# predi = torch.argmax(probs)\n",
    "\n",
    "# see_proba(probs[0].tolist(), None)\n",
    "# print(tokenizer.decode([predi.item()]))\n",
    "# print(f'for seq {str(seq)} the next token is {tokenizer.decode([predi.item()])}')\n",
    "\n",
    "# seq_gen = generate_sequence(mymodel, torch.tensor([tokenizer.encode(seq)]).to(device), context_lenght)\n",
    "\n",
    "# print(tokenizer.decode(seq_gen[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input = ActionOutcomeAction\n",
    "# Output = ActionOutcomeAction**Outcome**\n",
    "# Word is Action and Outcome\n",
    "\n",
    "# All words\n",
    "def create_all_words_action_outcome(action:list, feedback:list):\n",
    "    return action + feedback\n",
    "\n",
    "def create_all_words_action_outcome_enumerate(action:list, feedback:list):\n",
    "    all_words = create_all_words_action_outcome(action, feedback)\n",
    "    return {word:integer for integer, word in enumerate(all_words)}\n",
    "\n",
    "# create_all_words_action_outcome([0, 1, 2, 3], ['A', 'B'])\n",
    "# create_all_words_action_outcome_enumerate([0, 1, 2, 3], ['A', 'B'])\n",
    "# tokenizer_act_ff = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(['0', '1', '2', '3'], ['A', 'B']))\n",
    "# tempo = tokenizer_act_ff.encode(['0', 'A', '1', 'B'])\n",
    "# print(tempo)\n",
    "# print(tokenizer_act_ff.decode(tempo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3, 2, 3], ['A', 'B']]\n"
     ]
    }
   ],
   "source": [
    "My_list = [[0, 1, 2, 3], ['A', 'B']]\n",
    "My_list[0]\n",
    "\n",
    "My_list[0] = [0, 3, 2, 3]\n",
    "print(My_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# [(0, 'x'), (1, 'x'), (1, 'x'), (1, 'x'), (1, 'x'), (1, 'x'), (1, 'x'), (1, 'x')]\n",
    "# [[0], [0, x, 1], []\n",
    "def inter_action_and_feedback_size(history:list, size:int):\n",
    "    \"\"\"\n",
    "    Transform history into input and target.\n",
    "    history is a sequence of action and feedback.\n",
    "    We want to have all sequence of size size, associate with the feedback of the last action (targets).\n",
    "    Exemple:\n",
    "    history = [('0', 'x'), ('1', 'y'), ('0', 'x'), ('1', 'y'), ('0', 'x'), ('1', 'y'), ('0', 'x')]\n",
    "    size = 5\n",
    "    inter_action_and_feedback_size(history, size) => \n",
    "    inputs = [['0', 'x', '1', 'y', '0'], \n",
    "    ['1', 'y', '0', 'x', '1'],\n",
    "    ['0', 'x', '1', 'y', '0'],\n",
    "    ['1', 'y', '0', 'x', '1'],\n",
    "    ['0', 'x', '1', 'y', '0']]\n",
    "\n",
    "    targets = ['x', 'y', 'x', 'y', 'x']\n",
    "\n",
    "    \"\"\"\n",
    "    inputs, targets = [], []\n",
    "    for act, ff in history:\n",
    "        if inputs:\n",
    "            temp = inputs[-1][-int(size - 2):] + [targets[-1], act]\n",
    "            inputs.append(temp)\n",
    "        else:\n",
    "            inputs.append([act])\n",
    "        targets.append(ff)\n",
    "    return inputs[size - 1:], targets[size- 1:]\n",
    "\n",
    "inputs, targets = inter_action_and_feedback_size([('0', 'x'), ('1', 'y'), ('0', 'x')], 5)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# print(create_all_words_action_outcome_enumerate(['0', '1'], ['x', 'y']))\n",
    "# tokenizer = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(['0', '1'], ['x', 'y']))\n",
    "# print(targets)\n",
    "# print(tokenizer.encode(targets))\n",
    "\n",
    "# print(inputs)\n",
    "# for i, one_input in enumerate(inputs):\n",
    "#     inputs[i] = tokenizer.encode(inputs[i])\n",
    "\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ax': 0, 'ay': 1, 'bx': 2, 'by': 3}\n"
     ]
    }
   ],
   "source": [
    "def create_all_words_by_env(env):\n",
    "    vocab = []\n",
    "    action = env.get_actions()\n",
    "    feedback = env.get_outcomes()\n",
    "    for act in action:\n",
    "        for ff in feedback:\n",
    "            vocab.append(str(act) + str(ff))\n",
    "    return vocab\n",
    "\n",
    "def create_all_words_by_env_enumerate(env):\n",
    "    all_words = create_all_words_by_env(env)\n",
    "    return {word:integer for integer, word in enumerate(all_words)}\n",
    "\n",
    "env = env2Str()\n",
    "tokenizer = create_all_words_by_env_enumerate(env)\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x')]\n",
      "for 0 epochs, loss is 1.4357966184616089 and val_loss is 1.3223838806152344\n",
      "for 10 epochs, loss is 0.762743353843689 and val_loss is 0.7716611623764038\n",
      "for 20 epochs, loss is 0.6962653398513794 and val_loss is 0.7178255319595337\n",
      "for 30 epochs, loss is 0.6903412938117981 and val_loss is 0.7156215906143188\n",
      "for 40 epochs, loss is 0.6890097856521606 and val_loss is 0.7143491506576538\n",
      "for 50 epochs, loss is 0.6906518936157227 and val_loss is 0.7155564427375793\n",
      "for 60 epochs, loss is 0.6908204555511475 and val_loss is 0.7168586850166321\n",
      "for 70 epochs, loss is 0.6650678515434265 and val_loss is 0.6728503704071045\n",
      "for 80 epochs, loss is 0.9845237731933594 and val_loss is 1.04682457447052\n",
      "for 90 epochs, loss is 0.6982327699661255 and val_loss is 0.7714022994041443\n",
      "for 100 epochs, loss is 0.7036382555961609 and val_loss is 0.6939830183982849\n",
      "for 110 epochs, loss is 0.6964651346206665 and val_loss is 0.7358633279800415\n",
      "for 120 epochs, loss is 0.691999614238739 and val_loss is 0.7183887362480164\n",
      "for 130 epochs, loss is 0.6907417178153992 and val_loss is 0.709544837474823\n",
      "for 140 epochs, loss is 0.6864851713180542 and val_loss is 0.7146709561347961\n",
      "for 150 epochs, loss is 0.6708859205245972 and val_loss is 0.6938565969467163\n",
      "for 160 epochs, loss is 0.5723612308502197 and val_loss is 0.5663192868232727\n",
      "for 170 epochs, loss is 0.41026759147644043 and val_loss is 0.4381410479545593\n",
      "for 180 epochs, loss is 0.14951391518115997 and val_loss is 0.1704188883304596\n",
      "for 190 epochs, loss is 0.015977730974555016 and val_loss is 0.12000291049480438\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = []\n",
    "historyTest = []\n",
    "context_lenght = 21\n",
    "nb_seq_valance = 3\n",
    "\n",
    "env = env3Str()\n",
    "\n",
    "action = 0\n",
    "inputs = torch.tensor([[action]])\n",
    "targets = torch.tensor([0])\n",
    "\n",
    "valence = {\n",
    "    ('a', 'x') : 0,\n",
    "    ('a', 'y') : -1,\n",
    "    ('b', 'x') : 1,\n",
    "    ('b', 'y') : 0\n",
    "}\n",
    "\n",
    "# train\n",
    "for i in range(100):\n",
    "    action = np.random.choice(['a', 'b'])\n",
    "    feedback = env.outcome(action)\n",
    "    history.append((str(action), str(feedback)))\n",
    "\n",
    "# test\n",
    "for i in range(100):\n",
    "    action = np.random.choice(['a', 'b'])\n",
    "    feedback = env.outcome(action)\n",
    "    historyTest.append((str(action), str(feedback)))\n",
    "\n",
    "print(history)\n",
    "tmpInput, tmpTarget = inter_action_and_feedback_size(history, context_lenght)\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(['a', 'b', 'x', 'y'], []))\n",
    "\n",
    "inputs = []\n",
    "for i, one_input in enumerate(tmpInput):\n",
    "    inputs.append(tokenizer.encode(one_input))\n",
    "targets = tokenizer.encode(tmpTarget)\n",
    "\n",
    "inputs= torch.tensor(inputs, dtype=torch.long).to(device)\n",
    "targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "tmpXtest, tmpYtest = inter_action_and_feedback_size(historyTest, context_lenght)\n",
    "\n",
    "x_test = []\n",
    "for i, one_input in enumerate(tmpXtest):\n",
    "    x_test.append(tokenizer.encode(one_input))\n",
    "y_test = tokenizer.encode(tmpYtest)\n",
    "\n",
    "x_test = torch.tensor(x_test, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "mymodel = GPTModel({\n",
    "        \"vocab_size\": len(['a', 'b', 'x', 'y']),\n",
    "        \"context_length\": context_lenght,\n",
    "        \"emb_dim\": 16,\n",
    "        \"n_heads\": 4,\n",
    "        \"n_leayers\": 4,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": device\n",
    "    })\n",
    "\n",
    "optimizer = torch.optim.AdamW(mymodel.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "\n",
    "train_loss, val_loss = train_simple(mymodel, optimizer, inputs, targets, 200, x_test, y_test)\n",
    "\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for seq tensor([[1, 2, 1, 2, 1, 2, 0, 3, 0, 2, 0, 2, 1, 3, 0, 2, 0, 2, 1, 2, 0]],\n",
      "       device='cuda:0') the next token is ['y']\n"
     ]
    }
   ],
   "source": [
    "# test model \n",
    "\n",
    "seq =torch.tensor([[1, 2, 1, 2, 1, 2, 0, 3, 0, 2, 0, 2, 1, 3, 0, 2, 0, 2, 1, 2, 0]]).to(device)\n",
    "mymodel.eval()\n",
    "x = mymodel(seq, False)\n",
    "probs = nn.functional.softmax(x, dim=-1)\n",
    "predi = torch.argmax(probs)\n",
    "see_proba(probs[0].tolist(), None)\n",
    "print(f'for seq {str(seq)} the next token is {tokenizer.decode([predi.item()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, x, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, False)\n",
    "        # loss = nn.functional.cross_entropy(logits, y)\n",
    "        pred = torch.argmax(logits, dim=-1)\n",
    "        acc = (pred == y).float().mean()\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'y'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('a', 'y'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('b', 'x'), ('b', 'x'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('a', 'x'), ('b', 'y'), ('a', 'y'), ('b', 'y'), ('a', 'y'), ('a', 'x'), ('a', 'x'), ('b', 'y')]\n",
      "for 0 epochs, loss is 1.4709460735321045 and val_loss is 1.211422085762024\n",
      "for 10 epochs, loss is 0.6994051337242126 and val_loss is 0.7146291732788086\n",
      "for 20 epochs, loss is 0.6726356744766235 and val_loss is 0.7703437209129333\n",
      "for 30 epochs, loss is 0.6892731785774231 and val_loss is 0.7091843485832214\n",
      "for 40 epochs, loss is 0.687991201877594 and val_loss is 0.7133811116218567\n",
      "for 50 epochs, loss is 0.6860997676849365 and val_loss is 0.7106591463088989\n",
      "for 60 epochs, loss is 0.6839507222175598 and val_loss is 0.7106494307518005\n",
      "for 70 epochs, loss is 0.686225950717926 and val_loss is 0.7153082489967346\n",
      "for 80 epochs, loss is 0.6865352392196655 and val_loss is 0.7112587094306946\n",
      "for 90 epochs, loss is 0.6866699457168579 and val_loss is 0.7092968821525574\n",
      "0.48979589343070984\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = []\n",
    "historyTest = []\n",
    "context_lenght = 21\n",
    "nb_seq_valance = 3\n",
    "\n",
    "env = env3Str()\n",
    "\n",
    "action = 0\n",
    "inputs = torch.tensor([[action]])\n",
    "targets = torch.tensor([0])\n",
    "\n",
    "valence = {\n",
    "    ('a', 'x') : 0,\n",
    "    ('a', 'y') : -1,\n",
    "    ('b', 'x') : 1,\n",
    "    ('b', 'y') : 0\n",
    "}\n",
    "\n",
    "# test\n",
    "for i in range(1000):\n",
    "    action = np.random.choice(['a', 'b'])\n",
    "    feedback = env.outcome(action)\n",
    "    historyTest.append((str(action), str(feedback)))\n",
    "\n",
    "# TODO lunch test to see evolued accuracy and loss in function of the number of training\n",
    "# train\n",
    "for i in range(100):\n",
    "    action = np.random.choice(['a', 'b'])\n",
    "    feedback = env.outcome(action)\n",
    "    history.append((str(action), str(feedback)))\n",
    "\n",
    "\n",
    "print(history)\n",
    "tmpInput, tmpTarget = inter_action_and_feedback_size(history, context_lenght)\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(['a', 'b', 'x', 'y'], []))\n",
    "\n",
    "inputs = []\n",
    "for i, one_input in enumerate(tmpInput):\n",
    "    inputs.append(tokenizer.encode(one_input))\n",
    "targets = tokenizer.encode(tmpTarget)\n",
    "\n",
    "inputs= torch.tensor(inputs, dtype=torch.long).to(device)\n",
    "targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "\n",
    "tmpXtest, tmpYtest = inter_action_and_feedback_size(historyTest, context_lenght)\n",
    "\n",
    "x_test = []\n",
    "for i, one_input in enumerate(tmpXtest):\n",
    "    x_test.append(tokenizer.encode(one_input))\n",
    "y_test = tokenizer.encode(tmpYtest)\n",
    "\n",
    "x_test = torch.tensor(x_test, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "mymodel = GPTModel({\n",
    "        \"vocab_size\": len(['a', 'b', 'x', 'y']),\n",
    "        \"context_length\": context_lenght,\n",
    "        \"emb_dim\": 16 * 2,\n",
    "        \"n_heads\": 4,\n",
    "        \"n_leayers\": 4,\n",
    "        \"drop_rate\": 0.2,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": device\n",
    "    })\n",
    "\n",
    "optimizer = torch.optim.AdamW(mymodel.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "\n",
    "train_loss, val_loss = train_simple(mymodel, optimizer, inputs, targets, 100, x_test, y_test)\n",
    "\n",
    "print(accuracy(mymodel, x_test, y_test))\n",
    "\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot_loss(train_loss, val_loss, path:str=\"img_loss\", title:str=\"\"):\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    # if title == \"\" title = 'loss' + nb img save\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    if title == \"\":\n",
    "        title = 'loss' + str(len(os.listdir(path)))\n",
    "    plt.savefig(path + '/' + title + '.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') the next token is ['x']\n"
     ]
    }
   ],
   "source": [
    "# test model \n",
    "# seq =torch.tensor([tokenizer.encode(['b', 'x', 'a', 'x', 'b', 'x', 'b', 'y', 'a', 'x', 'b', 'x', 'a', 'x', 'b', 'x', 'b', 'x', 'a', 'y', 'a'])]).to(device)\n",
    "# Context = 21\n",
    "seq =torch.tensor([tokenizer.encode(['b', 'x', 'a' , 'y', 'a', 'y', 'a', 'x', 'a', 'x', 'a', 'x', 'a', 'x', 'a', 'x', 'a', 'x', 'b', 'x', 'b'])]).to(device)\n",
    "#=============================================================^noise=================================================================^noise\n",
    "# Context = 5\n",
    "# seq =torch.tensor([tokenizer.encode(['b', 'x', 'a' , 'y', 'b'])]).to(device)\n",
    "\n",
    "# Second noise have an impact on the prediction\n",
    "# Not the first one\n",
    "mymodel.eval()\n",
    "x = mymodel(seq, False)\n",
    "probs = nn.functional.softmax(x, dim=-1)\n",
    "predi = torch.argmax(probs)\n",
    "see_proba(probs[0].tolist(), None)\n",
    "print(f'for seq {str(seq)} the next token is {tokenizer.decode([predi.item()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_evolued_train_loss(train_loss):\n",
    "    for i, loss_list in enumerate(train_loss):\n",
    "        plt.plot(loss_list, label=f'Iteration {i}', color=plt.cm.viridis(i / len(train_loss)))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b']\n"
     ]
    }
   ],
   "source": [
    "# def make_tree_prediction(model, env, max_deep:int, last_sequence:list, seq_predi:list=[]):\n",
    "#     if max_deep == 0:\n",
    "#         return []\n",
    "#     # input(f'{last_sequence} | {seq_predi}')\n",
    "#     max_deep -= 1\n",
    "    \n",
    "#     fake_tree = []\n",
    "#     model.eval()\n",
    "#     for act in env.get_actions():\n",
    "#         seq_to_predict = tokenizer.encode(last_sequence + [act])\n",
    "#         seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.long).to(device)\n",
    "#         # input(f'seq to pass in model {seq_to_predict}')\n",
    "#         x = model(seq_to_predict, False)\n",
    "#         probs = nn.functional.softmax(x, dim=-1)\n",
    "#         predi = tokenizer.decode([torch.argmax(probs).item()])\n",
    "#         new__last_seq = last_sequence[2:] + [act, predi]\n",
    "#         new_seq_predi = seq_predi + [act, predi]\n",
    "#         # input(f'new last seq {new__last_seq} | new seq pred {new_seq_predi}')\n",
    "#         fake_tree.append(new_seq_predi) \n",
    "#         fake_tree += (make_tree_prediction(model, env, max_deep, new__last_seq, new_seq_predi))\n",
    "\n",
    "#     return fake_tree\n",
    "\n",
    "\n",
    "def make_tree_prediction(model, env, valence:dict, max_deep: int, last_sequence: list):\n",
    "    model.eval()\n",
    "    stack = [(last_sequence, [], 1)]\n",
    "    fake_tree = {}\n",
    "\n",
    "    while stack:\n",
    "        last_sequence, seq_predi, value = stack.pop()\n",
    "        # input(f'start wihle {stack} | \\n last seq {last_sequence} \\n seq predi {seq_predi}')\n",
    "        \n",
    "        if len(seq_predi) // 2 >= max_deep:\n",
    "            continue\n",
    "\n",
    "        for act in env.get_actions():\n",
    "            seq_to_predict = tokenizer.encode(last_sequence + [act])\n",
    "            seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.long).to(device)\n",
    "            \n",
    "            x = model(seq_to_predict, False)\n",
    "            probs = nn.functional.softmax(x, dim=-1)\n",
    "            predi = tokenizer.decode([torch.argmax(probs).item()])\n",
    "            best_proba = probs[0][torch.argmax(probs)].item()\n",
    "            # print(f'best proba {best_proba}')\n",
    "            \n",
    "            new_last_seq = last_sequence[2:] + [act, predi]\n",
    "            new_seq_predi = seq_predi + [act, predi]\n",
    "            new_value = value + best_proba * valence[(act, predi)]\n",
    "            fake_tree[str(new_seq_predi)] = new_value\n",
    "            stack.append((new_last_seq, new_seq_predi, new_value))\n",
    "\n",
    "    return fake_tree\n",
    "\n",
    "\n",
    "def model_in_env(model, tokenizer:SimpleTokenizerV1, env, valance:dict, iter:int, rand_iter:int = 10, path_save = \"loss_\", _lr=1e-2, weight_decay=1e-2, max_depth=3):\n",
    "    history = []\n",
    "    context_lenght = model.cfg[\"context_length\"]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=_lr, weight_decay=weight_decay)\n",
    "    evolued_train_loss = []\n",
    "    evolued_val_loss = []\n",
    "    all_predit = None\n",
    "    good_predicts:list[bool] = []\n",
    "    historyTest = []\n",
    "\n",
    "    for i in range(1000):\n",
    "        action = np.random.choice(env.get_actions())\n",
    "        feedback = env.outcome(action)\n",
    "        historyTest.append((str(action), str(feedback)))\n",
    "\n",
    "    tmpXtest, tmpYtest = inter_action_and_feedback_size(historyTest, context_lenght)\n",
    "\n",
    "    x_test = []\n",
    "    for i, one_input in enumerate(tmpXtest):\n",
    "        x_test.append(tokenizer.encode(one_input))\n",
    "    y_test = tokenizer.encode(tmpYtest)\n",
    "\n",
    "    x_val = torch.tensor(x_test, dtype=torch.long).to(device)\n",
    "    y_val = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(0, iter):\n",
    "        if i < rand_iter:\n",
    "            action = np.random.choice(env.get_actions())\n",
    "        else:\n",
    "            # train model\n",
    "            if i % 10 == 0:\n",
    "                model.apply(model._init_weights)\n",
    "                tmpInput, tmpTarget = inter_action_and_feedback_size(history, context_lenght)\n",
    "                inputs = []\n",
    "                for one_input in tmpInput:\n",
    "                    inputs.append(tokenizer.encode(one_input))\n",
    "                targets = tokenizer.encode(tmpTarget)\n",
    "\n",
    "                inputs= torch.tensor(inputs, dtype=torch.long).to(device)\n",
    "                targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "                time_start = time.time()\n",
    "                train_loss, val_loss = train_simple(model, optimizer, inputs, targets, 1000, verbose=False, x_val=x_val, y_val=y_val)\n",
    "                save_plot_loss(train_loss, val_loss, path=path_save, title=f'loss_{i}')\n",
    "                time_end = time.time()\n",
    "                print(f'time to train {time_end - time_start}')\n",
    "                evolued_train_loss.append(train_loss)\n",
    "                evolued_val_loss.append(val_loss)\n",
    "\n",
    "            # predict next action\n",
    "            tmp = tmpInput[-1][2:] + [tmpTarget[-1]]\n",
    "            time_start = time.time()\n",
    "            all_predit:dict = make_tree_prediction(\n",
    "                    model=model, env=env, max_deep=max_depth, valence=valance,\n",
    "                    last_sequence= tmp)\n",
    "            print(f'for this sequence {tmp} all prediction {all_predit}')\n",
    "            interact_max_val = max(all_predit.items())[0]\n",
    "            interact_max_val = eval(interact_max_val)\n",
    "            # input(f'max is {interact_max_val}')\n",
    "            action = interact_max_val[0]\n",
    "            predict = interact_max_val[1]\n",
    "            # input(f'action {action}')\n",
    "            # input(f'predict {predict}')\n",
    "            # input(f'for this sequence {tmp} all prediction {test}')\n",
    "            time_end = time.time()\n",
    "            print(f'time to make tree {time_end - time_start}')\n",
    "            # print(f'for this sequence {tmp}')\n",
    "            # print(f'all prediction {test}')\n",
    "            # max_potentiel = -np.inf\n",
    "            # for act in env.get_actions():\n",
    "            #     seq_to_predict = tokenizer.encode(tmpInput[-1][2:] + [tmpTarget[-1], act])\n",
    "            #     seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.long).to(device)\n",
    "            #     x = model(seq_to_predict, False)\n",
    "            #     probs = nn.functional.softmax(x, dim=-1)\n",
    "            #     predi = torch.argmax(probs)\n",
    "            #     val_predi = valance[(act, tokenizer.decode([predi.item()]))]\n",
    "            #     if val_predi > max_potentiel:\n",
    "            #         max_potentiel = val_predi\n",
    "            #         action = act\n",
    "            #     elif val_predi == max_potentiel:\n",
    "            #         action = np.random.choice([action, act])\n",
    "\n",
    "            # action = np.random.choice(env.get_actions()) # TODO delete this\n",
    "            \n",
    "\n",
    "        feedback = env.outcome(action)\n",
    "        if all_predit is not None:\n",
    "            list_keys = all_predit.keys()\n",
    "            # list_keys = [eval(key) for key in list_keys]\n",
    "            # input(f'I try to find if {str([str(action), feedback])} in {list_keys}')\n",
    "            good_predicts.append(str([str(action), feedback]) in list_keys)\n",
    "            print(f'% good predict : {sum(good_predicts[-10:]) * 10 if len(good_predicts) >= 10 else 0}')\n",
    "        print(f'iteration {i} action {action} feedback {feedback} predict {all_predit.keys() if all_predit else \"None\"}')\n",
    "        history.append((str(action), str(feedback)))\n",
    "\n",
    "    return evolued_train_loss, evolued_val_loss, good_predicts\n",
    "\n",
    "\n",
    "mymodel = GPTModel({\n",
    "        \"vocab_size\": len(['a', 'b', 'x', 'y']),\n",
    "        \"context_length\": 21,\n",
    "        \"emb_dim\": 16 *2,\n",
    "        \"n_heads\": 1, # 4\n",
    "        \"n_leayers\": 1, # 4\n",
    "        \"drop_rate\": 0.2,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": device\n",
    "    })\n",
    "\n",
    "\n",
    "valence = {\n",
    "    ('a', 'x') : -10,\n",
    "    ('a', 'y') : 10,\n",
    "    ('b', 'x') : -10,\n",
    "    ('b', 'y') : 10\n",
    "}\n",
    "\n",
    "env = env6Str()\n",
    "\n",
    "print(env.get_actions())\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(['a', 'b', 'x', 'y'], []))\n",
    "\n",
    "# evolued_train_loss, evolued_val_loss = model_in_env(mymodel, tokenizer, env, valence, 500, 100)\n",
    "\n",
    "# see_evolued_train_loss(evolued_train_loss)\n",
    "# see_evolued_train_loss(evolued_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicRt = {\n",
    "    str(['a',' b']): 1111\n",
    "}\n",
    "\n",
    "if str(['a',' c']) in dicRt:\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see_evolued_train_loss(evolued_train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forward', 'turn_left', 'turn_right', 'feel_front', 'feel_left', 'feel_right']\n"
     ]
    }
   ],
   "source": [
    "env = small_loop(1, 1, 0)\n",
    "\n",
    "mymodel = GPTModel({\n",
    "        \"vocab_size\": len(env.get_actions()) + len(env.get_outcomes()),\n",
    "        \"context_length\": 51,\n",
    "        \"emb_dim\": 16 * 4,\n",
    "        \"n_heads\": 1, # 4\n",
    "        \"n_leayers\": 1, # 4\n",
    "        \"drop_rate\": 0.2,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": device\n",
    "    })\n",
    "\n",
    "\n",
    "valence = {\n",
    "    ('forward', 'wall') : -10,\n",
    "    ('forward', 'empty') : 10,\n",
    "    ('turn_left', 'wall') : -1000000, # Can not produce\n",
    "    ('turn_left', 'empty') : -3,\n",
    "    ('turn_right', 'wall') : -1000000, # Can not produce\n",
    "    ('turn_right', 'empty') : -3,\n",
    "    ('feel_front', 'wall') : -3,\n",
    "    ('feel_front', 'empty') : -2,\n",
    "    ('feel_left', 'wall') : -3,\n",
    "    ('feel_left', 'empty') : -2,\n",
    "    ('feel_right', 'wall') : -3,\n",
    "    ('feel_right', 'empty') : -2\n",
    "\n",
    "}\n",
    "\n",
    "print(env.get_actions())\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(create_all_words_action_outcome_enumerate(env.get_actions(), env.get_outcomes()))\n",
    "\n",
    "# evolued_train_loss, evolued_val_loss, _ = model_in_env(mymodel, tokenizer, env, valence, 500, 100, \"loss_SL_samll_embeding\", _lr=1e-3, weight_decay=1e-2, max_depth=4)\n",
    "\n",
    "# see_evolued_train_loss(evolued_train_loss)\n",
    "# see_evolued_train_loss(evolued_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage superviser\n",
    "## Decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 59\u001b[0m\n\u001b[1;32m     40\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SimpleTokenizerV1(\n\u001b[1;32m     41\u001b[0m     create_all_words_action_outcome_enumerate(\n\u001b[1;32m     42\u001b[0m         env\u001b[38;5;241m.\u001b[39mget_actions(), env\u001b[38;5;241m.\u001b[39mget_outcomes()))\n\u001b[1;32m     44\u001b[0m valence \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     45\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwall\u001b[39m\u001b[38;5;124m'\u001b[39m) : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     46\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mempty\u001b[39m\u001b[38;5;124m'\u001b[39m) : \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeel_right\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mempty\u001b[39m\u001b[38;5;124m'\u001b[39m) : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     57\u001b[0m }\n\u001b[0;32m---> 59\u001b[0m x_fit, y_fit, x_val, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mmake_data_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m              \u001b[49m\u001b[43mrand_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m              \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# analyse data\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Correlation Matrix\u001b[39;00m\n\u001b[1;32m     66\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(x_fit)\n",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m, in \u001b[0;36mmake_data_set\u001b[0;34m(tokenizer, env, rand_iter)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_data_set\u001b[39m(tokenizer, env, rand_iter:\u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Create data val\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Not implemented"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "def make_data_set(tokenizer, env, rand_iter:int = 100):\n",
    "    history = []\n",
    "\n",
    "    # Create data val\n",
    "    historyTest = []\n",
    "    for i in range(1000):\n",
    "        action = np.random.choice(env.get_actions())\n",
    "        feedback = env.outcome(action)\n",
    "        historyTest.append((str(action), str(feedback)))\n",
    "\n",
    "    tmpXtest, tmpYtest = inter_action_and_feedback_size(historyTest, context_lenght)\n",
    "    x_test = []\n",
    "    for i, one_input in enumerate(tmpXtest):\n",
    "        x_test.append(tokenizer.encode(one_input))\n",
    "    y_test = tokenizer.encode(tmpYtest)\n",
    "\n",
    "    # Create first rand sequence\n",
    "    for i in range(rand_iter):\n",
    "        action = np.random.choice(env.get_actions())\n",
    "        feedback = env.outcome(action)\n",
    "        history.append((str(action), str(feedback)))\n",
    "\n",
    "    tmpXfit, tmpYfit = inter_action_and_feedback_size(history, context_lenght)\n",
    "    x_fit = []\n",
    "    for i, one_input in enumerate(tmpXfit):\n",
    "        x_fit.append(tokenizer.encode(one_input))\n",
    "    y_fit = tokenizer.encode(tmpYfit)\n",
    "\n",
    "    return x_fit, y_fit, x_test, y_test\n",
    "\n",
    "\n",
    "# env = small_loop(1, 1, 0)\n",
    "env = env6Str()\n",
    "tokenizer = SimpleTokenizerV1(\n",
    "    create_all_words_action_outcome_enumerate(\n",
    "        env.get_actions(), env.get_outcomes()))\n",
    "\n",
    "valence = {\n",
    "    ('forward', 'wall') : -10,\n",
    "    ('forward', 'empty') : 10,\n",
    "    ('turn_left', 'wall') : -1000000, # Can not produce\n",
    "    ('turn_left', 'empty') : -3,\n",
    "    ('turn_right', 'wall') : -1000000, # Can not produce\n",
    "    ('turn_right', 'empty') : -3,\n",
    "    ('feel_front', 'wall') : -3,\n",
    "    ('feel_front', 'empty') : -2,\n",
    "    ('feel_left', 'wall') : -3,\n",
    "    ('feel_left', 'empty') : -2,\n",
    "    ('feel_right', 'wall') : -3,\n",
    "    ('feel_right', 'empty') : -2\n",
    "}\n",
    "\n",
    "x_fit, y_fit, x_val, y_val = make_data_set(env=env,\n",
    "              rand_iter=200,\n",
    "              tokenizer=tokenizer,\n",
    "              )\n",
    "\n",
    "# analyse data\n",
    "# Correlation Matrix\n",
    "df = pd.DataFrame(x_fit)\n",
    "df['target'] = y_fit\n",
    "corr = df.corr()\n",
    "print(corr)\n",
    "\n",
    "# Count y_fit\n",
    "print(\"Count y_fit \")\n",
    "print(pd.Series(y_fit).value_counts())\n",
    "\n",
    "\n",
    "# Test with decision tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x_fit, y_fit)\n",
    "y_pred = clf.predict(x_val)\n",
    "print(f'Tree : accuracy {sk.metrics.accuracy_score(y_val, y_pred)}')\n",
    "# Confusion matrix\n",
    "print(sk.metrics.confusion_matrix(y_val, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe utilise plusieurs couche cachÃ©es\n",
    "    Pour dÃ©finir un rÃ©seau de neurones profonds\n",
    "    \n",
    "    - Il y a une couche d'entrÃ©e\n",
    "    - Plusieurs couches cachÃ©es dÃ©pendant de la liste hidden_size\n",
    "    - Une couche de sortie\n",
    "\n",
    "    les poids sont initialise de maniÃ¨re alÃ©atoire\n",
    "    \"\"\"\n",
    "    \n",
    "    name = \"DeepNetwork\"\n",
    "    \n",
    "    def __init__(self, input_size:int, hidden_size:list[int], output_size:int):\n",
    "        \"\"\"\n",
    "        Constructeur de la classe, applique le constructeur de la classe parent\n",
    "        Et crÃ©e les couches du rÃ©seau :\n",
    "        - 1 couche d'entrÃ©e\n",
    "        - Plusieurs couches cachÃ©es\n",
    "        - 1 couche de sortie\n",
    "        \n",
    "        Les couches cachÃ©es sont dÃ©finies mis en place avec nn.ModuleList, pour permettre\n",
    "        l'ajout dynamique de couches cachÃ©es. (Concretement nous pouvons passer le \n",
    "        modele.to(deviece) et la sauvgarde des poids par torch)\n",
    "\n",
    "        :param input_size: la taille des donnÃ©es d'entrÃ©e\n",
    "        :param output_size: la taille des donnÃ©es de sortie\n",
    "        :param hidden_size: la taille des couches cachÃ©es, \n",
    "        le nombre d'Ã©lÃ©ments dans la liste correspond au nombre de couches cachÃ©es       \n",
    "        \"\"\"\n",
    "        super(DeepNetwork, self).__init__()\n",
    "        # nn.Linear initialise les poids de maniÃ¨re alÃ©atoire\n",
    "        print(\"liste hidden init\",hidden_size)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for size in range(len(hidden_size) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size[size], hidden_size[size + 1]))\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = nn.functional.relu(layer(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, test_loader: torch.utils.data.DataLoader, loss_funct) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    MÃ©thode d'Ã©valuation du modÃ¨le\n",
    "\n",
    "    :param model: le modÃ¨le Ã  Ã©valuer\n",
    "    :param test_loader: le lecteur de donnÃ©es de test\n",
    "    :param loss_funct: la fonction de perte Ã  utiliser pour l'Ã©valuation\n",
    "    :param device: le device sur lequel effectuer les calculs\n",
    "    :return: une tuple contenant (le taux de rÃ©ussite, la perte moyenne)\n",
    "    \"\"\"\n",
    "    acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    model.to(device)\n",
    "    model.eval()  # Pour dÃ©sactiver les couches dropout ou batchnorm\n",
    "    with torch.no_grad():  # Pas besoin de calculer les gradients en mode Ã©valuation\n",
    "        for x, t in test_loader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            \n",
    "            # PrÃ©dictions\n",
    "            y = model(x)\n",
    "            \n",
    "            # Calcul de la loss\n",
    "            loss = loss_funct(y, t)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calcul de la prÃ©cision\n",
    "            if len(t.shape) > 1 and t.shape[1] > 1:  # Si les labels sont one-hot encodÃ©s\n",
    "                t = torch.argmax(t, dim=1)  # On convertit en indices de classes\n",
    "            acc += (torch.argmax(y, 1) == t).sum().item()\n",
    "            \n",
    "            total_samples += t.size(0)\n",
    "\n",
    "    avg_acc = acc / total_samples\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "\n",
    "    return avg_acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module,\n",
    "        train_loader:torch.utils.data.DataLoader,\n",
    "        validate_loader:torch.utils.data.DataLoader,\n",
    "        optimizer:torch.optim.SGD,\n",
    "        loss_func:torch.nn.MSELoss,\n",
    "        nb_epochs:int,\n",
    "        print_:bool=False\n",
    "        ) -> Tuple[float, nn.Module]:\n",
    "    \"\"\"\n",
    "    MÃ©thode d'entraÃ®nement du modÃ¨le\n",
    "\n",
    "    Pour chaque Ã©poque:\n",
    "    - on entraÃ®ne le modÃ¨le sur les donnÃ©es d'apprentissage\n",
    "    - Puis on Ã©value le modÃ¨le sur les donnÃ©es de validation\n",
    "\n",
    "    Au final nous renvoyons le modÃ¨le avec le meilleur taux de rÃ©ussite\n",
    "    (Nous Ã©vitons le sur-apprentissage tout en testant l'entiÃ¨retÃ© des Ã©poques)\n",
    "\n",
    "    :param model: le modÃ¨le Ã  entraÃ®ner\n",
    "    :param train_loader: le lecteur de donnÃ©es d'apprentissage\n",
    "    :param validate_loader: le lecteur de donnÃ©es de validation\n",
    "    :param optimizer: l'optimiseur liÃ© au modÃ¨le\n",
    "    :param loss_func: la fonction de loss\n",
    "    :param nb_epochs: le nombre d'Ã©poques\n",
    "    :param print_: afficher ou non les rÃ©sultats\n",
    "    :return: le taux de rÃ©ussite final et le meilleur modÃ¨le\n",
    "    \"\"\"\n",
    "    meilleur_acc:int = 0\n",
    "    meilleur_model = None\n",
    "    model.to(device)\n",
    "    loss_func.to(device)\n",
    "    for epoch in range(nb_epochs):\n",
    "        for x,t in train_loader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            # on calcule la sortie du modÃ¨le\n",
    "            y = model(x)\n",
    "            # on met Ã  jour les poids\n",
    "            if t.dim() > 1 and t.shape[1] > 1:  # VÃ©rifie si t est en one-hot encoding\n",
    "                t = torch.argmax(t, dim=1)  # Convertit en indices de classes\n",
    "\n",
    "            loss:torch.Tensor = loss_func(y,t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        acc, _ = evaluate(model=model, test_loader=validate_loader, loss_funct=loss_func)\n",
    "        if acc > meilleur_acc:\n",
    "            meilleur_acc = acc\n",
    "            meilleur_model = model\n",
    "\n",
    "        if print_:\n",
    "            print(f'Epoch {epoch+1}/{nb_epochs}, Accuracy: {acc}')\n",
    "    return meilleur_acc, meilleur_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env6Str()\n",
    "tokenizer = SimpleTokenizerV1(\n",
    "    create_all_words_action_outcome_enumerate(\n",
    "        env.get_actions(), env.get_outcomes()))\n",
    "\n",
    "\n",
    "# CrÃ©ation des donnÃ©es\n",
    "x_fit, y_fit, x_val, y_val = make_data_set(env=env,\n",
    "              rand_iter=200,\n",
    "              tokenizer=tokenizer,\n",
    "              )\n",
    "\n",
    "x_fit = torch.tensor(x_fit, dtype=torch.float).to(device)\n",
    "y_fit = torch.tensor(y_fit, dtype=torch.long).to(device)\n",
    "\n",
    "# Transforme y_fit en one hot\n",
    "y_fit = torch.nn.functional.one_hot(y_fit.to(torch.int64), len(env.get_actions()) + len(env.get_outcomes())).to(torch.float)\n",
    "\n",
    "x_val = torch.tensor(x_val, dtype=torch.float).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "# CrÃ©ation des DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(x_fit, y_fit), batch_size=32, shuffle=True\n",
    ")\n",
    "validate_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(x_val, y_val), batch_size=32, shuffle=False\n",
    ")\n",
    "\n",
    "# CrÃ©ation du modÃ¨le\n",
    "model = DeepNetwork(\n",
    "    input_size=len(x_fit[0]),\n",
    "    hidden_size=[64, 128, 64, 16],\n",
    "    output_size=len(env.get_actions()) + len(env.get_outcomes())\n",
    ")\n",
    "\n",
    "# CrÃ©ation de l'optimiseur\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "# CrÃ©ation de la fonction de perte\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# EntraÃ®nement du modÃ¨le\n",
    "best_acc, best_model = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    validate_loader=validate_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func,\n",
    "    nb_epochs=100,\n",
    "    print_=True\n",
    ")\n",
    "\n",
    "# Ã‰valuation du modÃ¨le\n",
    "acc, loss = evaluate(model=best_model, test_loader=validate_loader, loss_funct=loss_func)\n",
    "print(f'Accuracy: {acc}, Loss: {loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin de la biffurcasion on reprend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 0 epochs, loss is 1.8376810550689697 and val_loss is 1.8720433712005615\n",
      "for 1 epochs, loss is 1.6691237688064575 and val_loss is 1.7822015285491943\n",
      "for 2 epochs, loss is 1.4468965530395508 and val_loss is 1.702410340309143\n",
      "for 3 epochs, loss is 1.2476377487182617 and val_loss is 1.6349009275436401\n",
      "for 4 epochs, loss is 1.0759906768798828 and val_loss is 1.406278371810913\n",
      "for 5 epochs, loss is 0.4128521680831909 and val_loss is 1.4644794464111328\n",
      "for 6 epochs, loss is 0.26396575570106506 and val_loss is 1.538109302520752\n",
      "for 7 epochs, loss is 0.2062036246061325 and val_loss is 1.6027919054031372\n",
      "for 8 epochs, loss is 0.17597226798534393 and val_loss is 1.6581454277038574\n",
      "for 9 epochs, loss is 0.158736452460289 and val_loss is 1.7100735902786255\n",
      "for 10 epochs, loss is 0.13932417333126068 and val_loss is 1.758804440498352\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4686), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2767), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.5846), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2652), \"['a', 'x', 'a', 'x']\": np.float64(-7.4947), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2741), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0117), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2795), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0111), \"['a', 'x', 'a', 'y']\": np.float64(0.3182), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.578), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2816), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.6938), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2698), \"['a', 'x', 'b', 'x']\": np.float64(-7.6193), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.265), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0113), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2701), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0107), \"['a', 'x', 'b', 'y']\": np.float64(0.3075), \"['a', 'x']\": np.float64(-8.6558), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2746), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0117), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2796), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0113), \"['a', 'y', 'a', 'x']\": np.float64(-0.3182), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0116), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0118), \"['a', 'y', 'a', 'y']\": np.float64(0.0135), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2796), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.012), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2845), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0115), \"['a', 'y', 'b', 'x']\": np.float64(-0.3239), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0112), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0114), \"['a', 'y', 'b', 'y']\": np.float64(0.013), \"['a', 'y']\": np.float64(0.368), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.5813), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2817), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.6969), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2698), \"['b', 'x', 'a', 'x']\": np.float64(-7.6227), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2791), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.012), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2845), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0113), \"['b', 'x', 'a', 'y']\": np.float64(0.3239), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.691), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2866), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.8064), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2744), \"['b', 'x', 'b', 'x']\": np.float64(-7.7477), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2696), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0115), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2747), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0109), \"['b', 'x', 'b', 'y']\": np.float64(0.3128), \"['b', 'x']\": np.float64(-8.8017), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2667), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0114), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2714), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0109), \"['b', 'y', 'a', 'x']\": np.float64(-0.3089), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0113), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0115), \"['b', 'y', 'a', 'y']\": np.float64(0.0131), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2714), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0116), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2761), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0111), \"['b', 'y', 'b', 'x']\": np.float64(-0.3143), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0109), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0111), \"['b', 'y', 'b', 'y']\": np.float64(0.0127), \"['b', 'y']\": np.float64(0.3571)}\n",
      "It s train loss bro [1.8376810550689697, 1.6691237688064575, 1.4468965530395508, 1.2476377487182617, 1.0759906768798828, 0.4128521680831909, 0.26396575570106506, 0.2062036246061325, 0.17597226798534393, 0.158736452460289, 0.13932417333126068]\n",
      "% good predict : -1\n",
      "\u001b[0;34miteration 0 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.9023194313049316 and val_loss is 1.7517045736312866\n",
      "for 1 epochs, loss is 2.998197555541992 and val_loss is 1.7376391887664795\n",
      "for 2 epochs, loss is 2.9364049434661865 and val_loss is 1.719014048576355\n",
      "for 3 epochs, loss is 2.855684518814087 and val_loss is 1.6969044208526611\n",
      "for 4 epochs, loss is 2.759620428085327 and val_loss is 1.6719741821289062\n",
      "for 5 epochs, loss is 2.649564027786255 and val_loss is 1.644676685333252\n",
      "for 6 epochs, loss is 2.525425910949707 and val_loss is 1.6153477430343628\n",
      "for 7 epochs, loss is 2.3861913681030273 and val_loss is 1.5842595100402832\n",
      "for 8 epochs, loss is 2.2303225994110107 and val_loss is 1.5516576766967773\n",
      "for 9 epochs, loss is 2.0561726093292236 and val_loss is 1.5177903175354004\n",
      "for 10 epochs, loss is 1.8628036975860596 and val_loss is 1.4829381704330444\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.117), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5612), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.8317), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2516), \"['a', 'x', 'a', 'x']\": np.float64(-3.4471), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5583), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1479), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7463), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0665), \"['a', 'x', 'a', 'y']\": np.float64(0.9089), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.767), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.778), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.7787), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3349), \"['a', 'x', 'b', 'x']\": np.float64(-4.5983), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2446), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0687), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3339), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0297), \"['a', 'x', 'b', 'y']\": np.float64(0.4064), \"['a', 'x']\": np.float64(-5.5983), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7196), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1907), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9619), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0858), \"['a', 'y', 'a', 'x']\": np.float64(-1.1718), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1897), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0503), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2534), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0227), \"['a', 'y', 'a', 'y']\": np.float64(0.3088), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.94), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2643), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2829), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1142), \"['a', 'y', 'b', 'x']\": np.float64(-1.5623), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0833), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0234), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1136), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0101), \"['a', 'y', 'b', 'y']\": np.float64(0.1385), \"['a', 'y']\": np.float64(1.9029), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.0561), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8114), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.084), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3616), \"['b', 'x', 'a', 'x']\": np.float64(-4.9696), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8576), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2276), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1454), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1017), \"['b', 'x', 'a', 'y']\": np.float64(1.3943), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.0852), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1481), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.5705), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.492), \"['b', 'x', 'b', 'x']\": np.float64(-6.7761), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3603), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1012), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.491), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0435), \"['b', 'x', 'b', 'y']\": np.float64(0.5975), \"['b', 'x']\": np.float64(-8.2471), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2649), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0703), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3538), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0315), \"['b', 'y', 'a', 'x']\": np.float64(-0.4308), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0743), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0197), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0992), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0088), \"['b', 'y', 'a', 'y']\": np.float64(0.1208), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3539), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0994), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4823), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0428), \"['b', 'y', 'b', 'x']\": np.float64(-0.587), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0313), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0088), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0426), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0038), \"['b', 'y', 'b', 'y']\": np.float64(0.0519), \"['b', 'y']\": np.float64(0.7148)}\n",
      "It s train loss bro [2.9023194313049316, 2.998197555541992, 2.9364049434661865, 2.855684518814087, 2.759620428085327, 2.649564027786255, 2.525425910949707, 2.3861913681030273, 2.2303225994110107, 2.0561726093292236, 1.8628036975860596]\n",
      "% good predict : -1\n",
      "\u001b[0;34miteration 1 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.43005794286727905 and val_loss is 1.1796880960464478\n",
      "for 1 epochs, loss is 1.7730414867401123 and val_loss is 1.1598186492919922\n",
      "for 2 epochs, loss is 1.713325023651123 and val_loss is 1.1458380222320557\n",
      "for 3 epochs, loss is 1.5559024810791016 and val_loss is 1.1364926099777222\n",
      "for 4 epochs, loss is 1.3539841175079346 and val_loss is 1.1310127973556519\n",
      "for 5 epochs, loss is 1.1394248008728027 and val_loss is 1.12888503074646\n",
      "for 6 epochs, loss is 0.9300751686096191 and val_loss is 1.1267234086990356\n",
      "for 7 epochs, loss is 0.32549163699150085 and val_loss is 1.1277403831481934\n",
      "for 8 epochs, loss is 0.2887967526912689 and val_loss is 1.1342148780822754\n",
      "for 9 epochs, loss is 0.2706235945224762 and val_loss is 1.1464459896087646\n",
      "for 10 epochs, loss is 0.2544107735157013 and val_loss is 1.1641565561294556\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.6888), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7367), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7102), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7178), \"['a', 'x', 'a', 'x']\": np.float64(-5.9474), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8345), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1311), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8383), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1277), \"['a', 'x', 'a', 'y']\": np.float64(1.0585), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.72), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7416), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7415), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7225), \"['a', 'x', 'b', 'x']\": np.float64(-5.987), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8101), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1273), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8138), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.124), \"['a', 'x', 'b', 'y']\": np.float64(1.0276), \"['a', 'x']\": np.float64(-7.7115), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8281), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1301), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8318), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1268), \"['a', 'y', 'a', 'x']\": np.float64(-1.0503), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1473), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0231), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.148), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0225), \"['a', 'y', 'a', 'y']\": np.float64(0.1868), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8336), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.131), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8374), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1276), \"['a', 'y', 'b', 'x']\": np.float64(-1.0573), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.143), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0225), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1436), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0219), \"['a', 'y', 'b', 'y']\": np.float64(0.1813), \"['a', 'y']\": np.float64(1.3617), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.7167), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7411), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7383), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.722), \"['b', 'x', 'a', 'x']\": np.float64(-5.9828), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8395), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1319), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8433), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1285), \"['b', 'x', 'a', 'y']\": np.float64(1.0648), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.7481), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.746), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7698), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7268), \"['b', 'x', 'b', 'x']\": np.float64(-6.0226), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.815), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1281), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8187), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1247), \"['b', 'x', 'b', 'y']\": np.float64(1.0337), \"['b', 'x']\": np.float64(-7.7575), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8086), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1271), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8123), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1238), \"['b', 'y', 'a', 'x']\": np.float64(-1.0257), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1438), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0226), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1445), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.022), \"['b', 'y', 'a', 'y']\": np.float64(0.1824), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.814), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1279), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8177), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1246), \"['b', 'y', 'b', 'x']\": np.float64(-1.0325), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1396), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0219), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1403), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0214), \"['b', 'y', 'b', 'y']\": np.float64(0.1771), \"['b', 'y']\": np.float64(1.3298)}\n",
      "It s train loss bro [0.43005794286727905, 1.7730414867401123, 1.713325023651123, 1.5559024810791016, 1.3539841175079346, 1.1394248008728027, 0.9300751686096191, 0.32549163699150085, 0.2887967526912689, 0.2706235945224762, 0.2544107735157013]\n",
      "% good predict : -1\n",
      "\u001b[0;34miteration 2 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3003385066986084 and val_loss is 1.1865383386611938\n",
      "for 1 epochs, loss is 0.252875417470932 and val_loss is 1.2128292322158813\n",
      "for 2 epochs, loss is 0.23531314730644226 and val_loss is 1.2427644729614258\n",
      "for 3 epochs, loss is 0.2179989069700241 and val_loss is 1.2761127948760986\n",
      "for 4 epochs, loss is 0.20112518966197968 and val_loss is 1.3126449584960938\n",
      "for 5 epochs, loss is 0.18490876257419586 and val_loss is 1.352107286453247\n",
      "for 6 epochs, loss is 0.16956636309623718 and val_loss is 1.3941967487335205\n",
      "for 7 epochs, loss is 0.15529939532279968 and val_loss is 1.4385077953338623\n",
      "for 8 epochs, loss is 0.14230096340179443 and val_loss is 1.484430193901062\n",
      "for 9 epochs, loss is 0.13079828023910522 and val_loss is 1.5309734344482422\n",
      "for 10 epochs, loss is 0.1211337149143219 and val_loss is 1.5765299797058105\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.8939), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.369), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.8654), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3944), \"['a', 'x', 'a', 'x']\": np.float64(-7.725), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3401), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0182), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3387), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0194), \"['a', 'x', 'a', 'y']\": np.float64(0.381), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.9378), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3714), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.909), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3969), \"['a', 'x', 'b', 'x']\": np.float64(-7.774), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3484), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0187), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.347), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0199), \"['a', 'x', 'b', 'y']\": np.float64(0.3904), \"['a', 'x']\": np.float64(-8.83), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4328), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0229), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4313), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0236), \"['a', 'y', 'a', 'x']\": np.float64(-0.485), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0213), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0011), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0213), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0012), \"['a', 'y', 'a', 'y']\": np.float64(0.0239), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4356), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.023), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.434), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0238), \"['a', 'y', 'b', 'x']\": np.float64(-0.4881), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0219), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0012), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0218), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0012), \"['a', 'y', 'b', 'y']\": np.float64(0.0245), \"['a', 'y']\": np.float64(0.5544), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.889), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3688), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.8605), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3942), \"['b', 'x', 'a', 'x']\": np.float64(-7.7194), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3398), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0182), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3384), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0194), \"['b', 'x', 'a', 'y']\": np.float64(0.3808), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.9328), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3711), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.9041), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3967), \"['b', 'x', 'b', 'x']\": np.float64(-7.7685), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3482), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0186), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3467), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0199), \"['b', 'x', 'b', 'y']\": np.float64(0.3901), \"['b', 'x']\": np.float64(-8.8237), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4524), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0239), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4507), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0247), \"['b', 'y', 'a', 'x']\": np.float64(-0.5069), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0223), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0012), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0222), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0012), \"['b', 'y', 'a', 'y']\": np.float64(0.025), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4552), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0241), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4536), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0249), \"['b', 'y', 'b', 'x']\": np.float64(-0.5101), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0229), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0012), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0228), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0012), \"['b', 'y', 'b', 'y']\": np.float64(0.0256), \"['b', 'y']\": np.float64(0.5794)}\n",
      "It s train loss bro [0.3003385066986084, 0.252875417470932, 0.23531314730644226, 0.2179989069700241, 0.20112518966197968, 0.18490876257419586, 0.16956636309623718, 0.15529939532279968, 0.14230096340179443, 0.13079828023910522, 0.1211337149143219]\n",
      "% good predict : -1\n",
      "\u001b[0;34miteration 3 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.6257848739624023 and val_loss is 1.5754280090332031\n",
      "for 1 epochs, loss is 2.6898293495178223 and val_loss is 1.569543480873108\n",
      "for 2 epochs, loss is 2.669311285018921 and val_loss is 1.5596691370010376\n",
      "for 3 epochs, loss is 2.636941432952881 and val_loss is 1.5462946891784668\n",
      "for 4 epochs, loss is 2.5930728912353516 and val_loss is 1.5297064781188965\n",
      "for 5 epochs, loss is 2.5379552841186523 and val_loss is 1.5099855661392212\n",
      "for 6 epochs, loss is 2.47330641746521 and val_loss is 1.4870059490203857\n",
      "for 7 epochs, loss is 2.4037394523620605 and val_loss is 1.4605491161346436\n",
      "for 8 epochs, loss is 2.3355231285095215 and val_loss is 1.4307173490524292\n",
      "for 9 epochs, loss is 2.272744655609131 and val_loss is 1.3985295295715332\n",
      "for 10 epochs, loss is 2.215557098388672 and val_loss is 1.3659422397613525\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.5847), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4921), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.2063), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8651), \"['a', 'x', 'a', 'x']\": np.float64(-7.5742), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5272), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0394), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4969), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0693), \"['a', 'x', 'a', 'y']\": np.float64(0.6065), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.2058), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4638), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.8618), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8027), \"['a', 'x', 'b', 'x']\": np.float64(-7.1384), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.857), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0641), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8095), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1109), \"['a', 'x', 'b', 'y']\": np.float64(0.9858), \"['a', 'x']\": np.float64(-8.6972), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4949), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.037), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4664), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0651), \"['a', 'y', 'a', 'x']\": np.float64(-0.5693), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0396), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.003), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0373), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0052), \"['a', 'y', 'a', 'y']\": np.float64(0.0456), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4664), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0349), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4405), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0604), \"['a', 'y', 'b', 'x']\": np.float64(-0.5365), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0644), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0048), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0609), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0083), \"['a', 'y', 'b', 'y']\": np.float64(0.0741), \"['a', 'y']\": np.float64(0.6537), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.2107), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4642), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8529), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.817), \"['b', 'x', 'a', 'x']\": np.float64(-7.144), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4972), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0372), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4686), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0654), \"['b', 'x', 'a', 'y']\": np.float64(0.572), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.869), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4386), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.5431), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.76), \"['b', 'x', 'b', 'x']\": np.float64(-6.7509), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7935), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0593), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7494), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1028), \"['b', 'x', 'b', 'y']\": np.float64(0.9127), \"['b', 'x']\": np.float64(-8.2032), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8553), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0639), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8059), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1126), \"['b', 'y', 'a', 'x']\": np.float64(-0.9838), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0685), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0051), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0645), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.009), \"['b', 'y', 'a', 'y']\": np.float64(0.0787), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8082), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0604), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7632), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1047), \"['b', 'y', 'b', 'x']\": np.float64(-0.9296), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1093), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0082), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1032), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0142), \"['b', 'y', 'b', 'y']\": np.float64(0.1257), \"['b', 'y']\": np.float64(1.1296)}\n",
      "It s train loss bro [2.6257848739624023, 2.6898293495178223, 2.669311285018921, 2.636941432952881, 2.5930728912353516, 2.5379552841186523, 2.47330641746521, 2.4037394523620605, 2.3355231285095215, 2.272744655609131, 2.215557098388672]\n",
      "% good predict : -1\n",
      "\u001b[0;34miteration 4 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.17231373488903046 and val_loss is 1.1661689281463623\n",
      "for 1 epochs, loss is 0.25184953212738037 and val_loss is 1.0402946472167969\n",
      "for 2 epochs, loss is 0.32924482226371765 and val_loss is 0.9488359093666077\n",
      "for 3 epochs, loss is 0.4010174572467804 and val_loss is 0.8838963508605957\n",
      "for 4 epochs, loss is 0.4619919955730438 and val_loss is 0.8393339514732361\n",
      "for 5 epochs, loss is 0.5076847672462463 and val_loss is 0.813698410987854\n",
      "for 6 epochs, loss is 0.5371810793876648 and val_loss is 0.8017677664756775\n",
      "for 7 epochs, loss is 0.551434338092804 and val_loss is 0.7969056963920593\n",
      "for 8 epochs, loss is 0.5514296889305115 and val_loss is 0.7959159016609192\n",
      "for 9 epochs, loss is 0.5380168557167053 and val_loss is 0.7976782321929932\n",
      "for 10 epochs, loss is 0.51642245054245 and val_loss is 0.8020144104957581\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.1504), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1882), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.0717), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2438), \"['a', 'x', 'a', 'x']\": np.float64(-3.5894), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1864), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.6606), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1446), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6872), \"['a', 'x', 'a', 'y']\": np.float64(1.9832), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.111), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1664), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.0579), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1963), \"['a', 'x', 'b', 'x']\": np.float64(-3.5237), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2016), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6691), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.173), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.6819), \"['a', 'x', 'b', 'y']\": np.float64(2.0085), \"['a', 'x']\": np.float64(-5.9912), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1861), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.6554), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1427), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.6861), \"['a', 'y', 'a', 'x']\": np.float64(-1.9798), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.6589), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3669), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.6357), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3817), \"['a', 'y', 'a', 'y']\": np.float64(1.1014), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1658), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.6441), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1365), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6606), \"['a', 'y', 'b', 'x']\": np.float64(-1.9459), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.6635), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3695), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.6478), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3766), \"['a', 'y', 'b', 'y']\": np.float64(1.1092), \"['a', 'y']\": np.float64(3.3086), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.188), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2089), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.108), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2655), \"['b', 'x', 'a', 'x']\": np.float64(-3.6521), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2072), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.6722), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1647), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6992), \"['b', 'x', 'a', 'y']\": np.float64(2.0179), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.165), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1963), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.1106), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2268), \"['b', 'x', 'b', 'x']\": np.float64(-3.6138), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2047), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6708), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1761), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.6836), \"['b', 'x', 'b', 'y']\": np.float64(2.0137), \"['b', 'x']\": np.float64(-6.0959), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1168), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.6171), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.076), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.6459), \"['b', 'y', 'a', 'x']\": np.float64(-1.8641), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.6204), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3455), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.5986), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3593), \"['b', 'y', 'a', 'y']\": np.float64(1.0371), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1064), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.6113), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0786), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6269), \"['b', 'y', 'b', 'x']\": np.float64(-1.8468), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.6156), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3428), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.601), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3494), \"['b', 'y', 'b', 'y']\": np.float64(1.0291), \"['b', 'y']\": np.float64(3.1153)}\n",
      "It s train loss bro [0.17231373488903046, 0.25184953212738037, 0.32924482226371765, 0.4010174572467804, 0.4619919955730438, 0.5076847672462463, 0.5371810793876648, 0.551434338092804, 0.5514296889305115, 0.5380168557167053, 0.51642245054245]\n",
      "% good predict : -1\n",
      "\u001b[0;34miteration 5 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.9956414103507996 and val_loss is 0.8017795085906982\n",
      "for 1 epochs, loss is 0.9956286549568176 and val_loss is 0.800970196723938\n",
      "for 2 epochs, loss is 0.9889434576034546 and val_loss is 0.7996634840965271\n",
      "for 3 epochs, loss is 0.9783676266670227 and val_loss is 0.7979353666305542\n",
      "for 4 epochs, loss is 0.9651651382446289 and val_loss is 0.7958628535270691\n",
      "for 5 epochs, loss is 0.9500327706336975 and val_loss is 0.7935230731964111\n",
      "for 6 epochs, loss is 0.9332669973373413 and val_loss is 0.7909928560256958\n",
      "for 7 epochs, loss is 0.9149993658065796 and val_loss is 0.788347065448761\n",
      "for 8 epochs, loss is 0.8953278064727783 and val_loss is 0.7856575846672058\n",
      "for 9 epochs, loss is 0.8743572235107422 and val_loss is 0.7829937934875488\n",
      "for 10 epochs, loss is 0.8522048592567444 and val_loss is 0.7804219126701355\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-1.1713), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0344), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-1.3738), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8268), \"['a', 'x', 'a', 'x']\": np.float64(-2.3851), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0482), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.9261), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2297), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7401), \"['a', 'x', 'a', 'y']\": np.float64(2.1349), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-1.2574), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1105), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-1.4749), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8877), \"['a', 'x', 'b', 'x']\": np.float64(-2.5606), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9628), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.8506), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1295), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.6798), \"['a', 'x', 'b', 'y']\": np.float64(1.961), \"['a', 'x']\": np.float64(-4.889), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0439), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.9219), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2244), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7369), \"['a', 'y', 'a', 'x']\": np.float64(-2.1257), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.9346), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.8257), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-1.0965), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.6599), \"['a', 'y', 'a', 'y']\": np.float64(1.9036), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1209), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.9899), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3148), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7913), \"['a', 'y', 'b', 'x']\": np.float64(-2.2826), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.8583), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.7583), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-1.0069), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.606), \"['a', 'y', 'b', 'y']\": np.float64(1.7481), \"['a', 'y']\": np.float64(4.3583), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-1.3783), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2173), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-1.6168), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.973), \"['b', 'x', 'a', 'x']\": np.float64(-2.8068), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2335), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(1.0898), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4472), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.871), \"['b', 'x', 'a', 'y']\": np.float64(2.5125), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-1.4799), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3071), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-1.736), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0448), \"['b', 'x', 'b', 'x']\": np.float64(-3.0138), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1328), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(1.0008), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.329), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7998), \"['b', 'x', 'b', 'y']\": np.float64(2.3073), \"['b', 'x']\": np.float64(-5.7535), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.832), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7348), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.976), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5874), \"['b', 'y', 'a', 'x']\": np.float64(-1.6944), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.745), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.6582), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.874), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.526), \"['b', 'y', 'a', 'y']\": np.float64(1.5174), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8936), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7892), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0482), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6308), \"['b', 'y', 'b', 'x']\": np.float64(-1.8198), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.684), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.6043), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.8025), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.4829), \"['b', 'y', 'b', 'y']\": np.float64(1.3931), \"['b', 'y']\": np.float64(3.474)}\n",
      "It s train loss bro [0.9956414103507996, 0.9956286549568176, 0.9889434576034546, 0.9783676266670227, 0.9651651382446289, 0.9500327706336975, 0.9332669973373413, 0.9149993658065796, 0.8953278064727783, 0.8743572235107422, 0.8522048592567444]\n",
      "% good predict : -1\n",
      "\u001b[0;34miteration 6 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.6947154402732849 and val_loss is 0.7695249915122986\n",
      "for 1 epochs, loss is 0.7813458442687988 and val_loss is 0.7694790363311768\n",
      "for 2 epochs, loss is 0.789004921913147 and val_loss is 0.7694889307022095\n",
      "for 3 epochs, loss is 0.7925901412963867 and val_loss is 0.7694875597953796\n",
      "for 4 epochs, loss is 0.7925506234169006 and val_loss is 0.7694447636604309\n",
      "for 5 epochs, loss is 0.7893423438072205 and val_loss is 0.7693572640419006\n",
      "for 6 epochs, loss is 0.7834151387214661 and val_loss is 0.7692407369613647\n",
      "for 7 epochs, loss is 0.7751993536949158 and val_loss is 0.7691221237182617\n",
      "for 8 epochs, loss is 0.7650963068008423 and val_loss is 0.7707523107528687\n",
      "for 9 epochs, loss is 0.6612079739570618 and val_loss is 0.7787785530090332\n",
      "for 10 epochs, loss is 0.5717019438743591 and val_loss is 0.7938455939292908\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.0857), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.184), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.2106), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0426), \"['a', 'x', 'a', 'x']\": np.float64(-3.494), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.28), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7266), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3566), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6399), \"['a', 'x', 'a', 'y']\": np.float64(2.1443), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.2568), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2811), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.3919), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1282), \"['a', 'x', 'b', 'x']\": np.float64(-3.7806), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0985), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6236), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1643), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.5492), \"['a', 'x', 'b', 'y']\": np.float64(1.8403), \"['a', 'x']\": np.float64(-6.0328), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1455), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.6502), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.214), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5726), \"['a', 'y', 'a', 'x']\": np.float64(-1.9189), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.703), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.399), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.745), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3514), \"['a', 'y', 'a', 'y']\": np.float64(1.1776), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2394), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7036), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3136), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6196), \"['a', 'y', 'b', 'x']\": np.float64(-2.0763), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.6033), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3425), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.6394), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3016), \"['a', 'y', 'b', 'y']\": np.float64(1.0107), \"['a', 'y']\": np.float64(3.3131), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.1082), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1836), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.2261), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.05), \"['b', 'x', 'a', 'x']\": np.float64(-3.5186), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2938), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7264), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3661), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6444), \"['b', 'x', 'a', 'y']\": np.float64(2.1593), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.2811), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2807), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.4087), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1361), \"['b', 'x', 'b', 'x']\": np.float64(-3.8072), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1104), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6234), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1725), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.553), \"['b', 'x', 'b', 'y']\": np.float64(1.8532), \"['b', 'x']\": np.float64(-6.0752), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1307), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.6348), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1939), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5631), \"['b', 'y', 'a', 'x']\": np.float64(-1.8871), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.6939), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3896), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7327), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3456), \"['b', 'y', 'a', 'y']\": np.float64(1.1581), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2234), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.6868), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2918), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6093), \"['b', 'y', 'b', 'x']\": np.float64(-2.0419), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5955), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3343), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.6288), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.2966), \"['b', 'y', 'b', 'y']\": np.float64(0.9939), \"['b', 'y']\": np.float64(3.2582)}\n",
      "It s train loss bro [0.6947154402732849, 0.7813458442687988, 0.789004921913147, 0.7925901412963867, 0.7925506234169006, 0.7893423438072205, 0.7834151387214661, 0.7751993536949158, 0.7650963068008423, 0.6612079739570618, 0.5717019438743591]\n",
      "% good predict : -1\n",
      "\u001b[0;34miteration 7 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.5817785859107971 and val_loss is 0.8169524669647217\n",
      "for 1 epochs, loss is 0.4530814290046692 and val_loss is 0.8473378419876099\n",
      "for 2 epochs, loss is 0.39328303933143616 and val_loss is 0.8835170269012451\n",
      "for 3 epochs, loss is 0.34112662076950073 and val_loss is 0.9239721298217773\n",
      "for 4 epochs, loss is 0.29862338304519653 and val_loss is 0.9674444794654846\n",
      "for 5 epochs, loss is 0.2638390362262726 and val_loss is 1.012898564338684\n",
      "for 6 epochs, loss is 0.2347462773323059 and val_loss is 1.0594570636749268\n",
      "for 7 epochs, loss is 0.21005849540233612 and val_loss is 1.1063894033432007\n",
      "for 8 epochs, loss is 0.18894809484481812 and val_loss is 1.1531084775924683\n",
      "for 9 epochs, loss is 0.1708117127418518 and val_loss is 1.1991559267044067\n",
      "for 10 epochs, loss is 0.1551710069179535 and val_loss is 1.244187831878662\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4492), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7188), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.4454), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7071), \"['a', 'x', 'a', 'x']\": np.float64(-7.4516), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7396), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0824), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7392), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0811), \"['a', 'x', 'a', 'y']\": np.float64(0.8546), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.4592), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.72), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4555), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7083), \"['a', 'x', 'b', 'x']\": np.float64(-7.4634), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7107), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0792), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7102), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0779), \"['a', 'x', 'b', 'y']\": np.float64(0.8211), \"['a', 'x']\": np.float64(-8.6316), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7397), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0824), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7392), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0811), \"['a', 'y', 'a', 'x']\": np.float64(-0.8546), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0848), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0095), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0848), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0093), \"['a', 'y', 'a', 'y']\": np.float64(0.098), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7408), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0826), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7404), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0812), \"['a', 'y', 'b', 'x']\": np.float64(-0.856), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0815), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0091), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0815), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0089), \"['a', 'y', 'b', 'y']\": np.float64(0.0942), \"['a', 'y']\": np.float64(0.99), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4598), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7199), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.456), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7083), \"['b', 'x', 'a', 'x']\": np.float64(-7.4639), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7415), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0826), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.741), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0813), \"['b', 'x', 'a', 'y']\": np.float64(0.8567), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.4704), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7213), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4667), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7095), \"['b', 'x', 'b', 'x']\": np.float64(-7.4763), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7119), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0794), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7115), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0781), \"['b', 'x', 'b', 'y']\": np.float64(0.8226), \"['b', 'x']\": np.float64(-8.6466), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7108), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0792), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7103), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0779), \"['b', 'y', 'a', 'x']\": np.float64(-0.8212), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0816), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0091), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0815), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0089), \"['b', 'y', 'a', 'y']\": np.float64(0.0943), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7119), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0794), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7115), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0781), \"['b', 'y', 'b', 'x']\": np.float64(-0.8226), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0783), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0087), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0783), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0086), \"['b', 'y', 'b', 'y']\": np.float64(0.0905), \"['b', 'y']\": np.float64(0.9514)}\n",
      "It s train loss bro [0.5817785859107971, 0.4530814290046692, 0.39328303933143616, 0.34112662076950073, 0.29862338304519653, 0.2638390362262726, 0.2347462773323059, 0.21005849540233612, 0.18894809484481812, 0.1708117127418518, 0.1551710069179535]\n",
      "% good predict : -1\n",
      "\u001b[0;34miteration 8 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1614750474691391 and val_loss is 1.2884390354156494\n",
      "for 1 epochs, loss is 0.13634276390075684 and val_loss is 1.3315094709396362\n",
      "for 2 epochs, loss is 0.1247142106294632 and val_loss is 1.3732117414474487\n",
      "for 3 epochs, loss is 0.11455526202917099 and val_loss is 1.413417100906372\n",
      "for 4 epochs, loss is 0.10566650331020355 and val_loss is 1.452047348022461\n",
      "for 5 epochs, loss is 0.0978723019361496 and val_loss is 1.4890629053115845\n",
      "for 6 epochs, loss is 0.09101922810077667 and val_loss is 1.5244568586349487\n",
      "for 7 epochs, loss is 0.08497507125139236 and val_loss is 1.5582432746887207\n",
      "for 8 epochs, loss is 0.07962659746408463 and val_loss is 1.5904569625854492\n",
      "for 9 epochs, loss is 0.07487727701663971 and val_loss is 1.6211433410644531\n",
      "for 10 epochs, loss is 0.07064493745565414 and val_loss is 1.650357961654663\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.1612), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3545), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.1613), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3518), \"['a', 'x', 'a', 'x']\": np.float64(-8.729), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3628), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0158), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3628), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0156), \"['a', 'x', 'a', 'y']\": np.float64(0.388), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.1586), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3544), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.1587), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3517), \"['a', 'x', 'b', 'x']\": np.float64(-8.7263), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3583), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0156), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3583), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0154), \"['a', 'x', 'b', 'y']\": np.float64(0.3833), \"['a', 'x']\": np.float64(-9.3411), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3689), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.016), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3689), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0159), \"['a', 'y', 'a', 'x']\": np.float64(-0.3946), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0164), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0164), \"['a', 'y', 'a', 'y']\": np.float64(0.0175), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3688), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.016), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3688), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0159), \"['a', 'y', 'b', 'x']\": np.float64(-0.3944), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0162), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0162), \"['a', 'y', 'b', 'y']\": np.float64(0.0173), \"['a', 'y']\": np.float64(0.4222), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.1498), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.354), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.1498), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3513), \"['b', 'x', 'a', 'x']\": np.float64(-8.7168), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3623), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0157), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3623), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0156), \"['b', 'x', 'a', 'y']\": np.float64(0.3875), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.1472), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3539), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.1473), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3512), \"['b', 'x', 'b', 'x']\": np.float64(-8.7141), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3578), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0155), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3578), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0154), \"['b', 'x', 'b', 'y']\": np.float64(0.3827), \"['b', 'x']\": np.float64(-9.328), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3675), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.016), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3675), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0158), \"['b', 'y', 'a', 'x']\": np.float64(-0.393), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0163), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0163), \"['b', 'y', 'a', 'y']\": np.float64(0.0175), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3673), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.016), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3673), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0158), \"['b', 'y', 'b', 'x']\": np.float64(-0.3929), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0161), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0161), \"['b', 'y', 'b', 'y']\": np.float64(0.0173), \"['b', 'y']\": np.float64(0.4206)}\n",
      "It s train loss bro [0.1614750474691391, 0.13634276390075684, 0.1247142106294632, 0.11455526202917099, 0.10566650331020355, 0.0978723019361496, 0.09101922810077667, 0.08497507125139236, 0.07962659746408463, 0.07487727701663971, 0.07064493745565414]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 9 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.08028282225131989 and val_loss is 1.6783562898635864\n",
      "for 1 epochs, loss is 0.06518254429101944 and val_loss is 1.7050321102142334\n",
      "for 2 epochs, loss is 0.06207069382071495 and val_loss is 1.730451226234436\n",
      "for 3 epochs, loss is 0.05924162268638611 and val_loss is 1.7546793222427368\n",
      "for 4 epochs, loss is 0.056662287563085556 and val_loss is 1.777780294418335\n",
      "for 5 epochs, loss is 0.05430426821112633 and val_loss is 1.7998180389404297\n",
      "for 6 epochs, loss is 0.0521426759660244 and val_loss is 1.8208540678024292\n",
      "for 7 epochs, loss is 0.050155892968177795 and val_loss is 1.8409470319747925\n",
      "for 8 epochs, loss is 0.04832497984170914 and val_loss is 1.8601531982421875\n",
      "for 9 epochs, loss is 0.04663362726569176 and val_loss is 1.878525733947754\n",
      "for 10 epochs, loss is 0.04506738856434822 and val_loss is 1.8961164951324463\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.7777), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2231), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.7697), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2273), \"['a', 'x', 'a', 'x']\": np.float64(-9.1671), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2252), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0057), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.225), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0058), \"['a', 'x', 'a', 'y']\": np.float64(0.2351), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7659), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2228), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.7579), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.227), \"['a', 'x', 'b', 'x']\": np.float64(-9.1548), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2326), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0059), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2324), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.006), \"['a', 'x', 'b', 'y']\": np.float64(0.2429), \"['a', 'x']\": np.float64(-9.5747), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2278), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0058), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2276), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0059), \"['a', 'y', 'a', 'x']\": np.float64(-0.2379), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0058), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0058), \"['a', 'y', 'a', 'y']\": np.float64(0.0061), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2275), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0058), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2273), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0059), \"['a', 'y', 'b', 'x']\": np.float64(-0.2376), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.006), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.006), \"['a', 'y', 'b', 'y']\": np.float64(0.0063), \"['a', 'y']\": np.float64(0.2485), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.7634), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2228), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.7554), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2269), \"['b', 'x', 'a', 'x']\": np.float64(-9.1521), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2248), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0057), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2246), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0058), \"['b', 'x', 'a', 'y']\": np.float64(0.2348), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7516), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2225), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.7436), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2266), \"['b', 'x', 'b', 'x']\": np.float64(-9.1398), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2322), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0059), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.232), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.006), \"['b', 'x', 'b', 'y']\": np.float64(0.2425), \"['b', 'x']\": np.float64(-9.5591), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2346), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.006), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2344), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0061), \"['b', 'y', 'a', 'x']\": np.float64(-0.245), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.006), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.006), \"['b', 'y', 'a', 'y']\": np.float64(0.0063), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2343), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.006), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2341), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0061), \"['b', 'y', 'b', 'x']\": np.float64(-0.2447), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0062), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0062), \"['b', 'y', 'b', 'y']\": np.float64(0.0065), \"['b', 'y']\": np.float64(0.2559)}\n",
      "It s train loss bro [0.08028282225131989, 0.06518254429101944, 0.06207069382071495, 0.05924162268638611, 0.056662287563085556, 0.05430426821112633, 0.0521426759660244, 0.050155892968177795, 0.04832497984170914, 0.04663362726569176, 0.04506738856434822]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 10 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 3.3162381649017334 and val_loss is 1.8952046632766724\n",
      "for 1 epochs, loss is 3.6801774501800537 and val_loss is 1.8917025327682495\n",
      "for 2 epochs, loss is 3.6702165603637695 and val_loss is 1.885959506034851\n",
      "for 3 epochs, loss is 3.6548638343811035 and val_loss is 1.8782808780670166\n",
      "for 4 epochs, loss is 3.6351847648620605 and val_loss is 1.8689329624176025\n",
      "for 5 epochs, loss is 3.612229824066162 and val_loss is 1.8581467866897583\n",
      "for 6 epochs, loss is 3.586852550506592 and val_loss is 1.8461207151412964\n",
      "for 7 epochs, loss is 3.5595543384552 and val_loss is 1.8330262899398804\n",
      "for 8 epochs, loss is 3.5305464267730713 and val_loss is 1.8190135955810547\n",
      "for 9 epochs, loss is 3.499922037124634 and val_loss is 1.8042103052139282\n",
      "for 10 epochs, loss is 3.4677658081054688 and val_loss is 1.7887308597564697\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.5812), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.285), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5635), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2899), \"['a', 'x', 'a', 'x']\": np.float64(-9.0329), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2733), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0091), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2728), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0092), \"['a', 'x', 'a', 'y']\": np.float64(0.2877), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.5645), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2845), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.5468), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2894), \"['a', 'x', 'b', 'x']\": np.float64(-9.0153), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2825), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0094), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2819), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0095), \"['a', 'x', 'b', 'y']\": np.float64(0.2974), \"['a', 'x']\": np.float64(-9.5006), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2811), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0093), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2806), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0095), \"['a', 'y', 'a', 'x']\": np.float64(-0.2959), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.009), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0089), \"['a', 'y', 'a', 'y']\": np.float64(0.0094), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2806), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0093), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.28), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0095), \"['a', 'y', 'b', 'x']\": np.float64(-0.2954), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0093), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0092), \"['a', 'y', 'b', 'y']\": np.float64(0.0097), \"['a', 'y']\": np.float64(0.3113), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.5631), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2844), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5452), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2896), \"['b', 'x', 'a', 'x']\": np.float64(-9.0139), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2728), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0091), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2722), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0092), \"['b', 'x', 'a', 'y']\": np.float64(0.2871), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.5464), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2839), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.5286), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.289), \"['b', 'x', 'b', 'x']\": np.float64(-8.9964), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2819), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0094), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2813), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0095), \"['b', 'x', 'b', 'y']\": np.float64(0.2967), \"['b', 'x']\": np.float64(-9.4806), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2896), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0096), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.289), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0098), \"['b', 'y', 'a', 'x']\": np.float64(-0.3048), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0092), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0092), \"['b', 'y', 'a', 'y']\": np.float64(0.0097), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.289), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0096), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2884), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0098), \"['b', 'y', 'b', 'x']\": np.float64(-0.3042), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0095), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0095), \"['b', 'y', 'b', 'y']\": np.float64(0.01), \"['b', 'y']\": np.float64(0.3206)}\n",
      "It s train loss bro [3.3162381649017334, 3.6801774501800537, 3.6702165603637695, 3.6548638343811035, 3.6351847648620605, 3.612229824066162, 3.586852550506592, 3.5595543384552, 3.5305464267730713, 3.499922037124634, 3.4677658081054688]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 11 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.057638756930828094 and val_loss is 1.651512622833252\n",
      "for 1 epochs, loss is 0.06771818548440933 and val_loss is 1.5319029092788696\n",
      "for 2 epochs, loss is 0.08390853554010391 and val_loss is 1.428609013557434\n",
      "for 3 epochs, loss is 0.10157104581594467 and val_loss is 1.3405272960662842\n",
      "for 4 epochs, loss is 0.12020233273506165 and val_loss is 1.2664904594421387\n",
      "for 5 epochs, loss is 0.13919225335121155 and val_loss is 1.2050281763076782\n",
      "for 6 epochs, loss is 0.1578918844461441 and val_loss is 1.1545730829238892\n",
      "for 7 epochs, loss is 0.17567400634288788 and val_loss is 1.1136479377746582\n",
      "for 8 epochs, loss is 0.19197894632816315 and val_loss is 1.0809475183486938\n",
      "for 9 epochs, loss is 0.20634882152080536 and val_loss is 1.055355191230774\n",
      "for 10 epochs, loss is 0.21844665706157684 and val_loss is 1.0359172821044922\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.3016), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0218), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.2057), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0997), \"['a', 'x', 'a', 'x']\": np.float64(-6.5393), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0482), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.202), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0293), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2174), \"['a', 'x', 'a', 'y']\": np.float64(1.293), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.2254), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0071), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.1309), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0839), \"['a', 'x', 'b', 'x']\": np.float64(-6.4453), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1038), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2127), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0838), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.229), \"['a', 'x', 'b', 'y']\": np.float64(1.3615), \"['a', 'x']\": np.float64(-8.0965), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0287), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1983), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0101), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2134), \"['a', 'y', 'a', 'x']\": np.float64(-1.2689), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2034), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0392), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1997), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0422), \"['a', 'y', 'a', 'y']\": np.float64(0.2509), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0139), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1954), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9956), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2103), \"['a', 'y', 'b', 'x']\": np.float64(-1.2506), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2142), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0413), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2103), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0444), \"['a', 'y', 'b', 'y']\": np.float64(0.2642), \"['a', 'y']\": np.float64(1.571), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.1841), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9991), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0903), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0753), \"['b', 'x', 'a', 'x']\": np.float64(-6.3944), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.025), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1975), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0065), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2126), \"['b', 'x', 'a', 'y']\": np.float64(1.2643), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.1096), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9848), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.0172), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0598), \"['b', 'x', 'b', 'x']\": np.float64(-6.3025), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0793), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.208), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0598), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2239), \"['b', 'x', 'b', 'y']\": np.float64(1.3313), \"['b', 'x']\": np.float64(-7.917), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.133), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2184), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1125), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.235), \"['b', 'y', 'a', 'x']\": np.float64(-1.3975), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.224), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0432), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.22), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0465), \"['b', 'y', 'a', 'y']\": np.float64(0.2763), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1167), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2152), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0965), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2316), \"['b', 'y', 'b', 'x']\": np.float64(-1.3774), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2359), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0455), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2316), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0489), \"['b', 'y', 'b', 'y']\": np.float64(0.291), \"['b', 'y']\": np.float64(1.7303)}\n",
      "It s train loss bro [0.057638756930828094, 0.06771818548440933, 0.08390853554010391, 0.10157104581594467, 0.12020233273506165, 0.13919225335121155, 0.1578918844461441, 0.17567400634288788, 0.19197894632816315, 0.20634882152080536, 0.21844665706157684]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 12 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2577797770500183 and val_loss is 1.0220366716384888\n",
      "for 1 epochs, loss is 0.24042688310146332 and val_loss is 1.012852668762207\n",
      "for 2 epochs, loss is 0.24462471902370453 and val_loss is 1.0077816247940063\n",
      "for 3 epochs, loss is 0.2460848093032837 and val_loss is 1.0063378810882568\n",
      "for 4 epochs, loss is 0.2450145184993744 and val_loss is 1.0081182718276978\n",
      "for 5 epochs, loss is 0.2415107786655426 and val_loss is 1.0127849578857422\n",
      "for 6 epochs, loss is 0.23484572768211365 and val_loss is 1.0200430154800415\n",
      "for 7 epochs, loss is 0.22383148968219757 and val_loss is 1.0295659303665161\n",
      "for 8 epochs, loss is 0.2159164994955063 and val_loss is 1.0410983562469482\n",
      "for 9 epochs, loss is 0.20879270136356354 and val_loss is 1.054441213607788\n",
      "for 10 epochs, loss is 0.20115813612937927 and val_loss is 1.0694067478179932\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.6952), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9803), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.6278), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0352), \"['a', 'x', 'a', 'x']\": np.float64(-6.8638), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9802), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1687), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9686), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1782), \"['a', 'x', 'a', 'y']\": np.float64(1.1814), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.6278), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9687), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.5612), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0229), \"['a', 'x', 'b', 'x']\": np.float64(-6.7825), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0352), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1782), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0229), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1882), \"['a', 'x', 'b', 'y']\": np.float64(1.2476), \"['a', 'x']\": np.float64(-8.2721), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0029), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1726), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.991), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1823), \"['a', 'y', 'a', 'x']\": np.float64(-1.2086), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1726), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0297), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1706), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0314), \"['a', 'y', 'a', 'y']\": np.float64(0.208), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.991), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1706), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9793), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1801), \"['a', 'y', 'b', 'x']\": np.float64(-1.1943), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1823), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0314), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1801), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0331), \"['a', 'y', 'b', 'y']\": np.float64(0.2197), \"['a', 'y']\": np.float64(1.4566), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.6451), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9717), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.5783), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0261), \"['b', 'x', 'a', 'x']\": np.float64(-6.8034), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9716), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1672), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9601), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1766), \"['b', 'x', 'a', 'y']\": np.float64(1.171), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.5783), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9602), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.5123), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0139), \"['b', 'x', 'b', 'x']\": np.float64(-6.7229), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0261), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1766), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0139), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1865), \"['b', 'x', 'b', 'y']\": np.float64(1.2366), \"['b', 'x']\": np.float64(-8.1993), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0384), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1787), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0261), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1887), \"['b', 'y', 'a', 'x']\": np.float64(-1.2514), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1787), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0308), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1766), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0325), \"['b', 'y', 'a', 'y']\": np.float64(0.2154), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0261), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1766), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0139), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1865), \"['b', 'y', 'b', 'x']\": np.float64(-1.2366), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1887), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0325), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1865), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0343), \"['b', 'y', 'b', 'y']\": np.float64(0.2275), \"['b', 'y']\": np.float64(1.5082)}\n",
      "It s train loss bro [0.2577797770500183, 0.24042688310146332, 0.24462471902370453, 0.2460848093032837, 0.2450145184993744, 0.2415107786655426, 0.23484572768211365, 0.22383148968219757, 0.2159164994955063, 0.20879270136356354, 0.20115813612937927]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 13 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.22916007041931152 and val_loss is 1.0860717296600342\n",
      "for 1 epochs, loss is 0.19110144674777985 and val_loss is 1.1040313243865967\n",
      "for 2 epochs, loss is 0.18235190212726593 and val_loss is 1.1231127977371216\n",
      "for 3 epochs, loss is 0.1736573874950409 and val_loss is 1.1431505680084229\n",
      "for 4 epochs, loss is 0.16512446105480194 and val_loss is 1.1639848947525024\n",
      "for 5 epochs, loss is 0.15683548152446747 and val_loss is 1.185465931892395\n",
      "for 6 epochs, loss is 0.14885088801383972 and val_loss is 1.207452416419983\n",
      "for 7 epochs, loss is 0.1412128359079361 and val_loss is 1.2298134565353394\n",
      "for 8 epochs, loss is 0.1339481621980667 and val_loss is 1.252428650856018\n",
      "for 9 epochs, loss is 0.12707115709781647 and val_loss is 1.2751883268356323\n",
      "for 10 epochs, loss is 0.12058640271425247 and val_loss is 1.2979938983917236\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.173), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6853), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.1673), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6884), \"['a', 'x', 'a', 'x']\": np.float64(-8.0173), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6819), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0651), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6813), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0654), \"['a', 'x', 'a', 'y']\": np.float64(0.7621), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.1444), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6826), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.1387), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6857), \"['a', 'x', 'b', 'x']\": np.float64(-7.9853), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7039), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0672), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7033), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0676), \"['a', 'x', 'b', 'y']\": np.float64(0.7867), \"['a', 'x']\": np.float64(-8.954), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6818), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0651), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6813), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0654), \"['a', 'y', 'a', 'x']\": np.float64(-0.7621), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0648), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0062), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0648), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0062), \"['a', 'y', 'a', 'y']\": np.float64(0.0724), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6791), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0649), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6786), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0652), \"['a', 'y', 'b', 'x']\": np.float64(-0.759), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0669), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0064), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0669), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0064), \"['a', 'y', 'b', 'y']\": np.float64(0.0748), \"['a', 'y']\": np.float64(0.8511), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.1444), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6826), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.1387), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6857), \"['b', 'x', 'a', 'x']\": np.float64(-7.9853), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6791), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0649), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6786), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0652), \"['b', 'x', 'a', 'y']\": np.float64(0.7591), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.1159), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6798), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.1102), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6829), \"['b', 'x', 'b', 'x']\": np.float64(-7.9534), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7011), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.067), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7005), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0673), \"['b', 'x', 'b', 'y']\": np.float64(0.7836), \"['b', 'x']\": np.float64(-8.9182), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7039), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0672), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7033), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0676), \"['b', 'y', 'a', 'x']\": np.float64(-0.7867), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0669), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0064), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0669), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0064), \"['b', 'y', 'a', 'y']\": np.float64(0.0748), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7011), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.067), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7005), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0673), \"['b', 'y', 'b', 'x']\": np.float64(-0.7836), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0691), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0066), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.069), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0066), \"['b', 'y', 'b', 'y']\": np.float64(0.0772), \"['b', 'y']\": np.float64(0.8786)}\n",
      "It s train loss bro [0.22916007041931152, 0.19110144674777985, 0.18235190212726593, 0.1736573874950409, 0.16512446105480194, 0.15683548152446747, 0.14885088801383972, 0.1412128359079361, 0.1339481621980667, 0.12707115709781647, 0.12058640271425247]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 14 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.13981157541275024 and val_loss is 1.3210068941116333\n",
      "for 1 epochs, loss is 0.10866536945104599 and val_loss is 1.3438771963119507\n",
      "for 2 epochs, loss is 0.1032639890909195 and val_loss is 1.3665374517440796\n",
      "for 3 epochs, loss is 0.09822257608175278 and val_loss is 1.3889305591583252\n",
      "for 4 epochs, loss is 0.09352219849824905 and val_loss is 1.4110074043273926\n",
      "for 5 epochs, loss is 0.08914294838905334 and val_loss is 1.4327280521392822\n",
      "for 6 epochs, loss is 0.08506430685520172 and val_loss is 1.4540596008300781\n",
      "for 7 epochs, loss is 0.08126673102378845 and val_loss is 1.4749757051467896\n",
      "for 8 epochs, loss is 0.0777304545044899 and val_loss is 1.4954568147659302\n",
      "for 9 epochs, loss is 0.07443661987781525 and val_loss is 1.515487790107727\n",
      "for 10 epochs, loss is 0.07136746495962143 and val_loss is 1.5350584983825684\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.1678), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4526), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.1628), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4527), \"['a', 'x', 'a', 'x']\": np.float64(-8.7416), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4437), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0246), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4434), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0246), \"['a', 'x', 'a', 'y']\": np.float64(0.4748), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.1662), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4525), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.1612), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4526), \"['a', 'x', 'b', 'x']\": np.float64(-8.7399), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4447), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0246), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4444), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0246), \"['a', 'x', 'b', 'y']\": np.float64(0.4759), \"['a', 'x']\": np.float64(-9.3502), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4448), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0246), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4446), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0247), \"['a', 'y', 'a', 'x']\": np.float64(-0.4761), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0242), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0013), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0241), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0013), \"['a', 'y', 'a', 'y']\": np.float64(0.0259), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4447), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0246), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4445), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0246), \"['a', 'y', 'b', 'x']\": np.float64(-0.476), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0242), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0013), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0242), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0013), \"['a', 'y', 'b', 'y']\": np.float64(0.0259), \"['a', 'y']\": np.float64(0.5092), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.157), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.452), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.152), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4521), \"['b', 'x', 'a', 'x']\": np.float64(-8.7301), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4431), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0246), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4428), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0246), \"['b', 'x', 'a', 'y']\": np.float64(0.4742), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.1553), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4519), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.1504), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.452), \"['b', 'x', 'b', 'x']\": np.float64(-8.7283), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4441), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0246), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4438), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0246), \"['b', 'x', 'b', 'y']\": np.float64(0.4753), \"['b', 'x']\": np.float64(-9.3379), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4523), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0251), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4521), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0251), \"['b', 'y', 'a', 'x']\": np.float64(-0.4841), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0246), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0014), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0246), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0014), \"['b', 'y', 'a', 'y']\": np.float64(0.0263), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4522), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0251), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.452), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0251), \"['b', 'y', 'b', 'x']\": np.float64(-0.484), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0246), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0014), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0246), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0014), \"['b', 'y', 'b', 'y']\": np.float64(0.0264), \"['b', 'y']\": np.float64(0.5178)}\n",
      "It s train loss bro [0.13981157541275024, 0.10866536945104599, 0.1032639890909195, 0.09822257608175278, 0.09352219849824905, 0.08914294838905334, 0.08506430685520172, 0.08126673102378845, 0.0777304545044899, 0.07443661987781525, 0.07136746495962143]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 15 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.08009908348321915 and val_loss is 1.5542287826538086\n",
      "for 1 epochs, loss is 0.06598412990570068 and val_loss is 1.5729299783706665\n",
      "for 2 epochs, loss is 0.06348167359828949 and val_loss is 1.5911628007888794\n",
      "for 3 epochs, loss is 0.06114351004362106 and val_loss is 1.6089309453964233\n",
      "for 4 epochs, loss is 0.05895688384771347 and val_loss is 1.62623929977417\n",
      "for 5 epochs, loss is 0.056909844279289246 and val_loss is 1.6430957317352295\n",
      "for 6 epochs, loss is 0.05499153211712837 and val_loss is 1.6595078706741333\n",
      "for 7 epochs, loss is 0.05319206789135933 and val_loss is 1.6754865646362305\n",
      "for 8 epochs, loss is 0.05150202661752701 and val_loss is 1.6910415887832642\n",
      "for 9 epochs, loss is 0.049912892282009125 and val_loss is 1.7061854600906372\n",
      "for 10 epochs, loss is 0.048417095094919205 and val_loss is 1.720929503440857\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6943), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3133), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6942), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3134), \"['a', 'x', 'a', 'x']\": np.float64(-9.1079), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3219), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0116), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3219), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0116), \"['a', 'x', 'a', 'y']\": np.float64(0.3372), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.694), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3133), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.6939), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3134), \"['a', 'x', 'b', 'x']\": np.float64(-9.1076), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3182), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0115), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3182), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0115), \"['a', 'x', 'b', 'y']\": np.float64(0.3333), \"['a', 'x']\": np.float64(-9.546), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3138), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0113), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3138), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0113), \"['a', 'y', 'a', 'x']\": np.float64(-0.3287), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0116), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0116), \"['a', 'y', 'a', 'y']\": np.float64(0.0122), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3138), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0113), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3138), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0113), \"['a', 'y', 'b', 'x']\": np.float64(-0.3287), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0115), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0115), \"['a', 'y', 'b', 'y']\": np.float64(0.012), \"['a', 'y']\": np.float64(0.3445), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6941), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3133), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6941), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3134), \"['b', 'x', 'a', 'x']\": np.float64(-9.1078), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3219), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0116), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3219), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0116), \"['b', 'x', 'a', 'y']\": np.float64(0.3372), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.6939), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3133), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.6938), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3134), \"['b', 'x', 'b', 'x']\": np.float64(-9.1075), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3182), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0115), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3182), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0115), \"['b', 'x', 'b', 'y']\": np.float64(0.3333), \"['b', 'x']\": np.float64(-9.5458), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3133), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0113), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3133), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0113), \"['b', 'y', 'a', 'x']\": np.float64(-0.3282), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0116), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0116), \"['b', 'y', 'a', 'y']\": np.float64(0.0121), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3133), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0113), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3133), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0113), \"['b', 'y', 'b', 'x']\": np.float64(-0.3282), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0115), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0115), \"['b', 'y', 'b', 'y']\": np.float64(0.012), \"['b', 'y']\": np.float64(0.344)}\n",
      "It s train loss bro [0.08009908348321915, 0.06598412990570068, 0.06348167359828949, 0.06114351004362106, 0.05895688384771347, 0.056909844279289246, 0.05499153211712837, 0.05319206789135933, 0.05150202661752701, 0.049912892282009125, 0.048417095094919205]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 16 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 3.23549222946167 and val_loss is 1.7198554277420044\n",
      "for 1 epochs, loss is 3.3713343143463135 and val_loss is 1.7164504528045654\n",
      "for 2 epochs, loss is 3.3643555641174316 and val_loss is 1.710998296737671\n",
      "for 3 epochs, loss is 3.353123903274536 and val_loss is 1.7037489414215088\n",
      "for 4 epochs, loss is 3.3381526470184326 and val_loss is 1.6949254274368286\n",
      "for 5 epochs, loss is 3.3198957443237305 and val_loss is 1.6847256422042847\n",
      "for 6 epochs, loss is 3.298753023147583 and val_loss is 1.673323631286621\n",
      "for 7 epochs, loss is 3.2750773429870605 and val_loss is 1.6608760356903076\n",
      "for 8 epochs, loss is 3.2491796016693115 and val_loss is 1.647519588470459\n",
      "for 9 epochs, loss is 3.221332311630249 and val_loss is 1.6333765983581543\n",
      "for 10 epochs, loss is 3.1917717456817627 and val_loss is 1.6185551881790161\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.4537), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3784), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.4538), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3784), \"['a', 'x', 'a', 'x']\": np.float64(-8.9387), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3784), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0169), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3784), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0169), \"['a', 'x', 'a', 'y']\": np.float64(0.4002), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.4538), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3784), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.4539), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3784), \"['a', 'x', 'b', 'x']\": np.float64(-8.9388), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3784), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0169), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3784), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0169), \"['a', 'x', 'b', 'y']\": np.float64(0.4002), \"['a', 'x']\": np.float64(-9.4515), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3872), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0173), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3872), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0173), \"['a', 'y', 'a', 'x']\": np.float64(-0.4094), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0173), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0173), \"['a', 'y', 'a', 'y']\": np.float64(0.0183), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3872), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0173), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3872), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0173), \"['a', 'y', 'b', 'x']\": np.float64(-0.4094), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0173), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0173), \"['a', 'y', 'b', 'y']\": np.float64(0.0183), \"['a', 'y']\": np.float64(0.4329), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.4547), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3785), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.4548), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3785), \"['b', 'x', 'a', 'x']\": np.float64(-8.9398), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3785), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0169), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3785), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0169), \"['b', 'x', 'a', 'y']\": np.float64(0.4002), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.4548), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3785), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.4549), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3785), \"['b', 'x', 'b', 'x']\": np.float64(-8.9399), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3785), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0169), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3785), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0169), \"['b', 'x', 'b', 'y']\": np.float64(0.4002), \"['b', 'x']\": np.float64(-9.4527), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3831), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0172), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3831), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0171), \"['b', 'y', 'a', 'x']\": np.float64(-0.4051), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0172), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0172), \"['b', 'y', 'a', 'y']\": np.float64(0.0181), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3831), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0172), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3831), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0172), \"['b', 'y', 'b', 'x']\": np.float64(-0.4051), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0172), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0172), \"['b', 'y', 'b', 'y']\": np.float64(0.0181), \"['b', 'y']\": np.float64(0.4283)}\n",
      "It s train loss bro [3.23549222946167, 3.3713343143463135, 3.3643555641174316, 3.353123903274536, 3.3381526470184326, 3.3198957443237305, 3.298753023147583, 3.2750773429870605, 3.2491796016693115, 3.221332311630249, 3.1917717456817627]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 17 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.06310289353132248 and val_loss is 1.488293170928955\n",
      "for 1 epochs, loss is 0.07273691147565842 and val_loss is 1.3765439987182617\n",
      "for 2 epochs, loss is 0.09077637642621994 and val_loss is 1.2818520069122314\n",
      "for 3 epochs, loss is 0.11037784814834595 and val_loss is 1.2025939226150513\n",
      "for 4 epochs, loss is 0.13087128102779388 and val_loss is 1.1370800733566284\n",
      "for 5 epochs, loss is 0.15149423480033875 and val_loss is 1.0836338996887207\n",
      "for 6 epochs, loss is 0.17146801948547363 and val_loss is 1.0406572818756104\n",
      "for 7 epochs, loss is 0.19009464979171753 and val_loss is 1.006680965423584\n",
      "for 8 epochs, loss is 0.2068253755569458 and val_loss is 0.9803974628448486\n",
      "for 9 epochs, loss is 0.22123925387859344 and val_loss is 0.9606799483299255\n",
      "for 10 epochs, loss is 0.2330101579427719 and val_loss is 0.9465798735618591\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.8356), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.214), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.8567), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.194), \"['a', 'x', 'a', 'x']\": np.float64(-6.1756), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1923), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2993), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1975), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2944), \"['a', 'x', 'a', 'y']\": np.float64(1.5227), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.8393), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2149), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.8603), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1949), \"['a', 'x', 'b', 'x']\": np.float64(-6.1802), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1901), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2988), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1953), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2938), \"['a', 'x', 'b', 'y']\": np.float64(1.5198), \"['a', 'x']\": np.float64(-7.8585), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1923), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2993), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1975), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2944), \"['a', 'y', 'a', 'x']\": np.float64(-1.5227), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.294), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0738), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2953), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0726), \"['a', 'y', 'a', 'y']\": np.float64(0.3755), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1932), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2996), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1984), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2946), \"['a', 'y', 'b', 'x']\": np.float64(-1.5239), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2934), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0737), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2947), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0725), \"['a', 'y', 'b', 'y']\": np.float64(0.3748), \"['a', 'y']\": np.float64(1.9377), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.839), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2148), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.8596), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1954), \"['b', 'x', 'a', 'x']\": np.float64(-6.1799), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1931), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2995), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1982), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2947), \"['b', 'x', 'a', 'y']\": np.float64(1.5238), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.8426), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2157), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.8632), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1963), \"['b', 'x', 'b', 'x']\": np.float64(-6.1845), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1909), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.299), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.196), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2942), \"['b', 'x', 'b', 'y']\": np.float64(1.5209), \"['b', 'x']\": np.float64(-7.8639), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1904), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2988), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1955), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2941), \"['b', 'y', 'a', 'x']\": np.float64(-1.5203), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2935), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0737), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2948), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0725), \"['b', 'y', 'a', 'y']\": np.float64(0.3749), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1913), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2991), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1964), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2943), \"['b', 'y', 'b', 'x']\": np.float64(-1.5214), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.293), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0735), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2942), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0724), \"['b', 'y', 'b', 'y']\": np.float64(0.3742), \"['b', 'y']\": np.float64(1.9346)}\n",
      "It s train loss bro [0.06310289353132248, 0.07273691147565842, 0.09077637642621994, 0.11037784814834595, 0.13087128102779388, 0.15149423480033875, 0.17146801948547363, 0.19009464979171753, 0.2068253755569458, 0.22123925387859344, 0.2330101579427719]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 18 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.25377610325813293 and val_loss is 0.9372666478157043\n",
      "for 1 epochs, loss is 0.24793599545955658 and val_loss is 0.9321634769439697\n",
      "for 2 epochs, loss is 0.2515023350715637 and val_loss is 0.9307515621185303\n",
      "for 3 epochs, loss is 0.2524026036262512 and val_loss is 0.9326087832450867\n",
      "for 4 epochs, loss is 0.25090256333351135 and val_loss is 0.9373880624771118\n",
      "for 5 epochs, loss is 0.24731816351413727 and val_loss is 0.9447966814041138\n",
      "for 6 epochs, loss is 0.24198968708515167 and val_loss is 0.9545813202857971\n",
      "for 7 epochs, loss is 0.23525990545749664 and val_loss is 0.9665149450302124\n",
      "for 8 epochs, loss is 0.2274564802646637 and val_loss is 0.9803870916366577\n",
      "for 9 epochs, loss is 0.21888071298599243 and val_loss is 0.9959981441497803\n",
      "for 10 epochs, loss is 0.20979931950569153 and val_loss is 1.013156533241272\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.4705), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1044), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.4786), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0937), \"['a', 'x', 'a', 'x']\": np.float64(-6.6922), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0999), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.222), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1015), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2199), \"['a', 'x', 'a', 'y']\": np.float64(1.3455), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.4774), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1058), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.4855), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.095), \"['a', 'x', 'b', 'x']\": np.float64(-6.7007), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0923), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2205), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0939), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2184), \"['a', 'x', 'b', 'y']\": np.float64(1.3362), \"['a', 'x']\": np.float64(-8.1837), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0946), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.221), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0962), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2188), \"['a', 'y', 'a', 'x']\": np.float64(-1.339), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2201), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0444), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2204), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.044), \"['a', 'y', 'a', 'y']\": np.float64(0.2692), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.096), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2212), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0976), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2191), \"['a', 'y', 'b', 'x']\": np.float64(-1.3407), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2186), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0441), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2189), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0437), \"['a', 'y', 'b', 'y']\": np.float64(0.2674), \"['a', 'y']\": np.float64(1.6374), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.473), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1049), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.4811), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0942), \"['b', 'x', 'a', 'x']\": np.float64(-6.6952), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1004), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2221), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.102), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.22), \"['b', 'x', 'a', 'y']\": np.float64(1.3461), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.4799), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1063), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.488), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0955), \"['b', 'x', 'b', 'x']\": np.float64(-6.7037), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0928), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2206), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0944), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2185), \"['b', 'x', 'b', 'y']\": np.float64(1.3368), \"['b', 'x']\": np.float64(-8.1874), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0928), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2206), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0944), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2185), \"['b', 'y', 'a', 'x']\": np.float64(-1.3368), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2197), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0444), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.22), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0439), \"['b', 'y', 'a', 'y']\": np.float64(0.2688), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0942), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2209), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0958), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2187), \"['b', 'y', 'b', 'x']\": np.float64(-1.3385), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2182), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.044), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2185), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0436), \"['b', 'y', 'b', 'y']\": np.float64(0.2669), \"['b', 'y']\": np.float64(1.6347)}\n",
      "It s train loss bro [0.25377610325813293, 0.24793599545955658, 0.2515023350715637, 0.2524026036262512, 0.25090256333351135, 0.24731816351413727, 0.24198968708515167, 0.23525990545749664, 0.2274564802646637, 0.21888071298599243, 0.20979931950569153]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 19 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.21573799848556519 and val_loss is 1.0316996574401855\n",
      "for 1 epochs, loss is 0.19170284271240234 and val_loss is 1.051438331604004\n",
      "for 2 epochs, loss is 0.1823505014181137 and val_loss is 1.0721981525421143\n",
      "for 3 epochs, loss is 0.1731644570827484 and val_loss is 1.093812108039856\n",
      "for 4 epochs, loss is 0.16424016654491425 and val_loss is 1.1161205768585205\n",
      "for 5 epochs, loss is 0.15564706921577454 and val_loss is 1.138974666595459\n",
      "for 6 epochs, loss is 0.14743302762508392 and val_loss is 1.1622346639633179\n",
      "for 7 epochs, loss is 0.13962776958942413 and val_loss is 1.1857733726501465\n",
      "for 8 epochs, loss is 0.13224710524082184 and val_loss is 1.2094738483428955\n",
      "for 9 epochs, loss is 0.12529534101486206 and val_loss is 1.2332305908203125\n",
      "for 10 epochs, loss is 0.11876843869686127 and val_loss is 1.2569509744644165\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.1438), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7426), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.1358), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7485), \"['a', 'x', 'a', 'x']\": np.float64(-7.9914), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7426), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0772), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7418), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0778), \"['a', 'x', 'a', 'y']\": np.float64(0.8307), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.1358), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7418), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.1278), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7477), \"['a', 'x', 'b', 'x']\": np.float64(-7.9824), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7485), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0778), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7477), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0784), \"['a', 'x', 'b', 'y']\": np.float64(0.8373), \"['a', 'x']\": np.float64(-8.9394), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7426), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0772), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7418), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0778), \"['a', 'y', 'a', 'x']\": np.float64(-0.8307), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0772), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.008), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0771), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0081), \"['a', 'y', 'a', 'y']\": np.float64(0.0864), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7418), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0771), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.741), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0777), \"['a', 'y', 'b', 'x']\": np.float64(-0.8298), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0778), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0081), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0777), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0082), \"['a', 'y', 'b', 'y']\": np.float64(0.087), \"['a', 'y']\": np.float64(0.9293), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.1334), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7472), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.1309), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.748), \"['b', 'x', 'a', 'x']\": np.float64(-7.9858), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7416), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0777), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7413), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0778), \"['b', 'x', 'a', 'y']\": np.float64(0.8302), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.1254), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7463), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.1229), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7472), \"['b', 'x', 'b', 'x']\": np.float64(-7.9769), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7474), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0783), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7472), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0784), \"['b', 'x', 'b', 'y']\": np.float64(0.8367), \"['b', 'x']\": np.float64(-8.9332), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7456), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0781), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7454), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0782), \"['b', 'y', 'a', 'x']\": np.float64(-0.8347), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0775), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0081), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0775), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0081), \"['b', 'y', 'a', 'y']\": np.float64(0.0868), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7448), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.078), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7445), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0781), \"['b', 'y', 'b', 'x']\": np.float64(-0.8338), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0781), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0082), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0781), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0082), \"['b', 'y', 'b', 'y']\": np.float64(0.0875), \"['b', 'y']\": np.float64(0.9338)}\n",
      "It s train loss bro [0.21573799848556519, 0.19170284271240234, 0.1823505014181137, 0.1731644570827484, 0.16424016654491425, 0.15564706921577454, 0.14743302762508392, 0.13962776958942413, 0.13224710524082184, 0.12529534101486206, 0.11876843869686127]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 20 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.2768051624298096 and val_loss is 1.2574172019958496\n",
      "for 1 epochs, loss is 2.3645102977752686 and val_loss is 1.2560956478118896\n",
      "for 2 epochs, loss is 2.3612658977508545 and val_loss is 1.2531914710998535\n",
      "for 3 epochs, loss is 2.3542697429656982 and val_loss is 1.248889684677124\n",
      "for 4 epochs, loss is 2.3438704013824463 and val_loss is 1.2433608770370483\n",
      "for 5 epochs, loss is 2.3303511142730713 and val_loss is 1.236757516860962\n",
      "for 6 epochs, loss is 2.3139758110046387 and val_loss is 1.2292184829711914\n",
      "for 7 epochs, loss is 2.295090675354004 and val_loss is 1.2208688259124756\n",
      "for 8 epochs, loss is 2.2742035388946533 and val_loss is 1.21182119846344\n",
      "for 9 epochs, loss is 2.2518937587738037 and val_loss is 1.202176809310913\n",
      "for 10 epochs, loss is 2.2285733222961426 and val_loss is 1.192026972770691\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.7502), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8479), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.7518), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8403), \"['a', 'x', 'a', 'x']\": np.float64(-7.7001), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.84), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1055), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8402), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1046), \"['a', 'x', 'a', 'y']\": np.float64(0.9582), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.7458), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8473), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.7474), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8398), \"['a', 'x', 'b', 'x']\": np.float64(-7.6951), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8397), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1055), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8399), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1045), \"['a', 'x', 'b', 'y']\": np.float64(0.9579), \"['a', 'x']\": np.float64(-8.7761), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8366), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1042), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8362), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1041), \"['a', 'y', 'a', 'x']\": np.float64(-0.9536), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1041), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.013), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.104), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0129), \"['a', 'y', 'a', 'y']\": np.float64(0.1187), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8361), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1041), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8356), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.104), \"['a', 'y', 'b', 'x']\": np.float64(-0.953), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1041), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.013), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.104), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0129), \"['a', 'y', 'b', 'y']\": np.float64(0.1186), \"['a', 'y']\": np.float64(1.0869), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.74), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8515), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.7458), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8396), \"['b', 'x', 'a', 'x']\": np.float64(-7.6933), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8387), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.106), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8394), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1045), \"['b', 'x', 'a', 'y']\": np.float64(0.9573), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.7356), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8509), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.7414), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.839), \"['b', 'x', 'b', 'x']\": np.float64(-7.6883), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8384), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1059), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8391), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1044), \"['b', 'x', 'b', 'y']\": np.float64(0.957), \"['b', 'x']\": np.float64(-8.7683), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8402), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1046), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8397), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1045), \"['b', 'y', 'a', 'x']\": np.float64(-0.9576), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1045), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.013), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1045), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.013), \"['b', 'y', 'a', 'y']\": np.float64(0.1192), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8396), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1046), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8391), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1044), \"['b', 'y', 'b', 'x']\": np.float64(-0.957), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1045), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.013), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1045), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.013), \"['b', 'y', 'b', 'y']\": np.float64(0.1191), \"['b', 'y']\": np.float64(1.0914)}\n",
      "It s train loss bro [2.2768051624298096, 2.3645102977752686, 2.3612658977508545, 2.3542697429656982, 2.3438704013824463, 2.3303511142730713, 2.3139758110046387, 2.295090675354004, 2.2742035388946533, 2.2518937587738037, 2.2285733222961426]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 21 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.13165755569934845 and val_loss is 1.105499029159546\n",
      "for 1 epochs, loss is 0.15971790254116058 and val_loss is 1.0348563194274902\n",
      "for 2 epochs, loss is 0.18972888588905334 and val_loss is 0.9780213236808777\n",
      "for 3 epochs, loss is 0.219903826713562 and val_loss is 0.932917058467865\n",
      "for 4 epochs, loss is 0.24946622550487518 and val_loss is 0.8978725671768188\n",
      "for 5 epochs, loss is 0.2773441970348358 and val_loss is 0.871296763420105\n",
      "for 6 epochs, loss is 0.301786333322525 and val_loss is 0.8517360687255859\n",
      "for 7 epochs, loss is 0.3218236267566681 and val_loss is 0.8379610180854797\n",
      "for 8 epochs, loss is 0.3371063768863678 and val_loss is 0.8289692401885986\n",
      "for 9 epochs, loss is 0.3475300967693329 and val_loss is 0.8239885568618774\n",
      "for 10 epochs, loss is 0.3531888425350189 and val_loss is 0.8224486112594604\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.5101), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3726), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.4988), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3826), \"['a', 'x', 'a', 'x']\": np.float64(-4.9726), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3726), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5367), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3682), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.5406), \"['a', 'x', 'a', 'y']\": np.float64(1.9445), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.4985), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3718), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.4959), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3741), \"['a', 'x', 'b', 'x']\": np.float64(-4.9613), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3766), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.5398), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3756), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.5407), \"['a', 'x', 'b', 'y']\": np.float64(1.9522), \"['a', 'x']\": np.float64(-7.0445), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3817), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.5403), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3773), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5443), \"['a', 'y', 'a', 'x']\": np.float64(-1.9575), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.5403), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.2113), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.5386), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.2128), \"['a', 'y', 'a', 'y']\": np.float64(0.7655), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3772), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.54), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3762), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.5409), \"['a', 'y', 'b', 'x']\": np.float64(-1.9531), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5419), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.2125), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.5415), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.2128), \"['a', 'y', 'b', 'y']\": np.float64(0.7685), \"['a', 'y']\": np.float64(2.7731), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.4916), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3654), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.4804), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3753), \"['b', 'x', 'a', 'x']\": np.float64(-4.9464), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3743), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5374), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3699), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.5413), \"['b', 'x', 'a', 'y']\": np.float64(1.947), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.4885), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3679), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.486), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3702), \"['b', 'x', 'b', 'x']\": np.float64(-4.9472), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3716), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.5378), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3706), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.5387), \"['b', 'x', 'b', 'y']\": np.float64(1.9451), \"['b', 'x']\": np.float64(-7.0221), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3885), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.543), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3841), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5469), \"['b', 'y', 'a', 'x']\": np.float64(-1.9671), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.5465), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.2137), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.5448), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.2153), \"['b', 'y', 'a', 'y']\": np.float64(0.7743), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3873), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.544), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3863), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.5449), \"['b', 'y', 'b', 'x']\": np.float64(-1.9674), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5455), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.2139), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.5451), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.2142), \"['b', 'y', 'b', 'y']\": np.float64(0.7735), \"['b', 'y']\": np.float64(2.7925)}\n",
      "It s train loss bro [0.13165755569934845, 0.15971790254116058, 0.18972888588905334, 0.219903826713562, 0.24946622550487518, 0.2773441970348358, 0.301786333322525, 0.3218236267566681, 0.3371063768863678, 0.3475300967693329, 0.3531888425350189]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 22 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.36815086007118225 and val_loss is 0.8238486647605896\n",
      "for 1 epochs, loss is 0.35307562351226807 and val_loss is 0.828015923500061\n",
      "for 2 epochs, loss is 0.34696394205093384 and val_loss is 0.8347469568252563\n",
      "for 3 epochs, loss is 0.33787059783935547 and val_loss is 0.8438888788223267\n",
      "for 4 epochs, loss is 0.32640886306762695 and val_loss is 0.855315625667572\n",
      "for 5 epochs, loss is 0.313172847032547 and val_loss is 0.8689064383506775\n",
      "for 6 epochs, loss is 0.29871171712875366 and val_loss is 0.8845347762107849\n",
      "for 7 epochs, loss is 0.2835118770599365 and val_loss is 0.9020606279373169\n",
      "for 8 epochs, loss is 0.2679888904094696 and val_loss is 0.9213271737098694\n",
      "for 9 epochs, loss is 0.25248393416404724 and val_loss is 0.9421634078025818\n",
      "for 10 epochs, loss is 0.23726682364940643 and val_loss is 0.9643857479095459\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.1063), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1796), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.1101), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1735), \"['a', 'x', 'a', 'x']\": np.float64(-6.3839), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1798), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2724), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1804), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2711), \"['a', 'x', 'a', 'y']\": np.float64(1.4747), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.1068), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1797), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.1107), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1736), \"['a', 'x', 'b', 'x']\": np.float64(-6.3845), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1789), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2722), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1796), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2709), \"['a', 'x', 'b', 'y']\": np.float64(1.4736), \"['a', 'x']\": np.float64(-7.9811), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1992), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.277), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2002), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2756), \"['a', 'y', 'a', 'x']\": np.float64(-1.4993), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2769), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0639), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2771), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0636), \"['a', 'y', 'a', 'y']\": np.float64(0.3462), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1992), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.277), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2001), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2756), \"['a', 'y', 'b', 'x']\": np.float64(-1.4992), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2768), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0639), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.277), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0636), \"['a', 'y', 'b', 'y']\": np.float64(0.346), \"['a', 'y']\": np.float64(1.8741), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.1179), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1822), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.1217), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1762), \"['b', 'x', 'a', 'x']\": np.float64(-6.3983), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1824), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.273), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1831), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2717), \"['b', 'x', 'a', 'y']\": np.float64(1.478), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.1185), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1824), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.1223), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1763), \"['b', 'x', 'b', 'x']\": np.float64(-6.3991), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1816), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2728), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1823), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2715), \"['b', 'x', 'b', 'y']\": np.float64(1.477), \"['b', 'x']\": np.float64(-7.9992), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1806), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2727), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1815), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2713), \"['b', 'y', 'a', 'x']\": np.float64(-1.476), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2726), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.063), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2728), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0626), \"['b', 'y', 'a', 'y']\": np.float64(0.3408), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1806), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2727), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1815), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2713), \"['b', 'y', 'b', 'x']\": np.float64(-1.4759), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2725), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0629), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2727), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0626), \"['b', 'y', 'b', 'y']\": np.float64(0.3407), \"['b', 'y']\": np.float64(1.845)}\n",
      "It s train loss bro [0.36815086007118225, 0.35307562351226807, 0.34696394205093384, 0.33787059783935547, 0.32640886306762695, 0.313172847032547, 0.29871171712875366, 0.2835118770599365, 0.2679888904094696, 0.25248393416404724, 0.23726682364940643]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 23 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.6151928901672363 and val_loss is 0.9653433561325073\n",
      "for 1 epochs, loss is 1.6822381019592285 and val_loss is 0.9650591015815735\n",
      "for 2 epochs, loss is 1.6813470125198364 and val_loss is 0.9636669158935547\n",
      "for 3 epochs, loss is 1.6775619983673096 and val_loss is 0.9612915515899658\n",
      "for 4 epochs, loss is 1.6711989641189575 and val_loss is 0.9580507278442383\n",
      "for 5 epochs, loss is 1.6625475883483887 and val_loss is 0.9540512561798096\n",
      "for 6 epochs, loss is 1.6518688201904297 and val_loss is 0.9493926167488098\n",
      "for 7 epochs, loss is 1.6393998861312866 and val_loss is 0.9441654682159424\n",
      "for 8 epochs, loss is 1.6253554821014404 and val_loss is 0.9384533166885376\n",
      "for 9 epochs, loss is 1.6099293231964111 and val_loss is 0.9323306083679199\n",
      "for 10 epochs, loss is 1.5932966470718384 and val_loss is 0.9258663654327393\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.704), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2513), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7125), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2405), \"['a', 'x', 'a', 'x']\": np.float64(-6.0485), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2513), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3329), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2535), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.33), \"['a', 'x', 'a', 'y']\": np.float64(1.6089), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.7164), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2546), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7249), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2438), \"['a', 'x', 'b', 'x']\": np.float64(-6.0645), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2355), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3287), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2377), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3258), \"['a', 'x', 'b', 'y']\": np.float64(1.5886), \"['a', 'x']\": np.float64(-7.7772), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2513), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3329), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2535), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.33), \"['a', 'y', 'a', 'x']\": np.float64(-1.6089), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3328), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0885), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3334), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0878), \"['a', 'y', 'a', 'y']\": np.float64(0.428), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2546), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3337), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2569), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3308), \"['a', 'y', 'b', 'x']\": np.float64(-1.6132), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3287), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0874), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3292), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0867), \"['a', 'y', 'b', 'y']\": np.float64(0.4226), \"['a', 'y']\": np.float64(2.0688), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.7144), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2541), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7229), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2432), \"['b', 'x', 'a', 'x']\": np.float64(-6.0618), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.254), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3336), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2563), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3307), \"['b', 'x', 'a', 'y']\": np.float64(1.6125), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.7268), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2574), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7353), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2465), \"['b', 'x', 'b', 'x']\": np.float64(-6.0778), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2382), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3294), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2405), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3265), \"['b', 'x', 'b', 'y']\": np.float64(1.5921), \"['b', 'x']\": np.float64(-7.7943), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2393), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3297), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2416), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3268), \"['b', 'y', 'a', 'x']\": np.float64(-1.5936), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3297), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0877), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3303), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0869), \"['b', 'y', 'a', 'y']\": np.float64(0.4239), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2426), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3305), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2449), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3277), \"['b', 'y', 'b', 'x']\": np.float64(-1.5978), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3255), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0866), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3261), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0858), \"['b', 'y', 'b', 'y']\": np.float64(0.4186), \"['b', 'y']\": np.float64(2.049)}\n",
      "It s train loss bro [1.6151928901672363, 1.6822381019592285, 1.6813470125198364, 1.6775619983673096, 1.6711989641189575, 1.6625475883483887, 1.6518688201904297, 1.6393998861312866, 1.6253554821014404, 1.6099293231964111, 1.5932966470718384]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 24 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2460257112979889 and val_loss is 0.8733381032943726\n",
      "for 1 epochs, loss is 0.29141396284103394 and val_loss is 0.8337315320968628\n",
      "for 2 epochs, loss is 0.33177661895751953 and val_loss is 0.8046554327011108\n",
      "for 3 epochs, loss is 0.3685953915119171 and val_loss is 0.7839155197143555\n",
      "for 4 epochs, loss is 0.40033894777297974 and val_loss is 0.7696344256401062\n",
      "for 5 epochs, loss is 0.4259222149848938 and val_loss is 0.7603113651275635\n",
      "for 6 epochs, loss is 0.44470441341400146 and val_loss is 0.7548255324363708\n",
      "for 7 epochs, loss is 0.45646077394485474 and val_loss is 0.752403736114502\n",
      "for 8 epochs, loss is 0.4613466262817383 and val_loss is 0.7525646686553955\n",
      "for 9 epochs, loss is 0.4598388075828552 and val_loss is 0.7550513744354248\n",
      "for 10 epochs, loss is 0.45265594124794006 and val_loss is 0.7597643136978149\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.6454), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3988), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.6522), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3922), \"['a', 'x', 'a', 'x']\": np.float64(-4.1103), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4316), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.757), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4353), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7534), \"['a', 'x', 'a', 'y']\": np.float64(2.2244), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.6814), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4179), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.6892), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4102), \"['a', 'x', 'b', 'x']\": np.float64(-4.1663), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3935), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.7368), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3975), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7328), \"['a', 'x', 'b', 'y']\": np.float64(2.1651), \"['a', 'x']\": np.float64(-6.4361), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3988), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7396), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4024), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7362), \"['a', 'y', 'a', 'x']\": np.float64(-2.1734), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.757), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.4003), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7589), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3984), \"['a', 'y', 'a', 'y']\": np.float64(1.1761), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4178), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7497), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4219), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7456), \"['a', 'y', 'b', 'x']\": np.float64(-2.203), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7368), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3896), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.7389), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3875), \"['a', 'y', 'b', 'y']\": np.float64(1.1448), \"['a', 'y']\": np.float64(3.4031), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.6648), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4091), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.672), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4021), \"['b', 'x', 'a', 'x']\": np.float64(-4.1404), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4421), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7625), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.446), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7588), \"['b', 'x', 'a', 'y']\": np.float64(2.2407), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.7011), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4283), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.709), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4204), \"['b', 'x', 'b', 'x']\": np.float64(-4.1968), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4037), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.7422), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4078), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7381), \"['b', 'x', 'b', 'y']\": np.float64(2.181), \"['b', 'x']\": np.float64(-6.4832), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3781), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7287), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3818), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7251), \"['b', 'y', 'a', 'x']\": np.float64(-2.1413), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.7458), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3944), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7478), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3924), \"['b', 'y', 'a', 'y']\": np.float64(1.1588), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3969), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7386), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.401), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7346), \"['b', 'y', 'b', 'x']\": np.float64(-2.1704), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7259), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3839), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.7281), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3817), \"['b', 'y', 'b', 'y']\": np.float64(1.1279), \"['b', 'y']\": np.float64(3.3529)}\n",
      "It s train loss bro [0.2460257112979889, 0.29141396284103394, 0.33177661895751953, 0.3685953915119171, 0.40033894777297974, 0.4259222149848938, 0.44470441341400146, 0.45646077394485474, 0.4613466262817383, 0.4598388075828552, 0.45265594124794006]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 25 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.7278196811676025 and val_loss is 0.7601538896560669\n",
      "for 1 epochs, loss is 0.43949514627456665 and val_loss is 0.7661705613136292\n",
      "for 2 epochs, loss is 0.42858603596687317 and val_loss is 0.7746185064315796\n",
      "for 3 epochs, loss is 0.4191676080226898 and val_loss is 0.7854229211807251\n",
      "for 4 epochs, loss is 0.40092742443084717 and val_loss is 0.7985872626304626\n",
      "for 5 epochs, loss is 0.38040515780448914 and val_loss is 0.8140999674797058\n",
      "for 6 epochs, loss is 0.35882508754730225 and val_loss is 0.8319041728973389\n",
      "for 7 epochs, loss is 0.33690884709358215 and val_loss is 0.8518918752670288\n",
      "for 8 epochs, loss is 0.31513211131095886 and val_loss is 0.8739038705825806\n",
      "for 9 epochs, loss is 0.29377928376197815 and val_loss is 0.8977399468421936\n",
      "for 10 epochs, loss is 0.27310487627983093 and val_loss is 0.9231666922569275\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.6798), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2371), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.6915), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2218), \"['a', 'x', 'a', 'x']\": np.float64(-6.027), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.237), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.327), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.24), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.323), \"['a', 'x', 'a', 'y']\": np.float64(1.5931), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.6911), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2401), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7028), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2248), \"['a', 'x', 'b', 'x']\": np.float64(-6.0416), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2216), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3229), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2247), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.319), \"['a', 'x', 'b', 'y']\": np.float64(1.5733), \"['a', 'x']\": np.float64(-7.7615), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2395), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3277), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2426), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3236), \"['a', 'y', 'a', 'x']\": np.float64(-1.5964), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3276), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0866), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3284), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0855), \"['a', 'y', 'a', 'y']\": np.float64(0.4219), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2425), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3285), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2456), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3244), \"['a', 'y', 'b', 'x']\": np.float64(-1.6002), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3236), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0855), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3244), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0845), \"['a', 'y', 'b', 'y']\": np.float64(0.4167), \"['a', 'y']\": np.float64(2.0558), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.6934), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2407), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7086), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2195), \"['b', 'x', 'a', 'x']\": np.float64(-6.0445), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2405), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3279), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2445), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3223), \"['b', 'x', 'a', 'y']\": np.float64(1.5976), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.7047), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2437), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7199), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2225), \"['b', 'x', 'b', 'x']\": np.float64(-6.0591), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2252), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3239), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2292), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3184), \"['b', 'x', 'b', 'y']\": np.float64(1.5779), \"['b', 'x']\": np.float64(-7.784), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2216), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3229), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2255), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3174), \"['b', 'y', 'a', 'x']\": np.float64(-1.5732), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3229), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0854), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3239), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0839), \"['b', 'y', 'a', 'y']\": np.float64(0.4158), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2245), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3237), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2285), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3182), \"['b', 'y', 'b', 'x']\": np.float64(-1.577), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3189), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0843), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3199), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0829), \"['b', 'y', 'b', 'y']\": np.float64(0.4107), \"['b', 'y']\": np.float64(2.026)}\n",
      "It s train loss bro [2.7278196811676025, 0.43949514627456665, 0.42858603596687317, 0.4191676080226898, 0.40092742443084717, 0.38040515780448914, 0.35882508754730225, 0.33690884709358215, 0.31513211131095886, 0.29377928376197815, 0.27310487627983093]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 26 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2781771123409271 and val_loss is 0.9499797224998474\n",
      "for 1 epochs, loss is 0.23749200999736786 and val_loss is 0.9779167771339417\n",
      "for 2 epochs, loss is 0.22018641233444214 and val_loss is 1.0067250728607178\n",
      "for 3 epochs, loss is 0.20418979227542877 and val_loss is 1.0361640453338623\n",
      "for 4 epochs, loss is 0.1894899308681488 and val_loss is 1.0660099983215332\n",
      "for 5 epochs, loss is 0.1760348528623581 and val_loss is 1.0960593223571777\n",
      "for 6 epochs, loss is 0.16375239193439484 and val_loss is 1.126131296157837\n",
      "for 7 epochs, loss is 0.15256059169769287 and val_loss is 1.1560673713684082\n",
      "for 8 epochs, loss is 0.1423746794462204 and val_loss is 1.185731053352356\n",
      "for 9 epochs, loss is 0.1331099420785904 and val_loss is 1.2150077819824219\n",
      "for 10 epochs, loss is 0.124684639275074 and val_loss is 1.2438018321990967\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.031), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7534), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.0188), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7617), \"['a', 'x', 'a', 'x']\": np.float64(-7.9042), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7579), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0812), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7566), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0821), \"['a', 'x', 'a', 'y']\": np.float64(0.852), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.0225), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7525), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.0108), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7604), \"['a', 'x', 'b', 'x']\": np.float64(-7.8947), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7619), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0816), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7607), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0825), \"['a', 'x', 'b', 'y']\": np.float64(0.8566), \"['a', 'x']\": np.float64(-8.8901), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7587), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0813), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7574), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0822), \"['a', 'y', 'a', 'x']\": np.float64(-0.853), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0818), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0088), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0816), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0089), \"['a', 'y', 'a', 'y']\": np.float64(0.0919), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7578), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0812), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7566), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.082), \"['a', 'y', 'b', 'x']\": np.float64(-0.8519), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0822), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0088), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0821), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0089), \"['a', 'y', 'b', 'y']\": np.float64(0.0924), \"['a', 'y']\": np.float64(0.9593), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.0236), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7526), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.0096), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.762), \"['b', 'x', 'a', 'x']\": np.float64(-7.8959), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7571), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0811), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7556), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0821), \"['b', 'x', 'a', 'y']\": np.float64(0.8511), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.0156), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7518), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.0016), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7611), \"['b', 'x', 'b', 'x']\": np.float64(-7.8869), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7608), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0815), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7593), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0825), \"['b', 'x', 'b', 'y']\": np.float64(0.8553), \"['b', 'x']\": np.float64(-8.8808), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7618), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0816), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7603), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0826), \"['b', 'y', 'a', 'x']\": np.float64(-0.8564), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0821), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0088), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.082), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0089), \"['b', 'y', 'a', 'y']\": np.float64(0.0923), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7609), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0815), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7594), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0825), \"['b', 'y', 'b', 'x']\": np.float64(-0.8554), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0825), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0088), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0824), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.009), \"['b', 'y', 'b', 'y']\": np.float64(0.0928), \"['b', 'y']\": np.float64(0.9632)}\n",
      "It s train loss bro [0.2781771123409271, 0.23749200999736786, 0.22018641233444214, 0.20418979227542877, 0.1894899308681488, 0.1760348528623581, 0.16375239193439484, 0.15256059169769287, 0.1423746794462204, 0.1331099420785904, 0.124684639275074]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 27 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.2107913494110107 and val_loss is 1.2449146509170532\n",
      "for 1 epochs, loss is 2.336507558822632 and val_loss is 1.244317650794983\n",
      "for 2 epochs, loss is 2.334726095199585 and val_loss is 1.242197036743164\n",
      "for 3 epochs, loss is 2.3294243812561035 and val_loss is 1.2387248277664185\n",
      "for 4 epochs, loss is 2.3209927082061768 and val_loss is 1.234057068824768\n",
      "for 5 epochs, loss is 2.3097829818725586 and val_loss is 1.2283377647399902\n",
      "for 6 epochs, loss is 2.2961127758026123 and val_loss is 1.221695899963379\n",
      "for 7 epochs, loss is 2.280266761779785 and val_loss is 1.2142497301101685\n",
      "for 8 epochs, loss is 2.2625017166137695 and val_loss is 1.206106185913086\n",
      "for 9 epochs, loss is 2.2430472373962402 and val_loss is 1.1973614692687988\n",
      "for 10 epochs, loss is 2.2221107482910156 and val_loss is 1.1881023645401\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.7009), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8331), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.6824), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8472), \"['a', 'x', 'a', 'x']\": np.float64(-7.6569), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.834), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1037), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8317), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1054), \"['a', 'x', 'a', 'y']\": np.float64(0.953), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.6871), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8314), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.6649), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8467), \"['a', 'x', 'b', 'x']\": np.float64(-7.6411), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8425), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1047), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8397), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1067), \"['a', 'x', 'b', 'y']\": np.float64(0.9627), \"['a', 'x']\": np.float64(-8.7498), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8348), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1038), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8325), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1055), \"['a', 'y', 'a', 'x']\": np.float64(-0.9539), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1039), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0129), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1036), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0131), \"['a', 'y', 'a', 'y']\": np.float64(0.1187), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8331), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1036), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8304), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1055), \"['a', 'y', 'b', 'x']\": np.float64(-0.952), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.105), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.013), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1046), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0133), \"['a', 'y', 'b', 'y']\": np.float64(0.1199), \"['a', 'y']\": np.float64(1.0901), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.6799), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8305), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.6658), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8397), \"['b', 'x', 'a', 'x']\": np.float64(-7.6329), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8314), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1034), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8296), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1045), \"['b', 'x', 'a', 'y']\": np.float64(0.95), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.6585), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8278), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.6368), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8427), \"['b', 'x', 'b', 'x']\": np.float64(-7.6085), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8455), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1051), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8427), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.107), \"['b', 'x', 'b', 'y']\": np.float64(0.9661), \"['b', 'x']\": np.float64(-8.7224), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8486), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1055), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8468), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1067), \"['b', 'y', 'a', 'x']\": np.float64(-0.9696), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1056), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0131), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1054), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0133), \"['b', 'y', 'a', 'y']\": np.float64(0.1207), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8459), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1052), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8431), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1071), \"['b', 'y', 'b', 'x']\": np.float64(-0.9665), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1074), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0134), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1071), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0136), \"['b', 'y', 'b', 'y']\": np.float64(0.1227), \"['b', 'y']\": np.float64(1.1081)}\n",
      "It s train loss bro [2.2107913494110107, 2.336507558822632, 2.334726095199585, 2.3294243812561035, 2.3209927082061768, 2.3097829818725586, 2.2961127758026123, 2.280266761779785, 2.2625017166137695, 2.2430472373962402, 2.2221107482910156]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 28 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1528053730726242 and val_loss is 1.10930597782135\n",
      "for 1 epochs, loss is 0.1637173742055893 and val_loss is 1.0447760820388794\n",
      "for 2 epochs, loss is 0.1922144591808319 and val_loss is 0.9927346110343933\n",
      "for 3 epochs, loss is 0.22026216983795166 and val_loss is 0.9514327645301819\n",
      "for 4 epochs, loss is 0.2467346042394638 and val_loss is 0.9192307591438293\n",
      "for 5 epochs, loss is 0.27063050866127014 and val_loss is 0.8946534991264343\n",
      "for 6 epochs, loss is 0.29115059971809387 and val_loss is 0.8764267563819885\n",
      "for 7 epochs, loss is 0.3077440559864044 and val_loss is 0.8634873032569885\n",
      "for 8 epochs, loss is 0.3201202154159546 and val_loss is 0.8549765944480896\n",
      "for 9 epochs, loss is 0.32823312282562256 and val_loss is 0.8502206206321716\n",
      "for 10 epochs, loss is 0.3322462737560272 and val_loss is 0.8487021327018738\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.7651), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3364), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.7096), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3851), \"['a', 'x', 'a', 'x']\": np.float64(-5.2117), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3347), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4738), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3154), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4907), \"['a', 'x', 'a', 'y']\": np.float64(1.8475), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.6796), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3061), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.6244), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3549), \"['a', 'x', 'b', 'x']\": np.float64(-5.0933), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4097), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.5004), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3886), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.5191), \"['a', 'x', 'b', 'y']\": np.float64(1.9514), \"['a', 'x']\": np.float64(-7.2119), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.345), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4774), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3252), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4948), \"['a', 'y', 'a', 'x']\": np.float64(-1.8618), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4768), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1692), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4699), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1753), \"['a', 'y', 'a', 'y']\": np.float64(0.66), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3145), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4666), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2948), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.484), \"['a', 'y', 'b', 'x']\": np.float64(-1.8196), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5036), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1787), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.496), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1854), \"['a', 'y', 'b', 'y']\": np.float64(0.6971), \"['a', 'y']\": np.float64(2.5764), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.7221), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3211), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.6388), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3939), \"['b', 'x', 'a', 'x']\": np.float64(-5.1521), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3195), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4683), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.29), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4941), \"['b', 'x', 'a', 'y']\": np.float64(1.8264), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.6499), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2955), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.5801), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3568), \"['b', 'x', 'b', 'x']\": np.float64(-5.0521), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3833), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.491), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3569), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.5142), \"['b', 'x', 'b', 'y']\": np.float64(1.9148), \"['b', 'x']\": np.float64(-7.1295), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.382), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4905), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3511), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5175), \"['b', 'y', 'a', 'x']\": np.float64(-1.9129), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4899), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1739), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.479), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1834), \"['b', 'y', 'a', 'y']\": np.float64(0.6781), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3552), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.481), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3293), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.5038), \"['b', 'y', 'b', 'x']\": np.float64(-1.8758), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5136), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1823), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.5038), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1909), \"['b', 'y', 'b', 'y']\": np.float64(0.7109), \"['b', 'y']\": np.float64(2.6471)}\n",
      "It s train loss bro [0.1528053730726242, 0.1637173742055893, 0.1922144591808319, 0.22026216983795166, 0.2467346042394638, 0.27063050866127014, 0.29115059971809387, 0.3077440559864044, 0.3201202154159546, 0.32823312282562256, 0.3322462737560272]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 29 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3591688275337219 and val_loss is 0.8501189947128296\n",
      "for 1 epochs, loss is 0.3333215117454529 and val_loss is 0.8541161417961121\n",
      "for 2 epochs, loss is 0.32723402976989746 and val_loss is 0.8604599833488464\n",
      "for 3 epochs, loss is 0.31870633363723755 and val_loss is 0.8689602613449097\n",
      "for 4 epochs, loss is 0.30832669138908386 and val_loss is 0.8794525265693665\n",
      "for 5 epochs, loss is 0.29661825299263 and val_loss is 0.8917858004570007\n",
      "for 6 epochs, loss is 0.28402215242385864 and val_loss is 0.9058157801628113\n",
      "for 7 epochs, loss is 0.2709057629108429 and val_loss is 0.9213992357254028\n",
      "for 8 epochs, loss is 0.257571280002594 and val_loss is 0.9383932948112488\n",
      "for 9 epochs, loss is 0.24426454305648804 and val_loss is 0.9566527009010315\n",
      "for 10 epochs, loss is 0.231178417801857 and val_loss is 0.976033627986908\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.1512), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1659), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.1649), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1498), \"['a', 'x', 'a', 'x']\": np.float64(-6.4257), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1666), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.264), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1697), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2604), \"['a', 'x', 'a', 'y']\": np.float64(1.4552), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.1652), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1691), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.179), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.153), \"['a', 'x', 'b', 'x']\": np.float64(-6.4432), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.15), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2603), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.153), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2567), \"['a', 'x', 'b', 'y']\": np.float64(1.4345), \"['a', 'x']\": np.float64(-8.016), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1666), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.264), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1697), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2604), \"['a', 'y', 'a', 'x']\": np.float64(-1.4552), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2642), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0598), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2649), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.059), \"['a', 'y', 'a', 'y']\": np.float64(0.3296), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1697), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2648), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1729), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2611), \"['a', 'y', 'b', 'x']\": np.float64(-1.4592), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2604), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0589), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2611), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0581), \"['a', 'y', 'b', 'y']\": np.float64(0.3249), \"['a', 'y']\": np.float64(1.8154), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.1609), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1685), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.1749), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.152), \"['b', 'x', 'a', 'x']\": np.float64(-6.4381), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1688), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2646), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.172), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2609), \"['b', 'x', 'a', 'y']\": np.float64(1.4581), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.1749), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1717), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.189), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1552), \"['b', 'x', 'b', 'x']\": np.float64(-6.4556), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1522), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2609), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1553), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2572), \"['b', 'x', 'b', 'y']\": np.float64(1.4373), \"['b', 'x']\": np.float64(-8.0316), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1538), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2612), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.157), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2576), \"['b', 'y', 'a', 'x']\": np.float64(-1.4394), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2613), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0592), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.262), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0583), \"['b', 'y', 'a', 'y']\": np.float64(0.326), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.157), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.262), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1601), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2583), \"['b', 'y', 'b', 'x']\": np.float64(-1.4433), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2576), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0583), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2583), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0575), \"['b', 'y', 'b', 'y']\": np.float64(0.3213), \"['b', 'y']\": np.float64(1.7956)}\n",
      "It s train loss bro [0.3591688275337219, 0.3333215117454529, 0.32723402976989746, 0.31870633363723755, 0.30832669138908386, 0.29661825299263, 0.28402215242385864, 0.2709057629108429, 0.257571280002594, 0.24426454305648804, 0.231178417801857]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 30 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.610484004020691 and val_loss is 0.9768100380897522\n",
      "for 1 epochs, loss is 1.7083101272583008 and val_loss is 0.9764481782913208\n",
      "for 2 epochs, loss is 1.707750916481018 and val_loss is 0.9750704169273376\n",
      "for 3 epochs, loss is 1.7044154405593872 and val_loss is 0.972790539264679\n",
      "for 4 epochs, loss is 1.6986339092254639 and val_loss is 0.9697142839431763\n",
      "for 5 epochs, loss is 1.6906957626342773 and val_loss is 0.9659390449523926\n",
      "for 6 epochs, loss is 1.6808581352233887 and val_loss is 0.9615546464920044\n",
      "for 7 epochs, loss is 1.6693482398986816 and val_loss is 0.9566431045532227\n",
      "for 8 epochs, loss is 1.6563708782196045 and val_loss is 0.9512796401977539\n",
      "for 9 epochs, loss is 1.6421098709106445 and val_loss is 0.9455322027206421\n",
      "for 10 epochs, loss is 1.6267294883728027 and val_loss is 0.9394633769989014\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.7958), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2239), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7711), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.246), \"['a', 'x', 'a', 'x']\": np.float64(-6.1242), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2316), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3143), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2252), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.32), \"['a', 'x', 'a', 'y']\": np.float64(1.5727), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.7761), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2188), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.729), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2596), \"['a', 'x', 'b', 'x']\": np.float64(-6.099), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2482), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3185), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2358), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3292), \"['a', 'x', 'b', 'y']\": np.float64(1.5939), \"['a', 'x']\": np.float64(-7.8302), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2242), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3123), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2179), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3181), \"['a', 'y', 'a', 'x']\": np.float64(-1.5633), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3144), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0802), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3128), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0817), \"['a', 'y', 'a', 'y']\": np.float64(0.4014), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2192), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.311), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2071), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3215), \"['a', 'y', 'b', 'x']\": np.float64(-1.5569), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3186), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0813), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3155), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.084), \"['a', 'y', 'b', 'y']\": np.float64(0.4069), \"['a', 'y']\": np.float64(1.9987), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.7718), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2177), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7469), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.24), \"['b', 'x', 'a', 'x']\": np.float64(-6.0935), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2254), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3127), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.219), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3184), \"['b', 'x', 'a', 'y']\": np.float64(1.5648), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.7307), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2073), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.6843), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2474), \"['b', 'x', 'b', 'x']\": np.float64(-6.0411), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2598), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3215), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2474), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3322), \"['b', 'x', 'b', 'y']\": np.float64(1.6088), \"['b', 'x']\": np.float64(-7.7909), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2456), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3177), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2391), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3237), \"['b', 'y', 'a', 'x']\": np.float64(-1.5906), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3199), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0816), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3182), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0831), \"['b', 'y', 'a', 'y']\": np.float64(0.4085), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2349), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.315), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2227), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3256), \"['b', 'y', 'b', 'x']\": np.float64(-1.5769), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3289), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0839), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3256), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0867), \"['b', 'y', 'b', 'y']\": np.float64(0.4199), \"['b', 'y']\": np.float64(2.0336)}\n",
      "It s train loss bro [1.610484004020691, 1.7083101272583008, 1.707750916481018, 1.7044154405593872, 1.6986339092254639, 1.6906957626342773, 1.6808581352233887, 1.6693482398986816, 1.6563708782196045, 1.6421098709106445, 1.6267294883728027]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 31 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.5651192665100098 and val_loss is 0.9331813454627991\n",
      "for 1 epochs, loss is 1.5875046253204346 and val_loss is 0.926699161529541\n",
      "for 2 epochs, loss is 1.5703039169311523 and val_loss is 0.9200600981712341\n",
      "for 3 epochs, loss is 1.5523576736450195 and val_loss is 0.9133034944534302\n",
      "for 4 epochs, loss is 1.533774733543396 and val_loss is 0.9064644575119019\n",
      "for 5 epochs, loss is 1.514651894569397 and val_loss is 0.8995746970176697\n",
      "for 6 epochs, loss is 1.4950761795043945 and val_loss is 0.8926624655723572\n",
      "for 7 epochs, loss is 1.4751297235488892 and val_loss is 0.8857529163360596\n",
      "for 8 epochs, loss is 1.4548888206481934 and val_loss is 0.8788685202598572\n",
      "for 9 epochs, loss is 1.4344252347946167 and val_loss is 0.8720292448997498\n",
      "for 10 epochs, loss is 1.413806676864624 and val_loss is 0.865253210067749\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.004), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3288), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.9899), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3406), \"['a', 'x', 'a', 'x']\": np.float64(-5.4315), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3287), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.441), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.324), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4449), \"['a', 'x', 'a', 'y']\": np.float64(1.8024), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.9879), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3234), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.9738), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3352), \"['a', 'x', 'b', 'x']\": np.float64(-5.4097), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3424), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4455), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3377), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4495), \"['a', 'x', 'b', 'y']\": np.float64(1.821), \"['a', 'x']\": np.float64(-7.3678), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3319), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.442), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3273), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.446), \"['a', 'y', 'a', 'x']\": np.float64(-1.8068), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.442), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1467), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4405), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.148), \"['a', 'y', 'a', 'y']\": np.float64(0.5996), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3266), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4402), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3219), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4442), \"['a', 'y', 'b', 'x']\": np.float64(-1.7996), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4466), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1482), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.445), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1495), \"['a', 'y', 'b', 'y']\": np.float64(0.6058), \"['a', 'y']\": np.float64(2.451), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.9832), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3219), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.9691), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3337), \"['b', 'x', 'a', 'x']\": np.float64(-5.4033), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3218), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4387), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3171), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4426), \"['b', 'x', 'a', 'y']\": np.float64(1.793), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.9672), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3165), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.9532), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3284), \"['b', 'x', 'b', 'x']\": np.float64(-5.3816), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3355), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4432), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3307), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4472), \"['b', 'x', 'b', 'y']\": np.float64(1.8116), \"['b', 'x']\": np.float64(-7.3296), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3491), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4477), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3443), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4517), \"['b', 'y', 'a', 'x']\": np.float64(-1.8301), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4477), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1486), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4461), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1499), \"['b', 'y', 'a', 'y']\": np.float64(0.6073), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3437), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4459), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3389), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4499), \"['b', 'y', 'b', 'x']\": np.float64(-1.8227), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4523), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1501), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4507), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1514), \"['b', 'y', 'b', 'y']\": np.float64(0.6136), \"['b', 'y']\": np.float64(2.4825)}\n",
      "It s train loss bro [1.5651192665100098, 1.5875046253204346, 1.5703039169311523, 1.5523576736450195, 1.533774733543396, 1.514651894569397, 1.4950761795043945, 1.4751297235488892, 1.4548888206481934, 1.4344252347946167, 1.413806676864624]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 32 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.35247430205345154 and val_loss is 0.8121592998504639\n",
      "for 1 epochs, loss is 0.3807714581489563 and val_loss is 0.7752015590667725\n",
      "for 2 epochs, loss is 0.4423655569553375 and val_loss is 0.7506855726242065\n",
      "for 3 epochs, loss is 0.49987921118736267 and val_loss is 0.7352237701416016\n",
      "for 4 epochs, loss is 0.5496857762336731 and val_loss is 0.725974440574646\n",
      "for 5 epochs, loss is 0.5895839929580688 and val_loss is 0.7207461595535278\n",
      "for 6 epochs, loss is 0.6185863614082336 and val_loss is 0.7179939150810242\n",
      "for 7 epochs, loss is 0.6367207765579224 and val_loss is 0.7167499661445618\n",
      "for 8 epochs, loss is 0.6447900533676147 and val_loss is 0.7165163159370422\n",
      "for 9 epochs, loss is 0.6436581611633301 and val_loss is 0.7171462178230286\n",
      "for 10 epochs, loss is 0.6338464617729187 and val_loss is 0.7187294960021973\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-1.562), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2887), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-1.5692), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2779), \"['a', 'x', 'a', 'x']\": np.float64(-2.9043), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2821), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(1.0564), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2873), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(1.0483), \"['a', 'x', 'a', 'y']\": np.float64(2.3825), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-1.5577), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2852), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-1.561), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2784), \"['a', 'x', 'b', 'x']\": np.float64(-2.8963), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2858), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(1.0594), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2878), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(1.0546), \"['a', 'x', 'b', 'y']\": np.float64(2.3893), \"['a', 'x']\": np.float64(-5.3892), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2811), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(1.0574), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2873), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(1.0483), \"['a', 'y', 'a', 'x']\": np.float64(-2.3825), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-1.0512), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.8666), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-1.0558), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.8598), \"['a', 'y', 'a', 'y']\": np.float64(1.954), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2775), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(1.0545), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2805), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(1.0486), \"['a', 'y', 'b', 'x']\": np.float64(-2.3758), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-1.0544), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.8692), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-1.0563), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.8651), \"['a', 'y', 'b', 'y']\": np.float64(1.96), \"['a', 'y']\": np.float64(4.4207), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-1.5558), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2836), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-1.5628), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2731), \"['b', 'x', 'a', 'x']\": np.float64(-2.8928), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.277), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(1.0522), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.282), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(1.0444), \"['b', 'x', 'a', 'y']\": np.float64(2.3731), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-1.5498), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2786), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-1.5529), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2719), \"['b', 'x', 'b', 'x']\": np.float64(-2.8815), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2818), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(1.0562), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2838), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(1.0515), \"['b', 'x', 'b', 'y']\": np.float64(2.382), \"['b', 'x']\": np.float64(-5.3679), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2859), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(1.0614), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.292), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(1.0525), \"['b', 'y', 'a', 'x']\": np.float64(-2.3915), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-1.0552), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.8698), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-1.0596), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.8632), \"['b', 'y', 'a', 'y']\": np.float64(1.9614), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2808), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(1.0572), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2837), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(1.0514), \"['b', 'y', 'b', 'x']\": np.float64(-2.382), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-1.0593), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.8733), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-1.0612), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.8692), \"['b', 'y', 'b', 'y']\": np.float64(1.9691), \"['b', 'y']\": np.float64(4.4373)}\n",
      "It s train loss bro [0.35247430205345154, 0.3807714581489563, 0.4423655569553375, 0.49987921118736267, 0.5496857762336731, 0.5895839929580688, 0.6185863614082336, 0.6367207765579224, 0.6447900533676147, 0.6436581611633301, 0.6338464617729187]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 33 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.6271892786026001 and val_loss is 0.7214674949645996\n",
      "for 1 epochs, loss is 0.6047989130020142 and val_loss is 0.7256730794906616\n",
      "for 2 epochs, loss is 0.577594518661499 and val_loss is 0.731636643409729\n",
      "for 3 epochs, loss is 0.5472770929336548 and val_loss is 0.739607036113739\n",
      "for 4 epochs, loss is 0.5151491761207581 and val_loss is 0.7497664093971252\n",
      "for 5 epochs, loss is 0.4822922945022583 and val_loss is 0.7622195482254028\n",
      "for 6 epochs, loss is 0.44956451654434204 and val_loss is 0.7769922614097595\n",
      "for 7 epochs, loss is 0.41761597990989685 and val_loss is 0.7940399646759033\n",
      "for 8 epochs, loss is 0.3869132697582245 and val_loss is 0.813256561756134\n",
      "for 9 epochs, loss is 0.3577709496021271 and val_loss is 0.8344889879226685\n",
      "for 10 epochs, loss is 0.3303796350955963 and val_loss is 0.857547402381897\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.9718), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3428), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.9809), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.33), \"['a', 'x', 'a', 'x']\": np.float64(-5.4024), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3436), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4542), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3467), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4499), \"['a', 'x', 'a', 'y']\": np.float64(1.8276), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.9852), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3476), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.9947), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3346), \"['a', 'x', 'b', 'x']\": np.float64(-5.421), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.324), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4477), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3272), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4434), \"['a', 'x', 'b', 'y']\": np.float64(1.801), \"['a', 'x']\": np.float64(-7.3493), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3446), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4546), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3477), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4503), \"['a', 'y', 'a', 'x']\": np.float64(-1.8289), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4549), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1538), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4559), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1523), \"['a', 'y', 'a', 'y']\": np.float64(0.6187), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3491), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4562), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3523), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4518), \"['a', 'y', 'b', 'x']\": np.float64(-1.8352), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4482), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1516), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4493), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1501), \"['a', 'y', 'b', 'y']\": np.float64(0.6097), \"['a', 'y']\": np.float64(2.488), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.9828), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3466), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.9921), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3338), \"['b', 'x', 'a', 'x']\": np.float64(-5.4175), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3476), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4556), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3507), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4513), \"['b', 'x', 'a', 'y']\": np.float64(1.833), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.9964), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3515), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.0061), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3385), \"['b', 'x', 'b', 'x']\": np.float64(-5.4364), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3278), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.449), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.331), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4447), \"['b', 'x', 'b', 'y']\": np.float64(1.8062), \"['b', 'x']\": np.float64(-7.3702), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3303), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4498), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3334), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4455), \"['b', 'y', 'a', 'x']\": np.float64(-1.8095), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4501), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1522), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4512), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1507), \"['b', 'y', 'a', 'y']\": np.float64(0.6122), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3348), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4514), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3381), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4471), \"['b', 'y', 'b', 'x']\": np.float64(-1.8158), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4435), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.15), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4446), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1485), \"['b', 'y', 'b', 'y']\": np.float64(0.6033), \"['b', 'y']\": np.float64(2.4617)}\n",
      "It s train loss bro [0.6271892786026001, 0.6047989130020142, 0.577594518661499, 0.5472770929336548, 0.5151491761207581, 0.4822922945022583, 0.44956451654434204, 0.41761597990989685, 0.3869132697582245, 0.3577709496021271, 0.3303796350955963]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 34 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.3063410520553589 and val_loss is 0.8590570092201233\n",
      "for 1 epochs, loss is 1.3946986198425293 and val_loss is 0.8597033619880676\n",
      "for 2 epochs, loss is 1.396572470664978 and val_loss is 0.8595702648162842\n",
      "for 3 epochs, loss is 1.3961070775985718 and val_loss is 0.8587391972541809\n",
      "for 4 epochs, loss is 1.3935450315475464 and val_loss is 0.8572880625724792\n",
      "for 5 epochs, loss is 1.3891106843948364 and val_loss is 0.8552901148796082\n",
      "for 6 epochs, loss is 1.3830103874206543 and val_loss is 0.8528137803077698\n",
      "for 7 epochs, loss is 1.3754328489303589 and val_loss is 0.8499230742454529\n",
      "for 8 epochs, loss is 1.366550326347351 and val_loss is 0.8466767072677612\n",
      "for 9 epochs, loss is 1.3565196990966797 and val_loss is 0.8431286811828613\n",
      "for 10 epochs, loss is 1.3454842567443848 and val_loss is 0.8393284678459167\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.7433), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3663), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.7615), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3403), \"['a', 'x', 'a', 'x']\": np.float64(-5.1918), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.367), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4989), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3736), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4895), \"['a', 'x', 'a', 'y']\": np.float64(1.896), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.7614), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3729), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.7797), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3468), \"['a', 'x', 'b', 'x']\": np.float64(-5.2169), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3449), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4908), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3515), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4816), \"['a', 'x', 'b', 'y']\": np.float64(1.8654), \"['a', 'x']\": np.float64(-7.2046), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3688), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4987), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3752), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.49), \"['a', 'y', 'a', 'x']\": np.float64(-1.8981), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4999), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1821), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.5022), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1789), \"['a', 'y', 'a', 'y']\": np.float64(0.6931), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3755), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.5012), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3818), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4924), \"['a', 'y', 'b', 'x']\": np.float64(-1.9073), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4918), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1792), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4941), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1761), \"['a', 'y', 'b', 'y']\": np.float64(0.682), \"['a', 'y']\": np.float64(2.634), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.7641), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3743), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.7826), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3479), \"['b', 'x', 'a', 'x']\": np.float64(-5.221), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3746), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5018), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3813), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4922), \"['b', 'x', 'a', 'y']\": np.float64(1.9066), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.7823), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.381), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.8009), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3544), \"['b', 'x', 'b', 'x']\": np.float64(-5.2462), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3524), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4937), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3591), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4843), \"['b', 'x', 'b', 'y']\": np.float64(1.8758), \"['b', 'x']\": np.float64(-7.2451), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3416), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4889), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3478), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4803), \"['b', 'y', 'a', 'x']\": np.float64(-1.8604), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4899), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1785), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4922), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1754), \"['b', 'y', 'a', 'y']\": np.float64(0.6794), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3481), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4913), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3544), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4826), \"['b', 'y', 'b', 'x']\": np.float64(-1.8694), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.482), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1756), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4843), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1726), \"['b', 'y', 'b', 'y']\": np.float64(0.6684), \"['b', 'y']\": np.float64(2.5816)}\n",
      "It s train loss bro [1.3063410520553589, 1.3946986198425293, 1.396572470664978, 1.3961070775985718, 1.3935450315475464, 1.3891106843948364, 1.3830103874206543, 1.3754328489303589, 1.366550326347351, 1.3565196990966797, 1.3454842567443848]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 35 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.33768734335899353 and val_loss is 0.809417724609375\n",
      "for 1 epochs, loss is 0.3649136424064636 and val_loss is 0.7876743078231812\n",
      "for 2 epochs, loss is 0.39819496870040894 and val_loss is 0.7722633481025696\n",
      "for 3 epochs, loss is 0.42594993114471436 and val_loss is 0.7617115378379822\n",
      "for 4 epochs, loss is 0.4475465416908264 and val_loss is 0.7548949718475342\n",
      "for 5 epochs, loss is 0.46280115842819214 and val_loss is 0.7510045766830444\n",
      "for 6 epochs, loss is 0.4718211889266968 and val_loss is 0.7494941353797913\n",
      "for 7 epochs, loss is 0.4748433530330658 and val_loss is 0.7500239610671997\n",
      "for 8 epochs, loss is 0.472293496131897 and val_loss is 0.7524102330207825\n",
      "for 9 epochs, loss is 0.4647986888885498 and val_loss is 0.7565750479698181\n",
      "for 10 epochs, loss is 0.4531121850013733 and val_loss is 0.762505054473877\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.6496), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4163), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.6841), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3789), \"['a', 'x', 'a', 'x']\": np.float64(-4.1309), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4118), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7547), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4302), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7347), \"['a', 'x', 'a', 'y']\": np.float64(2.2011), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.6988), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4428), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.733), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4058), \"['a', 'x', 'b', 'x']\": np.float64(-4.2078), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3568), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.7254), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.374), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7068), \"['a', 'x', 'b', 'y']\": np.float64(2.1155), \"['a', 'x']\": np.float64(-6.4337), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4032), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7501), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4215), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7303), \"['a', 'y', 'a', 'x']\": np.float64(-2.1878), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.7477), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3997), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7574), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3891), \"['a', 'y', 'a', 'y']\": np.float64(1.1657), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4293), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7641), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4474), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7445), \"['a', 'y', 'b', 'x']\": np.float64(-2.2285), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7186), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3842), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.7277), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3743), \"['a', 'y', 'b', 'y']\": np.float64(1.1204), \"['a', 'y']\": np.float64(3.4073), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.675), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4268), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.7067), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3906), \"['b', 'x', 'a', 'x']\": np.float64(-4.1657), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4254), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7603), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4423), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.741), \"['b', 'x', 'a', 'y']\": np.float64(2.2198), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.7239), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4548), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.7561), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4177), \"['b', 'x', 'b', 'x']\": np.float64(-4.2433), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3695), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.7314), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3856), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7128), \"['b', 'x', 'b', 'y']\": np.float64(2.1334), \"['b', 'x']\": np.float64(-6.488), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3812), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7366), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3975), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.718), \"['b', 'y', 'a', 'x']\": np.float64(-2.1509), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.736), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3925), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7447), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3826), \"['b', 'y', 'a', 'y']\": np.float64(1.1461), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4065), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7511), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.423), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.732), \"['b', 'y', 'b', 'x']\": np.float64(-2.191), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7071), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3776), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.7154), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.368), \"['b', 'y', 'b', 'y']\": np.float64(1.1015), \"['b', 'y']\": np.float64(3.3499)}\n",
      "It s train loss bro [0.33768734335899353, 0.3649136424064636, 0.39819496870040894, 0.42594993114471436, 0.4475465416908264, 0.46280115842819214, 0.4718211889266968, 0.4748433530330658, 0.472293496131897, 0.4647986888885498, 0.4531121850013733]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 36 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.4563327431678772 and val_loss is 0.7701520919799805\n",
      "for 1 epochs, loss is 0.4272438585758209 and val_loss is 0.7796400785446167\n",
      "for 2 epochs, loss is 0.40866225957870483 and val_loss is 0.790980339050293\n",
      "for 3 epochs, loss is 0.38873785734176636 and val_loss is 0.8041569590568542\n",
      "for 4 epochs, loss is 0.36807751655578613 and val_loss is 0.8191208243370056\n",
      "for 5 epochs, loss is 0.3471834361553192 and val_loss is 0.8357903361320496\n",
      "for 6 epochs, loss is 0.3264455795288086 and val_loss is 0.8540533185005188\n",
      "for 7 epochs, loss is 0.30616244673728943 and val_loss is 0.8737736940383911\n",
      "for 8 epochs, loss is 0.28657740354537964 and val_loss is 0.8947963118553162\n",
      "for 9 epochs, loss is 0.2678954303264618 and val_loss is 0.9169553518295288\n",
      "for 10 epochs, loss is 0.2502608597278595 and val_loss is 0.940079927444458\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.9113), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2393), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.9348), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2072), \"['a', 'x', 'a', 'x']\": np.float64(-6.2271), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2559), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3169), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2619), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3087), \"['a', 'x', 'a', 'y']\": np.float64(1.5923), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.9458), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2481), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.9696), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.216), \"['a', 'x', 'b', 'x']\": np.float64(-6.2709), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2099), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3053), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2157), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2975), \"['a', 'x', 'b', 'y']\": np.float64(1.534), \"['a', 'x']\": np.float64(-7.9131), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2173), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3069), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.223), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2992), \"['a', 'y', 'a', 'x']\": np.float64(-1.5433), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3116), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0785), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3131), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0766), \"['a', 'y', 'a', 'y']\": np.float64(0.3951), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2261), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3091), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2319), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3014), \"['a', 'y', 'b', 'x']\": np.float64(-1.5545), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2999), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0756), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3014), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0737), \"['a', 'y', 'b', 'y']\": np.float64(0.3803), \"['a', 'y']\": np.float64(1.9616), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.9205), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2417), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.9442), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2097), \"['b', 'x', 'a', 'x']\": np.float64(-6.2388), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2597), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3179), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2658), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3097), \"['b', 'x', 'a', 'y']\": np.float64(1.5973), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.9563), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2509), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.9804), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2188), \"['b', 'x', 'b', 'x']\": np.float64(-6.2844), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2127), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3061), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2186), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2982), \"['b', 'x', 'b', 'y']\": np.float64(1.5377), \"['b', 'x']\": np.float64(-7.9299), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1982), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3021), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2039), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2945), \"['b', 'y', 'a', 'x']\": np.float64(-1.5191), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3072), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0774), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3086), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0755), \"['b', 'y', 'a', 'y']\": np.float64(0.3894), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2073), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3044), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.213), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2969), \"['b', 'y', 'b', 'x']\": np.float64(-1.5306), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2954), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0745), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2968), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0726), \"['b', 'y', 'b', 'y']\": np.float64(0.3745), \"['b', 'y']\": np.float64(1.9314)}\n",
      "It s train loss bro [0.4563327431678772, 0.4272438585758209, 0.40866225957870483, 0.38873785734176636, 0.36807751655578613, 0.3471834361553192, 0.3264455795288086, 0.30616244673728943, 0.28657740354537964, 0.2678954303264618, 0.2502608597278595]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 37 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.25277069211006165 and val_loss is 0.9639971256256104\n",
      "for 1 epochs, loss is 0.21873004734516144 and val_loss is 0.9885623455047607\n",
      "for 2 epochs, loss is 0.2040245532989502 and val_loss is 1.0136085748672485\n",
      "for 3 epochs, loss is 0.1903984099626541 and val_loss is 1.0389769077301025\n",
      "for 4 epochs, loss is 0.17799977958202362 and val_loss is 1.064521312713623\n",
      "for 5 epochs, loss is 0.16670405864715576 and val_loss is 1.090112566947937\n",
      "for 6 epochs, loss is 0.15634407103061676 and val_loss is 1.1156357526779175\n",
      "for 7 epochs, loss is 0.1468089371919632 and val_loss is 1.1410000324249268\n",
      "for 8 epochs, loss is 0.1380234807729721 and val_loss is 1.1661370992660522\n",
      "for 9 epochs, loss is 0.1299266368150711 and val_loss is 1.1909838914871216\n",
      "for 10 epochs, loss is 0.12246424704790115 and val_loss is 1.2154688835144043\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.0477), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8001), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.0405), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8042), \"['a', 'x', 'a', 'x']\": np.float64(-7.9196), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8001), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0908), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7993), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0913), \"['a', 'x', 'a', 'y']\": np.float64(0.8991), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.0406), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7993), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.0334), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8034), \"['a', 'x', 'b', 'x']\": np.float64(-7.9117), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8036), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0912), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8028), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0917), \"['a', 'x', 'b', 'y']\": np.float64(0.903), \"['a', 'x']\": np.float64(-8.8995), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7988), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0907), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.798), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0912), \"['a', 'y', 'a', 'x']\": np.float64(-0.8976), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0907), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0103), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0906), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0103), \"['a', 'y', 'a', 'y']\": np.float64(0.1019), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.798), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0906), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7972), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0911), \"['a', 'y', 'b', 'x']\": np.float64(-0.8967), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0911), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0103), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.091), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0104), \"['a', 'y', 'b', 'y']\": np.float64(0.1024), \"['a', 'y']\": np.float64(1.0087), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.0397), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7992), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.0321), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8039), \"['b', 'x', 'a', 'x']\": np.float64(-7.9107), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7992), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0907), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7984), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0913), \"['b', 'x', 'a', 'y']\": np.float64(0.8981), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.0326), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7984), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.0251), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8031), \"['b', 'x', 'b', 'x']\": np.float64(-7.9027), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8027), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0911), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8018), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0917), \"['b', 'x', 'b', 'y']\": np.float64(0.902), \"['b', 'x']\": np.float64(-8.8894), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8035), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0912), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8026), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0918), \"['b', 'y', 'a', 'x']\": np.float64(-0.9029), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0912), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0104), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0911), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0104), \"['b', 'y', 'a', 'y']\": np.float64(0.1025), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8027), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0911), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8018), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0917), \"['b', 'y', 'b', 'x']\": np.float64(-0.902), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0916), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0104), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0915), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0105), \"['b', 'y', 'b', 'y']\": np.float64(0.103), \"['b', 'y']\": np.float64(1.0146)}\n",
      "It s train loss bro [0.25277069211006165, 0.21873004734516144, 0.2040245532989502, 0.1903984099626541, 0.17799977958202362, 0.16670405864715576, 0.15634407103061676, 0.1468089371919632, 0.1380234807729721, 0.1299266368150711, 0.12246424704790115]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 38 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.228341579437256 and val_loss is 1.2163742780685425\n",
      "for 1 epochs, loss is 2.2830185890197754 and val_loss is 1.215827465057373\n",
      "for 2 epochs, loss is 2.2814226150512695 and val_loss is 1.2139843702316284\n",
      "for 3 epochs, loss is 2.2767956256866455 and val_loss is 1.210989236831665\n",
      "for 4 epochs, loss is 2.269463300704956 and val_loss is 1.206973910331726\n",
      "for 5 epochs, loss is 2.2597193717956543 and val_loss is 1.2020596265792847\n",
      "for 6 epochs, loss is 2.2478315830230713 and val_loss is 1.1963567733764648\n",
      "for 7 epochs, loss is 2.2340431213378906 and val_loss is 1.1899657249450684\n",
      "for 8 epochs, loss is 2.2185771465301514 and val_loss is 1.1829777956008911\n",
      "for 9 epochs, loss is 2.2016353607177734 and val_loss is 1.1754746437072754\n",
      "for 10 epochs, loss is 2.183400869369507 and val_loss is 1.1675313711166382\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.7355), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8924), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.731), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8841), \"['a', 'x', 'a', 'x']\": np.float64(-7.6961), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8691), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1151), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8685), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1141), \"['a', 'x', 'a', 'y']\": np.float64(0.993), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.713), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8898), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.7094), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8809), \"['a', 'x', 'b', 'x']\": np.float64(-7.6708), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8825), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.117), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.882), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1158), \"['a', 'x', 'b', 'y']\": np.float64(1.0084), \"['a', 'x']\": np.float64(-8.7727), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8691), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1152), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8685), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1141), \"['a', 'y', 'a', 'x']\": np.float64(-0.993), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1121), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0149), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1121), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0147), \"['a', 'y', 'a', 'y']\": np.float64(0.1281), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8662), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1148), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8657), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1137), \"['a', 'y', 'b', 'x']\": np.float64(-0.9898), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1139), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0151), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1138), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0149), \"['a', 'y', 'b', 'y']\": np.float64(0.1301), \"['a', 'y']\": np.float64(1.132), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.7158), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8897), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.7097), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8821), \"['b', 'x', 'a', 'x']\": np.float64(-7.6736), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8665), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1148), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8657), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1138), \"['b', 'x', 'a', 'y']\": np.float64(0.9901), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.6934), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8872), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.6877), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8792), \"['b', 'x', 'b', 'x']\": np.float64(-7.6484), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8799), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1166), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8792), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1156), \"['b', 'x', 'b', 'y']\": np.float64(1.0055), \"['b', 'x']\": np.float64(-8.7471), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8815), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1168), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8807), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1158), \"['b', 'y', 'a', 'x']\": np.float64(-1.0073), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1137), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0151), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1136), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0149), \"['b', 'y', 'a', 'y']\": np.float64(0.13), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8786), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1165), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8778), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1154), \"['b', 'y', 'b', 'x']\": np.float64(-1.004), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1155), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0153), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1154), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0152), \"['b', 'y', 'b', 'y']\": np.float64(0.132), \"['b', 'y']\": np.float64(1.1482)}\n",
      "It s train loss bro [2.228341579437256, 2.2830185890197754, 2.2814226150512695, 2.2767956256866455, 2.269463300704956, 2.2597193717956543, 2.2478315830230713, 2.2340431213378906, 2.2185771465301514, 2.2016353607177734, 2.183400869369507]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 39 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.14028143882751465 and val_loss is 1.0998820066452026\n",
      "for 1 epochs, loss is 0.15546657145023346 and val_loss is 1.0443168878555298\n",
      "for 2 epochs, loss is 0.17955021560192108 and val_loss is 0.999165952205658\n",
      "for 3 epochs, loss is 0.20238018035888672 and val_loss is 0.9629045128822327\n",
      "for 4 epochs, loss is 0.222622811794281 and val_loss is 0.9341586828231812\n",
      "for 5 epochs, loss is 0.24035000801086426 and val_loss is 0.9117250442504883\n",
      "for 6 epochs, loss is 0.25606539845466614 and val_loss is 0.8945909738540649\n",
      "for 7 epochs, loss is 0.26939618587493896 and val_loss is 0.8819252252578735\n",
      "for 8 epochs, loss is 0.2799607813358307 and val_loss is 0.8730542659759521\n",
      "for 9 epochs, loss is 0.28763532638549805 and val_loss is 0.8674386143684387\n",
      "for 10 epochs, loss is 0.29246509075164795 and val_loss is 0.864651620388031\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.0978), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4215), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.1975), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3441), \"['a', 'x', 'a', 'x']\": np.float64(-5.6094), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2858), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.446), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3162), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4222), \"['a', 'x', 'a', 'y']\": np.float64(1.7601), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.0908), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4191), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.1907), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3415), \"['a', 'x', 'b', 'x']\": np.float64(-5.5998), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2915), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.448), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3222), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4239), \"['a', 'x', 'b', 'y']\": np.float64(1.7679), \"['a', 'x']\": np.float64(-7.4549), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3104), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4546), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3423), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4298), \"['a', 'y', 'a', 'x']\": np.float64(-1.7938), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4222), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1465), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4322), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1386), \"['a', 'y', 'a', 'y']\": np.float64(0.5779), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3161), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4566), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3483), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4316), \"['a', 'y', 'b', 'x']\": np.float64(-1.8016), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4184), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1451), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4283), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1373), \"['a', 'y', 'b', 'y']\": np.float64(0.5727), \"['a', 'y']\": np.float64(2.4043), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.1301), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4328), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2314), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.354), \"['b', 'x', 'a', 'x']\": np.float64(-5.6536), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2959), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4496), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3269), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4252), \"['b', 'x', 'a', 'y']\": np.float64(1.774), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.1229), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4302), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2244), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3514), \"['b', 'x', 'b', 'x']\": np.float64(-5.6438), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3018), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4516), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.333), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.427), \"['b', 'x', 'b', 'y']\": np.float64(1.782), \"['b', 'x']\": np.float64(-7.5137), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2932), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4486), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3249), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.424), \"['b', 'y', 'a', 'x']\": np.float64(-1.7702), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4166), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1445), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4266), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1367), \"['b', 'y', 'a', 'y']\": np.float64(0.5703), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2987), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4505), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3307), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4257), \"['b', 'y', 'b', 'x']\": np.float64(-1.7778), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4129), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1432), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4228), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1354), \"['b', 'y', 'b', 'y']\": np.float64(0.5652), \"['b', 'y']\": np.float64(2.3727)}\n",
      "It s train loss bro [0.14028143882751465, 0.15546657145023346, 0.17955021560192108, 0.20238018035888672, 0.222622811794281, 0.24035000801086426, 0.25606539845466614, 0.26939618587493896, 0.2799607813358307, 0.28763532638549805, 0.29246509075164795]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 40 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.3584132194519043 and val_loss is 0.8637972474098206\n",
      "for 1 epochs, loss is 1.4193215370178223 and val_loss is 0.862369179725647\n",
      "for 2 epochs, loss is 1.4149835109710693 and val_loss is 0.8604342341423035\n",
      "for 3 epochs, loss is 1.4090797901153564 and val_loss is 0.8580540418624878\n",
      "for 4 epochs, loss is 1.4017900228500366 and val_loss is 0.8552855849266052\n",
      "for 5 epochs, loss is 1.393276572227478 and val_loss is 0.8521811962127686\n",
      "for 6 epochs, loss is 1.3836865425109863 and val_loss is 0.8487886786460876\n",
      "for 7 epochs, loss is 1.3731534481048584 and val_loss is 0.8451516628265381\n",
      "for 8 epochs, loss is 1.3617960214614868 and val_loss is 0.8413103222846985\n",
      "for 9 epochs, loss is 1.3497215509414673 and val_loss is 0.8373001217842102\n",
      "for 10 epochs, loss is 1.3370263576507568 and val_loss is 0.8331544399261475\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.7781), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3584), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.6611), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4463), \"['a', 'x', 'a', 'x']\": np.float64(-5.1993), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3844), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4978), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3415), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.53), \"['a', 'x', 'a', 'y']\": np.float64(1.9051), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.6867), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3256), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.5871), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3985), \"['a', 'x', 'b', 'x']\": np.float64(-5.0735), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.456), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.5235), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4166), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.5523), \"['a', 'x', 'b', 'y']\": np.float64(2.0036), \"['a', 'x']\": np.float64(-7.2043), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.382), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.5065), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3485), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5327), \"['a', 'y', 'a', 'x']\": np.float64(-1.915), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.5064), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1856), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4942), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1952), \"['a', 'y', 'a', 'y']\": np.float64(0.7018), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3486), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4942), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3213), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.5151), \"['a', 'y', 'b', 'x']\": np.float64(-1.8688), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5326), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1952), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.5218), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.2034), \"['a', 'y', 'b', 'y']\": np.float64(0.738), \"['a', 'y']\": np.float64(2.6537), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.696), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3271), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.5785), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4137), \"['b', 'x', 'a', 'x']\": np.float64(-5.0819), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3543), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4863), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3113), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.518), \"['b', 'x', 'a', 'y']\": np.float64(1.8621), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.6209), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3001), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.52), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3724), \"['b', 'x', 'b', 'x']\": np.float64(-4.9786), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4117), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.5069), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3724), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.5351), \"['b', 'x', 'b', 'y']\": np.float64(1.941), \"['b', 'x']\": np.float64(-7.0417), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4563), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.526), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4136), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5584), \"['b', 'y', 'a', 'x']\": np.float64(-2.0075), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.5336), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1928), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.518), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.2046), \"['b', 'y', 'a', 'y']\": np.float64(0.7357), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4267), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.5153), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3905), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.5421), \"['b', 'y', 'b', 'x']\": np.float64(-1.9668), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5562), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.2009), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.5421), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.2114), \"['b', 'y', 'b', 'y']\": np.float64(0.7668), \"['b', 'y']\": np.float64(2.7818)}\n",
      "It s train loss bro [1.3584132194519043, 1.4193215370178223, 1.4149835109710693, 1.4090797901153564, 1.4017900228500366, 1.393276572227478, 1.3836865425109863, 1.3731534481048584, 1.3617960214614868, 1.3497215509414673, 1.3370263576507568]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 41 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.2447574138641357 and val_loss is 0.8289482593536377\n",
      "for 1 epochs, loss is 1.268071174621582 and val_loss is 0.8246744871139526\n",
      "for 2 epochs, loss is 1.2538584470748901 and val_loss is 0.8203538060188293\n",
      "for 3 epochs, loss is 1.239088773727417 and val_loss is 0.8160058856010437\n",
      "for 4 epochs, loss is 1.223852515220642 and val_loss is 0.8116475343704224\n",
      "for 5 epochs, loss is 1.208210825920105 and val_loss is 0.8072941899299622\n",
      "for 6 epochs, loss is 1.1922074556350708 and val_loss is 0.8029597401618958\n",
      "for 7 epochs, loss is 1.1758754253387451 and val_loss is 0.7986560463905334\n",
      "for 8 epochs, loss is 1.1592400074005127 and val_loss is 0.7943946719169617\n",
      "for 9 epochs, loss is 1.1423237323760986 and val_loss is 0.790185272693634\n",
      "for 10 epochs, loss is 1.1251431703567505 and val_loss is 0.7860366106033325\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.046), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4086), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.9571), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4814), \"['a', 'x', 'a', 'x']\": np.float64(-4.5196), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4106), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.6523), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3653), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6882), \"['a', 'x', 'a', 'y']\": np.float64(2.093), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.9963), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3354), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.875), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4397), \"['a', 'x', 'b', 'x']\": np.float64(-4.3934), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4978), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6676), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4326), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7222), \"['a', 'x', 'b', 'y']\": np.float64(2.1962), \"['a', 'x']\": np.float64(-6.7094), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4205), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.6579), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3798), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.6912), \"['a', 'y', 'a', 'x']\": np.float64(-2.1089), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.6578), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3047), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.637), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3211), \"['a', 'y', 'a', 'y']\": np.float64(0.9766), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.381), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.6312), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3365), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6692), \"['a', 'y', 'b', 'x']\": np.float64(-2.0423), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.6961), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3181), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.6715), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3385), \"['a', 'y', 'b', 'y']\": np.float64(1.0294), \"['a', 'y']\": np.float64(3.1307), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.9723), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3746), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.8867), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4447), \"['b', 'x', 'a', 'x']\": np.float64(-4.4103), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3602), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.629), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3166), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6636), \"['b', 'x', 'a', 'y']\": np.float64(2.0182), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.9144), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2989), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.7975), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3995), \"['b', 'x', 'b', 'x']\": np.float64(-4.2733), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4563), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6491), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3929), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7021), \"['b', 'x', 'b', 'y']\": np.float64(2.1353), \"['b', 'x']\": np.float64(-6.5249), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4965), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.6931), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4542), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7278), \"['b', 'y', 'a', 'x']\": np.float64(-2.2217), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.6848), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3172), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.6632), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3343), \"['b', 'y', 'a', 'y']\": np.float64(1.0167), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.45), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.6627), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4037), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7022), \"['b', 'y', 'b', 'x']\": np.float64(-2.1443), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7308), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.334), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.705), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3554), \"['b', 'y', 'b', 'y']\": np.float64(1.0807), \"['b', 'y']\": np.float64(3.2869)}\n",
      "It s train loss bro [1.2447574138641357, 1.268071174621582, 1.2538584470748901, 1.239088773727417, 1.223852515220642, 1.208210825920105, 1.1922074556350708, 1.1758754253387451, 1.1592400074005127, 1.1423237323760986, 1.1251431703567505]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 42 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.44495731592178345 and val_loss is 0.7546403408050537\n",
      "for 1 epochs, loss is 0.5084575414657593 and val_loss is 0.7343717813491821\n",
      "for 2 epochs, loss is 0.5851048231124878 and val_loss is 0.7223650813102722\n",
      "for 3 epochs, loss is 0.6482987999916077 and val_loss is 0.7158915996551514\n",
      "for 4 epochs, loss is 0.6918694972991943 and val_loss is 0.7127037048339844\n",
      "for 5 epochs, loss is 0.7149778008460999 and val_loss is 0.7112313508987427\n",
      "for 6 epochs, loss is 0.7207310795783997 and val_loss is 0.7111645340919495\n",
      "for 7 epochs, loss is 0.7198609113693237 and val_loss is 0.7111514210700989\n",
      "for 8 epochs, loss is 0.7176479697227478 and val_loss is 0.7111876606941223\n",
      "for 9 epochs, loss is 0.7142794728279114 and val_loss is 0.7122577428817749\n",
      "for 10 epochs, loss is 0.6744382381439209 and val_loss is 0.7144787311553955\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-1.5477), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2851), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-1.526), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3012), \"['a', 'x', 'a', 'x']\": np.float64(-2.8787), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2968), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(1.0768), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2786), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(1.0903), \"['a', 'x', 'a', 'y']\": np.float64(2.4121), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-1.5344), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2724), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-1.5119), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2892), \"['a', 'x', 'b', 'x']\": np.float64(-2.8521), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3084), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(1.085), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2892), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(1.0993), \"['a', 'x', 'b', 'y']\": np.float64(2.432), \"['a', 'x']\": np.float64(-5.3805), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.283), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(1.0654), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2651), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(1.0787), \"['a', 'y', 'a', 'x']\": np.float64(-2.3865), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-1.0751), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.8928), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-1.0601), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.904), \"['a', 'y', 'a', 'y']\": np.float64(1.9998), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2721), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(1.0548), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2534), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(1.0688), \"['a', 'y', 'b', 'x']\": np.float64(-2.3645), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-1.0847), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.8995), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-1.0688), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.9114), \"['a', 'y', 'b', 'y']\": np.float64(2.0163), \"['a', 'y']\": np.float64(4.4606), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-1.523), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2724), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-1.5069), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2849), \"['b', 'x', 'a', 'x']\": np.float64(-2.8427), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2702), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(1.0613), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2568), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(1.0717), \"['b', 'x', 'a', 'y']\": np.float64(2.371), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-1.5115), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2524), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-1.4895), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2701), \"['b', 'x', 'b', 'x']\": np.float64(-2.81), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2888), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(1.068), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2701), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(1.0831), \"['b', 'x', 'b', 'y']\": np.float64(2.3961), \"['b', 'x']\": np.float64(-5.3009), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2986), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(1.0849), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2849), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(1.0956), \"['b', 'y', 'a', 'x']\": np.float64(-2.4239), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-1.0831), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.905), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-1.0718), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.9139), \"['b', 'y', 'a', 'y']\": np.float64(2.0218), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2889), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(1.068), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2701), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(1.0831), \"['b', 'y', 'b', 'x']\": np.float64(-2.3961), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-1.099), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.9107), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-1.0831), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.9235), \"['b', 'y', 'b', 'y']\": np.float64(2.0432), \"['b', 'y']\": np.float64(4.5201)}\n",
      "It s train loss bro [0.44495731592178345, 0.5084575414657593, 0.5851048231124878, 0.6482987999916077, 0.6918694972991943, 0.7149778008460999, 0.7207310795783997, 0.7198609113693237, 0.7176479697227478, 0.7142794728279114, 0.6744382381439209]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 43 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.6204565763473511 and val_loss is 0.718064546585083\n",
      "for 1 epochs, loss is 0.5775318145751953 and val_loss is 0.723206639289856\n",
      "for 2 epochs, loss is 0.54734867811203 and val_loss is 0.7300239205360413\n",
      "for 3 epochs, loss is 0.5167742967605591 and val_loss is 0.7385760545730591\n",
      "for 4 epochs, loss is 0.4864540100097656 and val_loss is 0.7488744854927063\n",
      "for 5 epochs, loss is 0.4568427801132202 and val_loss is 0.7608928680419922\n",
      "for 6 epochs, loss is 0.42825111746788025 and val_loss is 0.7745758295059204\n",
      "for 7 epochs, loss is 0.40087583661079407 and val_loss is 0.7898460030555725\n",
      "for 8 epochs, loss is 0.37482795119285583 and val_loss is 0.8066096305847168\n",
      "for 9 epochs, loss is 0.3501562178134918 and val_loss is 0.8247619271278381\n",
      "for 10 epochs, loss is 0.32686787843704224 and val_loss is 0.844189465045929\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.4556), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3695), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.6041), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2388), \"['a', 'x', 'a', 'x']\": np.float64(-4.8992), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4385), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5141), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.454), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4997), \"['a', 'x', 'a', 'y']\": np.float64(1.9765), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.633), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4398), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.7877), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3035), \"['a', 'x', 'b', 'x']\": np.float64(-5.1507), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.276), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.456), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2892), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4436), \"['a', 'x', 'b', 'y']\": np.float64(1.7531), \"['a', 'x']\": np.float64(-6.9878), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4649), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.5841), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.5311), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5261), \"['a', 'y', 'a', 'x']\": np.float64(-2.0811), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.5318), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1907), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.5382), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1849), \"['a', 'y', 'a', 'y']\": np.float64(0.7315), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4768), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.5888), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.543), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.5308), \"['a', 'y', 'b', 'x']\": np.float64(-2.098), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5192), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1862), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.5252), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1807), \"['a', 'y', 'b', 'y']\": np.float64(0.7141), \"['a', 'y']\": np.float64(2.8463), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.6408), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4433), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.7992), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3031), \"['b', 'x', 'a', 'x']\": np.float64(-5.1623), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.5157), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5418), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.5327), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.5257), \"['b', 'x', 'a', 'y']\": np.float64(2.0826), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.8275), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.5174), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.9927), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3713), \"['b', 'x', 'b', 'x']\": np.float64(-5.4271), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3446), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4807), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3593), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4668), \"['b', 'x', 'b', 'y']\": np.float64(1.8475), \"['b', 'x']\": np.float64(-7.363), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2962), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.517), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3555), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4649), \"['b', 'y', 'a', 'x']\": np.float64(-1.8417), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4706), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1688), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4764), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1634), \"['b', 'y', 'a', 'y']\": np.float64(0.6473), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3067), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.5212), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.366), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.469), \"['b', 'y', 'b', 'x']\": np.float64(-1.8566), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4595), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1648), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.465), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1597), \"['b', 'y', 'b', 'y']\": np.float64(0.632), \"['b', 'y']\": np.float64(2.5189)}\n",
      "It s train loss bro [0.6204565763473511, 0.5775318145751953, 0.54734867811203, 0.5167742967605591, 0.4864540100097656, 0.4568427801132202, 0.42825111746788025, 0.40087583661079407, 0.37482795119285583, 0.3501562178134918, 0.32686787843704224]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 44 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.355954647064209 and val_loss is 0.8454328775405884\n",
      "for 1 epochs, loss is 1.3684272766113281 and val_loss is 0.8459775447845459\n",
      "for 2 epochs, loss is 1.3696417808532715 and val_loss is 0.845891535282135\n",
      "for 3 epochs, loss is 1.36875319480896 and val_loss is 0.8452402353286743\n",
      "for 4 epochs, loss is 1.366007685661316 and val_loss is 0.8440853953361511\n",
      "for 5 epochs, loss is 1.3616206645965576 and val_loss is 0.8424847722053528\n",
      "for 6 epochs, loss is 1.355776309967041 and val_loss is 0.8404926061630249\n",
      "for 7 epochs, loss is 1.3486297130584717 and val_loss is 0.8381580710411072\n",
      "for 8 epochs, loss is 1.340314269065857 and val_loss is 0.8355274200439453\n",
      "for 9 epochs, loss is 1.3309462070465088 and val_loss is 0.8326419591903687\n",
      "for 10 epochs, loss is 1.3206288814544678 and val_loss is 0.8295396566390991\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.7517), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3859), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.7666), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3625), \"['a', 'x', 'a', 'x']\": np.float64(-5.1919), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3719), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5068), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3774), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4982), \"['a', 'x', 'a', 'y']\": np.float64(1.8986), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.7551), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3871), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.7684), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3669), \"['a', 'x', 'b', 'x']\": np.float64(-5.1966), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3624), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.5033), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3672), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4959), \"['a', 'x', 'b', 'y']\": np.float64(1.8854), \"['a', 'x']\": np.float64(-7.1677), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4165), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.518), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4187), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5132), \"['a', 'y', 'a', 'x']\": np.float64(-1.9556), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.518), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1894), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.5188), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1877), \"['a', 'y', 'a', 'y']\": np.float64(0.7151), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4178), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.5185), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4194), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.5148), \"['a', 'y', 'b', 'x']\": np.float64(-1.9573), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5144), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1881), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.515), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1868), \"['a', 'y', 'b', 'y']\": np.float64(0.7102), \"['a', 'y']\": np.float64(2.6998), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.7982), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.403), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.8146), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3787), \"['b', 'x', 'a', 'x']\": np.float64(-5.2564), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3889), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5131), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3949), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.5042), \"['b', 'x', 'a', 'y']\": np.float64(1.9221), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.8021), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4045), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.8178), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3813), \"['b', 'x', 'b', 'x']\": np.float64(-5.2618), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3796), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.5096), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3853), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.5012), \"['b', 'x', 'b', 'y']\": np.float64(1.9093), \"['b', 'x']\": np.float64(-7.2567), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3765), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.5034), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3791), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4985), \"['b', 'y', 'a', 'x']\": np.float64(-1.9004), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.5034), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1841), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.5043), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1823), \"['b', 'y', 'a', 'y']\": np.float64(0.6949), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3779), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.5039), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3803), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4994), \"['b', 'y', 'b', 'x']\": np.float64(-1.9023), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1828), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.5009), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1812), \"['b', 'y', 'b', 'y']\": np.float64(0.6903), \"['b', 'y']\": np.float64(2.6236)}\n",
      "It s train loss bro [1.355954647064209, 1.3684272766113281, 1.3696417808532715, 1.36875319480896, 1.366007685661316, 1.3616206645965576, 1.355776309967041, 1.3486297130584717, 1.340314269065857, 1.3309462070465088, 1.3206288814544678]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 45 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.336289644241333 and val_loss is 0.8048624992370605\n",
      "for 1 epochs, loss is 0.3585456609725952 and val_loss is 0.7910045385360718\n",
      "for 2 epochs, loss is 0.3862648010253906 and val_loss is 0.7848697900772095\n",
      "for 3 epochs, loss is 0.4092791974544525 and val_loss is 0.8337226510047913\n",
      "for 4 epochs, loss is 0.4270859956741333 and val_loss is 0.8667551279067993\n",
      "for 5 epochs, loss is 0.4394697844982147 and val_loss is 0.8989077210426331\n",
      "for 6 epochs, loss is 0.4464882016181946 and val_loss is 0.9061887264251709\n",
      "for 7 epochs, loss is 0.4484291076660156 and val_loss is 0.9569025039672852\n",
      "for 8 epochs, loss is 0.4457542896270752 and val_loss is 0.9689784646034241\n",
      "for 9 epochs, loss is 0.4390384554862976 and val_loss is 1.0839570760726929\n",
      "for 10 epochs, loss is 0.4289132356643677 and val_loss is 1.1116441488265991\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.8056), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4284), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.8463), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3893), \"['a', 'x', 'a', 'x']\": np.float64(-4.2857), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.427), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7283), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4491), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7073), \"['a', 'x', 'a', 'y']\": np.float64(2.1819), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.8973), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4032), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.8888), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4082), \"['a', 'x', 'b', 'x']\": np.float64(-4.3479), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.409), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6896), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4101), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.6874), \"['a', 'x', 'b', 'y']\": np.float64(2.1223), \"['a', 'x']\": np.float64(-6.5466), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4269), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7264), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4476), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7066), \"['a', 'y', 'a', 'x']\": np.float64(-2.1797), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.7276), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3713), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7388), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3606), \"['a', 'y', 'a', 'y']\": np.float64(1.1124), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4749), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7143), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4706), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7168), \"['a', 'y', 'b', 'x']\": np.float64(-2.2134), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7173), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3511), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.7178), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3499), \"['a', 'y', 'b', 'y']\": np.float64(1.0804), \"['a', 'y']\": np.float64(3.3326), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.8957), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4741), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.9365), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4348), \"['b', 'x', 'a', 'x']\": np.float64(-4.4232), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4011), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.715), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4221), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6949), \"['b', 'x', 'a', 'y']\": np.float64(2.1422), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.9438), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4183), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.9286), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4298), \"['b', 'x', 'b', 'x']\": np.float64(-4.4102), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4344), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6917), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4276), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.697), \"['b', 'x', 'b', 'y']\": np.float64(2.1499), \"['b', 'x']\": np.float64(-6.6378), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4102), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7179), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4301), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.6987), \"['b', 'y', 'a', 'x']\": np.float64(-2.1542), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.6896), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3519), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.6999), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.342), \"['b', 'y', 'a', 'y']\": np.float64(1.0543), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4389), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.6933), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4315), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6989), \"['b', 'y', 'b', 'x']\": np.float64(-2.1557), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7011), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3381), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.6978), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3407), \"['b', 'y', 'b', 'y']\": np.float64(1.0508), \"['b', 'y']\": np.float64(3.2446)}\n",
      "It s train loss bro [0.336289644241333, 0.3585456609725952, 0.3862648010253906, 0.4092791974544525, 0.4270859956741333, 0.4394697844982147, 0.4464882016181946, 0.4484291076660156, 0.4457542896270752, 0.4390384554862976, 0.4289132356643677]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 46 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.4372677505016327 and val_loss is 1.161350131034851\n",
      "for 1 epochs, loss is 0.40849795937538147 and val_loss is 1.238710880279541\n",
      "for 2 epochs, loss is 0.3895522952079773 and val_loss is 1.2629207372665405\n",
      "for 3 epochs, loss is 0.36972591280937195 and val_loss is 1.2776151895523071\n",
      "for 4 epochs, loss is 0.3506634831428528 and val_loss is 1.2906320095062256\n",
      "for 5 epochs, loss is 0.33197954297065735 and val_loss is 1.3035787343978882\n",
      "for 6 epochs, loss is 0.3135582506656647 and val_loss is 1.316534161567688\n",
      "for 7 epochs, loss is 0.2955321669578552 and val_loss is 1.3420356512069702\n",
      "for 8 epochs, loss is 0.27803826332092285 and val_loss is 1.4439218044281006\n",
      "for 9 epochs, loss is 0.26116305589675903 and val_loss is 1.5574008226394653\n",
      "for 10 epochs, loss is 0.24495993554592133 and val_loss is 1.6235897541046143\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.0321), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2341), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0477), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2173), \"['a', 'x', 'a', 'x']\": np.float64(-6.3266), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2341), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3027), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.238), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2985), \"['a', 'x', 'a', 'y']\": np.float64(1.5516), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.0405), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.243), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.04), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2426), \"['a', 'x', 'b', 'x']\": np.float64(-6.3461), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2155), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2998), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2154), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2996), \"['a', 'x', 'b', 'y']\": np.float64(1.5304), \"['a', 'x']\": np.float64(-7.9539), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2342), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3027), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.238), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2985), \"['a', 'y', 'a', 'x']\": np.float64(-1.5516), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3027), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0742), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3036), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0732), \"['a', 'y', 'a', 'y']\": np.float64(0.3805), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2362), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3049), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2361), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3047), \"['a', 'y', 'b', 'x']\": np.float64(-1.5564), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2981), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0735), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2981), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0735), \"['a', 'y', 'b', 'y']\": np.float64(0.3753), \"['a', 'y']\": np.float64(1.9508), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.0405), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2362), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0561), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2193), \"['b', 'x', 'a', 'x']\": np.float64(-6.3371), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.243), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3048), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2468), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3007), \"['b', 'x', 'a', 'y']\": np.float64(1.5627), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.0329), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2411), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.0324), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2407), \"['b', 'x', 'b', 'x']\": np.float64(-6.3365), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2408), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.306), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2407), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3059), \"['b', 'x', 'b', 'y']\": np.float64(1.5622), \"['b', 'x']\": np.float64(-7.9786), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2156), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2981), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2193), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.294), \"['b', 'y', 'a', 'x']\": np.float64(-1.5282), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2998), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0735), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3007), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0725), \"['b', 'y', 'a', 'y']\": np.float64(0.3769), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2137), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2993), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2136), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2992), \"['b', 'y', 'b', 'x']\": np.float64(-1.5281), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2992), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0738), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2992), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0738), \"['b', 'y', 'b', 'y']\": np.float64(0.3767), \"['b', 'y']\": np.float64(1.9241)}\n",
      "It s train loss bro [0.4372677505016327, 0.40849795937538147, 0.3895522952079773, 0.36972591280937195, 0.3506634831428528, 0.33197954297065735, 0.3135582506656647, 0.2955321669578552, 0.27803826332092285, 0.26116305589675903, 0.24495993554592133]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 47 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.25498950481414795 and val_loss is 1.6636877059936523\n",
      "for 1 epochs, loss is 0.22842249274253845 and val_loss is 1.693922519683838\n",
      "for 2 epochs, loss is 0.21380114555358887 and val_loss is 1.7196985483169556\n",
      "for 3 epochs, loss is 0.19988365471363068 and val_loss is 1.7431769371032715\n",
      "for 4 epochs, loss is 0.18676072359085083 and val_loss is 1.7652703523635864\n",
      "for 5 epochs, loss is 0.1744820475578308 and val_loss is 1.7863764762878418\n",
      "for 6 epochs, loss is 0.16306264698505402 and val_loss is 1.806736707687378\n",
      "for 7 epochs, loss is 0.1524910032749176 and val_loss is 1.8266215324401855\n",
      "for 8 epochs, loss is 0.14273710548877716 and val_loss is 1.8463560342788696\n",
      "for 9 epochs, loss is 0.1337582767009735 and val_loss is 1.8662017583847046\n",
      "for 10 epochs, loss is 0.12550471723079681 and val_loss is 1.8861885070800781\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.0205), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8242), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.036), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8063), \"['a', 'x', 'a', 'x']\": np.float64(-7.8991), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8242), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0968), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.826), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0946), \"['a', 'x', 'a', 'y']\": np.float64(0.9273), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.0543), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8072), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.0534), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8064), \"['a', 'x', 'b', 'x']\": np.float64(-7.9166), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.808), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0927), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8083), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0924), \"['a', 'x', 'b', 'y']\": np.float64(0.9072), \"['a', 'x']\": np.float64(-8.8877), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8242), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0968), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.826), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0947), \"['a', 'y', 'a', 'x']\": np.float64(-0.9273), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0968), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0114), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.097), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0111), \"['a', 'y', 'a', 'y']\": np.float64(0.1089), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8282), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0948), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8281), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0947), \"['a', 'y', 'b', 'x']\": np.float64(-0.9294), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0949), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0109), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0949), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0108), \"['a', 'y', 'b', 'y']\": np.float64(0.1065), \"['a', 'y']\": np.float64(1.0434), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.0543), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8282), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.0699), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8102), \"['b', 'x', 'a', 'x']\": np.float64(-7.9372), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8072), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0948), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.809), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0927), \"['b', 'x', 'a', 'y']\": np.float64(0.9082), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.0718), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8092), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.0709), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8084), \"['b', 'x', 'b', 'x']\": np.float64(-7.9362), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8082), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0927), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8084), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0924), \"['b', 'x', 'b', 'y']\": np.float64(0.9074), \"['b', 'x']\": np.float64(-8.9074), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.808), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0949), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8098), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0928), \"['b', 'y', 'a', 'x']\": np.float64(-0.9092), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0927), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0109), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0929), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0106), \"['b', 'y', 'a', 'y']\": np.float64(0.1043), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8104), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0927), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8103), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0926), \"['b', 'y', 'b', 'x']\": np.float64(-0.9094), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0926), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0106), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0926), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0106), \"['b', 'y', 'b', 'y']\": np.float64(0.104), \"['b', 'y']\": np.float64(1.0207)}\n",
      "It s train loss bro [0.25498950481414795, 0.22842249274253845, 0.21380114555358887, 0.19988365471363068, 0.18676072359085083, 0.1744820475578308, 0.16306264698505402, 0.1524910032749176, 0.14273710548877716, 0.1337582767009735, 0.12550471723079681]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 48 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.13512951135635376 and val_loss is 1.9062427282333374\n",
      "for 1 epochs, loss is 0.11204157024621964 and val_loss is 1.9258413314819336\n",
      "for 2 epochs, loss is 0.10556288808584213 and val_loss is 1.9448639154434204\n",
      "for 3 epochs, loss is 0.09960733354091644 and val_loss is 1.9633411169052124\n",
      "for 4 epochs, loss is 0.09412987530231476 and val_loss is 1.9814540147781372\n",
      "for 5 epochs, loss is 0.08908896893262863 and val_loss is 1.9994488954544067\n",
      "for 6 epochs, loss is 0.08444572240114212 and val_loss is 2.017536163330078\n",
      "for 7 epochs, loss is 0.08016455173492432 and val_loss is 2.0358052253723145\n",
      "for 8 epochs, loss is 0.07621276378631592 and val_loss is 2.0541977882385254\n",
      "for 9 epochs, loss is 0.07256075739860535 and val_loss is 2.072547197341919\n",
      "for 10 epochs, loss is 0.06918174773454666 and val_loss is 2.0906519889831543\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.2154), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5104), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.2086), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.515), \"['a', 'x', 'a', 'x']\": np.float64(-8.7712), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5104), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0317), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.51), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.032), \"['a', 'x', 'a', 'y']\": np.float64(0.5449), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.2081), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5088), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.2072), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.509), \"['a', 'x', 'b', 'x']\": np.float64(-8.7639), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5149), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0319), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5149), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0319), \"['a', 'x', 'b', 'y']\": np.float64(0.5498), \"['a', 'x']\": np.float64(-9.3646), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5148), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.032), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5144), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0323), \"['a', 'y', 'a', 'x']\": np.float64(-0.5496), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.032), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.002), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.032), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.002), \"['a', 'y', 'a', 'y']\": np.float64(0.0341), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5143), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0319), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5143), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0319), \"['a', 'y', 'b', 'x']\": np.float64(-0.5492), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0323), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.002), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0323), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.002), \"['a', 'y', 'b', 'y']\": np.float64(0.0345), \"['a', 'y']\": np.float64(0.5868), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.1976), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5093), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.1908), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5139), \"['b', 'x', 'a', 'x']\": np.float64(-8.7522), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5082), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0316), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5078), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0319), \"['b', 'x', 'a', 'y']\": np.float64(0.5426), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.1962), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5081), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.1953), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5082), \"['b', 'x', 'b', 'x']\": np.float64(-8.7512), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5082), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0315), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5082), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0315), \"['b', 'x', 'b', 'y']\": np.float64(0.5427), \"['b', 'x']\": np.float64(-9.3448), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.526), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0327), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5256), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.033), \"['b', 'y', 'a', 'x']\": np.float64(-0.5616), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0326), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.002), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0326), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.002), \"['b', 'y', 'a', 'y']\": np.float64(0.0348), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.526), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0326), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5259), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0326), \"['b', 'y', 'b', 'x']\": np.float64(-0.5616), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0326), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.002), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0326), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.002), \"['b', 'y', 'b', 'y']\": np.float64(0.0348), \"['b', 'y']\": np.float64(0.5997)}\n",
      "It s train loss bro [0.13512951135635376, 0.11204157024621964, 0.10556288808584213, 0.09960733354091644, 0.09412987530231476, 0.08908896893262863, 0.08444572240114212, 0.08016455173492432, 0.07621276378631592, 0.07256075739860535, 0.06918174773454666]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 49 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.707655429840088 and val_loss is 2.0903046131134033\n",
      "for 1 epochs, loss is 2.8426589965820312 and val_loss is 2.0879273414611816\n",
      "for 2 epochs, loss is 2.8377671241760254 and val_loss is 2.0837185382843018\n",
      "for 3 epochs, loss is 2.828878402709961 and val_loss is 2.077951192855835\n",
      "for 4 epochs, loss is 2.8164873123168945 and val_loss is 2.071260452270508\n",
      "for 5 epochs, loss is 2.801039457321167 and val_loss is 2.065195083618164\n",
      "for 6 epochs, loss is 2.7829339504241943 and val_loss is 2.0627517700195312\n",
      "for 7 epochs, loss is 2.7625274658203125 and val_loss is 2.0673885345458984\n",
      "for 8 epochs, loss is 2.740138292312622 and val_loss is 2.080204725265503\n",
      "for 9 epochs, loss is 2.7160515785217285 and val_loss is 2.0994434356689453\n",
      "for 10 epochs, loss is 2.690518856048584 and val_loss is 2.121307134628296\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.9176), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5926), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.909), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5988), \"['a', 'x', 'a', 'x']\": np.float64(-8.5581), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5926), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0444), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5919), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0448), \"['a', 'x', 'a', 'y']\": np.float64(0.6405), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.9058), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5933), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.9047), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5936), \"['a', 'x', 'b', 'x']\": np.float64(-8.5487), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5985), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0449), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5984), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0449), \"['a', 'x', 'b', 'y']\": np.float64(0.6472), \"['a', 'x']\": np.float64(-9.2503), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5947), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0445), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.594), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.045), \"['a', 'y', 'a', 'x']\": np.float64(-0.6428), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0445), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0033), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0445), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0034), \"['a', 'y', 'a', 'y']\": np.float64(0.0481), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5938), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0446), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5937), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0446), \"['a', 'y', 'b', 'x']\": np.float64(-0.6421), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.045), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0034), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.045), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0034), \"['a', 'y', 'b', 'y']\": np.float64(0.0486), \"['a', 'y']\": np.float64(0.6948), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.8929), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5907), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.8842), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5969), \"['b', 'x', 'a', 'x']\": np.float64(-8.5313), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5923), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0443), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5917), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0448), \"['b', 'x', 'a', 'y']\": np.float64(0.6402), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.8886), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.592), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.8875), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5923), \"['b', 'x', 'b', 'x']\": np.float64(-8.5301), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5923), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0445), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5923), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0445), \"['b', 'x', 'b', 'y']\": np.float64(0.6405), \"['b', 'x']\": np.float64(-9.225), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6114), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0458), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6107), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0462), \"['b', 'y', 'a', 'x']\": np.float64(-0.6608), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0459), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0034), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0459), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0035), \"['b', 'y', 'a', 'y']\": np.float64(0.0496), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6111), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0459), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.611), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0459), \"['b', 'y', 'b', 'x']\": np.float64(-0.6608), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0459), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0034), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0459), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0034), \"['b', 'y', 'b', 'y']\": np.float64(0.0496), \"['b', 'y']\": np.float64(0.7146)}\n",
      "It s train loss bro [2.707655429840088, 2.8426589965820312, 2.8377671241760254, 2.828878402709961, 2.8164873123168945, 2.801039457321167, 2.7829339504241943, 2.7625274658203125, 2.740138292312622, 2.7160515785217285, 2.690518856048584]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 50 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.08436596393585205 and val_loss is 2.1607038974761963\n",
      "for 1 epochs, loss is 0.09901194274425507 and val_loss is 2.115441083908081\n",
      "for 2 epochs, loss is 0.1204259842634201 and val_loss is 2.0698771476745605\n",
      "for 3 epochs, loss is 0.14192835986614227 and val_loss is 2.030686140060425\n",
      "for 4 epochs, loss is 0.16271421313285828 and val_loss is 1.9983948469161987\n",
      "for 5 epochs, loss is 0.18209904432296753 and val_loss is 1.972493290901184\n",
      "for 6 epochs, loss is 0.19954802095890045 and val_loss is 1.952269434928894\n",
      "for 7 epochs, loss is 0.2146836817264557 and val_loss is 1.9370120763778687\n",
      "for 8 epochs, loss is 0.22727812826633453 and val_loss is 1.926067590713501\n",
      "for 9 epochs, loss is 0.2372371107339859 and val_loss is 1.9188538789749146\n",
      "for 10 epochs, loss is 0.2445785105228424 and val_loss is 1.9148585796356201\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.7951), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2707), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7729), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.289), \"['a', 'x', 'a', 'x']\": np.float64(-6.1231), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2707), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3368), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2649), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3416), \"['a', 'x', 'a', 'y']\": np.float64(1.6227), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.7587), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2768), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7509), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2832), \"['a', 'x', 'b', 'x']\": np.float64(-6.0949), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2849), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.345), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2831), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3465), \"['a', 'x', 'b', 'y']\": np.float64(1.6461), \"['a', 'x']\": np.float64(-7.819), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2811), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3395), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2752), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3444), \"['a', 'y', 'a', 'x']\": np.float64(-1.6359), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3395), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.09), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3379), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0913), \"['a', 'y', 'a', 'y']\": np.float64(0.4335), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2714), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3411), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2693), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3428), \"['a', 'y', 'b', 'x']\": np.float64(-1.6284), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3433), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0922), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3428), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0926), \"['a', 'y', 'b', 'y']\": np.float64(0.4398), \"['a', 'y']\": np.float64(2.089), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.7539), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2598), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.732), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.278), \"['b', 'x', 'a', 'x']\": np.float64(-6.0705), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2755), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.338), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2697), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3429), \"['b', 'x', 'a', 'y']\": np.float64(1.6288), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.732), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2697), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7243), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.276), \"['b', 'x', 'b', 'x']\": np.float64(-6.0607), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2778), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3431), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.276), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3446), \"['b', 'x', 'b', 'y']\": np.float64(1.6369), \"['b', 'x']\": np.float64(-7.7751), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2962), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3435), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2903), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3485), \"['b', 'y', 'a', 'x']\": np.float64(-1.6552), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.348), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0922), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3464), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0935), \"['b', 'y', 'a', 'y']\": np.float64(0.4444), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2905), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3463), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2884), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.348), \"['b', 'y', 'b', 'x']\": np.float64(-1.6529), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3485), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0936), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.348), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.094), \"['b', 'y', 'b', 'y']\": np.float64(0.4464), \"['b', 'y']\": np.float64(2.1204)}\n",
      "It s train loss bro [0.08436596393585205, 0.09901194274425507, 0.1204259842634201, 0.14192835986614227, 0.16271421313285828, 0.18209904432296753, 0.19954802095890045, 0.2146836817264557, 0.22727812826633453, 0.2372371107339859, 0.2445785105228424]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 51 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2591203451156616 and val_loss is 1.9136946201324463\n",
      "for 1 epochs, loss is 0.2519570291042328 and val_loss is 1.9149070978164673\n",
      "for 2 epochs, loss is 0.2523866891860962 and val_loss is 1.9181549549102783\n",
      "for 3 epochs, loss is 0.25095006823539734 and val_loss is 1.9231451749801636\n",
      "for 4 epochs, loss is 0.24790966510772705 and val_loss is 1.9296196699142456\n",
      "for 5 epochs, loss is 0.24352754652500153 and val_loss is 1.9373562335968018\n",
      "for 6 epochs, loss is 0.23805513978004456 and val_loss is 1.9461617469787598\n",
      "for 7 epochs, loss is 0.23172545433044434 and val_loss is 1.9558699131011963\n",
      "for 8 epochs, loss is 0.2247493863105774 and val_loss is 1.9663350582122803\n",
      "for 9 epochs, loss is 0.2173127979040146 and val_loss is 1.9774301052093506\n",
      "for 10 epochs, loss is 0.20957604050636292 and val_loss is 1.9890445470809937\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.4669), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1647), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.4604), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1683), \"['a', 'x', 'a', 'x']\": np.float64(-6.6867), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1646), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2482), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1634), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2489), \"['a', 'x', 'a', 'y']\": np.float64(1.4246), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.4621), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1607), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.4576), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1635), \"['a', 'x', 'b', 'x']\": np.float64(-6.6787), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1679), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2489), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1677), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2489), \"['a', 'x', 'b', 'y']\": np.float64(1.429), \"['a', 'x']\": np.float64(-8.1787), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1619), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2475), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1605), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2483), \"['a', 'y', 'a', 'x']\": np.float64(-1.4212), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2476), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0528), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2473), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0529), \"['a', 'y', 'a', 'y']\": np.float64(0.3029), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.161), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2467), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.16), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2473), \"['a', 'y', 'b', 'x']\": np.float64(-1.4196), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2482), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0529), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2482), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0529), \"['a', 'y', 'b', 'y']\": np.float64(0.3037), \"['a', 'y']\": np.float64(1.7384), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.4588), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.163), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.4523), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1666), \"['b', 'x', 'a', 'x']\": np.float64(-6.6768), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.16), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2472), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1587), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2479), \"['b', 'x', 'a', 'y']\": np.float64(1.4189), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.4561), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1594), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.4516), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1622), \"['b', 'x', 'b', 'x']\": np.float64(-6.6713), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1624), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2478), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1622), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2478), \"['b', 'x', 'b', 'y']\": np.float64(1.4223), \"['b', 'x']\": np.float64(-8.164), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1663), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2485), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1649), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2493), \"['b', 'y', 'a', 'x']\": np.float64(-1.4266), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2486), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.053), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2483), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0531), \"['b', 'y', 'a', 'y']\": np.float64(0.3041), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1665), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2479), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1655), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2485), \"['b', 'y', 'b', 'x']\": np.float64(-1.4263), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2485), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.053), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2485), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.053), \"['b', 'y', 'b', 'y']\": np.float64(0.3041), \"['b', 'y']\": np.float64(1.7454)}\n",
      "It s train loss bro [0.2591203451156616, 0.2519570291042328, 0.2523866891860962, 0.25095006823539734, 0.24790966510772705, 0.24352754652500153, 0.23805513978004456, 0.23172545433044434, 0.2247493863105774, 0.2173127979040146, 0.20957604050636292]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 52 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.21225079894065857 and val_loss is 2.001103639602661\n",
      "for 1 epochs, loss is 0.19370797276496887 and val_loss is 2.013491630554199\n",
      "for 2 epochs, loss is 0.1857832968235016 and val_loss is 2.0261332988739014\n",
      "for 3 epochs, loss is 0.17797313630580902 and val_loss is 2.0389597415924072\n",
      "for 4 epochs, loss is 0.17033356428146362 and val_loss is 2.051910877227783\n",
      "for 5 epochs, loss is 0.1629071831703186 and val_loss is 2.064927339553833\n",
      "for 6 epochs, loss is 0.15572521090507507 and val_loss is 2.0779595375061035\n",
      "for 7 epochs, loss is 0.14880897104740143 and val_loss is 2.0909578800201416\n",
      "for 8 epochs, loss is 0.1421724259853363 and val_loss is 2.103874921798706\n",
      "for 9 epochs, loss is 0.13582317531108856 and val_loss is 2.1166672706604004\n",
      "for 10 epochs, loss is 0.12976393103599548 and val_loss is 2.129286527633667\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.8679), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8671), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.8743), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8584), \"['a', 'x', 'a', 'x']\": np.float64(-7.7859), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.867), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1096), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8679), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1083), \"['a', 'x', 'a', 'y']\": np.float64(0.983), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.8847), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8573), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.8829), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8572), \"['a', 'x', 'b', 'x']\": np.float64(-7.7932), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8589), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1078), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8595), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.107), \"['a', 'x', 'b', 'y']\": np.float64(0.9732), \"['a', 'x']\": np.float64(-8.8266), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8614), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1088), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8622), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1077), \"['a', 'y', 'a', 'x']\": np.float64(-0.9766), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1089), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0138), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.109), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0136), \"['a', 'y', 'a', 'y']\": np.float64(0.1234), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8637), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1076), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8635), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1075), \"['a', 'y', 'b', 'x']\": np.float64(-0.9777), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1077), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0135), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1078), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0134), \"['a', 'y', 'b', 'y']\": np.float64(0.122), \"['a', 'y']\": np.float64(1.1073), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.8809), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8687), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.8873), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8601), \"['b', 'x', 'a', 'x']\": np.float64(-7.8007), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8567), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1083), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8577), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1071), \"['b', 'x', 'a', 'y']\": np.float64(0.9714), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.8895), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8579), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.8878), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8578), \"['b', 'x', 'b', 'x']\": np.float64(-7.7987), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8572), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1075), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8578), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1068), \"['b', 'x', 'b', 'y']\": np.float64(0.9713), \"['b', 'x']\": np.float64(-8.83), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8553), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.108), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8561), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1069), \"['b', 'y', 'a', 'x']\": np.float64(-0.9696), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1073), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0136), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1074), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0134), \"['b', 'y', 'a', 'y']\": np.float64(0.1216), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8572), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1067), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.857), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1067), \"['b', 'y', 'b', 'x']\": np.float64(-0.9703), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1066), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0134), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1067), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0133), \"['b', 'y', 'b', 'y']\": np.float64(0.1208), \"['b', 'y']\": np.float64(1.0986)}\n",
      "It s train loss bro [0.21225079894065857, 0.19370797276496887, 0.1857832968235016, 0.17797313630580902, 0.17033356428146362, 0.1629071831703186, 0.15572521090507507, 0.14880897104740143, 0.1421724259853363, 0.13582317531108856, 0.12976393103599548]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 53 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.123845338821411 and val_loss is 2.129854440689087\n",
      "for 1 epochs, loss is 2.1968555450439453 and val_loss is 2.12980055809021\n",
      "for 2 epochs, loss is 2.1945993900299072 and val_loss is 2.1291825771331787\n",
      "for 3 epochs, loss is 2.1895294189453125 and val_loss is 2.128042459487915\n",
      "for 4 epochs, loss is 2.1819610595703125 and val_loss is 2.126415252685547\n",
      "for 5 epochs, loss is 2.1721832752227783 and val_loss is 2.1243345737457275\n",
      "for 6 epochs, loss is 2.160457134246826 and val_loss is 2.121835231781006\n",
      "for 7 epochs, loss is 2.147020101547241 and val_loss is 2.1189444065093994\n",
      "for 8 epochs, loss is 2.132086992263794 and val_loss is 2.1156980991363525\n",
      "for 9 epochs, loss is 2.1158499717712402 and val_loss is 2.112126350402832\n",
      "for 10 epochs, loss is 2.0984835624694824 and val_loss is 2.1082615852355957\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.5506), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9429), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.5626), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9287), \"['a', 'x', 'a', 'x']\": np.float64(-7.5445), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9427), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1359), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9447), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1336), \"['a', 'x', 'a', 'y']\": np.float64(1.086), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.5772), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.93), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.5755), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9298), \"['a', 'x', 'b', 'x']\": np.float64(-7.5583), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9297), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1326), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9306), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1316), \"['a', 'x', 'b', 'y']\": np.float64(1.0696), \"['a', 'x']\": np.float64(-8.6891), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9358), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1347), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9375), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1327), \"['a', 'y', 'a', 'x']\": np.float64(-1.0778), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1349), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0194), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1351), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0191), \"['a', 'y', 'a', 'y']\": np.float64(0.1553), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9398), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1329), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9396), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1329), \"['a', 'y', 'b', 'x']\": np.float64(-1.08), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1328), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0189), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1329), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0188), \"['a', 'y', 'b', 'y']\": np.float64(0.1528), \"['a', 'y']\": np.float64(1.2415), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.5744), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9463), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.5865), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9321), \"['b', 'x', 'a', 'x']\": np.float64(-7.5719), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9294), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.134), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9314), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1317), \"['b', 'x', 'a', 'y']\": np.float64(1.0706), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.5874), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9314), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.5857), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9312), \"['b', 'x', 'b', 'x']\": np.float64(-7.5701), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9303), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1327), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9313), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1317), \"['b', 'x', 'b', 'y']\": np.float64(1.0704), \"['b', 'x']\": np.float64(-8.7015), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9245), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1331), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9262), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1311), \"['b', 'y', 'a', 'x']\": np.float64(-1.0648), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1318), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.019), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1321), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0187), \"['b', 'y', 'a', 'y']\": np.float64(0.1519), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9275), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1311), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9272), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1311), \"['b', 'y', 'b', 'x']\": np.float64(-1.0658), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.131), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0187), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1311), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0185), \"['b', 'y', 'b', 'y']\": np.float64(0.1507), \"['b', 'y']\": np.float64(1.2251)}\n",
      "It s train loss bro [2.123845338821411, 2.1968555450439453, 2.1945993900299072, 2.1895294189453125, 2.1819610595703125, 2.1721832752227783, 2.160457134246826, 2.147020101547241, 2.132086992263794, 2.1158499717712402, 2.0984835624694824]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 54 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.16532252728939056 and val_loss is 2.073331832885742\n",
      "for 1 epochs, loss is 0.16641351580619812 and val_loss is 2.0429062843322754\n",
      "for 2 epochs, loss is 0.19115717709064484 and val_loss is 2.017718553543091\n",
      "for 3 epochs, loss is 0.21463078260421753 and val_loss is 1.9974372386932373\n",
      "for 4 epochs, loss is 0.23612192273139954 and val_loss is 1.9814698696136475\n",
      "for 5 epochs, loss is 0.2550863027572632 and val_loss is 1.9692014455795288\n",
      "for 6 epochs, loss is 0.2711550295352936 and val_loss is 1.9600743055343628\n",
      "for 7 epochs, loss is 0.2841261327266693 and val_loss is 1.9536118507385254\n",
      "for 8 epochs, loss is 0.2939477264881134 and val_loss is 1.9494229555130005\n",
      "for 9 epochs, loss is 0.30069270730018616 and val_loss is 1.9471901655197144\n",
      "for 10 epochs, loss is 0.3045324683189392 and val_loss is 1.9466625452041626\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.0223), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3866), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.0524), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3561), \"['a', 'x', 'a', 'x']\": np.float64(-5.4559), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3862), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4784), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3971), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4674), \"['a', 'x', 'a', 'y']\": np.float64(1.8808), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.075), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3753), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.0766), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3725), \"['a', 'x', 'b', 'x']\": np.float64(-5.4967), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3615), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4621), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3642), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4593), \"['a', 'x', 'b', 'y']\": np.float64(1.8394), \"['a', 'x']\": np.float64(-7.4004), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3652), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4706), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3754), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4603), \"['a', 'y', 'a', 'x']\": np.float64(-1.8518), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.471), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1626), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4747), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1588), \"['a', 'y', 'a', 'y']\": np.float64(0.6391), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3836), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.467), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3842), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.466), \"['a', 'y', 'b', 'x']\": np.float64(-1.8663), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4622), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1569), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4631), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1559), \"['a', 'y', 'b', 'y']\": np.float64(0.6244), \"['a', 'y']\": np.float64(2.5126), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.0664), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4019), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.0969), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.371), \"['b', 'x', 'a', 'x']\": np.float64(-5.5157), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.372), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4735), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3828), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4626), \"['b', 'x', 'a', 'y']\": np.float64(1.8616), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.0907), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3806), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.0923), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3778), \"['b', 'x', 'b', 'x']\": np.float64(-5.5179), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3751), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4667), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3778), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4638), \"['b', 'x', 'b', 'y']\": np.float64(1.8577), \"['b', 'x']\": np.float64(-7.4401), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3481), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4647), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3582), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4545), \"['b', 'y', 'a', 'x']\": np.float64(-1.8286), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4574), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1579), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.461), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1542), \"['b', 'y', 'a', 'y']\": np.float64(0.6206), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3583), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4584), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3588), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4575), \"['b', 'y', 'b', 'x']\": np.float64(-1.8322), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4566), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.155), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4575), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.154), \"['b', 'y', 'b', 'y']\": np.float64(0.6168), \"['b', 'y']\": np.float64(2.4704)}\n",
      "It s train loss bro [0.16532252728939056, 0.16641351580619812, 0.19115717709064484, 0.21463078260421753, 0.23612192273139954, 0.2550863027572632, 0.2711550295352936, 0.2841261327266693, 0.2939477264881134, 0.30069270730018616, 0.3045324683189392]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 55 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3242102265357971 and val_loss is 1.9477081298828125\n",
      "for 1 epochs, loss is 0.3045285940170288 and val_loss is 1.9500962495803833\n",
      "for 2 epochs, loss is 0.30128875374794006 and val_loss is 1.9537039995193481\n",
      "for 3 epochs, loss is 0.2963096499443054 and val_loss is 1.9584301710128784\n",
      "for 4 epochs, loss is 0.28990185260772705 and val_loss is 1.96419358253479\n",
      "for 5 epochs, loss is 0.28235745429992676 and val_loss is 1.9709221124649048\n",
      "for 6 epochs, loss is 0.2739420533180237 and val_loss is 1.9785535335540771\n",
      "for 7 epochs, loss is 0.26489248871803284 and val_loss is 1.9870295524597168\n",
      "for 8 epochs, loss is 0.25541406869888306 and val_loss is 1.99629545211792\n",
      "for 9 epochs, loss is 0.24568253755569458 and val_loss is 2.0062975883483887\n",
      "for 10 epochs, loss is 0.2358439862728119 and val_loss is 2.0169835090637207\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.079), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2366), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0746), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2402), \"['a', 'x', 'a', 'x']\": np.float64(-6.3645), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2365), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3011), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2355), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.302), \"['a', 'x', 'a', 'y']\": np.float64(1.5496), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.073), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.238), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.0717), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2377), \"['a', 'x', 'b', 'x']\": np.float64(-6.3588), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2401), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3021), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2395), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3025), \"['a', 'x', 'b', 'y']\": np.float64(1.5541), \"['a', 'x']\": np.float64(-7.9752), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2394), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3018), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2383), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3027), \"['a', 'y', 'a', 'x']\": np.float64(-1.5531), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3018), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0735), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3016), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0737), \"['a', 'y', 'a', 'y']\": np.float64(0.3782), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2381), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3021), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2378), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3021), \"['a', 'y', 'b', 'x']\": np.float64(-1.5519), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3027), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0737), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3025), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0738), \"['a', 'y', 'b', 'y']\": np.float64(0.3793), \"['a', 'y']\": np.float64(1.9464), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.0703), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2345), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0659), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2381), \"['b', 'x', 'a', 'x']\": np.float64(-6.3536), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2372), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3013), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2363), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3022), \"['b', 'x', 'a', 'y']\": np.float64(1.5505), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.0675), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2367), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.0661), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2363), \"['b', 'x', 'b', 'x']\": np.float64(-6.3519), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2368), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3013), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2363), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3017), \"['b', 'x', 'b', 'y']\": np.float64(1.5501), \"['b', 'x']\": np.float64(-7.9639), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2451), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3032), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.244), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.304), \"['b', 'y', 'a', 'x']\": np.float64(-1.5603), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3033), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0739), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.303), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0741), \"['b', 'y', 'a', 'y']\": np.float64(0.3801), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2442), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3036), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2439), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3036), \"['b', 'y', 'b', 'x']\": np.float64(-1.5596), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3037), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.074), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3036), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0741), \"['b', 'y', 'b', 'y']\": np.float64(0.3806), \"['b', 'y']\": np.float64(1.9554)}\n",
      "It s train loss bro [0.3242102265357971, 0.3045285940170288, 0.30128875374794006, 0.2963096499443054, 0.28990185260772705, 0.28235745429992676, 0.2739420533180237, 0.26489248871803284, 0.25541406869888306, 0.24568253755569458, 0.2358439862728119]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 56 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.571081519126892 and val_loss is 2.0174810886383057\n",
      "for 1 epochs, loss is 1.6374620199203491 and val_loss is 2.0174150466918945\n",
      "for 2 epochs, loss is 1.6368809938430786 and val_loss is 2.016843318939209\n",
      "for 3 epochs, loss is 1.6344106197357178 and val_loss is 2.015821933746338\n",
      "for 4 epochs, loss is 1.6302508115768433 and val_loss is 2.01440167427063\n",
      "for 5 epochs, loss is 1.6245853900909424 and val_loss is 2.012629508972168\n",
      "for 6 epochs, loss is 1.6175801753997803 and val_loss is 2.010549783706665\n",
      "for 7 epochs, loss is 1.6093884706497192 and val_loss is 2.0082027912139893\n",
      "for 8 epochs, loss is 1.6001474857330322 and val_loss is 2.0056254863739014\n",
      "for 9 epochs, loss is 1.589982509613037 and val_loss is 2.002851724624634\n",
      "for 10 epochs, loss is 1.5790071487426758 and val_loss is 1.9999123811721802\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.8194), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2779), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.8135), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2829), \"['a', 'x', 'a', 'x']\": np.float64(-6.1455), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2779), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3389), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2764), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3402), \"['a', 'x', 'a', 'y']\": np.float64(1.6296), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.8108), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2799), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.8096), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2796), \"['a', 'x', 'b', 'x']\": np.float64(-6.138), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2826), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3405), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2819), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.341), \"['a', 'x', 'b', 'y']\": np.float64(1.6359), \"['a', 'x']\": np.float64(-7.8365), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2808), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3396), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2793), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.341), \"['a', 'y', 'a', 'x']\": np.float64(-1.6333), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3396), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0901), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3392), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0904), \"['a', 'y', 'a', 'y']\": np.float64(0.4331), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2786), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3402), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2783), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3401), \"['a', 'y', 'b', 'x']\": np.float64(-1.6314), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3409), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0905), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3407), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0906), \"['a', 'y', 'b', 'y']\": np.float64(0.4348), \"['a', 'y']\": np.float64(2.0828), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.8085), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.275), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.8026), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.28), \"['b', 'x', 'a', 'x']\": np.float64(-6.1316), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2793), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3392), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2778), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3406), \"['b', 'x', 'a', 'y']\": np.float64(1.6314), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.8045), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2783), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.8034), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2779), \"['b', 'x', 'b', 'x']\": np.float64(-6.1301), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2786), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3394), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2779), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.34), \"['b', 'x', 'b', 'y']\": np.float64(1.6309), \"['b', 'x']\": np.float64(-7.8232), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2876), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3414), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2861), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3428), \"['b', 'y', 'a', 'x']\": np.float64(-1.6419), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3418), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0906), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3414), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.091), \"['b', 'y', 'a', 'y']\": np.float64(0.4359), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2862), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3422), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2859), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3421), \"['b', 'y', 'b', 'x']\": np.float64(-1.641), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3423), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0909), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3421), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.091), \"['b', 'y', 'b', 'y']\": np.float64(0.4366), \"['b', 'y']\": np.float64(2.0943)}\n",
      "It s train loss bro [1.571081519126892, 1.6374620199203491, 1.6368809938430786, 1.6344106197357178, 1.6302508115768433, 1.6245853900909424, 1.6175801753997803, 1.6093884706497192, 1.6001474857330322, 1.589982509613037, 1.5790071487426758]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 57 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3478131890296936 and val_loss is 1.975968599319458\n",
      "for 1 epochs, loss is 0.2722620964050293 and val_loss is 1.957433819770813\n",
      "for 2 epochs, loss is 0.2981491684913635 and val_loss is 1.9433330297470093\n",
      "for 3 epochs, loss is 0.3211304843425751 and val_loss is 1.932845115661621\n",
      "for 4 epochs, loss is 0.34075653553009033 and val_loss is 1.9252965450286865\n",
      "for 5 epochs, loss is 0.35674965381622314 and val_loss is 1.9201501607894897\n",
      "for 6 epochs, loss is 0.36899325251579285 and val_loss is 1.9169878959655762\n",
      "for 7 epochs, loss is 0.377511590719223 and val_loss is 1.9154938459396362\n",
      "for 8 epochs, loss is 0.38244691491127014 and val_loss is 1.9154366254806519\n",
      "for 9 epochs, loss is 0.3840349614620209 and val_loss is 1.9166513681411743\n",
      "for 10 epochs, loss is 0.38257911801338196 and val_loss is 1.919022798538208\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.2299), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4359), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.2225), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4423), \"['a', 'x', 'a', 'x']\": np.float64(-4.7064), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4359), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.6383), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4326), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6412), \"['a', 'x', 'a', 'y']\": np.float64(2.0922), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.2183), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4368), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.2171), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4372), \"['a', 'x', 'b', 'x']\": np.float64(-4.6955), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4408), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6426), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4399), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.6433), \"['a', 'x', 'b', 'y']\": np.float64(2.1016), \"['a', 'x']\": np.float64(-6.8577), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4387), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.6396), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4354), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.6425), \"['a', 'y', 'a', 'x']\": np.float64(-2.0964), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.6396), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.2843), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.6381), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.2856), \"['a', 'y', 'a', 'y']\": np.float64(0.9319), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4335), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.64), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.433), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6402), \"['a', 'y', 'b', 'x']\": np.float64(-2.0915), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.6418), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.2862), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.6414), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.2865), \"['a', 'y', 'b', 'y']\": np.float64(0.9361), \"['a', 'y']\": np.float64(3.0546), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.2181), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4306), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.2107), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.437), \"['b', 'x', 'a', 'x']\": np.float64(-4.6892), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4367), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.6387), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4334), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6416), \"['b', 'x', 'a', 'y']\": np.float64(2.0934), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.2128), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4343), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.2115), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4347), \"['b', 'x', 'b', 'x']\": np.float64(-4.6874), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4356), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6403), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4347), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.6409), \"['b', 'x', 'b', 'y']\": np.float64(2.094), \"['b', 'x']\": np.float64(-6.8414), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4431), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.6415), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4397), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.6444), \"['b', 'y', 'a', 'x']\": np.float64(-2.1027), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.6436), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.2861), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.6421), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.2874), \"['b', 'y', 'a', 'y']\": np.float64(0.9378), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4403), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.643), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4398), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6432), \"['b', 'y', 'b', 'x']\": np.float64(-2.1014), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.6436), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.287), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.6432), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.2873), \"['b', 'y', 'b', 'y']\": np.float64(0.9388), \"['b', 'y']\": np.float64(3.0671)}\n",
      "It s train loss bro [0.3478131890296936, 0.2722620964050293, 0.2981491684913635, 0.3211304843425751, 0.34075653553009033, 0.35674965381622314, 0.36899325251579285, 0.377511590719223, 0.38244691491127014, 0.3840349614620209, 0.38257911801338196]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 58 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3862072229385376 and val_loss is 1.9225096702575684\n",
      "for 1 epochs, loss is 0.37207144498825073 and val_loss is 1.9270116090774536\n",
      "for 2 epochs, loss is 0.36375099420547485 and val_loss is 1.9324798583984375\n",
      "for 3 epochs, loss is 0.35383492708206177 and val_loss is 1.9388585090637207\n",
      "for 4 epochs, loss is 0.3426727056503296 and val_loss is 1.9460629224777222\n",
      "for 5 epochs, loss is 0.33058634400367737 and val_loss is 1.953950047492981\n",
      "for 6 epochs, loss is 0.3178648054599762 and val_loss is 1.9623034000396729\n",
      "for 7 epochs, loss is 0.30476152896881104 and val_loss is 1.9708991050720215\n",
      "for 8 epochs, loss is 0.2914939224720001 and val_loss is 1.9799914360046387\n",
      "for 9 epochs, loss is 0.27824416756629944 and val_loss is 1.9923630952835083\n",
      "for 10 epochs, loss is 0.26516082882881165 and val_loss is 2.0188260078430176\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.6958), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2999), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.6932), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3016), \"['a', 'x', 'a', 'x']\": np.float64(-6.0401), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2997), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3599), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2992), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3603), \"['a', 'x', 'a', 'y']\": np.float64(1.672), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.6948), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2982), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.6917), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3), \"['a', 'x', 'b', 'x']\": np.float64(-6.0368), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3015), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3603), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3012), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3605), \"['a', 'x', 'b', 'y']\": np.float64(1.6742), \"['a', 'x']\": np.float64(-7.7694), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3029), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3607), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3022), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3612), \"['a', 'y', 'a', 'x']\": np.float64(-1.676), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3608), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0999), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3606), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1), \"['a', 'y', 'a', 'y']\": np.float64(0.4641), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3028), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3603), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.302), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3608), \"['a', 'y', 'b', 'x']\": np.float64(-1.6753), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3612), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3611), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1), \"['a', 'y', 'b', 'y']\": np.float64(0.4646), \"['a', 'y']\": np.float64(2.156), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.6914), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2987), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.6888), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3004), \"['b', 'x', 'a', 'x']\": np.float64(-6.0345), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2971), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3592), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2965), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3595), \"['b', 'x', 'a', 'y']\": np.float64(1.6686), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.6899), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2968), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.6868), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2986), \"['b', 'x', 'b', 'x']\": np.float64(-6.0305), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.299), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3596), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2986), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3598), \"['b', 'x', 'b', 'y']\": np.float64(1.6709), \"['b', 'x']\": np.float64(-7.7595), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.307), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3618), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3063), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3623), \"['b', 'y', 'a', 'x']\": np.float64(-1.6812), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3618), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1002), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3617), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1003), \"['b', 'y', 'a', 'y']\": np.float64(0.4654), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3071), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3614), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3063), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3619), \"['b', 'y', 'b', 'x']\": np.float64(-1.6808), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.362), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1002), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3619), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1003), \"['b', 'y', 'b', 'y']\": np.float64(0.4657), \"['b', 'y']\": np.float64(2.1626)}\n",
      "It s train loss bro [0.3862072229385376, 0.37207144498825073, 0.36375099420547485, 0.35383492708206177, 0.3426727056503296, 0.33058634400367737, 0.3178648054599762, 0.30476152896881104, 0.2914939224720001, 0.27824416756629944, 0.26516082882881165]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 59 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2769489884376526 and val_loss is 2.0794637203216553\n",
      "for 1 epochs, loss is 0.23994910717010498 and val_loss is 2.1732230186462402\n",
      "for 2 epochs, loss is 0.2279774695634842 and val_loss is 2.2718772888183594\n",
      "for 3 epochs, loss is 0.21649624407291412 and val_loss is 2.3548364639282227\n",
      "for 4 epochs, loss is 0.2055368423461914 and val_loss is 2.4198224544525146\n",
      "for 5 epochs, loss is 0.19511668384075165 and val_loss is 2.471024990081787\n",
      "for 6 epochs, loss is 0.18524213135242462 and val_loss is 2.512643814086914\n",
      "for 7 epochs, loss is 0.17591068148612976 and val_loss is 2.547698497772217\n",
      "for 8 epochs, loss is 0.16711273789405823 and val_loss is 2.578190565109253\n",
      "for 9 epochs, loss is 0.1588335782289505 and val_loss is 2.605443000793457\n",
      "for 10 epochs, loss is 0.1510547250509262 and val_loss is 2.6303274631500244\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4963), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9623), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.4962), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9617), \"['a', 'x', 'a', 'x']\": np.float64(-7.4996), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.962), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1428), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9623), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1424), \"['a', 'x', 'a', 'y']\": np.float64(1.1109), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.5033), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9563), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4966), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9613), \"['a', 'x', 'b', 'x']\": np.float64(-7.4996), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9618), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1423), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9618), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1423), \"['a', 'x', 'b', 'y']\": np.float64(1.1102), \"['a', 'x']\": np.float64(-8.658), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9652), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.143), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9652), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1429), \"['a', 'y', 'a', 'x']\": np.float64(-1.1143), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1432), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0213), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1432), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0212), \"['a', 'y', 'a', 'y']\": np.float64(0.1654), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9666), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1421), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9656), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1429), \"['a', 'y', 'b', 'x']\": np.float64(-1.1146), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1429), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0211), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1429), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0211), \"['a', 'y', 'b', 'y']\": np.float64(0.1649), \"['a', 'y']\": np.float64(1.2867), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4977), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9625), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.4977), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9619), \"['b', 'x', 'a', 'x']\": np.float64(-7.5013), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9552), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1418), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9555), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1414), \"['b', 'x', 'a', 'y']\": np.float64(1.103), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.4981), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9555), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4915), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9606), \"['b', 'x', 'b', 'x']\": np.float64(-7.4936), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9606), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1422), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9606), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1421), \"['b', 'x', 'b', 'y']\": np.float64(1.1089), \"['b', 'x']\": np.float64(-8.6505), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9694), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1436), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9694), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1435), \"['b', 'y', 'a', 'x']\": np.float64(-1.1191), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1434), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0213), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1435), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0212), \"['b', 'y', 'a', 'y']\": np.float64(0.1656), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9704), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1427), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9695), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1435), \"['b', 'y', 'b', 'x']\": np.float64(-1.1191), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1434), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0212), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1434), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0212), \"['b', 'y', 'b', 'y']\": np.float64(0.1656), \"['b', 'y']\": np.float64(1.2919)}\n",
      "It s train loss bro [0.2769489884376526, 0.23994910717010498, 0.2279774695634842, 0.21649624407291412, 0.2055368423461914, 0.19511668384075165, 0.18524213135242462, 0.17591068148612976, 0.16711273789405823, 0.1588335782289505, 0.1510547250509262]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 60 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.15556514263153076 and val_loss is 2.6538078784942627\n",
      "for 1 epochs, loss is 0.13689596951007843 and val_loss is 2.6757686138153076\n",
      "for 2 epochs, loss is 0.13047178089618683 and val_loss is 2.696531295776367\n",
      "for 3 epochs, loss is 0.12445773184299469 and val_loss is 2.7163124084472656\n",
      "for 4 epochs, loss is 0.11882909387350082 and val_loss is 2.7352678775787354\n",
      "for 5 epochs, loss is 0.11356186866760254 and val_loss is 2.7535064220428467\n",
      "for 6 epochs, loss is 0.10863286256790161 and val_loss is 2.771111011505127\n",
      "for 7 epochs, loss is 0.1040196344256401 and val_loss is 2.7881417274475098\n",
      "for 8 epochs, loss is 0.09970056265592575 and val_loss is 2.80464243888855\n",
      "for 9 epochs, loss is 0.09565563499927521 and val_loss is 2.8206498622894287\n",
      "for 10 epochs, loss is 0.09186551719903946 and val_loss is 2.8361918926239014\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.6715), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6739), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.6708), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6741), \"['a', 'x', 'a', 'x']\": np.float64(-8.379), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6737), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0594), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6739), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0592), \"['a', 'x', 'a', 'y']\": np.float64(0.7361), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.6761), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6695), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.6709), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6734), \"['a', 'x', 'b', 'x']\": np.float64(-8.3782), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6741), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0592), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6741), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0592), \"['a', 'x', 'b', 'y']\": np.float64(0.7363), \"['a', 'x']\": np.float64(-9.1517), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6771), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0595), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6771), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0595), \"['a', 'y', 'a', 'x']\": np.float64(-0.7396), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0597), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0053), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0597), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0052), \"['a', 'y', 'a', 'y']\": np.float64(0.0652), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6778), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0591), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6773), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0595), \"['a', 'y', 'b', 'x']\": np.float64(-0.7398), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0595), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0052), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0595), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0052), \"['a', 'y', 'b', 'y']\": np.float64(0.065), \"['a', 'y']\": np.float64(0.808), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.6716), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.674), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.6709), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6741), \"['b', 'x', 'a', 'x']\": np.float64(-8.379), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6689), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.059), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6691), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0588), \"['b', 'x', 'a', 'y']\": np.float64(0.7308), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.6717), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6691), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.6664), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.673), \"['b', 'x', 'b', 'x']\": np.float64(-8.3733), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.673), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0592), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6731), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0591), \"['b', 'x', 'b', 'y']\": np.float64(0.7351), \"['b', 'x']\": np.float64(-9.1454), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6811), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0598), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6811), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0599), \"['b', 'y', 'a', 'x']\": np.float64(-0.7439), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0598), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0053), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0599), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0053), \"['b', 'y', 'a', 'y']\": np.float64(0.0654), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6817), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0595), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6812), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0598), \"['b', 'y', 'b', 'x']\": np.float64(-0.744), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0598), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0053), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0598), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0052), \"['b', 'y', 'b', 'y']\": np.float64(0.0653), \"['b', 'y']\": np.float64(0.8126)}\n",
      "It s train loss bro [0.15556514263153076, 0.13689596951007843, 0.13047178089618683, 0.12445773184299469, 0.11882909387350082, 0.11356186866760254, 0.10863286256790161, 0.1040196344256401, 0.09970056265592575, 0.09565563499927521, 0.09186551719903946]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 61 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.09882621467113495 and val_loss is 2.851436138153076\n",
      "for 1 epochs, loss is 0.08496477454900742 and val_loss is 2.866236448287964\n",
      "for 2 epochs, loss is 0.0818236842751503 and val_loss is 2.88061261177063\n",
      "for 3 epochs, loss is 0.07887432724237442 and val_loss is 2.8945837020874023\n",
      "for 4 epochs, loss is 0.07610229402780533 and val_loss is 2.9081625938415527\n",
      "for 5 epochs, loss is 0.07349503040313721 and val_loss is 2.9213662147521973\n",
      "for 6 epochs, loss is 0.07104030251502991 and val_loss is 2.93420672416687\n",
      "for 7 epochs, loss is 0.06872722506523132 and val_loss is 2.9466969966888428\n",
      "for 8 epochs, loss is 0.06654564291238785 and val_loss is 2.958850145339966\n",
      "for 9 epochs, loss is 0.0644858181476593 and val_loss is 2.970677375793457\n",
      "for 10 epochs, loss is 0.06253956258296967 and val_loss is 2.982191801071167\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3346), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4934), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3331), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4944), \"['a', 'x', 'a', 'x']\": np.float64(-8.8552), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4933), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0294), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4934), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0292), \"['a', 'x', 'a', 'y']\": np.float64(0.5242), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3339), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.493), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3327), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4934), \"['a', 'x', 'b', 'x']\": np.float64(-8.8537), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4943), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0293), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4944), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0293), \"['a', 'x', 'b', 'y']\": np.float64(0.5253), \"['a', 'x']\": np.float64(-9.4083), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4966), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0294), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4965), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0295), \"['a', 'y', 'a', 'x']\": np.float64(-0.5276), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0295), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0018), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0295), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0018), \"['a', 'y', 'a', 'y']\": np.float64(0.0314), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4968), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0294), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4967), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0294), \"['a', 'y', 'b', 'x']\": np.float64(-0.5277), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0294), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0017), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0294), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0017), \"['a', 'y', 'b', 'y']\": np.float64(0.0313), \"['a', 'y']\": np.float64(0.5608), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3308), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4932), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3293), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4942), \"['b', 'x', 'a', 'x']\": np.float64(-8.8512), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4926), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0293), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4927), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0292), \"['b', 'x', 'a', 'y']\": np.float64(0.5235), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3304), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4927), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3292), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4932), \"['b', 'x', 'b', 'x']\": np.float64(-8.8499), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4932), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0293), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4932), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0292), \"['b', 'x', 'b', 'y']\": np.float64(0.5241), \"['b', 'x']\": np.float64(-9.4032), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5002), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0296), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5001), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0297), \"['b', 'y', 'a', 'x']\": np.float64(-0.5314), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0297), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0018), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0297), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0018), \"['b', 'y', 'a', 'y']\": np.float64(0.0315), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5003), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0296), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5002), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0296), \"['b', 'y', 'b', 'x']\": np.float64(-0.5315), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0296), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0018), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0296), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0018), \"['b', 'y', 'b', 'y']\": np.float64(0.0315), \"['b', 'y']\": np.float64(0.5647)}\n",
      "It s train loss bro [0.09882621467113495, 0.08496477454900742, 0.0818236842751503, 0.07887432724237442, 0.07610229402780533, 0.07349503040313721, 0.07104030251502991, 0.06872722506523132, 0.06654564291238785, 0.0644858181476593, 0.06253956258296967]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 62 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.0678631141781807 and val_loss is 2.9934842586517334\n",
      "for 1 epochs, loss is 0.05894744023680687 and val_loss is 3.0044760704040527\n",
      "for 2 epochs, loss is 0.05728883296251297 and val_loss is 3.01517915725708\n",
      "for 3 epochs, loss is 0.05571587011218071 and val_loss is 3.0256025791168213\n",
      "for 4 epochs, loss is 0.05422274395823479 and val_loss is 3.035759210586548\n",
      "for 5 epochs, loss is 0.052803948521614075 and val_loss is 3.045657157897949\n",
      "for 6 epochs, loss is 0.051454585045576096 and val_loss is 3.0553066730499268\n",
      "for 7 epochs, loss is 0.05016983672976494 and val_loss is 3.0647192001342773\n",
      "for 8 epochs, loss is 0.04894541949033737 and val_loss is 3.073901414871216\n",
      "for 9 epochs, loss is 0.04777734726667404 and val_loss is 3.0828630924224854\n",
      "for 10 epochs, loss is 0.046662185341119766 and val_loss is 3.0916128158569336\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.7211), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3831), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.7193), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3845), \"['a', 'x', 'a', 'x']\": np.float64(-9.127), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.383), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0169), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3831), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0169), \"['a', 'x', 'a', 'y']\": np.float64(0.401), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7163), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.386), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.7187), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3834), \"['a', 'x', 'b', 'x']\": np.float64(-9.1252), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3845), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.017), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3845), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0169), \"['a', 'x', 'b', 'y']\": np.float64(0.4024), \"['a', 'x']\": np.float64(-9.5518), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3862), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.017), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3861), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.017), \"['a', 'y', 'a', 'x']\": np.float64(-0.4041), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0171), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0171), \"['a', 'y', 'a', 'y']\": np.float64(0.0179), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3861), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0171), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3862), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.017), \"['a', 'y', 'b', 'x']\": np.float64(-0.4042), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.017), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.017), \"['a', 'y', 'b', 'y']\": np.float64(0.0178), \"['a', 'y']\": np.float64(0.4231), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.7141), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3828), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.7123), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3842), \"['b', 'x', 'a', 'x']\": np.float64(-9.1197), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3858), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0171), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3859), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.017), \"['b', 'x', 'a', 'y']\": np.float64(0.4039), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7135), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3859), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.716), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3833), \"['b', 'x', 'b', 'x']\": np.float64(-9.1223), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3832), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0169), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3833), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0169), \"['b', 'x', 'b', 'y']\": np.float64(0.4012), \"['b', 'x']\": np.float64(-9.5475), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3893), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0171), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3892), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0172), \"['b', 'y', 'a', 'x']\": np.float64(-0.4074), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0172), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0172), \"['b', 'y', 'a', 'y']\": np.float64(0.018), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3892), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0172), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3893), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0171), \"['b', 'y', 'b', 'x']\": np.float64(-0.4074), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0171), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0171), \"['b', 'y', 'b', 'y']\": np.float64(0.0179), \"['b', 'y']\": np.float64(0.4264)}\n",
      "It s train loss bro [0.0678631141781807, 0.05894744023680687, 0.05728883296251297, 0.05571587011218071, 0.05422274395823479, 0.052803948521614075, 0.051454585045576096, 0.05016983672976494, 0.04894541949033737, 0.04777734726667404, 0.046662185341119766]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 63 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.04952803626656532 and val_loss is 3.100196361541748\n",
      "for 1 epochs, loss is 0.04457395151257515 and val_loss is 3.1085808277130127\n",
      "for 2 epochs, loss is 0.04359535872936249 and val_loss is 3.116772413253784\n",
      "for 3 epochs, loss is 0.042657848447561264 and val_loss is 3.1247808933258057\n",
      "for 4 epochs, loss is 0.04175890237092972 and val_loss is 3.1326122283935547\n",
      "for 5 epochs, loss is 0.040896225720644 and val_loss is 3.1402738094329834\n",
      "for 6 epochs, loss is 0.04006773605942726 and val_loss is 3.147770881652832\n",
      "for 7 epochs, loss is 0.039271339774131775 and val_loss is 3.15511155128479\n",
      "for 8 epochs, loss is 0.038505278527736664 and val_loss is 3.1623005867004395\n",
      "for 9 epochs, loss is 0.0377676747739315 and val_loss is 3.169344425201416\n",
      "for 10 epochs, loss is 0.037057094275951385 and val_loss is 3.1762475967407227\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.9659), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3115), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.964), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3131), \"['a', 'x', 'a', 'x']\": np.float64(-9.2971), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3115), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0109), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3115), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0109), \"['a', 'x', 'a', 'y']\": np.float64(0.323), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.9582), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3169), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.9633), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3119), \"['a', 'x', 'b', 'x']\": np.float64(-9.2951), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.313), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0109), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3131), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0109), \"['a', 'x', 'b', 'y']\": np.float64(0.3247), \"['a', 'x']\": np.float64(-9.6406), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3144), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0109), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3143), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.011), \"['a', 'y', 'a', 'x']\": np.float64(-0.326), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.011), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.011), \"['a', 'y', 'a', 'y']\": np.float64(0.0114), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3142), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0111), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3144), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0109), \"['a', 'y', 'b', 'x']\": np.float64(-0.326), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.011), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.011), \"['a', 'y', 'b', 'y']\": np.float64(0.0114), \"['a', 'y']\": np.float64(0.3381), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.9567), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3112), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9548), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3128), \"['b', 'x', 'a', 'x']\": np.float64(-9.2876), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3167), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0111), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3167), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0111), \"['b', 'x', 'a', 'y']\": np.float64(0.3285), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.956), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3168), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.9611), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3119), \"['b', 'x', 'b', 'x']\": np.float64(-9.2929), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3118), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0109), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3119), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0109), \"['b', 'x', 'b', 'y']\": np.float64(0.3234), \"['b', 'x']\": np.float64(-9.6369), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3171), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.011), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.317), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0111), \"['b', 'y', 'a', 'x']\": np.float64(-0.3288), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0111), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0111), \"['b', 'y', 'a', 'y']\": np.float64(0.0115), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3169), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0112), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3171), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.011), \"['b', 'y', 'b', 'x']\": np.float64(-0.3288), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.011), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.011), \"['b', 'y', 'b', 'y']\": np.float64(0.0114), \"['b', 'y']\": np.float64(0.341)}\n",
      "It s train loss bro [0.04952803626656532, 0.04457395151257515, 0.04359535872936249, 0.042657848447561264, 0.04175890237092972, 0.040896225720644, 0.04006773605942726, 0.039271339774131775, 0.038505278527736664, 0.0377676747739315, 0.037057094275951385]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 64 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.04130443558096886 and val_loss is 3.1830830574035645\n",
      "for 1 epochs, loss is 0.03570666164159775 and val_loss is 3.1897826194763184\n",
      "for 2 epochs, loss is 0.03506471589207649 and val_loss is 3.196352481842041\n",
      "for 3 epochs, loss is 0.03444493189454079 and val_loss is 3.2027955055236816\n",
      "for 4 epochs, loss is 0.03384608030319214 and val_loss is 3.2091188430786133\n",
      "for 5 epochs, loss is 0.033267054706811905 and val_loss is 3.215324640274048\n",
      "for 6 epochs, loss is 0.03270672634243965 and val_loss is 3.2214176654815674\n",
      "for 7 epochs, loss is 0.03216443955898285 and val_loss is 3.227403163909912\n",
      "for 8 epochs, loss is 0.03163906931877136 and val_loss is 3.233283042907715\n",
      "for 9 epochs, loss is 0.031129945069551468 and val_loss is 3.2390620708465576\n",
      "for 10 epochs, loss is 0.030636053532361984 and val_loss is 3.2447423934936523\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.1345), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2615), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.1326), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.263), \"['a', 'x', 'a', 'x']\": np.float64(-9.4134), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2614), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0075), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2614), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0075), \"['a', 'x', 'a', 'y']\": np.float64(0.2695), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.125), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2684), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.132), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2619), \"['a', 'x', 'b', 'x']\": np.float64(-9.4115), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.263), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0076), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.263), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0075), \"['a', 'x', 'b', 'y']\": np.float64(0.2711), \"['a', 'x']\": np.float64(-9.7008), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2641), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0076), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2641), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0076), \"['a', 'y', 'a', 'x']\": np.float64(-0.2722), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0076), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0076), \"['a', 'y', 'a', 'y']\": np.float64(0.0078), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2639), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0078), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2641), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0076), \"['a', 'y', 'b', 'x']\": np.float64(-0.2722), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0076), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0076), \"['a', 'y', 'b', 'y']\": np.float64(0.0078), \"['a', 'y']\": np.float64(0.2806), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.1239), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2612), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.122), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2627), \"['b', 'x', 'a', 'x']\": np.float64(-9.4025), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2683), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0077), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2683), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0077), \"['b', 'x', 'a', 'y']\": np.float64(0.2766), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.1233), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2684), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.1303), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2618), \"['b', 'x', 'b', 'x']\": np.float64(-9.4097), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2618), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0075), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2618), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0075), \"['b', 'x', 'b', 'y']\": np.float64(0.2699), \"['b', 'x']\": np.float64(-9.6977), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2665), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0076), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2664), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0077), \"['b', 'y', 'a', 'x']\": np.float64(-0.2746), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0077), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0077), \"['b', 'y', 'a', 'y']\": np.float64(0.0079), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2663), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0078), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2665), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0076), \"['b', 'y', 'b', 'x']\": np.float64(-0.2746), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0076), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0076), \"['b', 'y', 'b', 'y']\": np.float64(0.0079), \"['b', 'y']\": np.float64(0.283)}\n",
      "It s train loss bro [0.04130443558096886, 0.03570666164159775, 0.03506471589207649, 0.03444493189454079, 0.03384608030319214, 0.033267054706811905, 0.03270672634243965, 0.03216443955898285, 0.03163906931877136, 0.031129945069551468, 0.030636053532361984]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 65 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.03493533283472061 and val_loss is 3.250387191772461\n",
      "for 1 epochs, loss is 0.02968735061585903 and val_loss is 3.2559328079223633\n",
      "for 2 epochs, loss is 0.029231879860162735 and val_loss is 3.261383533477783\n",
      "for 3 epochs, loss is 0.028789404779672623 and val_loss is 3.26674222946167\n",
      "for 4 epochs, loss is 0.02835959754884243 and val_loss is 3.2720117568969727\n",
      "for 5 epochs, loss is 0.02794177643954754 and val_loss is 3.2771949768066406\n",
      "for 6 epochs, loss is 0.027535611763596535 and val_loss is 3.2822959423065186\n",
      "for 7 epochs, loss is 0.027140185236930847 and val_loss is 3.2873153686523438\n",
      "for 8 epochs, loss is 0.026755398139357567 and val_loss is 3.292257308959961\n",
      "for 9 epochs, loss is 0.02638067863881588 and val_loss is 3.2971205711364746\n",
      "for 10 epochs, loss is 0.026015575975179672 and val_loss is 3.301912307739258\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.2584), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2243), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.2566), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2258), \"['a', 'x', 'a', 'x']\": np.float64(-9.4984), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2243), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0055), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2243), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0055), \"['a', 'x', 'a', 'y']\": np.float64(0.2301), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.2477), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2324), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.2561), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2247), \"['a', 'x', 'b', 'x']\": np.float64(-9.4966), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2258), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0055), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2258), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0055), \"['a', 'x', 'b', 'y']\": np.float64(0.2317), \"['a', 'x']\": np.float64(-9.7447), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2268), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0055), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2268), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0055), \"['a', 'y', 'a', 'x']\": np.float64(-0.2327), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0055), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0055), \"['a', 'y', 'a', 'y']\": np.float64(0.0057), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2266), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0057), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2268), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0055), \"['a', 'y', 'b', 'x']\": np.float64(-0.2327), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0055), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0055), \"['a', 'y', 'b', 'y']\": np.float64(0.0057), \"['a', 'y']\": np.float64(0.2388), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.2469), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.224), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.2451), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2256), \"['b', 'x', 'a', 'x']\": np.float64(-9.4866), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2323), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0057), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2323), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0057), \"['b', 'x', 'a', 'y']\": np.float64(0.2384), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.2463), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2324), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.2547), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2247), \"['b', 'x', 'b', 'x']\": np.float64(-9.4952), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2246), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0055), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2247), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0055), \"['b', 'x', 'b', 'y']\": np.float64(0.2305), \"['b', 'x']\": np.float64(-9.7419), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2288), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0055), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2288), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0056), \"['b', 'y', 'a', 'x']\": np.float64(-0.2348), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0056), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0056), \"['b', 'y', 'a', 'y']\": np.float64(0.0057), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2286), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0057), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2289), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0056), \"['b', 'y', 'b', 'x']\": np.float64(-0.2348), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0056), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0056), \"['b', 'y', 'b', 'y']\": np.float64(0.0057), \"['b', 'y']\": np.float64(0.2409)}\n",
      "It s train loss bro [0.03493533283472061, 0.02968735061585903, 0.029231879860162735, 0.028789404779672623, 0.02835959754884243, 0.02794177643954754, 0.027535611763596535, 0.027140185236930847, 0.026755398139357567, 0.02638067863881588, 0.026015575975179672]\n",
      "% good predict : 0\n",
      "\u001b[0;34miteration 66 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.029770897701382637 and val_loss is 3.30668306350708\n",
      "for 1 epochs, loss is 0.025309262797236443 and val_loss is 3.31137752532959\n",
      "for 2 epochs, loss is 0.02496783807873726 and val_loss is 3.3159983158111572\n",
      "for 3 epochs, loss is 0.024635018780827522 and val_loss is 3.3205480575561523\n",
      "for 4 epochs, loss is 0.02431023307144642 and val_loss is 3.3250274658203125\n",
      "for 5 epochs, loss is 0.023993488401174545 and val_loss is 3.3294386863708496\n",
      "for 6 epochs, loss is 0.023684093728661537 and val_loss is 3.3337838649749756\n",
      "for 7 epochs, loss is 0.023382173851132393 and val_loss is 3.338064193725586\n",
      "for 8 epochs, loss is 0.02308715134859085 and val_loss is 3.342280387878418\n",
      "for 9 epochs, loss is 0.022798800840973854 and val_loss is 3.346435070037842\n",
      "for 10 epochs, loss is 0.02251701056957245 and val_loss is 3.3505287170410156\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.3537), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1956), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.352), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.197), \"['a', 'x', 'a', 'x']\": np.float64(-9.5635), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1955), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0041), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1955), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0041), \"['a', 'x', 'a', 'y']\": np.float64(0.2), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.3422), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2044), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.3515), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1959), \"['a', 'x', 'b', 'x']\": np.float64(-9.5618), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.197), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0041), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.197), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0041), \"['a', 'x', 'b', 'y']\": np.float64(0.2014), \"['a', 'x']\": np.float64(-9.7781), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1979), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0041), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1979), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0042), \"['a', 'y', 'a', 'x']\": np.float64(-0.2023), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0042), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0042), \"['a', 'y', 'a', 'y']\": np.float64(0.0043), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1977), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0043), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1979), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0041), \"['a', 'y', 'b', 'x']\": np.float64(-0.2023), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0042), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0042), \"['a', 'y', 'b', 'y']\": np.float64(0.0043), \"['a', 'y']\": np.float64(0.2069), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.3416), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1953), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.3399), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1967), \"['b', 'x', 'a', 'x']\": np.float64(-9.5511), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.2044), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0043), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2044), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0043), \"['b', 'x', 'a', 'y']\": np.float64(0.209), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.3411), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.2044), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.3504), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1959), \"['b', 'x', 'b', 'x']\": np.float64(-9.5607), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1959), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0041), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1959), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0041), \"['b', 'x', 'b', 'y']\": np.float64(0.2003), \"['b', 'x']\": np.float64(-9.7757), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1997), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0042), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1996), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0042), \"['b', 'y', 'a', 'x']\": np.float64(-0.2041), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0042), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0042), \"['b', 'y', 'a', 'y']\": np.float64(0.0043), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1995), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0044), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1997), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0042), \"['b', 'y', 'b', 'x']\": np.float64(-0.2042), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0042), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0042), \"['b', 'y', 'b', 'y']\": np.float64(0.0043), \"['b', 'y']\": np.float64(0.2088)}\n",
      "It s train loss bro [0.029770897701382637, 0.025309262797236443, 0.02496783807873726, 0.024635018780827522, 0.02431023307144642, 0.023993488401174545, 0.023684093728661537, 0.023382173851132393, 0.02308715134859085, 0.022798800840973854, 0.02251701056957245]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 67 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.027307815849781036 and val_loss is 3.3545594215393066\n",
      "for 1 epochs, loss is 0.021968817338347435 and val_loss is 3.3585305213928223\n",
      "for 2 epochs, loss is 0.021702541038393974 and val_loss is 3.362441062927246\n",
      "for 3 epochs, loss is 0.021442145109176636 and val_loss is 3.3662922382354736\n",
      "for 4 epochs, loss is 0.021187398582696915 and val_loss is 3.370086908340454\n",
      "for 5 epochs, loss is 0.020938074216246605 and val_loss is 3.3738226890563965\n",
      "for 6 epochs, loss is 0.020694177597761154 and val_loss is 3.377502679824829\n",
      "for 7 epochs, loss is 0.020455241203308105 and val_loss is 3.381126642227173\n",
      "for 8 epochs, loss is 0.020221389830112457 and val_loss is 3.384695291519165\n",
      "for 9 epochs, loss is 0.019992155954241753 and val_loss is 3.3882100582122803\n",
      "for 10 epochs, loss is 0.01976766251027584 and val_loss is 3.3916678428649902\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.4295), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1726), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.428), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1739), \"['a', 'x', 'a', 'x']\": np.float64(-9.6153), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1725), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0032), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1725), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0032), \"['a', 'x', 'a', 'y']\": np.float64(0.176), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.418), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1815), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.4276), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1729), \"['a', 'x', 'b', 'x']\": np.float64(-9.6137), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1739), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0032), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1739), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0032), \"['a', 'x', 'b', 'y']\": np.float64(0.1774), \"['a', 'x']\": np.float64(-9.8047), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1747), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0032), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1746), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0032), \"['a', 'y', 'a', 'x']\": np.float64(-0.1781), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0032), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0032), \"['a', 'y', 'a', 'y']\": np.float64(0.0033), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1745), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0034), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1747), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0032), \"['a', 'y', 'b', 'x']\": np.float64(-0.1781), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0032), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0032), \"['a', 'y', 'b', 'y']\": np.float64(0.0033), \"['a', 'y']\": np.float64(0.1816), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.4175), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1724), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.4159), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1737), \"['b', 'x', 'a', 'x']\": np.float64(-9.603), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1814), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0033), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1814), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0033), \"['b', 'x', 'a', 'y']\": np.float64(0.185), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.417), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1815), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.4266), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1729), \"['b', 'x', 'b', 'x']\": np.float64(-9.6127), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1728), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0032), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1729), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0032), \"['b', 'x', 'b', 'y']\": np.float64(0.1763), \"['b', 'x']\": np.float64(-9.8025), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1763), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0032), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1763), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0033), \"['b', 'y', 'a', 'x']\": np.float64(-0.1798), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0033), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0033), \"['b', 'y', 'a', 'y']\": np.float64(0.0033), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1761), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0034), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1763), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0032), \"['b', 'y', 'b', 'x']\": np.float64(-0.1798), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0032), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0032), \"['b', 'y', 'b', 'y']\": np.float64(0.0033), \"['b', 'y']\": np.float64(0.1834)}\n",
      "It s train loss bro [0.027307815849781036, 0.021968817338347435, 0.021702541038393974, 0.021442145109176636, 0.021187398582696915, 0.020938074216246605, 0.020694177597761154, 0.020455241203308105, 0.020221389830112457, 0.019992155954241753, 0.01976766251027584]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 68 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.02243063971400261 and val_loss is 3.395122766494751\n",
      "for 1 epochs, loss is 0.01932939887046814 and val_loss is 3.3985178470611572\n",
      "for 2 epochs, loss is 0.01911574974656105 and val_loss is 3.401855230331421\n",
      "for 3 epochs, loss is 0.01890626735985279 and val_loss is 3.405134677886963\n",
      "for 4 epochs, loss is 0.018701069056987762 and val_loss is 3.4083571434020996\n",
      "for 5 epochs, loss is 0.01849980838596821 and val_loss is 3.411520004272461\n",
      "for 6 epochs, loss is 0.018302368000149727 and val_loss is 3.41462779045105\n",
      "for 7 epochs, loss is 0.018108636140823364 and val_loss is 3.417677640914917\n",
      "for 8 epochs, loss is 0.01791861467063427 and val_loss is 3.420670986175537\n",
      "for 9 epochs, loss is 0.01773206889629364 and val_loss is 3.4236059188842773\n",
      "for 10 epochs, loss is 0.017548885196447372 and val_loss is 3.426481008529663\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.4913), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1538), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.4898), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1551), \"['a', 'x', 'a', 'x']\": np.float64(-9.6572), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1538), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0025), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1538), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0025), \"['a', 'x', 'a', 'y']\": np.float64(0.1565), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.4796), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1629), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.4894), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.154), \"['a', 'x', 'b', 'x']\": np.float64(-9.6558), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.155), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0025), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.155), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0025), \"['a', 'x', 'b', 'y']\": np.float64(0.1578), \"['a', 'x']\": np.float64(-9.8261), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1557), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0025), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1557), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0025), \"['a', 'y', 'a', 'x']\": np.float64(-0.1585), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0025), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0025), \"['a', 'y', 'a', 'y']\": np.float64(0.0026), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1556), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0027), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1557), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0025), \"['a', 'y', 'b', 'x']\": np.float64(-0.1585), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0025), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0025), \"['a', 'y', 'b', 'y']\": np.float64(0.0026), \"['a', 'y']\": np.float64(0.1612), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.4792), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1536), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.4777), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1549), \"['b', 'x', 'a', 'x']\": np.float64(-9.6449), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1628), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0027), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1628), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0027), \"['b', 'x', 'a', 'y']\": np.float64(0.1657), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.4788), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1629), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.4886), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.154), \"['b', 'x', 'b', 'x']\": np.float64(-9.6549), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.154), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0025), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.154), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0025), \"['b', 'x', 'b', 'y']\": np.float64(0.1567), \"['b', 'x']\": np.float64(-9.8242), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1572), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0025), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1572), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0026), \"['b', 'y', 'a', 'x']\": np.float64(-0.16), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0026), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0026), \"['b', 'y', 'a', 'y']\": np.float64(0.0026), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1571), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0027), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1572), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0026), \"['b', 'y', 'b', 'x']\": np.float64(-0.16), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0026), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0026), \"['b', 'y', 'b', 'y']\": np.float64(0.0026), \"['b', 'y']\": np.float64(0.1628)}\n",
      "It s train loss bro [0.02243063971400261, 0.01932939887046814, 0.01911574974656105, 0.01890626735985279, 0.018701069056987762, 0.01849980838596821, 0.018302368000149727, 0.018108636140823364, 0.01791861467063427, 0.01773206889629364, 0.017548885196447372]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 69 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.020839892327785492 and val_loss is 3.4293456077575684\n",
      "for 1 epochs, loss is 0.01718968339264393 and val_loss is 3.432147264480591\n",
      "for 2 epochs, loss is 0.017013784497976303 and val_loss is 3.434880256652832\n",
      "for 3 epochs, loss is 0.016841018572449684 and val_loss is 3.4375503063201904\n",
      "for 4 epochs, loss is 0.016671624034643173 and val_loss is 3.440150737762451\n",
      "for 5 epochs, loss is 0.016505248844623566 and val_loss is 3.442685604095459\n",
      "for 6 epochs, loss is 0.016341660171747208 and val_loss is 3.44515061378479\n",
      "for 7 epochs, loss is 0.01618109457194805 and val_loss is 3.4475502967834473\n",
      "for 8 epochs, loss is 0.016023436561226845 and val_loss is 3.449875831604004\n",
      "for 9 epochs, loss is 0.015868335962295532 and val_loss is 3.452131509780884\n",
      "for 10 epochs, loss is 0.015715908259153366 and val_loss is 3.4543137550354004\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.5426), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1381), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.5413), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1393), \"['a', 'x', 'a', 'x']\": np.float64(-9.6921), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1381), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.002), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1381), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.002), \"['a', 'x', 'a', 'y']\": np.float64(0.1403), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.5309), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1473), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.541), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1383), \"['a', 'x', 'b', 'x']\": np.float64(-9.6907), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1393), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.002), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1393), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.002), \"['a', 'x', 'b', 'y']\": np.float64(0.1415), \"['a', 'x']\": np.float64(-9.8439), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1399), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.002), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1399), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.002), \"['a', 'y', 'a', 'x']\": np.float64(-0.1421), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.002), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.002), \"['a', 'y', 'a', 'y']\": np.float64(0.0021), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1398), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0022), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1399), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.002), \"['a', 'y', 'b', 'x']\": np.float64(-0.1421), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.002), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.002), \"['a', 'y', 'b', 'y']\": np.float64(0.0021), \"['a', 'y']\": np.float64(0.1444), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.5305), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1379), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.5292), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1391), \"['b', 'x', 'a', 'x']\": np.float64(-9.6798), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1473), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0021), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1473), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0022), \"['b', 'x', 'a', 'y']\": np.float64(0.1496), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.5302), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1473), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.5403), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1383), \"['b', 'x', 'b', 'x']\": np.float64(-9.6901), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1383), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.002), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1383), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.002), \"['b', 'x', 'b', 'y']\": np.float64(0.1405), \"['b', 'x']\": np.float64(-9.8422), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1412), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.002), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1412), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0021), \"['b', 'y', 'a', 'x']\": np.float64(-0.1434), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0021), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0021), \"['b', 'y', 'a', 'y']\": np.float64(0.0021), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1411), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0022), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1412), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.002), \"['b', 'y', 'b', 'x']\": np.float64(-0.1435), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.002), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.002), \"['b', 'y', 'b', 'y']\": np.float64(0.0021), \"['b', 'y']\": np.float64(0.1457)}\n",
      "It s train loss bro [0.020839892327785492, 0.01718968339264393, 0.017013784497976303, 0.016841018572449684, 0.016671624034643173, 0.016505248844623566, 0.016341660171747208, 0.01618109457194805, 0.016023436561226845, 0.015868335962295532, 0.015715908259153366]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 70 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.01895422674715519 and val_loss is 3.4564335346221924\n",
      "for 1 epochs, loss is 0.015416973270475864 and val_loss is 3.45847225189209\n",
      "for 2 epochs, loss is 0.015270582400262356 and val_loss is 3.460430145263672\n",
      "for 3 epochs, loss is 0.015126870945096016 and val_loss is 3.4622974395751953\n",
      "for 4 epochs, loss is 0.014985370449721813 and val_loss is 3.464077949523926\n",
      "for 5 epochs, loss is 0.014846316538751125 and val_loss is 3.4657673835754395\n",
      "for 6 epochs, loss is 0.01470947451889515 and val_loss is 3.4673633575439453\n",
      "for 7 epochs, loss is 0.014574846252799034 and val_loss is 3.4688613414764404\n",
      "for 8 epochs, loss is 0.014442315325140953 and val_loss is 3.4702563285827637\n",
      "for 9 epochs, loss is 0.014311999082565308 and val_loss is 3.471543073654175\n",
      "for 10 epochs, loss is 0.014183547347784042 and val_loss is 3.472714900970459\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.5859), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1249), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.5846), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.126), \"['a', 'x', 'a', 'x']\": np.float64(-9.7214), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1249), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0016), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1249), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0016), \"['a', 'x', 'a', 'y']\": np.float64(0.1266), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.5743), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1339), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.5844), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.125), \"['a', 'x', 'b', 'x']\": np.float64(-9.7201), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.126), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0017), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.126), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0016), \"['a', 'x', 'b', 'y']\": np.float64(0.1278), \"['a', 'x']\": np.float64(-9.8589), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1266), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0016), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1266), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0017), \"['a', 'y', 'a', 'x']\": np.float64(-0.1284), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0017), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0017), \"['a', 'y', 'a', 'y']\": np.float64(0.0017), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1264), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0018), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1266), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0017), \"['a', 'y', 'b', 'x']\": np.float64(-0.1284), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0017), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0017), \"['a', 'y', 'b', 'y']\": np.float64(0.0017), \"['a', 'y']\": np.float64(0.1302), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.5741), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1247), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.5728), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1258), \"['b', 'x', 'a', 'x']\": np.float64(-9.7094), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1339), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0018), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1339), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0018), \"['b', 'x', 'a', 'y']\": np.float64(0.1358), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.5738), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1339), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.5838), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.125), \"['b', 'x', 'b', 'x']\": np.float64(-9.7196), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.125), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0016), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.125), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0016), \"['b', 'x', 'b', 'y']\": np.float64(0.1268), \"['b', 'x']\": np.float64(-9.8573), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1278), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0017), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1277), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0017), \"['b', 'y', 'a', 'x']\": np.float64(-0.1296), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0017), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0017), \"['b', 'y', 'a', 'y']\": np.float64(0.0017), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1276), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0018), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1278), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0017), \"['b', 'y', 'b', 'x']\": np.float64(-0.1296), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0017), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0017), \"['b', 'y', 'b', 'y']\": np.float64(0.0017), \"['b', 'y']\": np.float64(0.1314)}\n",
      "It s train loss bro [0.01895422674715519, 0.015416973270475864, 0.015270582400262356, 0.015126870945096016, 0.014985370449721813, 0.014846316538751125, 0.01470947451889515, 0.014574846252799034, 0.014442315325140953, 0.014311999082565308, 0.014183547347784042]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 71 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.01672508381307125 and val_loss is 3.4737467765808105\n",
      "for 1 epochs, loss is 0.013930941931903362 and val_loss is 3.4746551513671875\n",
      "for 2 epochs, loss is 0.013806908391416073 and val_loss is 3.475428342819214\n",
      "for 3 epochs, loss is 0.013684858568012714 and val_loss is 3.4760634899139404\n",
      "for 4 epochs, loss is 0.013564675115048885 and val_loss is 3.4765560626983643\n",
      "for 5 epochs, loss is 0.013446359895169735 and val_loss is 3.4768893718719482\n",
      "for 6 epochs, loss is 0.013329911977052689 and val_loss is 3.4770638942718506\n",
      "for 7 epochs, loss is 0.01321521494537592 and val_loss is 3.477081060409546\n",
      "for 8 epochs, loss is 0.0131021523848176 and val_loss is 3.4769158363342285\n",
      "for 9 epochs, loss is 0.012990842573344707 and val_loss is 3.4765625\n",
      "for 10 epochs, loss is 0.012880932539701462 and val_loss is 3.4760255813598633\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.6228), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1136), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.6216), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1146), \"['a', 'x', 'a', 'x']\": np.float64(-9.7464), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1136), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0014), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1136), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0014), \"['a', 'x', 'a', 'y']\": np.float64(0.115), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.6114), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1226), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.6214), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1137), \"['a', 'x', 'b', 'x']\": np.float64(-9.7452), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1146), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0014), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1146), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0014), \"['a', 'x', 'b', 'y']\": np.float64(0.1161), \"['a', 'x']\": np.float64(-9.8716), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1152), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0014), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1152), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0014), \"['a', 'y', 'a', 'x']\": np.float64(-0.1167), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0014), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0014), \"['a', 'y', 'a', 'y']\": np.float64(0.0014), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.115), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0015), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1152), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0014), \"['a', 'y', 'b', 'x']\": np.float64(-0.1166), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0014), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0014), \"['a', 'y', 'b', 'y']\": np.float64(0.0014), \"['a', 'y']\": np.float64(0.1182), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.6112), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1134), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.61), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1145), \"['b', 'x', 'a', 'x']\": np.float64(-9.7346), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1225), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0015), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1225), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0015), \"['b', 'x', 'a', 'y']\": np.float64(0.1241), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.611), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1226), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.621), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1137), \"['b', 'x', 'b', 'x']\": np.float64(-9.7448), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1137), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0014), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1137), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0013), \"['b', 'x', 'b', 'y']\": np.float64(0.1151), \"['b', 'x']\": np.float64(-9.8701), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1162), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0014), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1162), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0014), \"['b', 'y', 'a', 'x']\": np.float64(-0.1177), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0014), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0014), \"['b', 'y', 'a', 'y']\": np.float64(0.0014), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1161), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0015), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1162), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0014), \"['b', 'y', 'b', 'x']\": np.float64(-0.1177), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0014), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0014), \"['b', 'y', 'b', 'y']\": np.float64(0.0014), \"['b', 'y']\": np.float64(0.1192)}\n",
      "It s train loss bro [0.01672508381307125, 0.013930941931903362, 0.013806908391416073, 0.013684858568012714, 0.013564675115048885, 0.013446359895169735, 0.013329911977052689, 0.01321521494537592, 0.0131021523848176, 0.012990842573344707, 0.012880932539701462]\n",
      "% good predict : 60\n",
      "\u001b[0;34miteration 72 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.015620850957930088 and val_loss is 3.4754300117492676\n",
      "for 1 epochs, loss is 0.012664605863392353 and val_loss is 3.4746334552764893\n",
      "for 2 epochs, loss is 0.012558073736727238 and val_loss is 3.4736108779907227\n",
      "for 3 epochs, loss is 0.012453177943825722 and val_loss is 3.472362756729126\n",
      "for 4 epochs, loss is 0.012349803000688553 and val_loss is 3.4708645343780518\n",
      "for 5 epochs, loss is 0.012247947044670582 and val_loss is 3.469128131866455\n",
      "for 6 epochs, loss is 0.012147611938416958 and val_loss is 3.4671409130096436\n",
      "for 7 epochs, loss is 0.01204868033528328 and val_loss is 3.4649083614349365\n",
      "for 8 epochs, loss is 0.011951270513236523 and val_loss is 3.462425708770752\n",
      "for 9 epochs, loss is 0.011855028569698334 and val_loss is 3.459699869155884\n",
      "for 10 epochs, loss is 0.011760309338569641 and val_loss is 3.4567244052886963\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.6546), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1038), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.6536), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1047), \"['a', 'x', 'a', 'x']\": np.float64(-9.7679), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1038), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0011), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1038), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0011), \"['a', 'x', 'a', 'y']\": np.float64(0.105), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.6434), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1127), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.6534), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1039), \"['a', 'x', 'b', 'x']\": np.float64(-9.7668), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1047), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0011), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1047), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0011), \"['a', 'x', 'b', 'y']\": np.float64(0.106), \"['a', 'x']\": np.float64(-9.8825), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1053), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0011), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1053), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0011), \"['a', 'y', 'a', 'x']\": np.float64(-0.1065), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0011), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0011), \"['a', 'y', 'a', 'y']\": np.float64(0.0012), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1052), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0012), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1053), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0011), \"['a', 'y', 'b', 'x']\": np.float64(-0.1065), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0011), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0011), \"['a', 'y', 'b', 'y']\": np.float64(0.0012), \"['a', 'y']\": np.float64(0.1078), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.6433), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1037), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.6422), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1046), \"['b', 'x', 'a', 'x']\": np.float64(-9.7564), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1127), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0012), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1127), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0012), \"['b', 'x', 'a', 'y']\": np.float64(0.114), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.6431), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1127), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.6531), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1039), \"['b', 'x', 'b', 'x']\": np.float64(-9.7665), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1038), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0011), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1039), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0011), \"['b', 'x', 'b', 'y']\": np.float64(0.1051), \"['b', 'x']\": np.float64(-9.8813), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1062), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0011), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1062), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0012), \"['b', 'y', 'a', 'x']\": np.float64(-0.1075), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0012), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0012), \"['b', 'y', 'a', 'y']\": np.float64(0.0012), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1061), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0012), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1062), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0011), \"['b', 'y', 'b', 'x']\": np.float64(-0.1075), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0011), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0011), \"['b', 'y', 'b', 'y']\": np.float64(0.0012), \"['b', 'y']\": np.float64(0.1087)}\n",
      "It s train loss bro [0.015620850957930088, 0.012664605863392353, 0.012558073736727238, 0.012453177943825722, 0.012349803000688553, 0.012247947044670582, 0.012147611938416958, 0.01204868033528328, 0.011951270513236523, 0.011855028569698334, 0.011760309338569641]\n",
      "% good predict : 70\n",
      "\u001b[0;34miteration 73 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.018604889512062073 and val_loss is 3.4541306495666504\n",
      "for 1 epochs, loss is 0.011570372618734837 and val_loss is 3.451277494430542\n",
      "for 2 epochs, loss is 0.01147562637925148 and val_loss is 3.4482500553131104\n",
      "for 3 epochs, loss is 0.011382521130144596 and val_loss is 3.445033311843872\n",
      "for 4 epochs, loss is 0.011290940456092358 and val_loss is 3.441671371459961\n",
      "for 5 epochs, loss is 0.011200882494449615 and val_loss is 3.438276529312134\n",
      "for 6 epochs, loss is 0.011112113483250141 and val_loss is 3.4348301887512207\n",
      "for 7 epochs, loss is 0.011024869978427887 and val_loss is 3.43143892288208\n",
      "for 8 epochs, loss is 0.0109389154240489 and val_loss is 3.428140640258789\n",
      "for 9 epochs, loss is 0.010854250751435757 and val_loss is 3.4250166416168213\n",
      "for 10 epochs, loss is 0.010770758613944054 and val_loss is 3.4220802783966064\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.6829), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0951), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.6819), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.096), \"['a', 'x', 'a', 'x']\": np.float64(-9.787), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0951), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0951), \"['a', 'x', 'a', 'y']\": np.float64(0.0961), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.6719), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1039), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.6819), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0951), \"['a', 'x', 'b', 'x']\": np.float64(-9.786), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.096), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.096), \"['a', 'x', 'b', 'y']\": np.float64(0.097), \"['a', 'x']\": np.float64(-9.8922), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0965), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0965), \"['a', 'y', 'a', 'x']\": np.float64(-0.0976), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0964), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.001), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0965), \"['a', 'y', 'b', 'x']\": np.float64(-0.0976), \"['a', 'y']\": np.float64(0.0986), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.6718), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.095), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.6708), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0959), \"['b', 'x', 'a', 'x']\": np.float64(-9.7757), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1039), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.001), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1039), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.001), \"['b', 'x', 'a', 'y']\": np.float64(0.1051), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.6717), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1039), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.6817), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0951), \"['b', 'x', 'b', 'x']\": np.float64(-9.7858), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0951), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0951), \"['b', 'x', 'b', 'y']\": np.float64(0.0962), \"['b', 'x']\": np.float64(-9.8911), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0973), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0973), \"['b', 'y', 'a', 'x']\": np.float64(-0.0984), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0972), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.001), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0973), \"['b', 'y', 'b', 'x']\": np.float64(-0.0984), \"['b', 'y']\": np.float64(0.0995)}\n",
      "It s train loss bro [0.018604889512062073, 0.011570372618734837, 0.01147562637925148, 0.011382521130144596, 0.011290940456092358, 0.011200882494449615, 0.011112113483250141, 0.011024869978427887, 0.0109389154240489, 0.010854250751435757, 0.010770758613944054]\n",
      "% good predict : 80\n",
      "\u001b[0;34miteration 74 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.014115259051322937 and val_loss is 3.419381618499756\n",
      "for 1 epochs, loss is 0.01060587540268898 and val_loss is 3.41691255569458\n",
      "for 2 epochs, loss is 0.010524485260248184 and val_loss is 3.4146790504455566\n",
      "for 3 epochs, loss is 0.010444268584251404 and val_loss is 3.4126555919647217\n",
      "for 4 epochs, loss is 0.010365343652665615 and val_loss is 3.41080904006958\n",
      "for 5 epochs, loss is 0.010287472978234291 and val_loss is 3.4091546535491943\n",
      "for 6 epochs, loss is 0.010210777632892132 and val_loss is 3.407630443572998\n",
      "for 7 epochs, loss is 0.010135255753993988 and val_loss is 3.4062862396240234\n",
      "for 8 epochs, loss is 0.01006055437028408 and val_loss is 3.4051098823547363\n",
      "for 9 epochs, loss is 0.009986910037696362 and val_loss is 3.404132604598999\n",
      "for 10 epochs, loss is 0.00991420354694128 and val_loss is 3.4033753871917725\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7074), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0876), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7065), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0884), \"['a', 'x', 'a', 'x']\": np.float64(-9.8036), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0876), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0876), \"['a', 'x', 'a', 'y']\": np.float64(0.0884), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.6966), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0963), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7065), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0876), \"['a', 'x', 'b', 'x']\": np.float64(-9.8026), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0884), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0884), \"['a', 'x', 'b', 'y']\": np.float64(0.0893), \"['a', 'x']\": np.float64(-9.9006), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0889), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0889), \"['a', 'y', 'a', 'x']\": np.float64(-0.0898), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0888), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0889), \"['a', 'y', 'b', 'x']\": np.float64(-0.0898), \"['a', 'y']\": np.float64(0.0907), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.6965), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0875), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.6956), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0883), \"['b', 'x', 'a', 'x']\": np.float64(-9.7925), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0963), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0963), \"['b', 'x', 'a', 'y']\": np.float64(0.0972), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.6965), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0963), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7064), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0876), \"['b', 'x', 'b', 'x']\": np.float64(-9.8025), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0876), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0876), \"['b', 'x', 'b', 'y']\": np.float64(0.0884), \"['b', 'x']\": np.float64(-9.8996), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0896), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0896), \"['b', 'y', 'a', 'x']\": np.float64(-0.0905), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0895), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0896), \"['b', 'y', 'b', 'x']\": np.float64(-0.0905), \"['b', 'y']\": np.float64(0.0914)}\n",
      "It s train loss bro [0.014115259051322937, 0.01060587540268898, 0.010524485260248184, 0.010444268584251404, 0.010365343652665615, 0.010287472978234291, 0.010210777632892132, 0.010135255753993988, 0.01006055437028408, 0.009986910037696362, 0.00991420354694128]\n",
      "% good predict : 90\n",
      "\u001b[0;34miteration 75 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.012725342065095901 and val_loss is 3.4030637741088867\n",
      "for 1 epochs, loss is 0.009769956581294537 and val_loss is 3.4029998779296875\n",
      "for 2 epochs, loss is 0.009698533453047276 and val_loss is 3.403182029724121\n",
      "for 3 epochs, loss is 0.009628168307244778 and val_loss is 3.403623104095459\n",
      "for 4 epochs, loss is 0.009558742865920067 and val_loss is 3.4043004512786865\n",
      "for 5 epochs, loss is 0.009490256197750568 and val_loss is 3.405198335647583\n",
      "for 6 epochs, loss is 0.009422711096704006 and val_loss is 3.4062728881835938\n",
      "for 7 epochs, loss is 0.00935598649084568 and val_loss is 3.407475471496582\n",
      "for 8 epochs, loss is 0.00929020345211029 and val_loss is 3.4087603092193604\n",
      "for 9 epochs, loss is 0.00922512449324131 and val_loss is 3.4100844860076904\n",
      "for 10 epochs, loss is 0.009160986170172691 and val_loss is 3.4113969802856445\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7291), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0809), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7282), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0817), \"['a', 'x', 'a', 'x']\": np.float64(-9.8181), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0809), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0809), \"['a', 'x', 'a', 'y']\": np.float64(0.0817), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7184), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0895), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7283), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0809), \"['a', 'x', 'b', 'x']\": np.float64(-9.8173), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0817), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0817), \"['a', 'x', 'b', 'y']\": np.float64(0.0824), \"['a', 'x']\": np.float64(-9.908), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0822), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0822), \"['a', 'y', 'a', 'x']\": np.float64(-0.083), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0821), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0822), \"['a', 'y', 'b', 'x']\": np.float64(-0.083), \"['a', 'y']\": np.float64(0.0837), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7184), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0808), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7176), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0816), \"['b', 'x', 'a', 'x']\": np.float64(-9.8074), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0895), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0895), \"['b', 'x', 'a', 'y']\": np.float64(0.0903), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7184), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0895), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7282), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0809), \"['b', 'x', 'b', 'x']\": np.float64(-9.8173), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0809), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0809), \"['b', 'x', 'b', 'y']\": np.float64(0.0816), \"['b', 'x']\": np.float64(-9.9071), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0828), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0828), \"['b', 'y', 'a', 'x']\": np.float64(-0.0836), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0827), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0828), \"['b', 'y', 'b', 'x']\": np.float64(-0.0836), \"['b', 'y']\": np.float64(0.0843)}\n",
      "It s train loss bro [0.012725342065095901, 0.009769956581294537, 0.009698533453047276, 0.009628168307244778, 0.009558742865920067, 0.009490256197750568, 0.009422711096704006, 0.00935598649084568, 0.00929020345211029, 0.00922512449324131, 0.009160986170172691]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 76 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.011249330826103687 and val_loss is 3.412710666656494\n",
      "for 1 epochs, loss is 0.009034233167767525 and val_loss is 3.4139461517333984\n",
      "for 2 epochs, loss is 0.008971500210464 and val_loss is 3.41508412361145\n",
      "for 3 epochs, loss is 0.008909708820283413 and val_loss is 3.4161040782928467\n",
      "for 4 epochs, loss is 0.008848622441291809 and val_loss is 3.4170124530792236\n",
      "for 5 epochs, loss is 0.008788359351456165 and val_loss is 3.4177961349487305\n",
      "for 6 epochs, loss is 0.008728801272809505 and val_loss is 3.418463706970215\n",
      "for 7 epochs, loss is 0.008669950067996979 and val_loss is 3.4190359115600586\n",
      "for 8 epochs, loss is 0.008611802943050861 and val_loss is 3.4195101261138916\n",
      "for 9 epochs, loss is 0.008554362691938877 and val_loss is 3.419919490814209\n",
      "for 10 epochs, loss is 0.008497510105371475 and val_loss is 3.4202818870544434\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7482), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0751), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7474), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0757), \"['a', 'x', 'a', 'x']\": np.float64(-9.831), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0751), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0751), \"['a', 'x', 'a', 'y']\": np.float64(0.0757), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7378), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0835), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7475), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.075), \"['a', 'x', 'b', 'x']\": np.float64(-9.8302), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0757), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0757), \"['a', 'x', 'b', 'y']\": np.float64(0.0764), \"['a', 'x']\": np.float64(-9.9145), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0763), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0763), \"['a', 'y', 'a', 'x']\": np.float64(-0.0769), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0762), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0763), \"['a', 'y', 'b', 'x']\": np.float64(-0.0769), \"['a', 'y']\": np.float64(0.0776), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7378), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.075), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.737), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0757), \"['b', 'x', 'a', 'x']\": np.float64(-9.8205), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0835), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0835), \"['b', 'x', 'a', 'y']\": np.float64(0.0842), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7378), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0835), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7475), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.075), \"['b', 'x', 'b', 'x']\": np.float64(-9.8303), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.075), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.075), \"['b', 'x', 'b', 'y']\": np.float64(0.0756), \"['b', 'x']\": np.float64(-9.9138), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0768), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0768), \"['b', 'y', 'a', 'x']\": np.float64(-0.0775), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0767), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0768), \"['b', 'y', 'b', 'x']\": np.float64(-0.0775), \"['b', 'y']\": np.float64(0.0781)}\n",
      "It s train loss bro [0.011249330826103687, 0.009034233167767525, 0.008971500210464, 0.008909708820283413, 0.008848622441291809, 0.008788359351456165, 0.008728801272809505, 0.008669950067996979, 0.008611802943050861, 0.008554362691938877, 0.008497510105371475]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 77 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.009462624788284302 and val_loss is 3.4206271171569824\n",
      "for 1 epochs, loss is 0.00838521309196949 and val_loss is 3.420971632003784\n",
      "for 2 epochs, loss is 0.008329886943101883 and val_loss is 3.4213550090789795\n",
      "for 3 epochs, loss is 0.008275032043457031 and val_loss is 3.4217770099639893\n",
      "for 4 epochs, loss is 0.008220882155001163 and val_loss is 3.4222633838653564\n",
      "for 5 epochs, loss is 0.008167439140379429 and val_loss is 3.4228150844573975\n",
      "for 6 epochs, loss is 0.008114466443657875 and val_loss is 3.4234426021575928\n",
      "for 7 epochs, loss is 0.008062200620770454 and val_loss is 3.4241254329681396\n",
      "for 8 epochs, loss is 0.008010405115783215 and val_loss is 3.4248485565185547\n",
      "for 9 epochs, loss is 0.007959197275340557 and val_loss is 3.425628423690796\n",
      "for 10 epochs, loss is 0.007908578962087631 and val_loss is 3.4264256954193115\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7652), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0698), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7645), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0705), \"['a', 'x', 'a', 'x']\": np.float64(-9.8424), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0698), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0698), \"['a', 'x', 'a', 'y']\": np.float64(0.0704), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.755), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0781), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7646), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0697), \"['a', 'x', 'b', 'x']\": np.float64(-9.8417), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0704), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0705), \"['a', 'x', 'b', 'y']\": np.float64(0.071), \"['a', 'x']\": np.float64(-9.9203), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.071), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.071), \"['a', 'y', 'a', 'x']\": np.float64(-0.0716), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0709), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.071), \"['a', 'y', 'b', 'x']\": np.float64(-0.0716), \"['a', 'y']\": np.float64(0.0722), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.755), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0698), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7543), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0704), \"['b', 'x', 'a', 'x']\": np.float64(-9.8322), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0781), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0781), \"['b', 'x', 'a', 'y']\": np.float64(0.0787), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7551), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0781), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7647), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0697), \"['b', 'x', 'b', 'x']\": np.float64(-9.8419), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0697), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0697), \"['b', 'x', 'b', 'y']\": np.float64(0.0703), \"['b', 'x']\": np.float64(-9.9197), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0714), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0714), \"['b', 'y', 'a', 'x']\": np.float64(-0.072), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0714), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0714), \"['b', 'y', 'b', 'x']\": np.float64(-0.072), \"['b', 'y']\": np.float64(0.0726)}\n",
      "It s train loss bro [0.009462624788284302, 0.00838521309196949, 0.008329886943101883, 0.008275032043457031, 0.008220882155001163, 0.008167439140379429, 0.008114466443657875, 0.008062200620770454, 0.008010405115783215, 0.007959197275340557, 0.007908578962087631]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 78 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.010581695474684238 and val_loss is 3.427271842956543\n",
      "for 1 epochs, loss is 0.007807807996869087 and val_loss is 3.4281394481658936\n",
      "for 2 epochs, loss is 0.007757773622870445 and val_loss is 3.4290153980255127\n",
      "for 3 epochs, loss is 0.007708446122705936 and val_loss is 3.429906129837036\n",
      "for 4 epochs, loss is 0.007659589406102896 and val_loss is 3.430830478668213\n",
      "for 5 epochs, loss is 0.007611440494656563 and val_loss is 3.43179988861084\n",
      "for 6 epochs, loss is 0.0075636436231434345 and val_loss is 3.4328384399414062\n",
      "for 7 epochs, loss is 0.007516436744481325 and val_loss is 3.433969497680664\n",
      "for 8 epochs, loss is 0.00746981892734766 and val_loss is 3.4352192878723145\n",
      "for 9 epochs, loss is 0.007423553615808487 and val_loss is 3.4366159439086914\n",
      "for 10 epochs, loss is 0.007377996575087309 and val_loss is 3.4381861686706543\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7805), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0651), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7799), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0657), \"['a', 'x', 'a', 'x']\": np.float64(-9.8528), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0651), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0651), \"['a', 'x', 'a', 'y']\": np.float64(0.0656), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7706), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0732), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.78), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.065), \"['a', 'x', 'b', 'x']\": np.float64(-9.8521), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0657), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0657), \"['a', 'x', 'b', 'y']\": np.float64(0.0662), \"['a', 'x']\": np.float64(-9.9256), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0662), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0662), \"['a', 'y', 'a', 'x']\": np.float64(-0.0667), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0662), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0662), \"['a', 'y', 'b', 'x']\": np.float64(-0.0667), \"['a', 'y']\": np.float64(0.0672), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7707), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.065), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.77), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0656), \"['b', 'x', 'a', 'x']\": np.float64(-9.8429), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0732), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0732), \"['b', 'x', 'a', 'y']\": np.float64(0.0737), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7708), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0732), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7802), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.065), \"['b', 'x', 'b', 'x']\": np.float64(-9.8523), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.065), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.065), \"['b', 'x', 'b', 'y']\": np.float64(0.0654), \"['b', 'x']\": np.float64(-9.925), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0666), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0666), \"['b', 'y', 'a', 'x']\": np.float64(-0.0671), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0665), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0666), \"['b', 'y', 'b', 'x']\": np.float64(-0.0671), \"['b', 'y']\": np.float64(0.0676)}\n",
      "It s train loss bro [0.010581695474684238, 0.007807807996869087, 0.007757773622870445, 0.007708446122705936, 0.007659589406102896, 0.007611440494656563, 0.0075636436231434345, 0.007516436744481325, 0.00746981892734766, 0.007423553615808487, 0.007377996575087309]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 79 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.009643989615142345 and val_loss is 3.43999981880188\n",
      "for 1 epochs, loss is 0.007287112530320883 and val_loss is 3.442007303237915\n",
      "for 2 epochs, loss is 0.007242141291499138 and val_loss is 3.4442126750946045\n",
      "for 3 epochs, loss is 0.007197522558271885 and val_loss is 3.446619749069214\n",
      "for 4 epochs, loss is 0.007153493817895651 and val_loss is 3.4492223262786865\n",
      "for 5 epochs, loss is 0.007109936326742172 and val_loss is 3.4520065784454346\n",
      "for 6 epochs, loss is 0.007066850550472736 and val_loss is 3.454955816268921\n",
      "for 7 epochs, loss is 0.007024236489087343 and val_loss is 3.4580531120300293\n",
      "for 8 epochs, loss is 0.006982094142585993 and val_loss is 3.4612669944763184\n",
      "for 9 epochs, loss is 0.0069404239766299725 and val_loss is 3.4645822048187256\n",
      "for 10 epochs, loss is 0.006899106781929731 and val_loss is 3.4679670333862305\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7944), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0608), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7938), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0614), \"['a', 'x', 'a', 'x']\": np.float64(-9.8621), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0608), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0608), \"['a', 'x', 'a', 'y']\": np.float64(0.0613), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7848), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0687), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.794), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0607), \"['a', 'x', 'b', 'x']\": np.float64(-9.8615), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0614), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0614), \"['a', 'x', 'b', 'y']\": np.float64(0.0618), \"['a', 'x']\": np.float64(-9.9303), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0619), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0619), \"['a', 'y', 'a', 'x']\": np.float64(-0.0624), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0619), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0619), \"['a', 'y', 'b', 'x']\": np.float64(-0.0624), \"['a', 'y']\": np.float64(0.0628), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7848), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0608), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7843), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0613), \"['b', 'x', 'a', 'x']\": np.float64(-9.8525), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0687), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0687), \"['b', 'x', 'a', 'y']\": np.float64(0.0692), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.785), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0687), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7942), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0607), \"['b', 'x', 'b', 'x']\": np.float64(-9.8618), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0607), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0607), \"['b', 'x', 'b', 'y']\": np.float64(0.0611), \"['b', 'x']\": np.float64(-9.9298), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0622), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0622), \"['b', 'y', 'a', 'x']\": np.float64(-0.0626), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0622), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0622), \"['b', 'y', 'b', 'x']\": np.float64(-0.0627), \"['b', 'y']\": np.float64(0.0631)}\n",
      "It s train loss bro [0.009643989615142345, 0.007287112530320883, 0.007242141291499138, 0.007197522558271885, 0.007153493817895651, 0.007109936326742172, 0.007066850550472736, 0.007024236489087343, 0.006982094142585993, 0.0069404239766299725, 0.006899106781929731]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 80 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.009030216373503208 and val_loss is 3.4715473651885986\n",
      "for 1 epochs, loss is 0.006817414425313473 and val_loss is 3.4751524925231934\n",
      "for 2 epochs, loss is 0.006776920985430479 and val_loss is 3.478755474090576\n",
      "for 3 epochs, loss is 0.006736899726092815 and val_loss is 3.4823482036590576\n",
      "for 4 epochs, loss is 0.006697231903672218 and val_loss is 3.4859094619750977\n",
      "for 5 epochs, loss is 0.006658036261796951 and val_loss is 3.489433526992798\n",
      "for 6 epochs, loss is 0.006619194056838751 and val_loss is 3.4929022789001465\n",
      "for 7 epochs, loss is 0.006580824498087168 and val_loss is 3.4963314533233643\n",
      "for 8 epochs, loss is 0.006542808376252651 and val_loss is 3.4997057914733887\n",
      "for 9 epochs, loss is 0.006505146622657776 and val_loss is 3.503018379211426\n",
      "for 10 epochs, loss is 0.006467838305979967 and val_loss is 3.5062718391418457\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.807), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.057), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8064), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0575), \"['a', 'x', 'a', 'x']\": np.float64(-9.8706), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.057), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.057), \"['a', 'x', 'a', 'y']\": np.float64(0.0574), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7976), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0647), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8066), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0568), \"['a', 'x', 'b', 'x']\": np.float64(-9.87), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0575), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0575), \"['a', 'x', 'b', 'y']\": np.float64(0.0578), \"['a', 'x']\": np.float64(-9.9346), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.058), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.058), \"['a', 'y', 'a', 'x']\": np.float64(-0.0584), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.058), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.058), \"['a', 'y', 'b', 'x']\": np.float64(-0.0584), \"['a', 'y']\": np.float64(0.0588), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7977), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0569), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7971), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0574), \"['b', 'x', 'a', 'x']\": np.float64(-9.8612), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0647), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0647), \"['b', 'x', 'a', 'y']\": np.float64(0.0651), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7978), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0647), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8069), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0568), \"['b', 'x', 'b', 'x']\": np.float64(-9.8703), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0568), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0568), \"['b', 'x', 'b', 'y']\": np.float64(0.0572), \"['b', 'x']\": np.float64(-9.9341), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0583), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0583), \"['b', 'y', 'a', 'x']\": np.float64(-0.0586), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0582), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0583), \"['b', 'y', 'b', 'x']\": np.float64(-0.0587), \"['b', 'y']\": np.float64(0.059)}\n",
      "It s train loss bro [0.009030216373503208, 0.006817414425313473, 0.006776920985430479, 0.006736899726092815, 0.006697231903672218, 0.006658036261796951, 0.006619194056838751, 0.006580824498087168, 0.006542808376252651, 0.006505146622657776, 0.006467838305979967]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 81 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.008980597369372845 and val_loss is 3.5094680786132812\n",
      "for 1 epochs, loss is 0.006393099669367075 and val_loss is 3.5126073360443115\n",
      "for 2 epochs, loss is 0.006355668883770704 and val_loss is 3.5156967639923096\n",
      "for 3 epochs, loss is 0.006318947300314903 and val_loss is 3.518735885620117\n",
      "for 4 epochs, loss is 0.00628246134147048 and val_loss is 3.5217177867889404\n",
      "for 5 epochs, loss is 0.0062465667724609375 and val_loss is 3.52465558052063\n",
      "for 6 epochs, loss is 0.0062109073624014854 and val_loss is 3.5275349617004395\n",
      "for 7 epochs, loss is 0.006175721064209938 and val_loss is 3.530367851257324\n",
      "for 8 epochs, loss is 0.0061408886685967445 and val_loss is 3.533156394958496\n",
      "for 9 epochs, loss is 0.006106291897594929 and val_loss is 3.535897970199585\n",
      "for 10 epochs, loss is 0.006072168238461018 and val_loss is 3.5385940074920654\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8185), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0535), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.818), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0539), \"['a', 'x', 'a', 'x']\": np.float64(-9.8783), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0535), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0535), \"['a', 'x', 'a', 'y']\": np.float64(0.0538), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8093), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.061), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8182), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0532), \"['a', 'x', 'b', 'x']\": np.float64(-9.8778), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0539), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0539), \"['a', 'x', 'b', 'y']\": np.float64(0.0542), \"['a', 'x']\": np.float64(-9.9385), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0545), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0545), \"['a', 'y', 'a', 'x']\": np.float64(-0.0548), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0544), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0545), \"['a', 'y', 'b', 'x']\": np.float64(-0.0548), \"['a', 'y']\": np.float64(0.0551), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8094), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0534), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8089), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0539), \"['b', 'x', 'a', 'x']\": np.float64(-9.8692), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0609), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0609), \"['b', 'x', 'a', 'y']\": np.float64(0.0613), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8096), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.061), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8185), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0532), \"['b', 'x', 'b', 'x']\": np.float64(-9.8781), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0532), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0532), \"['b', 'x', 'b', 'y']\": np.float64(0.0536), \"['b', 'x']\": np.float64(-9.9381), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0546), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0546), \"['b', 'y', 'a', 'x']\": np.float64(-0.055), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0546), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0546), \"['b', 'y', 'b', 'x']\": np.float64(-0.055), \"['b', 'y']\": np.float64(0.0553)}\n",
      "It s train loss bro [0.008980597369372845, 0.006393099669367075, 0.006355668883770704, 0.006318947300314903, 0.00628246134147048, 0.0062465667724609375, 0.0062109073624014854, 0.006175721064209938, 0.0061408886685967445, 0.006106291897594929, 0.006072168238461018]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 82 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.008431551977992058 and val_loss is 3.541318416595459\n",
      "for 1 epochs, loss is 0.006003798451274633 and val_loss is 3.543994188308716\n",
      "for 2 epochs, loss is 0.005969671066850424 and val_loss is 3.5466251373291016\n",
      "for 3 epochs, loss is 0.005936016328632832 and val_loss is 3.5492069721221924\n",
      "for 4 epochs, loss is 0.005902716424316168 and val_loss is 3.55175518989563\n",
      "for 5 epochs, loss is 0.0058698891662061214 and val_loss is 3.5542612075805664\n",
      "for 6 epochs, loss is 0.00583729799836874 and val_loss is 3.556730270385742\n",
      "for 7 epochs, loss is 0.005805179942399263 and val_loss is 3.559161424636841\n",
      "for 8 epochs, loss is 0.005773297511041164 and val_loss is 3.561563014984131\n",
      "for 9 epochs, loss is 0.005741769913583994 and val_loss is 3.5639305114746094\n",
      "for 10 epochs, loss is 0.0057104784063994884 and val_loss is 3.566267251968384\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.829), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0502), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8286), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0506), \"['a', 'x', 'a', 'x']\": np.float64(-9.8854), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0502), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0502), \"['a', 'x', 'a', 'y']\": np.float64(0.0505), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8201), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0575), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8288), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.05), \"['a', 'x', 'b', 'x']\": np.float64(-9.8849), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0506), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0506), \"['a', 'x', 'b', 'y']\": np.float64(0.0509), \"['a', 'x']\": np.float64(-9.9421), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0512), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0512), \"['a', 'y', 'a', 'x']\": np.float64(-0.0515), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0511), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0512), \"['a', 'y', 'b', 'x']\": np.float64(-0.0515), \"['a', 'y']\": np.float64(0.0518), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8202), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0502), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8198), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0506), \"['b', 'x', 'a', 'x']\": np.float64(-9.8765), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0575), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0575), \"['b', 'x', 'a', 'y']\": np.float64(0.0579), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8205), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0575), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8291), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.05), \"['b', 'x', 'b', 'x']\": np.float64(-9.8853), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.05), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.05), \"['b', 'x', 'b', 'y']\": np.float64(0.0503), \"['b', 'x']\": np.float64(-9.9417), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0513), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0513), \"['b', 'y', 'a', 'x']\": np.float64(-0.0516), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0513), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0513), \"['b', 'y', 'b', 'x']\": np.float64(-0.0516), \"['b', 'y']\": np.float64(0.0519)}\n",
      "It s train loss bro [0.008431551977992058, 0.006003798451274633, 0.005969671066850424, 0.005936016328632832, 0.005902716424316168, 0.0058698891662061214, 0.00583729799836874, 0.005805179942399263, 0.005773297511041164, 0.005741769913583994, 0.0057104784063994884]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 83 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.007461891043931246 and val_loss is 3.5686564445495605\n",
      "for 1 epochs, loss is 0.005648366641253233 and val_loss is 3.5710091590881348\n",
      "for 2 epochs, loss is 0.005617308896034956 and val_loss is 3.5733249187469482\n",
      "for 3 epochs, loss is 0.005586724728345871 and val_loss is 3.575605630874634\n",
      "for 4 epochs, loss is 0.0055564953945577145 and val_loss is 3.577855348587036\n",
      "for 5 epochs, loss is 0.005526502151042223 and val_loss is 3.580077648162842\n",
      "for 6 epochs, loss is 0.00549686374142766 and val_loss is 3.582272529602051\n",
      "for 7 epochs, loss is 0.0054675801657140255 and val_loss is 3.584442615509033\n",
      "for 8 epochs, loss is 0.005438532680273056 and val_loss is 3.5865862369537354\n",
      "for 9 epochs, loss is 0.005409721285104752 and val_loss is 3.5887081623077393\n",
      "for 10 epochs, loss is 0.005381265189498663 and val_loss is 3.5908076763153076\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8386), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0473), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8382), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0476), \"['a', 'x', 'a', 'x']\": np.float64(-9.8918), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0473), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0473), \"['a', 'x', 'a', 'y']\": np.float64(0.0475), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.83), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0544), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8385), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.047), \"['a', 'x', 'b', 'x']\": np.float64(-9.8914), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0476), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0477), \"['a', 'x', 'b', 'y']\": np.float64(0.0479), \"['a', 'x']\": np.float64(-9.9453), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0482), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0482), \"['a', 'y', 'a', 'x']\": np.float64(-0.0485), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0482), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0482), \"['a', 'y', 'b', 'x']\": np.float64(-0.0485), \"['a', 'y']\": np.float64(0.0487), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8301), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0472), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8297), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0476), \"['b', 'x', 'a', 'x']\": np.float64(-9.8833), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0544), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0544), \"['b', 'x', 'a', 'y']\": np.float64(0.0547), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8304), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0544), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8388), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.047), \"['b', 'x', 'b', 'x']\": np.float64(-9.8918), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.047), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.047), \"['b', 'x', 'b', 'y']\": np.float64(0.0473), \"['b', 'x']\": np.float64(-9.9451), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0483), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0483), \"['b', 'y', 'a', 'x']\": np.float64(-0.0486), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0483), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0483), \"['b', 'y', 'b', 'x']\": np.float64(-0.0486), \"['b', 'y']\": np.float64(0.0488)}\n",
      "It s train loss bro [0.007461891043931246, 0.005648366641253233, 0.005617308896034956, 0.005586724728345871, 0.0055564953945577145, 0.005526502151042223, 0.00549686374142766, 0.0054675801657140255, 0.005438532680273056, 0.005409721285104752, 0.005381265189498663]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 84 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.006740570068359375 and val_loss is 3.592935562133789\n",
      "for 1 epochs, loss is 0.005324468482285738 and val_loss is 3.5950372219085693\n",
      "for 2 epochs, loss is 0.0052962470799684525 and val_loss is 3.5971171855926514\n",
      "for 3 epochs, loss is 0.005268261767923832 and val_loss is 3.599177122116089\n",
      "for 4 epochs, loss is 0.005240750499069691 and val_loss is 3.6012165546417236\n",
      "for 5 epochs, loss is 0.0052132378332316875 and val_loss is 3.6032373905181885\n",
      "for 6 epochs, loss is 0.005186199210584164 and val_loss is 3.605239152908325\n",
      "for 7 epochs, loss is 0.005159278400242329 and val_loss is 3.6072239875793457\n",
      "for 8 epochs, loss is 0.005132712423801422 and val_loss is 3.609193801879883\n",
      "for 9 epochs, loss is 0.005106264725327492 and val_loss is 3.6111481189727783\n",
      "for 10 epochs, loss is 0.00508017186075449 and val_loss is 3.613088607788086\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8475), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0446), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8471), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0449), \"['a', 'x', 'a', 'x']\": np.float64(-9.8978), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0446), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0446), \"['a', 'x', 'a', 'y']\": np.float64(0.0448), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.839), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0515), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8473), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0443), \"['a', 'x', 'b', 'x']\": np.float64(-9.8974), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0449), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0449), \"['a', 'x', 'b', 'y']\": np.float64(0.0452), \"['a', 'x']\": np.float64(-9.9483), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0455), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0455), \"['a', 'y', 'a', 'x']\": np.float64(-0.0457), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0454), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0455), \"['a', 'y', 'b', 'x']\": np.float64(-0.0457), \"['a', 'y']\": np.float64(0.0459), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8392), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0445), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8388), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0449), \"['b', 'x', 'a', 'x']\": np.float64(-9.8894), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0515), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0515), \"['b', 'x', 'a', 'y']\": np.float64(0.0518), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8394), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0515), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8477), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0443), \"['b', 'x', 'b', 'x']\": np.float64(-9.8978), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0443), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0443), \"['b', 'x', 'b', 'y']\": np.float64(0.0445), \"['b', 'x']\": np.float64(-9.9481), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0455), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0455), \"['b', 'y', 'a', 'x']\": np.float64(-0.0458), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0455), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0455), \"['b', 'y', 'b', 'x']\": np.float64(-0.0458), \"['b', 'y']\": np.float64(0.046)}\n",
      "It s train loss bro [0.006740570068359375, 0.005324468482285738, 0.0052962470799684525, 0.005268261767923832, 0.005240750499069691, 0.0052132378332316875, 0.005186199210584164, 0.005159278400242329, 0.005132712423801422, 0.005106264725327492, 0.00508017186075449]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 85 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.005671125371009111 and val_loss is 3.615030288696289\n",
      "for 1 epochs, loss is 0.00502834003418684 and val_loss is 3.616957664489746\n",
      "for 2 epochs, loss is 0.005002838093787432 and val_loss is 3.618870735168457\n",
      "for 3 epochs, loss is 0.004977335687726736 and val_loss is 3.620770215988159\n",
      "for 4 epochs, loss is 0.004952188581228256 and val_loss is 3.6226577758789062\n",
      "for 5 epochs, loss is 0.004927159287035465 and val_loss is 3.6245338916778564\n",
      "for 6 epochs, loss is 0.004902366548776627 and val_loss is 3.6263978481292725\n",
      "for 7 epochs, loss is 0.004877810832113028 and val_loss is 3.62825083732605\n",
      "for 8 epochs, loss is 0.004853491671383381 and val_loss is 3.630093574523926\n",
      "for 9 epochs, loss is 0.0048294090665876865 and val_loss is 3.631925582885742\n",
      "for 10 epochs, loss is 0.004805325530469418 and val_loss is 3.633746862411499\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8555), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0421), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8551), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0424), \"['a', 'x', 'a', 'x']\": np.float64(-9.9032), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0421), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0421), \"['a', 'x', 'a', 'y']\": np.float64(0.0423), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8473), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0488), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8554), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0418), \"['a', 'x', 'b', 'x']\": np.float64(-9.9028), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0424), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0424), \"['a', 'x', 'b', 'y']\": np.float64(0.0426), \"['a', 'x']\": np.float64(-9.951), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.043), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.043), \"['a', 'y', 'a', 'x']\": np.float64(-0.0432), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0429), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.043), \"['a', 'y', 'b', 'x']\": np.float64(-0.0432), \"['a', 'y']\": np.float64(0.0434), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8475), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0421), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8471), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0424), \"['b', 'x', 'a', 'x']\": np.float64(-9.8951), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0488), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0488), \"['b', 'x', 'a', 'y']\": np.float64(0.0491), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8477), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0488), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8559), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0418), \"['b', 'x', 'b', 'x']\": np.float64(-9.9032), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0418), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0418), \"['b', 'x', 'b', 'y']\": np.float64(0.042), \"['b', 'x']\": np.float64(-9.9509), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.043), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.043), \"['b', 'y', 'a', 'x']\": np.float64(-0.0432), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.043), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.043), \"['b', 'y', 'b', 'x']\": np.float64(-0.0432), \"['b', 'y']\": np.float64(0.0434)}\n",
      "It s train loss bro [0.005671125371009111, 0.00502834003418684, 0.005002838093787432, 0.004977335687726736, 0.004952188581228256, 0.004927159287035465, 0.004902366548776627, 0.004877810832113028, 0.004853491671383381, 0.0048294090665876865, 0.004805325530469418]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 86 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.005438650958240032 and val_loss is 3.635586738586426\n",
      "for 1 epochs, loss is 0.004757750779390335 and val_loss is 3.6374146938323975\n",
      "for 2 epochs, loss is 0.004734140355139971 and val_loss is 3.6392314434051514\n",
      "for 3 epochs, loss is 0.004710766952484846 and val_loss is 3.641036033630371\n",
      "for 4 epochs, loss is 0.004687392618507147 and val_loss is 3.6428310871124268\n",
      "for 5 epochs, loss is 0.004664492793381214 and val_loss is 3.6446170806884766\n",
      "for 6 epochs, loss is 0.004641592036932707 and val_loss is 3.646392583847046\n",
      "for 7 epochs, loss is 0.004619047045707703 and val_loss is 3.648159980773926\n",
      "for 8 epochs, loss is 0.004596501123160124 and val_loss is 3.649918794631958\n",
      "for 9 epochs, loss is 0.004574310965836048 and val_loss is 3.6516685485839844\n",
      "for 10 epochs, loss is 0.004552120342850685 and val_loss is 3.653411388397217\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8629), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0398), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8626), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0401), \"['a', 'x', 'a', 'x']\": np.float64(-9.9081), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0398), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0398), \"['a', 'x', 'a', 'y']\": np.float64(0.04), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8549), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0464), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8629), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0395), \"['a', 'x', 'b', 'x']\": np.float64(-9.9078), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0401), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0401), \"['a', 'x', 'b', 'y']\": np.float64(0.0403), \"['a', 'x']\": np.float64(-9.9536), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0407), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0407), \"['a', 'y', 'a', 'x']\": np.float64(-0.0409), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0407), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0407), \"['a', 'y', 'b', 'x']\": np.float64(-0.0409), \"['a', 'y']\": np.float64(0.0411), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8551), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0398), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8548), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0401), \"['b', 'x', 'a', 'x']\": np.float64(-9.9003), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0464), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0464), \"['b', 'x', 'a', 'y']\": np.float64(0.0466), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8554), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0464), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8633), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0395), \"['b', 'x', 'b', 'x']\": np.float64(-9.9083), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0395), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0395), \"['b', 'x', 'b', 'y']\": np.float64(0.0397), \"['b', 'x']\": np.float64(-9.9534), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0407), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0407), \"['b', 'y', 'a', 'x']\": np.float64(-0.0408), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0406), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0407), \"['b', 'y', 'b', 'x']\": np.float64(-0.0408), \"['b', 'y']\": np.float64(0.041)}\n",
      "It s train loss bro [0.005438650958240032, 0.004757750779390335, 0.004734140355139971, 0.004710766952484846, 0.004687392618507147, 0.004664492793381214, 0.004641592036932707, 0.004619047045707703, 0.004596501123160124, 0.004574310965836048, 0.004552120342850685]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 87 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.005571551155298948 and val_loss is 3.655168294906616\n",
      "for 1 epochs, loss is 0.004508093930780888 and val_loss is 3.656916618347168\n",
      "for 2 epochs, loss is 0.004486138932406902 and val_loss is 3.6586554050445557\n",
      "for 3 epochs, loss is 0.004464420955628157 and val_loss is 3.6603851318359375\n",
      "for 4 epochs, loss is 0.004442940000444651 and val_loss is 3.662106513977051\n",
      "for 5 epochs, loss is 0.004421577323228121 and val_loss is 3.663818836212158\n",
      "for 6 epochs, loss is 0.004400332923978567 and val_loss is 3.6655232906341553\n",
      "for 7 epochs, loss is 0.004379325080662966 and val_loss is 3.667221784591675\n",
      "for 8 epochs, loss is 0.004358554258942604 and val_loss is 3.6689112186431885\n",
      "for 9 epochs, loss is 0.004337902180850506 and val_loss is 3.670593738555908\n",
      "for 10 epochs, loss is 0.0043173679150640965 and val_loss is 3.6722700595855713\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8698), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0377), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8695), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.038), \"['a', 'x', 'a', 'x']\": np.float64(-9.9128), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0377), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0377), \"['a', 'x', 'a', 'y']\": np.float64(0.0379), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8621), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0441), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8698), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0374), \"['a', 'x', 'b', 'x']\": np.float64(-9.9125), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.038), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.038), \"['a', 'x', 'b', 'y']\": np.float64(0.0382), \"['a', 'x']\": np.float64(-9.9559), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0385), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0385), \"['a', 'y', 'a', 'x']\": np.float64(-0.0387), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0385), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0385), \"['a', 'y', 'b', 'x']\": np.float64(-0.0387), \"['a', 'y']\": np.float64(0.0389), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8623), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0377), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.862), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.038), \"['b', 'x', 'a', 'x']\": np.float64(-9.9052), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0441), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0441), \"['b', 'x', 'a', 'y']\": np.float64(0.0443), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8626), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0441), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8703), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0374), \"['b', 'x', 'b', 'x']\": np.float64(-9.913), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0374), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0374), \"['b', 'x', 'b', 'y']\": np.float64(0.0376), \"['b', 'x']\": np.float64(-9.9558), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0385), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0385), \"['b', 'y', 'a', 'x']\": np.float64(-0.0387), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0385), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0385), \"['b', 'y', 'b', 'x']\": np.float64(-0.0387), \"['b', 'y']\": np.float64(0.0388)}\n",
      "It s train loss bro [0.005571551155298948, 0.004508093930780888, 0.004486138932406902, 0.004464420955628157, 0.004442940000444651, 0.004421577323228121, 0.004400332923978567, 0.004379325080662966, 0.004358554258942604, 0.004337902180850506, 0.0043173679150640965]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 88 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.005769860465079546 and val_loss is 3.6739726066589355\n",
      "for 1 epochs, loss is 0.004276298452168703 and val_loss is 3.675666093826294\n",
      "for 2 epochs, loss is 0.004255881533026695 and val_loss is 3.6773502826690674\n",
      "for 3 epochs, loss is 0.004235583357512951 and val_loss is 3.6790249347686768\n",
      "for 4 epochs, loss is 0.004215521737933159 and val_loss is 3.6806914806365967\n",
      "for 5 epochs, loss is 0.00419557886198163 and val_loss is 3.682349920272827\n",
      "for 6 epochs, loss is 0.004175872541964054 and val_loss is 3.6840004920959473\n",
      "for 7 epochs, loss is 0.004156166221946478 and val_loss is 3.6856436729431152\n",
      "for 8 epochs, loss is 0.0041368152014911175 and val_loss is 3.687279462814331\n",
      "for 9 epochs, loss is 0.0041175829246640205 and val_loss is 3.688908338546753\n",
      "for 10 epochs, loss is 0.0040984689258039 and val_loss is 3.69053053855896\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8763), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0358), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.876), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.036), \"['a', 'x', 'a', 'x']\": np.float64(-9.9171), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0358), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0358), \"['a', 'x', 'a', 'y']\": np.float64(0.0359), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8687), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.042), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8763), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0354), \"['a', 'x', 'b', 'x']\": np.float64(-9.9168), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.036), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.036), \"['a', 'x', 'b', 'y']\": np.float64(0.0362), \"['a', 'x']\": np.float64(-9.9581), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0366), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0366), \"['a', 'y', 'a', 'x']\": np.float64(-0.0367), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0365), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0366), \"['a', 'y', 'b', 'x']\": np.float64(-0.0367), \"['a', 'y']\": np.float64(0.0369), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8689), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0357), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8687), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.036), \"['b', 'x', 'a', 'x']\": np.float64(-9.9097), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.042), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.042), \"['b', 'x', 'a', 'y']\": np.float64(0.0421), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8692), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.042), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8768), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0354), \"['b', 'x', 'b', 'x']\": np.float64(-9.9173), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0354), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0354), \"['b', 'x', 'b', 'y']\": np.float64(0.0356), \"['b', 'x']\": np.float64(-9.958), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0365), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0365), \"['b', 'y', 'a', 'x']\": np.float64(-0.0366), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0364), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0365), \"['b', 'y', 'b', 'x']\": np.float64(-0.0366), \"['b', 'y']\": np.float64(0.0368)}\n",
      "It s train loss bro [0.005769860465079546, 0.004276298452168703, 0.004255881533026695, 0.004235583357512951, 0.004215521737933159, 0.00419557886198163, 0.004175872541964054, 0.004156166221946478, 0.0041368152014911175, 0.0041175829246640205, 0.0040984689258039]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 89 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.005982942879199982 and val_loss is 3.692206859588623\n",
      "for 1 epochs, loss is 0.004060358740389347 and val_loss is 3.6938705444335938\n",
      "for 2 epochs, loss is 0.004041362088173628 and val_loss is 3.695521831512451\n",
      "for 3 epochs, loss is 0.004022484179586172 and val_loss is 3.6971635818481445\n",
      "for 4 epochs, loss is 0.004003843292593956 and val_loss is 3.69879412651062\n",
      "for 5 epochs, loss is 0.003985320683568716 and val_loss is 3.7004153728485107\n",
      "for 6 epochs, loss is 0.003967035561800003 and val_loss is 3.7020277976989746\n",
      "for 7 epochs, loss is 0.003948868252336979 and val_loss is 3.70363187789917\n",
      "for 8 epochs, loss is 0.003930819686502218 and val_loss is 3.7052271366119385\n",
      "for 9 epochs, loss is 0.0039130086079239845 and val_loss is 3.706815481185913\n",
      "for 10 epochs, loss is 0.00389519683085382 and val_loss is 3.70839524269104\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8822), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0339), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.882), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0342), \"['a', 'x', 'a', 'x']\": np.float64(-9.9211), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0339), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0339), \"['a', 'x', 'a', 'y']\": np.float64(0.0341), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8749), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.04), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8823), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0336), \"['a', 'x', 'b', 'x']\": np.float64(-9.9208), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0342), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0342), \"['a', 'x', 'b', 'y']\": np.float64(0.0343), \"['a', 'x']\": np.float64(-9.9601), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0347), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0347), \"['a', 'y', 'a', 'x']\": np.float64(-0.0348), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0347), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0347), \"['a', 'y', 'b', 'x']\": np.float64(-0.0348), \"['a', 'y']\": np.float64(0.035), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8751), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0339), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8749), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0341), \"['b', 'x', 'a', 'x']\": np.float64(-9.914), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.04), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.04), \"['b', 'x', 'a', 'y']\": np.float64(0.0401), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8754), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.04), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8828), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0336), \"['b', 'x', 'b', 'x']\": np.float64(-9.9214), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0336), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0336), \"['b', 'x', 'b', 'y']\": np.float64(0.0337), \"['b', 'x']\": np.float64(-9.9601), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0346), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0346), \"['b', 'y', 'a', 'x']\": np.float64(-0.0347), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0346), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0346), \"['b', 'y', 'b', 'x']\": np.float64(-0.0347), \"['b', 'y']\": np.float64(0.0349)}\n",
      "It s train loss bro [0.005982942879199982, 0.004060358740389347, 0.004041362088173628, 0.004022484179586172, 0.004003843292593956, 0.003985320683568716, 0.003967035561800003, 0.003948868252336979, 0.003930819686502218, 0.0039130086079239845, 0.00389519683085382]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 90 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.004598637111485004 and val_loss is 3.709998607635498\n",
      "for 1 epochs, loss is 0.0038599285762757063 and val_loss is 3.7115917205810547\n",
      "for 2 epochs, loss is 0.0038423535879701376 and val_loss is 3.7131752967834473\n",
      "for 3 epochs, loss is 0.003824896877631545 and val_loss is 3.7147505283355713\n",
      "for 4 epochs, loss is 0.0038075584452599287 and val_loss is 3.716317892074585\n",
      "for 5 epochs, loss is 0.0037904575001448393 and val_loss is 3.71787691116333\n",
      "for 6 epochs, loss is 0.003773474832996726 and val_loss is 3.719428777694702\n",
      "for 7 epochs, loss is 0.0037566106766462326 and val_loss is 3.720972776412964\n",
      "for 8 epochs, loss is 0.0037398652639240026 and val_loss is 3.722510576248169\n",
      "for 9 epochs, loss is 0.0037233568727970123 and val_loss is 3.72404146194458\n",
      "for 10 epochs, loss is 0.0037067292723804712 and val_loss is 3.725567102432251\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8878), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0322), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8876), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0325), \"['a', 'x', 'a', 'x']\": np.float64(-9.9248), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0322), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0322), \"['a', 'x', 'a', 'y']\": np.float64(0.0324), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8807), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0381), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8879), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0319), \"['a', 'x', 'b', 'x']\": np.float64(-9.9246), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0325), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0325), \"['a', 'x', 'b', 'y']\": np.float64(0.0326), \"['a', 'x']\": np.float64(-9.962), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.033), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.033), \"['a', 'y', 'a', 'x']\": np.float64(-0.0331), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.033), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.033), \"['a', 'y', 'b', 'x']\": np.float64(-0.0331), \"['a', 'y']\": np.float64(0.0332), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8809), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0322), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8807), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0324), \"['b', 'x', 'a', 'x']\": np.float64(-9.9179), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0381), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0381), \"['b', 'x', 'a', 'y']\": np.float64(0.0383), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8812), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0381), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8884), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0319), \"['b', 'x', 'b', 'x']\": np.float64(-9.9251), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0319), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0319), \"['b', 'x', 'b', 'y']\": np.float64(0.032), \"['b', 'x']\": np.float64(-9.962), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0329), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0329), \"['b', 'y', 'a', 'x']\": np.float64(-0.033), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0328), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0329), \"['b', 'y', 'b', 'x']\": np.float64(-0.033), \"['b', 'y']\": np.float64(0.0331)}\n",
      "It s train loss bro [0.004598637111485004, 0.0038599285762757063, 0.0038423535879701376, 0.003824896877631545, 0.0038075584452599287, 0.0037904575001448393, 0.003773474832996726, 0.0037566106766462326, 0.0037398652639240026, 0.0037233568727970123, 0.0037067292723804712]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 91 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.0049462574534118176 and val_loss is 3.727132558822632\n",
      "for 1 epochs, loss is 0.0036737113259732723 and val_loss is 3.7286875247955322\n",
      "for 2 epochs, loss is 0.0036572017706930637 and val_loss is 3.7302324771881104\n",
      "for 3 epochs, loss is 0.003640810726210475 and val_loss is 3.73176908493042\n",
      "for 4 epochs, loss is 0.0036245384253561497 and val_loss is 3.7332959175109863\n",
      "for 5 epochs, loss is 0.003608503146097064 and val_loss is 3.7348155975341797\n",
      "for 6 epochs, loss is 0.003592586610466242 and val_loss is 3.736326217651367\n",
      "for 7 epochs, loss is 0.0035766696091741323 and val_loss is 3.7378292083740234\n",
      "for 8 epochs, loss is 0.0035611088387668133 and val_loss is 3.739325761795044\n",
      "for 9 epochs, loss is 0.003545429091900587 and val_loss is 3.7408156394958496\n",
      "for 10 epochs, loss is 0.003529986599460244 and val_loss is 3.742297887802124\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.893), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0306), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8928), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0308), \"['a', 'x', 'a', 'x']\": np.float64(-9.9283), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0306), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0306), \"['a', 'x', 'a', 'y']\": np.float64(0.0308), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8861), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0364), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8931), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0303), \"['a', 'x', 'b', 'x']\": np.float64(-9.9281), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0308), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0308), \"['a', 'x', 'b', 'y']\": np.float64(0.031), \"['a', 'x']\": np.float64(-9.9637), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0314), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0314), \"['a', 'y', 'a', 'x']\": np.float64(-0.0315), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0314), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0314), \"['a', 'y', 'b', 'x']\": np.float64(-0.0315), \"['a', 'y']\": np.float64(0.0316), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8863), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0306), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8861), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0308), \"['b', 'x', 'a', 'x']\": np.float64(-9.9216), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0364), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0364), \"['b', 'x', 'a', 'y']\": np.float64(0.0365), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8867), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0364), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8937), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0303), \"['b', 'x', 'b', 'x']\": np.float64(-9.9287), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0303), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0303), \"['b', 'x', 'b', 'y']\": np.float64(0.0304), \"['b', 'x']\": np.float64(-9.9638), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0312), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0312), \"['b', 'y', 'a', 'x']\": np.float64(-0.0313), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0312), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0312), \"['b', 'y', 'b', 'x']\": np.float64(-0.0313), \"['b', 'y']\": np.float64(0.0314)}\n",
      "It s train loss bro [0.0049462574534118176, 0.0036737113259732723, 0.0036572017706930637, 0.003640810726210475, 0.0036245384253561497, 0.003608503146097064, 0.003592586610466242, 0.0035766696091741323, 0.0035611088387668133, 0.003545429091900587, 0.003529986599460244]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 92 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.004296714439988136 and val_loss is 3.743807077407837\n",
      "for 1 epochs, loss is 0.003499338636174798 and val_loss is 3.7453060150146484\n",
      "for 2 epochs, loss is 0.003483895445242524 and val_loss is 3.746795892715454\n",
      "for 3 epochs, loss is 0.003468808252364397 and val_loss is 3.7482800483703613\n",
      "for 4 epochs, loss is 0.00345372105948627 and val_loss is 3.749753952026367\n",
      "for 5 epochs, loss is 0.003438871121034026 and val_loss is 3.751220464706421\n",
      "for 6 epochs, loss is 0.0034240209497511387 and val_loss is 3.752681016921997\n",
      "for 7 epochs, loss is 0.003409408265724778 and val_loss is 3.754134178161621\n",
      "for 8 epochs, loss is 0.003394795348867774 and val_loss is 3.755581855773926\n",
      "for 9 epochs, loss is 0.0033803009428083897 and val_loss is 3.7570228576660156\n",
      "for 10 epochs, loss is 0.003365925280377269 and val_loss is 3.7584571838378906\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8979), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0292), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8977), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0294), \"['a', 'x', 'a', 'x']\": np.float64(-9.9316), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0292), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0292), \"['a', 'x', 'a', 'y']\": np.float64(0.0293), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8911), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0347), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.898), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0288), \"['a', 'x', 'b', 'x']\": np.float64(-9.9314), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0294), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0294), \"['a', 'x', 'b', 'y']\": np.float64(0.0295), \"['a', 'x']\": np.float64(-9.9654), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0299), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0299), \"['a', 'y', 'a', 'x']\": np.float64(-0.03), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0299), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0299), \"['a', 'y', 'b', 'x']\": np.float64(-0.03), \"['a', 'y']\": np.float64(0.0301), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8914), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0292), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8912), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0293), \"['b', 'x', 'a', 'x']\": np.float64(-9.9251), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0347), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0347), \"['b', 'x', 'a', 'y']\": np.float64(0.0349), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8917), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0347), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8986), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0288), \"['b', 'x', 'b', 'x']\": np.float64(-9.9319), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0288), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0288), \"['b', 'x', 'b', 'y']\": np.float64(0.0289), \"['b', 'x']\": np.float64(-9.9654), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0297), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0297), \"['b', 'y', 'a', 'x']\": np.float64(-0.0298), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0297), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0297), \"['b', 'y', 'b', 'x']\": np.float64(-0.0298), \"['b', 'y']\": np.float64(0.0299)}\n",
      "It s train loss bro [0.004296714439988136, 0.003499338636174798, 0.003483895445242524, 0.003468808252364397, 0.00345372105948627, 0.003438871121034026, 0.0034240209497511387, 0.003409408265724778, 0.003394795348867774, 0.0033803009428083897, 0.003365925280377269]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 93 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.004429528955370188 and val_loss is 3.759927988052368\n",
      "for 1 epochs, loss is 0.0033371730241924524 and val_loss is 3.761387825012207\n",
      "for 2 epochs, loss is 0.0033227966632694006 and val_loss is 3.7628395557403564\n",
      "for 3 epochs, loss is 0.0033083013258874416 and val_loss is 3.7642829418182373\n",
      "for 4 epochs, loss is 0.0032942809630185366 and val_loss is 3.765717029571533\n",
      "for 5 epochs, loss is 0.0032802606001496315 and val_loss is 3.7671449184417725\n",
      "for 6 epochs, loss is 0.0032663585152477026 and val_loss is 3.768564224243164\n",
      "for 7 epochs, loss is 0.003252575406804681 and val_loss is 3.7699763774871826\n",
      "for 8 epochs, loss is 0.0032387918327003717 and val_loss is 3.771383047103882\n",
      "for 9 epochs, loss is 0.0032252457458525896 and val_loss is 3.772782325744629\n",
      "for 10 epochs, loss is 0.0032116996590048075 and val_loss is 3.7741763591766357\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9024), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0278), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9022), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.028), \"['a', 'x', 'a', 'x']\": np.float64(-9.9346), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0278), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0278), \"['a', 'x', 'a', 'y']\": np.float64(0.0279), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8959), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0332), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9026), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0274), \"['a', 'x', 'b', 'x']\": np.float64(-9.9344), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.028), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.028), \"['a', 'x', 'b', 'y']\": np.float64(0.028), \"['a', 'x']\": np.float64(-9.9669), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0285), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0285), \"['a', 'y', 'a', 'x']\": np.float64(-0.0286), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0285), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0285), \"['a', 'y', 'b', 'x']\": np.float64(-0.0286), \"['a', 'y']\": np.float64(0.0287), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8961), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0278), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8959), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0279), \"['b', 'x', 'a', 'x']\": np.float64(-9.9283), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0332), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0332), \"['b', 'x', 'a', 'y']\": np.float64(0.0333), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8965), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0332), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9032), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0274), \"['b', 'x', 'b', 'x']\": np.float64(-9.935), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0274), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0274), \"['b', 'x', 'b', 'y']\": np.float64(0.0275), \"['b', 'x']\": np.float64(-9.967), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0283), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0283), \"['b', 'y', 'a', 'x']\": np.float64(-0.0284), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0283), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0283), \"['b', 'y', 'b', 'x']\": np.float64(-0.0284), \"['b', 'y']\": np.float64(0.0285)}\n",
      "It s train loss bro [0.004429528955370188, 0.0033371730241924524, 0.0033227966632694006, 0.0033083013258874416, 0.0032942809630185366, 0.0032802606001496315, 0.0032663585152477026, 0.003252575406804681, 0.0032387918327003717, 0.0032252457458525896, 0.0032116996590048075]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 94 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.0037010284140706062 and val_loss is 3.7755799293518066\n",
      "for 1 epochs, loss is 0.003184844274073839 and val_loss is 3.776977300643921\n",
      "for 2 epochs, loss is 0.0031715352088212967 and val_loss is 3.77836537361145\n",
      "for 3 epochs, loss is 0.0031583448871970177 and val_loss is 3.7797484397888184\n",
      "for 4 epochs, loss is 0.003145154332742095 and val_loss is 3.7811238765716553\n",
      "for 5 epochs, loss is 0.003132082289084792 and val_loss is 3.78249454498291\n",
      "for 6 epochs, loss is 0.00311924796551466 and val_loss is 3.7838590145111084\n",
      "for 7 epochs, loss is 0.003106294432654977 and val_loss is 3.785217523574829\n",
      "for 8 epochs, loss is 0.0030934596434235573 and val_loss is 3.7865707874298096\n",
      "for 9 epochs, loss is 0.0030808625742793083 and val_loss is 3.7879185676574707\n",
      "for 10 epochs, loss is 0.003068265039473772 and val_loss is 3.7892613410949707\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9067), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0265), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9065), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0267), \"['a', 'x', 'a', 'x']\": np.float64(-9.9375), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0265), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0265), \"['a', 'x', 'a', 'y']\": np.float64(0.0266), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9003), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0318), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9068), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0262), \"['a', 'x', 'b', 'x']\": np.float64(-9.9373), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0267), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0267), \"['a', 'x', 'b', 'y']\": np.float64(0.0267), \"['a', 'x']\": np.float64(-9.9684), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0272), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0272), \"['a', 'y', 'a', 'x']\": np.float64(-0.0273), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0272), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0272), \"['a', 'y', 'b', 'x']\": np.float64(-0.0273), \"['a', 'y']\": np.float64(0.0274), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9006), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0265), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9004), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0266), \"['b', 'x', 'a', 'x']\": np.float64(-9.9314), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0318), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0318), \"['b', 'x', 'a', 'y']\": np.float64(0.0319), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9009), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0318), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9074), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0262), \"['b', 'x', 'b', 'x']\": np.float64(-9.9379), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0262), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0262), \"['b', 'x', 'b', 'y']\": np.float64(0.0262), \"['b', 'x']\": np.float64(-9.9685), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.027), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.027), \"['b', 'y', 'a', 'x']\": np.float64(-0.027), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0269), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.027), \"['b', 'y', 'b', 'x']\": np.float64(-0.027), \"['b', 'y']\": np.float64(0.0271)}\n",
      "It s train loss bro [0.0037010284140706062, 0.003184844274073839, 0.0031715352088212967, 0.0031583448871970177, 0.003145154332742095, 0.003132082289084792, 0.00311924796551466, 0.003106294432654977, 0.0030934596434235573, 0.0030808625742793083, 0.003068265039473772]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 95 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.003903271397575736 and val_loss is 3.7906334400177\n",
      "for 1 epochs, loss is 0.0030430699698626995 and val_loss is 3.791996479034424\n",
      "for 2 epochs, loss is 0.003030471969395876 and val_loss is 3.793353319168091\n",
      "for 3 epochs, loss is 0.003017992712557316 and val_loss is 3.794701099395752\n",
      "for 4 epochs, loss is 0.003005632432177663 and val_loss is 3.79604172706604\n",
      "for 5 epochs, loss is 0.0029932716861367226 and val_loss is 3.7973761558532715\n",
      "for 6 epochs, loss is 0.0029810296837240458 and val_loss is 3.798704147338867\n",
      "for 7 epochs, loss is 0.0029690254013985395 and val_loss is 3.8000264167785645\n",
      "for 8 epochs, loss is 0.0029569019097834826 and val_loss is 3.8013410568237305\n",
      "for 9 epochs, loss is 0.002945016138255596 and val_loss is 3.8026514053344727\n",
      "for 10 epochs, loss is 0.0029332491103559732 and val_loss is 3.803955554962158\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9107), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0253), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9105), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0254), \"['a', 'x', 'a', 'x']\": np.float64(-9.9401), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0253), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0253), \"['a', 'x', 'a', 'y']\": np.float64(0.0254), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9045), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0304), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9108), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0249), \"['a', 'x', 'b', 'x']\": np.float64(-9.94), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0254), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0254), \"['a', 'x', 'b', 'y']\": np.float64(0.0255), \"['a', 'x']\": np.float64(-9.9697), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.026), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.026), \"['a', 'y', 'a', 'x']\": np.float64(-0.026), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0259), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.026), \"['a', 'y', 'b', 'x']\": np.float64(-0.026), \"['a', 'y']\": np.float64(0.0261), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9048), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0253), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9046), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0254), \"['b', 'x', 'a', 'x']\": np.float64(-9.9342), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0304), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0304), \"['b', 'x', 'a', 'y']\": np.float64(0.0305), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9051), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0304), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9115), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0249), \"['b', 'x', 'b', 'x']\": np.float64(-9.9406), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0249), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0249), \"['b', 'x', 'b', 'y']\": np.float64(0.025), \"['b', 'x']\": np.float64(-9.9698), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0257), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0257), \"['b', 'y', 'a', 'x']\": np.float64(-0.0258), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0257), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0257), \"['b', 'y', 'b', 'x']\": np.float64(-0.0258), \"['b', 'y']\": np.float64(0.0259)}\n",
      "It s train loss bro [0.003903271397575736, 0.0030430699698626995, 0.003030471969395876, 0.003017992712557316, 0.003005632432177663, 0.0029932716861367226, 0.0029810296837240458, 0.0029690254013985395, 0.0029569019097834826, 0.002945016138255596, 0.0029332491103559732]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 96 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.0036185993812978268 and val_loss is 3.805280923843384\n",
      "for 1 epochs, loss is 0.0029094768688082695 and val_loss is 3.806598424911499\n",
      "for 2 epochs, loss is 0.0028977093752473593 and val_loss is 3.8079073429107666\n",
      "for 3 epochs, loss is 0.0028859416488558054 and val_loss is 3.8092105388641357\n",
      "for 4 epochs, loss is 0.002874292666092515 and val_loss is 3.8105063438415527\n",
      "for 5 epochs, loss is 0.0028627626597881317 and val_loss is 3.8117947578430176\n",
      "for 6 epochs, loss is 0.0028513511642813683 and val_loss is 3.813077926635742\n",
      "for 7 epochs, loss is 0.0028400584124028683 and val_loss is 3.814354419708252\n",
      "for 8 epochs, loss is 0.002828646684065461 and val_loss is 3.815626382827759\n",
      "for 9 epochs, loss is 0.0028174726758152246 and val_loss is 3.8168911933898926\n",
      "for 10 epochs, loss is 0.0028064174111932516 and val_loss is 3.8181514739990234\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9144), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0241), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9143), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0243), \"['a', 'x', 'a', 'x']\": np.float64(-9.9427), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0241), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0241), \"['a', 'x', 'a', 'y']\": np.float64(0.0242), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9084), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0291), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9146), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0238), \"['a', 'x', 'b', 'x']\": np.float64(-9.9425), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0243), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0243), \"['a', 'x', 'b', 'y']\": np.float64(0.0243), \"['a', 'x']\": np.float64(-9.971), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0248), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0248), \"['a', 'y', 'a', 'x']\": np.float64(-0.0249), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0248), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0248), \"['a', 'y', 'b', 'x']\": np.float64(-0.0249), \"['a', 'y']\": np.float64(0.0249), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9087), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0241), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9085), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0243), \"['b', 'x', 'a', 'x']\": np.float64(-9.9369), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0291), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0291), \"['b', 'x', 'a', 'y']\": np.float64(0.0292), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.909), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0291), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9152), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0238), \"['b', 'x', 'b', 'x']\": np.float64(-9.9431), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0238), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0238), \"['b', 'x', 'b', 'y']\": np.float64(0.0239), \"['b', 'x']\": np.float64(-9.9711), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0245), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0245), \"['b', 'y', 'a', 'x']\": np.float64(-0.0246), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0245), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0245), \"['b', 'y', 'b', 'x']\": np.float64(-0.0246), \"['b', 'y']\": np.float64(0.0247)}\n",
      "It s train loss bro [0.0036185993812978268, 0.0029094768688082695, 0.0028977093752473593, 0.0028859416488558054, 0.002874292666092515, 0.0028627626597881317, 0.0028513511642813683, 0.0028400584124028683, 0.002828646684065461, 0.0028174726758152246, 0.0028064174111932516]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 97 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.003142658853903413 and val_loss is 3.8194186687469482\n",
      "for 1 epochs, loss is 0.002784187439829111 and val_loss is 3.8206799030303955\n",
      "for 2 epochs, loss is 0.002773250686004758 and val_loss is 3.8219356536865234\n",
      "for 3 epochs, loss is 0.002762313699349761 and val_loss is 3.82318377494812\n",
      "for 4 epochs, loss is 0.0027514954563230276 and val_loss is 3.824427366256714\n",
      "for 5 epochs, loss is 0.002740677213296294 and val_loss is 3.8256640434265137\n",
      "for 6 epochs, loss is 0.0027299777138978243 and val_loss is 3.8268966674804688\n",
      "for 7 epochs, loss is 0.002719396958127618 and val_loss is 3.828122854232788\n",
      "for 8 epochs, loss is 0.0027088159695267677 and val_loss is 3.8293464183807373\n",
      "for 9 epochs, loss is 0.0026982349809259176 and val_loss is 3.830562114715576\n",
      "for 10 epochs, loss is 0.002687772735953331 and val_loss is 3.831773519515991\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.918), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0231), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9178), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0232), \"['a', 'x', 'a', 'x']\": np.float64(-9.945), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0231), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0231), \"['a', 'x', 'a', 'y']\": np.float64(0.0231), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9121), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0279), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9181), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0227), \"['a', 'x', 'b', 'x']\": np.float64(-9.9449), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0232), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0232), \"['a', 'x', 'b', 'y']\": np.float64(0.0233), \"['a', 'x']\": np.float64(-9.9722), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0237), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0237), \"['a', 'y', 'a', 'x']\": np.float64(-0.0238), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0237), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0237), \"['a', 'y', 'b', 'x']\": np.float64(-0.0238), \"['a', 'y']\": np.float64(0.0239), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9124), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0231), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9122), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0232), \"['b', 'x', 'a', 'x']\": np.float64(-9.9394), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0279), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0279), \"['b', 'x', 'a', 'y']\": np.float64(0.028), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9127), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0279), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9188), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0227), \"['b', 'x', 'b', 'x']\": np.float64(-9.9455), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0227), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0227), \"['b', 'x', 'b', 'y']\": np.float64(0.0228), \"['b', 'x']\": np.float64(-9.9723), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0235), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0235), \"['b', 'y', 'a', 'x']\": np.float64(-0.0235), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0234), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0235), \"['b', 'y', 'b', 'x']\": np.float64(-0.0235), \"['b', 'y']\": np.float64(0.0236)}\n",
      "It s train loss bro [0.003142658853903413, 0.002784187439829111, 0.002773250686004758, 0.002762313699349761, 0.0027514954563230276, 0.002740677213296294, 0.0027299777138978243, 0.002719396958127618, 0.0027088159695267677, 0.0026982349809259176, 0.002687772735953331]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 98 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.0034976755268871784 and val_loss is 3.83302640914917\n",
      "for 1 epochs, loss is 0.002666848013177514 and val_loss is 3.8342695236206055\n",
      "for 2 epochs, loss is 0.002656385535374284 and val_loss is 3.835502862930298\n",
      "for 3 epochs, loss is 0.00264592282474041 and val_loss is 3.836729049682617\n",
      "for 4 epochs, loss is 0.002635579090565443 and val_loss is 3.8379480838775635\n",
      "for 5 epochs, loss is 0.002625472843647003 and val_loss is 3.8391590118408203\n",
      "for 6 epochs, loss is 0.0026153665967285633 and val_loss is 3.840362548828125\n",
      "for 7 epochs, loss is 0.0026052603498101234 and val_loss is 3.8415586948394775\n",
      "for 8 epochs, loss is 0.00259515387006104 and val_loss is 3.842750310897827\n",
      "for 9 epochs, loss is 0.00258516613394022 and val_loss is 3.843935012817383\n",
      "for 10 epochs, loss is 0.0025754161179065704 and val_loss is 3.8451125621795654\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9213), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0221), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9212), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0222), \"['a', 'x', 'a', 'x']\": np.float64(-9.9473), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0221), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0221), \"['a', 'x', 'a', 'y']\": np.float64(0.0221), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9156), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0268), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9215), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0217), \"['a', 'x', 'b', 'x']\": np.float64(-9.9471), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0222), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0222), \"['a', 'x', 'b', 'y']\": np.float64(0.0222), \"['a', 'x']\": np.float64(-9.9733), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0227), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0227), \"['a', 'y', 'a', 'x']\": np.float64(-0.0228), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0227), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0227), \"['a', 'y', 'b', 'x']\": np.float64(-0.0228), \"['a', 'y']\": np.float64(0.0228), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9159), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0221), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9158), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0222), \"['b', 'x', 'a', 'x']\": np.float64(-9.9418), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0268), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0268), \"['b', 'x', 'a', 'y']\": np.float64(0.0269), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9162), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0268), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9221), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0217), \"['b', 'x', 'b', 'x']\": np.float64(-9.9478), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0217), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0217), \"['b', 'x', 'b', 'y']\": np.float64(0.0218), \"['b', 'x']\": np.float64(-9.9735), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0224), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0224), \"['b', 'y', 'a', 'x']\": np.float64(-0.0225), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0224), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0224), \"['b', 'y', 'b', 'x']\": np.float64(-0.0225), \"['b', 'y']\": np.float64(0.0225)}\n",
      "It s train loss bro [0.0034976755268871784, 0.002666848013177514, 0.002656385535374284, 0.00264592282474041, 0.002635579090565443, 0.002625472843647003, 0.0026153665967285633, 0.0026052603498101234, 0.00259515387006104, 0.00258516613394022, 0.0025754161179065704]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 99 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.0037999581545591354 and val_loss is 3.8463408946990967\n",
      "for 1 epochs, loss is 0.002555440180003643 and val_loss is 3.847560167312622\n",
      "for 2 epochs, loss is 0.002545333234593272 and val_loss is 3.8487660884857178\n",
      "for 3 epochs, loss is 0.002535345032811165 and val_loss is 3.8499627113342285\n",
      "for 4 epochs, loss is 0.0025254758074879646 and val_loss is 3.8511505126953125\n",
      "for 5 epochs, loss is 0.0025157250929623842 and val_loss is 3.852329969406128\n",
      "for 6 epochs, loss is 0.002506093354895711 and val_loss is 3.8534998893737793\n",
      "for 7 epochs, loss is 0.0024964616168290377 and val_loss is 3.8546619415283203\n",
      "for 8 epochs, loss is 0.002486948622390628 and val_loss is 3.8558154106140137\n",
      "for 9 epochs, loss is 0.002477435627952218 and val_loss is 3.856962203979492\n",
      "for 10 epochs, loss is 0.002468041144311428 and val_loss is 3.8581020832061768\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9245), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0211), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9244), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0212), \"['a', 'x', 'a', 'x']\": np.float64(-9.9494), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0211), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0211), \"['a', 'x', 'a', 'y']\": np.float64(0.0212), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9189), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0257), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9247), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0208), \"['a', 'x', 'b', 'x']\": np.float64(-9.9493), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0212), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0212), \"['a', 'x', 'b', 'y']\": np.float64(0.0213), \"['a', 'x']\": np.float64(-9.9744), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0217), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0217), \"['a', 'y', 'a', 'x']\": np.float64(-0.0218), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0217), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0217), \"['a', 'y', 'b', 'x']\": np.float64(-0.0218), \"['a', 'y']\": np.float64(0.0218), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9193), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0211), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9191), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0212), \"['b', 'x', 'a', 'x']\": np.float64(-9.9442), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0257), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0257), \"['b', 'x', 'a', 'y']\": np.float64(0.0258), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9196), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0257), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9253), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0208), \"['b', 'x', 'b', 'x']\": np.float64(-9.9499), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0208), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0208), \"['b', 'x', 'b', 'y']\": np.float64(0.0208), \"['b', 'x']\": np.float64(-9.9746), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0214), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0214), \"['b', 'y', 'a', 'x']\": np.float64(-0.0215), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0214), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0214), \"['b', 'y', 'b', 'x']\": np.float64(-0.0215), \"['b', 'y']\": np.float64(0.0215)}\n",
      "It s train loss bro [0.0037999581545591354, 0.002555440180003643, 0.002545333234593272, 0.002535345032811165, 0.0025254758074879646, 0.0025157250929623842, 0.002506093354895711, 0.0024964616168290377, 0.002486948622390628, 0.002477435627952218, 0.002468041144311428]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 100 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.00381112121976912 and val_loss is 3.859299421310425\n",
      "for 1 epochs, loss is 0.002449014689773321 and val_loss is 3.860485315322876\n",
      "for 2 epochs, loss is 0.002439501229673624 and val_loss is 3.8616578578948975\n",
      "for 3 epochs, loss is 0.0024301065132021904 and val_loss is 3.862818479537964\n",
      "for 4 epochs, loss is 0.0024205928202718496 and val_loss is 3.8639674186706543\n",
      "for 5 epochs, loss is 0.002411317080259323 and val_loss is 3.8651070594787598\n",
      "for 6 epochs, loss is 0.0024021598510444164 and val_loss is 3.866238832473755\n",
      "for 7 epochs, loss is 0.0023930028546601534 and val_loss is 3.8673622608184814\n",
      "for 8 epochs, loss is 0.002383964601904154 and val_loss is 3.868473768234253\n",
      "for 9 epochs, loss is 0.0023750450927764177 and val_loss is 3.869580030441284\n",
      "for 10 epochs, loss is 0.0023660066071897745 and val_loss is 3.8706777095794678\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9275), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0202), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9274), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0203), \"['a', 'x', 'a', 'x']\": np.float64(-9.9514), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0202), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0202), \"['a', 'x', 'a', 'y']\": np.float64(0.0202), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9221), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0246), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9277), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0198), \"['a', 'x', 'b', 'x']\": np.float64(-9.9513), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0203), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0203), \"['a', 'x', 'b', 'y']\": np.float64(0.0203), \"['a', 'x']\": np.float64(-9.9754), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0208), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0208), \"['a', 'y', 'a', 'x']\": np.float64(-0.0208), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0208), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0208), \"['a', 'y', 'b', 'x']\": np.float64(-0.0208), \"['a', 'y']\": np.float64(0.0209), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9225), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0202), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9223), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0203), \"['b', 'x', 'a', 'x']\": np.float64(-9.9464), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0246), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0246), \"['b', 'x', 'a', 'y']\": np.float64(0.0247), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9228), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0246), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9284), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0198), \"['b', 'x', 'b', 'x']\": np.float64(-9.952), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0198), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0198), \"['b', 'x', 'b', 'y']\": np.float64(0.0199), \"['b', 'x']\": np.float64(-9.9756), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0205), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0205), \"['b', 'y', 'a', 'x']\": np.float64(-0.0205), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0205), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0205), \"['b', 'y', 'b', 'x']\": np.float64(-0.0205), \"['b', 'y']\": np.float64(0.0206)}\n",
      "It s train loss bro [0.00381112121976912, 0.002449014689773321, 0.002439501229673624, 0.0024301065132021904, 0.0024205928202718496, 0.002411317080259323, 0.0024021598510444164, 0.0023930028546601534, 0.002383964601904154, 0.0023750450927764177, 0.0023660066071897745]\n",
      "% good predict : 100\n",
      "\u001b[0;34miteration 101 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.0027305721305310726 and val_loss is 3.871774196624756\n",
      "for 1 epochs, loss is 0.0023484050761908293 and val_loss is 3.8728623390197754\n",
      "for 2 epochs, loss is 0.0023396043106913567 and val_loss is 3.8739445209503174\n",
      "for 3 epochs, loss is 0.0023309222888201475 and val_loss is 3.875018358230591\n",
      "for 4 epochs, loss is 0.0023222402669489384 and val_loss is 3.8760828971862793\n",
      "for 5 epochs, loss is 0.0023136769887059927 and val_loss is 3.877143383026123\n",
      "for 6 epochs, loss is 0.002305113710463047 and val_loss is 3.878194808959961\n",
      "for 7 epochs, loss is 0.0022966694086790085 and val_loss is 3.879242420196533\n",
      "for 8 epochs, loss is 0.0022882248740643263 and val_loss is 3.8802828788757324\n",
      "for 9 epochs, loss is 0.002279780339449644 and val_loss is 3.8813188076019287\n",
      "for 10 epochs, loss is 0.0022715735249221325 and val_loss is 3.8823471069335938\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9303), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0193), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9302), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0194), \"['a', 'x', 'a', 'x']\": np.float64(-9.9533), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0193), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0193), \"['a', 'x', 'a', 'y']\": np.float64(0.0194), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9251), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0237), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9306), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.019), \"['a', 'x', 'b', 'x']\": np.float64(-9.9532), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0194), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0194), \"['a', 'x', 'b', 'y']\": np.float64(0.0195), \"['a', 'x']\": np.float64(-9.9763), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0199), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0199), \"['a', 'y', 'a', 'x']\": np.float64(-0.02), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0199), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0199), \"['a', 'y', 'b', 'x']\": np.float64(-0.02), \"['a', 'y']\": np.float64(0.02), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.9254), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0193), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.9253), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0194), \"['b', 'x', 'a', 'x']\": np.float64(-9.9484), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0237), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0237), \"['b', 'x', 'a', 'y']\": np.float64(0.0237), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.9258), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0237), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.9312), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.019), \"['b', 'x', 'b', 'x']\": np.float64(-9.9539), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.019), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.019), \"['b', 'x', 'b', 'y']\": np.float64(0.019), \"['b', 'x']\": np.float64(-9.9766), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0196), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0196), \"['b', 'y', 'a', 'x']\": np.float64(-0.0197), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0196), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0196), \"['b', 'y', 'b', 'x']\": np.float64(-0.0197), \"['b', 'y']\": np.float64(0.0197)}\n",
      "It s train loss bro [0.0027305721305310726, 0.0023484050761908293, 0.0023396043106913567, 0.0023309222888201475, 0.0023222402669489384, 0.0023136769887059927, 0.002305113710463047, 0.0022966694086790085, 0.0022882248740643263, 0.002279780339449644, 0.0022715735249221325]\n",
      "% good predict : 90\n",
      "\u001b[0;34miteration 102 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 6.061942100524902 and val_loss is 3.879323720932007\n",
      "for 1 epochs, loss is 6.224942207336426 and val_loss is 3.873321294784546\n",
      "for 2 epochs, loss is 6.198923110961914 and val_loss is 3.8647050857543945\n",
      "for 3 epochs, loss is 6.163506507873535 and val_loss is 3.8537607192993164\n",
      "for 4 epochs, loss is 6.1201910972595215 and val_loss is 3.8407411575317383\n",
      "for 5 epochs, loss is 6.070163726806641 and val_loss is 3.8258564472198486\n",
      "for 6 epochs, loss is 6.014387130737305 and val_loss is 3.809267044067383\n",
      "for 7 epochs, loss is 5.953661918640137 and val_loss is 3.791030168533325\n",
      "for 8 epochs, loss is 5.8886566162109375 and val_loss is 3.770977258682251\n",
      "for 9 epochs, loss is 5.819931507110596 and val_loss is 3.7484257221221924\n",
      "for 10 epochs, loss is 5.747960567474365 and val_loss is 3.721524953842163\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.886), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0333), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8872), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0321), \"['a', 'x', 'a', 'x']\": np.float64(-9.9237), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0333), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0333), \"['a', 'x', 'a', 'y']\": np.float64(0.0334), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8829), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0368), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8887), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0318), \"['a', 'x', 'b', 'x']\": np.float64(-9.9249), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0321), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0321), \"['a', 'x', 'b', 'y']\": np.float64(0.0322), \"['a', 'x']\": np.float64(-9.9615), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0337), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0337), \"['a', 'y', 'a', 'x']\": np.float64(-0.0338), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0337), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0337), \"['a', 'y', 'b', 'x']\": np.float64(-0.0339), \"['a', 'y']\": np.float64(0.034), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.8833), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0333), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.8845), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0321), \"['b', 'x', 'a', 'x']\": np.float64(-9.921), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0368), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0368), \"['b', 'x', 'a', 'y']\": np.float64(0.0369), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.8848), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0368), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.8907), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0318), \"['b', 'x', 'b', 'x']\": np.float64(-9.9269), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0318), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0318), \"['b', 'x', 'b', 'y']\": np.float64(0.0319), \"['b', 'x']\": np.float64(-9.9632), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0321), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0321), \"['b', 'y', 'a', 'x']\": np.float64(-0.0322), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.032), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0321), \"['b', 'y', 'b', 'x']\": np.float64(-0.0322), \"['b', 'y']\": np.float64(0.0323)}\n",
      "It s train loss bro [6.061942100524902, 6.224942207336426, 6.198923110961914, 6.163506507873535, 6.1201910972595215, 6.070163726806641, 6.014387130737305, 5.953661918640137, 5.8886566162109375, 5.819931507110596, 5.747960567474365]\n",
      "% good predict : 80\n",
      "\u001b[0;34miteration 103 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 5.59249210357666 and val_loss is 3.686840534210205\n",
      "for 1 epochs, loss is 5.672615051269531 and val_loss is 3.641183614730835\n",
      "for 2 epochs, loss is 5.600327014923096 and val_loss is 3.6228795051574707\n",
      "for 3 epochs, loss is 5.526583194732666 and val_loss is 3.6248984336853027\n",
      "for 4 epochs, loss is 5.451663017272949 and val_loss is 3.624833583831787\n",
      "for 5 epochs, loss is 5.375802040100098 and val_loss is 3.63655686378479\n",
      "for 6 epochs, loss is 5.299206733703613 and val_loss is 3.6853444576263428\n",
      "for 7 epochs, loss is 5.222053527832031 and val_loss is 3.7368438243865967\n",
      "for 8 epochs, loss is 5.144493579864502 and val_loss is 3.755683422088623\n",
      "for 9 epochs, loss is 5.066657066345215 and val_loss is 3.755937099456787\n",
      "for 10 epochs, loss is 4.98865270614624 and val_loss is 3.747312307357788\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7463), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0773), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.753), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.071), \"['a', 'x', 'a', 'x']\": np.float64(-9.8302), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0773), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.0774), \"['a', 'x', 'a', 'y']\": np.float64(0.078), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.7561), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0739), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.7591), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0716), \"['a', 'x', 'b', 'x']\": np.float64(-9.8369), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.071), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.071), \"['a', 'x', 'b', 'y']\": np.float64(0.0716), \"['a', 'x']\": np.float64(-9.9147), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0773), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0773), \"['a', 'y', 'a', 'x']\": np.float64(-0.0779), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0774), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0774), \"['a', 'y', 'b', 'x']\": np.float64(-0.078), \"['a', 'y']\": np.float64(0.0786), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.7569), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0774), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.7636), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.071), \"['b', 'x', 'a', 'x']\": np.float64(-9.8409), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.0739), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.074), \"['b', 'x', 'a', 'y']\": np.float64(0.0745), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.763), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.0739), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.766), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.0716), \"['b', 'x', 'b', 'x']\": np.float64(-9.8439), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.0716), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.0716), \"['b', 'x', 'b', 'y']\": np.float64(0.0722), \"['b', 'x']\": np.float64(-9.9224), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0701), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.0702), \"['b', 'y', 'a', 'x']\": np.float64(-0.0707), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0702), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.0702), \"['b', 'y', 'b', 'x']\": np.float64(-0.0708), \"['b', 'y']\": np.float64(0.0713)}\n",
      "It s train loss bro [5.59249210357666, 5.672615051269531, 5.600327014923096, 5.526583194732666, 5.451663017272949, 5.375802040100098, 5.299206733703613, 5.222053527832031, 5.144493579864502, 5.066657066345215, 4.98865270614624]\n",
      "% good predict : 80\n",
      "\u001b[0;34miteration 104 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 4.580133438110352 and val_loss is 3.7333946228027344\n",
      "for 1 epochs, loss is 4.725010871887207 and val_loss is 3.7166073322296143\n",
      "for 2 epochs, loss is 4.643048286437988 and val_loss is 3.697935104370117\n",
      "for 3 epochs, loss is 4.559793472290039 and val_loss is 3.6779160499572754\n",
      "for 4 epochs, loss is 4.475254058837891 and val_loss is 3.6568636894226074\n",
      "for 5 epochs, loss is 4.389394283294678 and val_loss is 3.6349680423736572\n",
      "for 6 epochs, loss is 4.3021368980407715 and val_loss is 3.612344980239868\n",
      "for 7 epochs, loss is 4.213364601135254 and val_loss is 3.589064598083496\n",
      "for 8 epochs, loss is 4.122919082641602 and val_loss is 3.5651614665985107\n",
      "for 9 epochs, loss is 4.030601501464844 and val_loss is 3.540651321411133\n",
      "for 10 epochs, loss is 3.936161518096924 and val_loss is 3.5155272483825684\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.37), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.1996), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.4163), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1556), \"['a', 'x', 'a', 'x']\": np.float64(-9.5812), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1995), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0044), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.2007), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0033), \"['a', 'x', 'a', 'y']\": np.float64(0.2041), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.4707), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1488), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.4583), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1608), \"['a', 'x', 'b', 'x']\": np.float64(-9.6286), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1561), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0029), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1564), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0026), \"['a', 'x', 'b', 'y']\": np.float64(0.1591), \"['a', 'x']\": np.float64(-9.7972), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1833), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0039), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1842), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.003), \"['a', 'y', 'a', 'x']\": np.float64(-0.1874), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.004), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.004), \"['a', 'y', 'a', 'y']\": np.float64(0.0041), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1854), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0029), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1851), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0031), \"['a', 'y', 'b', 'x']\": np.float64(-0.1885), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.003), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.003), \"['a', 'y', 'b', 'y']\": np.float64(0.0031), \"['a', 'y']\": np.float64(0.1917), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.4591), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2015), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.5059), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1571), \"['b', 'x', 'a', 'x']\": np.float64(-9.6724), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.1485), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0032), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.1494), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0024), \"['b', 'x', 'a', 'y']\": np.float64(0.152), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.5013), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.1493), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.4888), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1613), \"['b', 'x', 'b', 'x']\": np.float64(-9.6597), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1611), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.003), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1614), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0027), \"['b', 'x', 'b', 'y']\": np.float64(0.1642), \"['b', 'x']\": np.float64(-9.8337), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1504), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0032), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1512), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0025), \"['b', 'y', 'a', 'x']\": np.float64(-0.1538), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0028), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0028), \"['b', 'y', 'a', 'y']\": np.float64(0.0028), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1516), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0024), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1514), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0026), \"['b', 'y', 'b', 'x']\": np.float64(-0.1541), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0025), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0025), \"['b', 'y', 'b', 'y']\": np.float64(0.0026), \"['b', 'y']\": np.float64(0.1568)}\n",
      "It s train loss bro [4.580133438110352, 4.725010871887207, 4.643048286437988, 4.559793472290039, 4.475254058837891, 4.389394283294678, 4.3021368980407715, 4.213364601135254, 4.122919082641602, 4.030601501464844, 3.936161518096924]\n",
      "% good predict : 80\n",
      "\u001b[0;34miteration 105 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.02541188895702362 and val_loss is 3.274153470993042\n",
      "for 1 epochs, loss is 0.058303941041231155 and val_loss is 3.050513744354248\n",
      "for 2 epochs, loss is 0.14001823961734772 and val_loss is 2.871246576309204\n",
      "for 3 epochs, loss is 0.27843159437179565 and val_loss is 2.741713523864746\n",
      "for 4 epochs, loss is 0.40786272287368774 and val_loss is 2.650545835494995\n",
      "for 5 epochs, loss is 0.4609449505805969 and val_loss is 2.5891945362091064\n",
      "for 6 epochs, loss is 0.4586431384086609 and val_loss is 2.5504889488220215\n",
      "for 7 epochs, loss is 0.43668225407600403 and val_loss is 2.5261566638946533\n",
      "for 8 epochs, loss is 0.4107990562915802 and val_loss is 2.5040621757507324\n",
      "for 9 epochs, loss is 0.38519787788391113 and val_loss is 2.4293830394744873\n",
      "for 10 epochs, loss is 0.36049243807792664 and val_loss is 2.5096895694732666\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.694), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4121), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.7893), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3326), \"['a', 'x', 'a', 'x']\": np.float64(-5.1654), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4118), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5401), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4491), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.509), \"['a', 'x', 'a', 'y']\": np.float64(1.9746), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.852), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3976), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.8813), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.372), \"['a', 'x', 'b', 'x']\": np.float64(-5.2987), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3485), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4963), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3654), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4821), \"['a', 'x', 'b', 'y']\": np.float64(1.8635), \"['a', 'x']\": np.float64(-7.223), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3675), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.5228), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4028), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4933), \"['a', 'y', 'a', 'x']\": np.float64(-1.9122), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.523), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.2001), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.5369), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1886), \"['a', 'y', 'a', 'y']\": np.float64(0.7316), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4268), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.5177), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4377), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.5082), \"['a', 'y', 'b', 'x']\": np.float64(-1.9627), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4989), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1836), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.5052), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1784), \"['a', 'y', 'b', 'y']\": np.float64(0.6895), \"['a', 'y']\": np.float64(2.6746), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.8486), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4712), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.9479), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3884), \"['b', 'x', 'a', 'x']\": np.float64(-5.3816), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.396), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5341), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4329), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.5033), \"['b', 'x', 'a', 'y']\": np.float64(1.9526), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.942), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4303), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.972), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4041), \"['b', 'x', 'b', 'x']\": np.float64(-5.4226), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3871), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.5106), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4045), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.496), \"['b', 'x', 'b', 'y']\": np.float64(1.9168), \"['b', 'x']\": np.float64(-7.4028), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3034), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4983), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3371), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4702), \"['b', 'y', 'a', 'x']\": np.float64(-1.8226), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4796), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1835), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4923), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1729), \"['b', 'y', 'a', 'y']\": np.float64(0.6709), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3416), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4868), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3518), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4779), \"['b', 'y', 'b', 'x']\": np.float64(-1.8455), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4716), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1736), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4775), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1686), \"['b', 'y', 'b', 'y']\": np.float64(0.6517), \"['b', 'y']\": np.float64(2.5187)}\n",
      "It s train loss bro [0.02541188895702362, 0.058303941041231155, 0.14001823961734772, 0.27843159437179565, 0.40786272287368774, 0.4609449505805969, 0.4586431384086609, 0.43668225407600403, 0.4107990562915802, 0.38519787788391113, 0.36049243807792664]\n",
      "% good predict : 70\n",
      "\u001b[0;34miteration 106 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3509082496166229 and val_loss is 2.519700765609741\n",
      "for 1 epochs, loss is 0.3133603036403656 and val_loss is 2.530704975128174\n",
      "for 2 epochs, loss is 0.29091447591781616 and val_loss is 2.5426011085510254\n",
      "for 3 epochs, loss is 0.269360214471817 and val_loss is 2.5565907955169678\n",
      "for 4 epochs, loss is 0.2488291710615158 and val_loss is 2.5719335079193115\n",
      "for 5 epochs, loss is 0.22943778336048126 and val_loss is 2.588127374649048\n",
      "for 6 epochs, loss is 0.2112693190574646 and val_loss is 2.6048548221588135\n",
      "for 7 epochs, loss is 0.1943691372871399 and val_loss is 2.621901273727417\n",
      "for 8 epochs, loss is 0.17874649167060852 and val_loss is 2.63911509513855\n",
      "for 9 epochs, loss is 0.16438086330890656 and val_loss is 2.656383991241455\n",
      "for 10 epochs, loss is 0.15122844278812408 and val_loss is 2.6736233234405518\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.5184), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9538), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.4907), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9837), \"['a', 'x', 'a', 'x']\": np.float64(-7.5249), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9575), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1355), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9496), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1441), \"['a', 'x', 'a', 'y']\": np.float64(1.1011), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.4069), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0371), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4726), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9695), \"['a', 'x', 'b', 'x']\": np.float64(-7.4929), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9856), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1419), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9808), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.147), \"['a', 'x', 'b', 'y']\": np.float64(1.1356), \"['a', 'x']\": np.float64(-8.6868), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9352), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1368), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9313), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1411), \"['a', 'y', 'a', 'x']\": np.float64(-1.0797), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1329), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0188), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1318), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.02), \"['a', 'y', 'a', 'y']\": np.float64(0.1528), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9155), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1482), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9249), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1385), \"['a', 'y', 'b', 'x']\": np.float64(-1.0707), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.141), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0203), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1403), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.021), \"['a', 'y', 'b', 'y']\": np.float64(0.1625), \"['a', 'y']\": np.float64(1.2415), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.3725), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9324), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.3455), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9617), \"['b', 'x', 'a', 'x']\": np.float64(-7.3565), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0355), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1466), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.027), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1558), \"['b', 'x', 'a', 'y']\": np.float64(1.1908), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.3548), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0286), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4199), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9616), \"['b', 'x', 'b', 'x']\": np.float64(-7.4319), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9661), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1391), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9614), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1441), \"['b', 'x', 'b', 'y']\": np.float64(1.1131), \"['b', 'x']\": np.float64(-8.6034), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0007), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1464), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9965), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.151), \"['b', 'y', 'a', 'x']\": np.float64(-1.1552), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1447), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0205), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1435), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0218), \"['b', 'y', 'a', 'y']\": np.float64(0.1663), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9831), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1591), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9931), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1487), \"['b', 'y', 'b', 'x']\": np.float64(-1.1497), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1496), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0215), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1489), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0223), \"['b', 'y', 'b', 'y']\": np.float64(0.1724), \"['b', 'y']\": np.float64(1.3311)}\n",
      "It s train loss bro [0.3509082496166229, 0.3133603036403656, 0.29091447591781616, 0.269360214471817, 0.2488291710615158, 0.22943778336048126, 0.2112693190574646, 0.1943691372871399, 0.17874649167060852, 0.16438086330890656, 0.15122844278812408]\n",
      "% good predict : 70\n",
      "\u001b[0;34miteration 107 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.938356876373291 and val_loss is 2.674546480178833\n",
      "for 1 epochs, loss is 2.075530767440796 and val_loss is 2.6746885776519775\n",
      "for 2 epochs, loss is 2.0725209712982178 and val_loss is 2.674144983291626\n",
      "for 3 epochs, loss is 2.064767360687256 and val_loss is 2.673001527786255\n",
      "for 4 epochs, loss is 2.052847146987915 and val_loss is 2.6713404655456543\n",
      "for 5 epochs, loss is 2.037283420562744 and val_loss is 2.6692352294921875\n",
      "for 6 epochs, loss is 2.0185508728027344 and val_loss is 2.6667540073394775\n",
      "for 7 epochs, loss is 1.997077465057373 and val_loss is 2.66395902633667\n",
      "for 8 epochs, loss is 1.9732489585876465 and val_loss is 2.6609058380126953\n",
      "for 9 epochs, loss is 1.947411060333252 and val_loss is 2.657644748687744\n",
      "for 10 epochs, loss is 1.9198753833770752 and val_loss is 2.654221534729004\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.9797), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0754), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9457), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1114), \"['a', 'x', 'a', 'x']\": np.float64(-7.1054), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0795), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.189), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0691), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2), \"['a', 'x', 'a', 'y']\": np.float64(1.2778), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.8782), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1395), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9227), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0938), \"['a', 'x', 'b', 'x']\": np.float64(-7.065), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1131), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.198), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1069), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2046), \"['a', 'x', 'b', 'y']\": np.float64(1.3206), \"['a', 'x']\": np.float64(-8.4429), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0558), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1899), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0498), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1962), \"['a', 'y', 'a', 'x']\": np.float64(-1.2545), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1855), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0325), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1837), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0344), \"['a', 'y', 'a', 'y']\": np.float64(0.2196), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0337), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2004), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0416), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1924), \"['a', 'y', 'b', 'x']\": np.float64(-1.2424), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1959), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0349), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1949), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.036), \"['a', 'y', 'b', 'y']\": np.float64(0.2325), \"['a', 'y']\": np.float64(1.485), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.842), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0506), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8088), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0858), \"['b', 'x', 'a', 'x']\": np.float64(-6.9417), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1368), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.199), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1259), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2107), \"['b', 'x', 'a', 'y']\": np.float64(1.3456), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.8193), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1281), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.8634), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0829), \"['b', 'x', 'b', 'x']\": np.float64(-6.9943), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0888), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1937), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0828), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2001), \"['b', 'x', 'b', 'y']\": np.float64(1.2918), \"['b', 'x']\": np.float64(-8.3434), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.128), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2029), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1216), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2097), \"['b', 'y', 'a', 'x']\": np.float64(-1.3404), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2014), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0353), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1995), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0373), \"['b', 'y', 'a', 'y']\": np.float64(0.2384), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.109), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.215), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1174), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2064), \"['b', 'y', 'b', 'x']\": np.float64(-1.333), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2077), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0369), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2065), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0382), \"['b', 'y', 'b', 'y']\": np.float64(0.2464), \"['b', 'y']\": np.float64(1.5902)}\n",
      "It s train loss bro [1.938356876373291, 2.075530767440796, 2.0725209712982178, 2.064767360687256, 2.052847146987915, 2.037283420562744, 2.0185508728027344, 1.997077465057373, 1.9732489585876465, 1.947411060333252, 1.9198753833770752]\n",
      "% good predict : 60\n",
      "\u001b[0;34miteration 108 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1817251741886139 and val_loss is 2.6277456283569336\n",
      "for 1 epochs, loss is 0.22276219725608826 and val_loss is 2.6100494861602783\n",
      "for 2 epochs, loss is 0.27572089433670044 and val_loss is 2.5990426540374756\n",
      "for 3 epochs, loss is 0.3275512754917145 and val_loss is 2.5924816131591797\n",
      "for 4 epochs, loss is 0.3751225173473358 and val_loss is 2.603114366531372\n",
      "for 5 epochs, loss is 0.41592085361480713 and val_loss is 3.1151576042175293\n",
      "for 6 epochs, loss is 0.44823527336120605 and val_loss is 3.167696952819824\n",
      "for 7 epochs, loss is 0.47118157148361206 and val_loss is 3.182502508163452\n",
      "for 8 epochs, loss is 0.4846196174621582 and val_loss is 3.1910817623138428\n",
      "for 9 epochs, loss is 0.4890153408050537 and val_loss is 3.19762921333313\n",
      "for 10 epochs, loss is 0.4852803945541382 and val_loss is 3.20332932472229\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.4455), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.443), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.4148), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4742), \"['a', 'x', 'a', 'x']\": np.float64(-3.9146), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4443), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.8501), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4248), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.8699), \"['a', 'x', 'a', 'y']\": np.float64(2.3098), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.4037), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4362), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.393), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4472), \"['a', 'x', 'b', 'x']\": np.float64(-3.8655), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4704), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.8738), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4608), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.8835), \"['a', 'x', 'b', 'y']\": np.float64(2.3598), \"['a', 'x']\": np.float64(-6.2661), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4326), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.8453), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4146), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.8636), \"['a', 'y', 'a', 'x']\": np.float64(-2.2932), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.844), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.4968), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.8326), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.5083), \"['a', 'y', 'a', 'y']\": np.float64(1.3498), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4068), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.8406), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4006), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.847), \"['a', 'y', 'b', 'x']\": np.float64(-2.2624), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.8606), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.5114), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.855), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.5171), \"['a', 'y', 'b', 'y']\": np.float64(1.3812), \"['a', 'y']\": np.float64(3.6674), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.3904), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4105), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.3604), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.441), \"['b', 'x', 'a', 'x']\": np.float64(-3.8263), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4296), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.8414), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4103), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.861), \"['b', 'x', 'a', 'y']\": np.float64(2.2863), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.3688), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4154), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.3582), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4261), \"['b', 'x', 'b', 'x']\": np.float64(-3.8093), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4354), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.853), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4261), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.8625), \"['b', 'x', 'b', 'y']\": np.float64(2.3037), \"['b', 'x']\": np.float64(-6.1532), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4722), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.8687), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4538), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.8875), \"['b', 'y', 'a', 'x']\": np.float64(-2.3566), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.8757), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.5154), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.8639), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.5274), \"['b', 'y', 'a', 'y']\": np.float64(1.4004), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4559), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.8699), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4494), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.8766), \"['b', 'y', 'b', 'x']\": np.float64(-2.3413), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.8823), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.5243), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.8766), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.5301), \"['b', 'y', 'b', 'y']\": np.float64(1.416), \"['b', 'y']\": np.float64(3.7821)}\n",
      "It s train loss bro [0.1817251741886139, 0.22276219725608826, 0.27572089433670044, 0.3275512754917145, 0.3751225173473358, 0.41592085361480713, 0.44823527336120605, 0.47118157148361206, 0.4846196174621582, 0.4890153408050537, 0.4852803945541382]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 109 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.522682785987854 and val_loss is 3.2088887691497803\n",
      "for 1 epochs, loss is 0.45792248845100403 and val_loss is 3.214452028274536\n",
      "for 2 epochs, loss is 0.4371079206466675 and val_loss is 3.2202694416046143\n",
      "for 3 epochs, loss is 0.41347283124923706 and val_loss is 3.2265195846557617\n",
      "for 4 epochs, loss is 0.3881795108318329 and val_loss is 3.233325719833374\n",
      "for 5 epochs, loss is 0.36221247911453247 and val_loss is 3.240755796432495\n",
      "for 6 epochs, loss is 0.33636364340782166 and val_loss is 3.2488319873809814\n",
      "for 7 epochs, loss is 0.31123512983322144 and val_loss is 3.2575409412384033\n",
      "for 8 epochs, loss is 0.2872559726238251 and val_loss is 3.2668399810791016\n",
      "for 9 epochs, loss is 0.26470595598220825 and val_loss is 3.2766664028167725\n",
      "for 10 epochs, loss is 0.24374254047870636 and val_loss is 3.2869479656219482\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.1057), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2504), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0947), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2623), \"['a', 'x', 'a', 'x']\": np.float64(-6.3912), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2521), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3045), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2476), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3093), \"['a', 'x', 'a', 'y']\": np.float64(1.5653), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.0822), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2609), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.0889), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2541), \"['a', 'x', 'b', 'x']\": np.float64(-6.3775), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2631), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3083), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2608), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3108), \"['a', 'x', 'b', 'y']\": np.float64(1.5801), \"['a', 'x']\": np.float64(-8.0005), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2421), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3042), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2394), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3071), \"['a', 'y', 'a', 'x']\": np.float64(-1.5548), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3024), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0735), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3014), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0747), \"['a', 'y', 'a', 'y']\": np.float64(0.3781), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2346), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3063), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2362), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3047), \"['a', 'y', 'b', 'x']\": np.float64(-1.5493), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.307), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0749), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3064), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0755), \"['a', 'y', 'b', 'y']\": np.float64(0.384), \"['a', 'y']\": np.float64(1.9437), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.0701), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2417), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0592), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2535), \"['b', 'x', 'a', 'x']\": np.float64(-6.3468), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2596), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3063), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2551), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3111), \"['b', 'x', 'a', 'y']\": np.float64(1.5747), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.0644), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2565), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.071), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2497), \"['b', 'x', 'b', 'x']\": np.float64(-6.3551), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2519), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3056), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2496), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3081), \"['b', 'x', 'b', 'y']\": np.float64(1.5662), \"['b', 'x']\": np.float64(-7.9643), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.266), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3101), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2633), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.313), \"['b', 'y', 'a', 'x']\": np.float64(-1.5848), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3094), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0752), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3083), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0764), \"['b', 'y', 'a', 'y']\": np.float64(0.3868), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2606), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3128), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2623), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3111), \"['b', 'y', 'b', 'x']\": np.float64(-1.5819), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3117), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0761), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3112), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0767), \"['b', 'y', 'b', 'y']\": np.float64(0.39), \"['b', 'y']\": np.float64(1.9826)}\n",
      "It s train loss bro [0.522682785987854, 0.45792248845100403, 0.4371079206466675, 0.41347283124923706, 0.3881795108318329, 0.36221247911453247, 0.33636364340782166, 0.31123512983322144, 0.2872559726238251, 0.26470595598220825, 0.24374254047870636]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 110 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.27640199661254883 and val_loss is 3.29787015914917\n",
      "for 1 epochs, loss is 0.20656070113182068 and val_loss is 3.3090670108795166\n",
      "for 2 epochs, loss is 0.19033119082450867 and val_loss is 3.320463180541992\n",
      "for 3 epochs, loss is 0.17564339935779572 and val_loss is 3.3319880962371826\n",
      "for 4 epochs, loss is 0.16238348186016083 and val_loss is 3.3435771465301514\n",
      "for 5 epochs, loss is 0.15043115615844727 and val_loss is 3.355172872543335\n",
      "for 6 epochs, loss is 0.13966509699821472 and val_loss is 3.3667211532592773\n",
      "for 7 epochs, loss is 0.12996861338615417 and val_loss is 3.3781747817993164\n",
      "for 8 epochs, loss is 0.12123213708400726 and val_loss is 3.3894870281219482\n",
      "for 9 epochs, loss is 0.11335412412881851 and val_loss is 3.4006147384643555\n",
      "for 10 epochs, loss is 0.10624208301305771 and val_loss is 3.411513090133667\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.4023), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7479), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.4046), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7453), \"['a', 'x', 'a', 'x']\": np.float64(-8.183), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.748), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0751), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7481), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0753), \"['a', 'x', 'a', 'y']\": np.float64(0.8268), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.3821), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7716), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.4077), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7442), \"['a', 'x', 'b', 'x']\": np.float64(-8.1855), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7454), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0749), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7456), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0749), \"['a', 'x', 'b', 'y']\": np.float64(0.8239), \"['a', 'x']\": np.float64(-9.0461), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7454), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0753), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7457), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0751), \"['a', 'y', 'a', 'x']\": np.float64(-0.8241), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0749), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0075), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0749), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0075), \"['a', 'y', 'a', 'y']\": np.float64(0.0828), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7432), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0777), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7458), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0749), \"['a', 'y', 'b', 'x']\": np.float64(-0.8241), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0751), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0075), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0751), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0075), \"['a', 'y', 'b', 'y']\": np.float64(0.083), \"['a', 'y']\": np.float64(0.9108), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.38), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7456), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.3822), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.743), \"['b', 'x', 'a', 'x']\": np.float64(-8.1583), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7716), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0775), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7716), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0777), \"['b', 'x', 'a', 'y']\": np.float64(0.8528), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.3831), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7717), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.4088), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7443), \"['b', 'x', 'b', 'x']\": np.float64(-8.1867), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7441), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0748), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7443), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0748), \"['b', 'x', 'b', 'y']\": np.float64(0.8225), \"['b', 'x']\": np.float64(-9.0463), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7479), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0756), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7481), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0753), \"['b', 'y', 'a', 'x']\": np.float64(-0.8267), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0752), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0076), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0752), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0076), \"['b', 'y', 'a', 'y']\": np.float64(0.0831), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7457), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0779), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7483), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0752), \"['b', 'y', 'b', 'x']\": np.float64(-0.8269), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0752), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0076), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0752), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0076), \"['b', 'y', 'b', 'y']\": np.float64(0.0831), \"['b', 'y']\": np.float64(0.9137)}\n",
      "It s train loss bro [0.27640199661254883, 0.20656070113182068, 0.19033119082450867, 0.17564339935779572, 0.16238348186016083, 0.15043115615844727, 0.13966509699821472, 0.12996861338615417, 0.12123213708400726, 0.11335412412881851, 0.10624208301305771]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 111 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.09893009811639786 and val_loss is 3.4221136569976807\n",
      "for 1 epochs, loss is 0.09400682896375656 and val_loss is 3.4323983192443848\n",
      "for 2 epochs, loss is 0.08873875439167023 and val_loss is 3.4423110485076904\n",
      "for 3 epochs, loss is 0.08395029604434967 and val_loss is 3.451786994934082\n",
      "for 4 epochs, loss is 0.07958950102329254 and val_loss is 3.460745096206665\n",
      "for 5 epochs, loss is 0.07561056315898895 and val_loss is 3.469080686569214\n",
      "for 6 epochs, loss is 0.07197277247905731 and val_loss is 3.4766557216644287\n",
      "for 7 epochs, loss is 0.06864041090011597 and val_loss is 3.483283758163452\n",
      "for 8 epochs, loss is 0.06558163464069366 and val_loss is 3.4886932373046875\n",
      "for 9 epochs, loss is 0.06276865303516388 and val_loss is 3.4925012588500977\n",
      "for 10 epochs, loss is 0.060176294296979904 and val_loss is 3.494122266769409\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.383), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4829), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3949), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4695), \"['a', 'x', 'a', 'x']\": np.float64(-8.8927), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4833), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0272), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4836), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.027), \"['a', 'x', 'a', 'y']\": np.float64(0.5123), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.2807), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5914), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.4065), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4699), \"['a', 'x', 'b', 'x']\": np.float64(-8.9053), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4699), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0264), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4701), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0263), \"['a', 'x', 'b', 'y']\": np.float64(0.498), \"['a', 'x']\": np.float64(-9.4334), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4735), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0273), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4741), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0265), \"['a', 'y', 'a', 'x']\": np.float64(-0.5022), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0267), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0015), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0267), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0015), \"['a', 'y', 'a', 'y']\": np.float64(0.0283), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4673), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0334), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4744), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0265), \"['a', 'y', 'b', 'x']\": np.float64(-0.5025), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0265), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0015), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0265), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0015), \"['a', 'y', 'b', 'y']\": np.float64(0.0281), \"['a', 'y']\": np.float64(0.5323), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.2738), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4766), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.2855), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4633), \"['b', 'x', 'a', 'x']\": np.float64(-8.7769), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5914), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0333), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5918), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0331), \"['b', 'x', 'a', 'y']\": np.float64(0.6269), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.2853), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5918), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.4112), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4702), \"['b', 'x', 'b', 'x']\": np.float64(-8.9102), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.47), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0264), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4702), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0263), \"['b', 'x', 'b', 'y']\": np.float64(0.4981), \"['b', 'x']\": np.float64(-9.4389), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4712), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0271), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4718), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0264), \"['b', 'y', 'a', 'x']\": np.float64(-0.4998), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0265), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0015), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0265), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0015), \"['b', 'y', 'a', 'y']\": np.float64(0.0281), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.465), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0332), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.472), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0264), \"['b', 'y', 'b', 'x']\": np.float64(-0.5), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0264), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0015), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0264), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0015), \"['b', 'y', 'b', 'y']\": np.float64(0.0279), \"['b', 'y']\": np.float64(0.5297)}\n",
      "It s train loss bro [0.09893009811639786, 0.09400682896375656, 0.08873875439167023, 0.08395029604434967, 0.07958950102329254, 0.07561056315898895, 0.07197277247905731, 0.06864041090011597, 0.06558163464069366, 0.06276865303516388, 0.060176294296979904]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 112 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.06326191872358322 and val_loss is 3.4924283027648926\n",
      "for 1 epochs, loss is 0.0555601492524147 and val_loss is 3.485910415649414\n",
      "for 2 epochs, loss is 0.053500715643167496 and val_loss is 3.471860408782959\n",
      "for 3 epochs, loss is 0.05158897861838341 and val_loss is 3.445540189743042\n",
      "for 4 epochs, loss is 0.04981093853712082 and val_loss is 3.399038076400757\n",
      "for 5 epochs, loss is 0.048154134303331375 and val_loss is 3.3223602771759033\n",
      "for 6 epochs, loss is 0.04660779982805252 and val_loss is 3.2191975116729736\n",
      "for 7 epochs, loss is 0.04516185075044632 and val_loss is 3.1258392333984375\n",
      "for 8 epochs, loss is 0.0438074916601181 and val_loss is 3.065730333328247\n",
      "for 9 epochs, loss is 0.04253675416111946 and val_loss is 3.0498151779174805\n",
      "for 10 epochs, loss is 0.04134241119027138 and val_loss is 3.0673632621765137\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.7678), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.383), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.815), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3359), \"['a', 'x', 'a', 'x']\": np.float64(-9.1752), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3849), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0148), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3851), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0147), \"['a', 'x', 'a', 'y']\": np.float64(0.4008), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.2698), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(2.1091), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8621), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3376), \"['a', 'x', 'b', 'x']\": np.float64(-9.2246), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3375), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.013), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3377), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0129), \"['a', 'x', 'b', 'y']\": np.float64(0.3515), \"['a', 'x']\": np.float64(-9.6015), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3395), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0148), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3413), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.013), \"['a', 'y', 'a', 'x']\": np.float64(-0.3553), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0131), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0131), \"['a', 'y', 'a', 'y']\": np.float64(0.0136), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2416), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0813), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3415), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.013), \"['a', 'y', 'b', 'x']\": np.float64(-0.3555), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.013), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.013), \"['a', 'y', 'b', 'y']\": np.float64(0.0135), \"['a', 'y']\": np.float64(0.37), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.2394), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2726), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.273), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.239), \"['b', 'x', 'a', 'x']\": np.float64(-6.5293), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-2.1091), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.081), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-2.1102), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0804), \"['b', 'x', 'a', 'y']\": np.float64(2.1964), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.2728), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(2.1101), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8663), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3378), \"['b', 'x', 'b', 'x']\": np.float64(-9.2289), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3377), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.013), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3378), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0129), \"['b', 'x', 'b', 'y']\": np.float64(0.3516), \"['b', 'x']\": np.float64(-9.6064), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3373), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0147), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3391), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0129), \"['b', 'y', 'a', 'x']\": np.float64(-0.3529), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.013), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.013), \"['b', 'y', 'a', 'y']\": np.float64(0.0135), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.24), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0807), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3392), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0129), \"['b', 'y', 'b', 'x']\": np.float64(-0.3531), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0129), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0129), \"['b', 'y', 'b', 'y']\": np.float64(0.0134), \"['b', 'y']\": np.float64(0.3675)}\n",
      "It s train loss bro [0.06326191872358322, 0.0555601492524147, 0.053500715643167496, 0.05158897861838341, 0.04981093853712082, 0.048154134303331375, 0.04660779982805252, 0.04516185075044632, 0.0438074916601181, 0.04253675416111946, 0.04134241119027138]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 113 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.046712130308151245 and val_loss is 3.0903544425964355\n",
      "for 1 epochs, loss is 0.03915098309516907 and val_loss is 3.1095774173736572\n",
      "for 2 epochs, loss is 0.038143888115882874 and val_loss is 3.124907970428467\n",
      "for 3 epochs, loss is 0.03719206154346466 and val_loss is 3.1375486850738525\n",
      "for 4 epochs, loss is 0.036291178315877914 and val_loss is 3.148435354232788\n",
      "for 5 epochs, loss is 0.03543723002076149 and val_loss is 3.158170461654663\n",
      "for 6 epochs, loss is 0.034626659005880356 and val_loss is 3.167123556137085\n",
      "for 7 epochs, loss is 0.0338563397526741 and val_loss is 3.175518035888672\n",
      "for 8 epochs, loss is 0.03312302380800247 and val_loss is 3.1834940910339355\n",
      "for 9 epochs, loss is 0.03242425248026848 and val_loss is 3.191138982772827\n",
      "for 10 epochs, loss is 0.031757328659296036 and val_loss is 3.1985113620758057\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8477), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3827), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9768), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2614), \"['a', 'x', 'a', 'x']\": np.float64(-9.2594), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3881), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0114), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3883), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0113), \"['a', 'x', 'a', 'y']\": np.float64(0.4005), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-0.0959), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7503), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.1075), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2651), \"['a', 'x', 'b', 'x']\": np.float64(-9.3945), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2651), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0078), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2652), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0077), \"['a', 'x', 'b', 'y']\": np.float64(0.2735), \"['a', 'x']\": np.float64(-9.6902), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2642), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0114), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.268), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0078), \"['a', 'y', 'a', 'x']\": np.float64(-0.2764), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0079), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0079), \"['a', 'y', 'a', 'y']\": np.float64(0.0081), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0028), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0221), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2681), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0078), \"['a', 'y', 'b', 'x']\": np.float64(-0.2766), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0078), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0078), \"['a', 'y', 'b', 'y']\": np.float64(0.008), \"['a', 'y']\": np.float64(0.2853), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-0.0946), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0041), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-0.096), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0028), \"['b', 'x', 'a', 'x']\": np.float64(-0.099), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7503), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.022), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7506), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0218), \"['b', 'x', 'a', 'y']\": np.float64(0.7742), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-0.096), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7506), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.1108), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2652), \"['b', 'x', 'b', 'x']\": np.float64(-9.3978), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2651), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0078), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2652), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0077), \"['b', 'x', 'b', 'y']\": np.float64(0.2735), \"['b', 'x']\": np.float64(-9.6939), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2626), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0114), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2664), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0078), \"['b', 'y', 'a', 'x']\": np.float64(-0.2748), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0078), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0078), \"['b', 'y', 'a', 'y']\": np.float64(0.0081), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0028), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.022), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2665), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0078), \"['b', 'y', 'b', 'x']\": np.float64(-0.2749), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0078), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0078), \"['b', 'y', 'b', 'y']\": np.float64(0.008), \"['b', 'y']\": np.float64(0.2836)}\n",
      "It s train loss bro [0.046712130308151245, 0.03915098309516907, 0.038143888115882874, 0.03719206154346466, 0.036291178315877914, 0.03543723002076149, 0.034626659005880356, 0.0338563397526741, 0.03312302380800247, 0.03242425248026848, 0.031757328659296036]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 114 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.03624346852302551 and val_loss is 3.2057013511657715\n",
      "for 1 epochs, loss is 0.0305065605789423 and val_loss is 3.2126848697662354\n",
      "for 2 epochs, loss is 0.029919346794486046 and val_loss is 3.2194788455963135\n",
      "for 3 epochs, loss is 0.02935667708516121 and val_loss is 3.2261009216308594\n",
      "for 4 epochs, loss is 0.028817202895879745 and val_loss is 3.232562780380249\n",
      "for 5 epochs, loss is 0.028299223631620407 and val_loss is 3.2388756275177\n",
      "for 6 epochs, loss is 0.027801498770713806 and val_loss is 3.245048999786377\n",
      "for 7 epochs, loss is 0.027322780340909958 and val_loss is 3.251089334487915\n",
      "for 8 epochs, loss is 0.026861704885959625 and val_loss is 3.257007360458374\n",
      "for 9 epochs, loss is 0.026417484506964684 and val_loss is 3.2628049850463867\n",
      "for 10 epochs, loss is 0.025988979265093803 and val_loss is 3.268490791320801\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.0933), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6857), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6573), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2054), \"['a', 'x', 'a', 'x']\": np.float64(-8.8807), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7332), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0175), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7335), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0174), \"['a', 'x', 'a', 'y']\": np.float64(0.7524), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-0.0266), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4443), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.2604), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2196), \"['a', 'x', 'b', 'x']\": np.float64(-9.4995), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2196), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0052), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2197), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0052), \"['a', 'x', 'b', 'y']\": np.float64(0.2254), \"['a', 'x']\": np.float64(-9.7447), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2074), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0176), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2218), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0053), \"['a', 'y', 'a', 'x']\": np.float64(-0.2276), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0053), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0053), \"['a', 'y', 'a', 'y']\": np.float64(0.0054), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0106), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2219), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0053), \"['a', 'y', 'b', 'x']\": np.float64(-0.2277), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0053), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0053), \"['a', 'y', 'b', 'y']\": np.float64(0.0054), \"['a', 'y']\": np.float64(0.2335), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-0.0249), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0021), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-0.0266), \"['b', 'x', 'a', 'x']\": np.float64(-0.0273), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4443), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0106), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4444), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0105), \"['b', 'x', 'a', 'y']\": np.float64(0.4559), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-0.0266), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4444), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.2628), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2196), \"['b', 'x', 'b', 'x']\": np.float64(-9.5021), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2196), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0052), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2196), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0052), \"['b', 'x', 'b', 'y']\": np.float64(0.2253), \"['b', 'x']\": np.float64(-9.7475), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2065), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0175), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2209), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0052), \"['b', 'y', 'a', 'x']\": np.float64(-0.2266), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0053), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0053), \"['b', 'y', 'a', 'y']\": np.float64(0.0054), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0106), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2209), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0052), \"['b', 'y', 'b', 'x']\": np.float64(-0.2267), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0052), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0052), \"['b', 'y', 'b', 'y']\": np.float64(0.0054), \"['b', 'y']\": np.float64(0.2325)}\n",
      "It s train loss bro [0.03624346852302551, 0.0305065605789423, 0.029919346794486046, 0.02935667708516121, 0.028817202895879745, 0.028299223631620407, 0.027801498770713806, 0.027322780340909958, 0.026861704885959625, 0.026417484506964684, 0.025988979265093803]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 115 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 3.648265838623047 and val_loss is 3.267439603805542\n",
      "for 1 epochs, loss is 3.755986452102661 and val_loss is 3.26491117477417\n",
      "for 2 epochs, loss is 3.747450113296509 and val_loss is 3.2610504627227783\n",
      "for 3 epochs, loss is 3.73459529876709 and val_loss is 3.255971908569336\n",
      "for 4 epochs, loss is 3.7179372310638428 and val_loss is 3.2497546672821045\n",
      "for 5 epochs, loss is 3.697939395904541 and val_loss is 3.242424249649048\n",
      "for 6 epochs, loss is 3.675018310546875 and val_loss is 3.23390531539917\n",
      "for 7 epochs, loss is 3.6495473384857178 and val_loss is 3.2238335609436035\n",
      "for 8 epochs, loss is 3.6218605041503906 and val_loss is 3.2109692096710205\n",
      "for 9 epochs, loss is 3.592256784439087 and val_loss is 3.191315174102783\n",
      "for 10 epochs, loss is 3.5610034465789795 and val_loss is 3.1550180912017822\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8326), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3861), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9583), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2681), \"['a', 'x', 'a', 'x']\": np.float64(-9.247), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3914), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0119), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3916), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0117), \"['a', 'x', 'a', 'y']\": np.float64(0.4042), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-0.1069), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7504), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.0853), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2721), \"['a', 'x', 'b', 'x']\": np.float64(-9.3786), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2718), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0082), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2719), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0081), \"['a', 'x', 'b', 'y']\": np.float64(0.2807), \"['a', 'x']\": np.float64(-9.6808), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2729), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0119), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2768), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0083), \"['a', 'y', 'a', 'x']\": np.float64(-0.2857), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0084), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0084), \"['a', 'y', 'a', 'y']\": np.float64(0.0087), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0033), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0229), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2769), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0083), \"['a', 'y', 'b', 'x']\": np.float64(-0.2859), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0083), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0083), \"['a', 'y', 'b', 'y']\": np.float64(0.0085), \"['a', 'y']\": np.float64(0.2951), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-0.1054), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0046), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-0.1069), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0032), \"['b', 'x', 'a', 'x']\": np.float64(-0.1104), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7505), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0227), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7509), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0225), \"['b', 'x', 'a', 'y']\": np.float64(0.7751), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-0.1069), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7508), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.0907), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2722), \"['b', 'x', 'b', 'x']\": np.float64(-9.3842), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2721), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0082), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2722), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0081), \"['b', 'x', 'b', 'y']\": np.float64(0.281), \"['b', 'x']\": np.float64(-9.6871), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2692), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0118), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.273), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0082), \"['b', 'y', 'a', 'x']\": np.float64(-0.2818), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0082), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0083), \"['b', 'y', 'a', 'y']\": np.float64(0.0085), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0032), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0226), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2731), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0082), \"['b', 'y', 'b', 'x']\": np.float64(-0.2819), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0082), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0082), \"['b', 'y', 'b', 'y']\": np.float64(0.0084), \"['b', 'y']\": np.float64(0.291)}\n",
      "It s train loss bro [3.648265838623047, 3.755986452102661, 3.747450113296509, 3.73459529876709, 3.7179372310638428, 3.697939395904541, 3.675018310546875, 3.6495473384857178, 3.6218605041503906, 3.592256784439087, 3.5610034465789795]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 116 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 3.411911964416504 and val_loss is 3.1382877826690674\n",
      "for 1 epochs, loss is 3.5047855377197266 and val_loss is 3.2835538387298584\n",
      "for 2 epochs, loss is 3.470445156097412 and val_loss is 3.477863073348999\n",
      "for 3 epochs, loss is 3.435253620147705 and val_loss is 3.569476842880249\n",
      "for 4 epochs, loss is 3.3993613719940186 and val_loss is 3.60530948638916\n",
      "for 5 epochs, loss is 3.3629021644592285 and val_loss is 3.6194863319396973\n",
      "for 6 epochs, loss is 3.3259952068328857 and val_loss is 3.6241447925567627\n",
      "for 7 epochs, loss is 3.2887461185455322 and val_loss is 3.6240274906158447\n",
      "for 8 epochs, loss is 3.2512474060058594 and val_loss is 3.6212680339813232\n",
      "for 9 epochs, loss is 3.213582992553711 and val_loss is 3.616943359375\n",
      "for 10 epochs, loss is 3.1758248805999756 and val_loss is 3.6116437911987305\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6259), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4185), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6523), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3912), \"['a', 'x', 'a', 'x']\": np.float64(-9.0671), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4193), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0193), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4198), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.019), \"['a', 'x', 'a', 'y']\": np.float64(0.4399), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.1184), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8812), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.6778), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3929), \"['a', 'x', 'b', 'x']\": np.float64(-9.0949), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3921), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.018), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3923), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0177), \"['a', 'x', 'b', 'y']\": np.float64(0.4112), \"['a', 'x']\": np.float64(-9.5309), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3996), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0194), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4009), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0181), \"['a', 'y', 'a', 'x']\": np.float64(-0.4201), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0184), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0184), \"['a', 'y', 'a', 'y']\": np.float64(0.0193), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3754), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0407), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4012), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0182), \"['a', 'y', 'b', 'x']\": np.float64(-0.4205), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0181), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0181), \"['a', 'y', 'b', 'y']\": np.float64(0.019), \"['a', 'y']\": np.float64(0.4406), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.1031), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3931), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.1279), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3675), \"['b', 'x', 'a', 'x']\": np.float64(-8.5176), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8814), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0405), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8823), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0398), \"['b', 'x', 'a', 'y']\": np.float64(0.9245), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.127), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8821), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.687), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3934), \"['b', 'x', 'b', 'x']\": np.float64(-9.1045), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3931), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.018), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3934), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0178), \"['b', 'x', 'b', 'y']\": np.float64(0.4123), \"['b', 'x']\": np.float64(-9.5421), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.392), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.019), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3932), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0178), \"['b', 'y', 'a', 'x']\": np.float64(-0.4121), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.018), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.018), \"['b', 'y', 'a', 'y']\": np.float64(0.0189), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3681), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.04), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3935), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0178), \"['b', 'y', 'b', 'x']\": np.float64(-0.4124), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0178), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0178), \"['b', 'y', 'b', 'y']\": np.float64(0.0187), \"['b', 'y']\": np.float64(0.4322)}\n",
      "It s train loss bro [3.411911964416504, 3.5047855377197266, 3.470445156097412, 3.435253620147705, 3.3993613719940186, 3.3629021644592285, 3.3259952068328857, 3.2887461185455322, 3.2512474060058594, 3.213582992553711, 3.1758248805999756]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 117 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.05116625502705574 and val_loss is 3.5521438121795654\n",
      "for 1 epochs, loss is 0.06550571322441101 and val_loss is 3.4977293014526367\n",
      "for 2 epochs, loss is 0.08679840713739395 and val_loss is 3.451910972595215\n",
      "for 3 epochs, loss is 0.11036717146635056 and val_loss is 3.4139623641967773\n",
      "for 4 epochs, loss is 0.13530175387859344 and val_loss is 3.382887840270996\n",
      "for 5 epochs, loss is 0.1606227457523346 and val_loss is 3.357722043991089\n",
      "for 6 epochs, loss is 0.18538515269756317 and val_loss is 3.3375818729400635\n",
      "for 7 epochs, loss is 0.20875439047813416 and val_loss is 3.3216848373413086\n",
      "for 8 epochs, loss is 0.23005245625972748 and val_loss is 3.3093550205230713\n",
      "for 9 epochs, loss is 0.24877525866031647 and val_loss is 3.3000173568725586\n",
      "for 10 epochs, loss is 0.2645905911922455 and val_loss is 3.2931931018829346\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3457), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3584), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.3514), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3544), \"['a', 'x', 'a', 'x']\": np.float64(-5.7353), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3572), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4255), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3604), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4233), \"['a', 'x', 'a', 'y']\": np.float64(1.7928), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.3555), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3576), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3561), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3567), \"['a', 'x', 'b', 'x']\": np.float64(-5.7429), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3537), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4239), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.356), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4222), \"['a', 'x', 'b', 'y']\": np.float64(1.7875), \"['a', 'x']\": np.float64(-7.5692), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3616), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4256), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3634), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4244), \"['a', 'y', 'a', 'x']\": np.float64(-1.797), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4265), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1337), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4275), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.133), \"['a', 'y', 'a', 'y']\": np.float64(0.5634), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3661), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4258), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3663), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4255), \"['a', 'y', 'b', 'x']\": np.float64(-1.8012), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4244), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1329), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4251), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1324), \"['a', 'y', 'b', 'y']\": np.float64(0.5604), \"['a', 'y']\": np.float64(2.3737), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3634), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.364), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.3692), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3599), \"['b', 'x', 'a', 'x']\": np.float64(-5.7587), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3588), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.426), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.362), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4238), \"['b', 'x', 'a', 'y']\": np.float64(1.795), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.3682), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3615), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3688), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3607), \"['b', 'x', 'b', 'x']\": np.float64(-5.7596), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3585), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4254), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3608), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4237), \"['b', 'x', 'b', 'y']\": np.float64(1.7938), \"['b', 'x']\": np.float64(-7.5931), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.352), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4226), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3538), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4214), \"['b', 'y', 'a', 'x']\": np.float64(-1.7844), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.423), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1326), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.424), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1319), \"['b', 'y', 'a', 'y']\": np.float64(0.5588), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3556), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4225), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3558), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4223), \"['b', 'y', 'b', 'x']\": np.float64(-1.7874), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4215), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.132), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4222), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1315), \"['b', 'y', 'b', 'y']\": np.float64(0.5565), \"['b', 'y']\": np.float64(2.3562)}\n",
      "It s train loss bro [0.05116625502705574, 0.06550571322441101, 0.08679840713739395, 0.11036717146635056, 0.13530175387859344, 0.1606227457523346, 0.18538515269756317, 0.20875439047813416, 0.23005245625972748, 0.24877525866031647, 0.2645905911922455]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 118 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.3121960163116455 and val_loss is 3.2923457622528076\n",
      "for 1 epochs, loss is 1.4316071271896362 and val_loss is 3.2913126945495605\n",
      "for 2 epochs, loss is 1.4249677658081055 and val_loss is 3.2901201248168945\n",
      "for 3 epochs, loss is 1.4171178340911865 and val_loss is 3.288790702819824\n",
      "for 4 epochs, loss is 1.4081975221633911 and val_loss is 3.2873458862304688\n",
      "for 5 epochs, loss is 1.3983343839645386 and val_loss is 3.285804033279419\n",
      "for 6 epochs, loss is 1.3876433372497559 and val_loss is 3.2841849327087402\n",
      "for 7 epochs, loss is 1.3762296438217163 and val_loss is 3.282503366470337\n",
      "for 8 epochs, loss is 1.3641862869262695 and val_loss is 3.2807726860046387\n",
      "for 9 epochs, loss is 1.3515992164611816 and val_loss is 3.2790071964263916\n",
      "for 10 epochs, loss is 1.3385448455810547 and val_loss is 3.27721905708313\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.8763), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4082), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.8834), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4029), \"['a', 'x', 'a', 'x']\": np.float64(-5.3146), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4071), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5124), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.411), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.5095), \"['a', 'x', 'a', 'y']\": np.float64(1.9308), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.8873), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4082), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.8894), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4062), \"['a', 'x', 'b', 'x']\": np.float64(-5.3245), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4024), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.5101), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4052), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.5079), \"['a', 'x', 'b', 'y']\": np.float64(1.9235), \"['a', 'x']\": np.float64(-7.2868), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4107), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.5125), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4133), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5106), \"['a', 'y', 'a', 'x']\": np.float64(-1.9342), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.5133), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1869), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.5147), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1859), \"['a', 'y', 'a', 'y']\": np.float64(0.7043), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.416), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.513), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4168), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.5122), \"['a', 'y', 'b', 'x']\": np.float64(-1.9395), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5107), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1857), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.5117), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1849), \"['a', 'y', 'b', 'y']\": np.float64(0.7004), \"['a', 'y']\": np.float64(2.654), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.8949), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.415), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.9021), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4097), \"['b', 'x', 'a', 'x']\": np.float64(-5.3402), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4098), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.5134), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4137), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.5105), \"['b', 'x', 'a', 'y']\": np.float64(1.9345), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.9009), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4131), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.9031), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4112), \"['b', 'x', 'b', 'x']\": np.float64(-5.3431), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4085), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.5123), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4113), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.5101), \"['b', 'x', 'b', 'y']\": np.float64(1.9318), \"['b', 'x']\": np.float64(-7.3145), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4002), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.5087), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4028), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.5068), \"['b', 'y', 'a', 'x']\": np.float64(-1.9197), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.5088), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1853), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.5102), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1843), \"['b', 'y', 'a', 'y']\": np.float64(0.6982), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4043), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.5087), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4051), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.508), \"['b', 'y', 'b', 'x']\": np.float64(-1.9235), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.5069), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1844), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.5079), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1836), \"['b', 'y', 'b', 'y']\": np.float64(0.6953), \"['b', 'y']\": np.float64(2.633)}\n",
      "It s train loss bro [1.3121960163116455, 1.4316071271896362, 1.4249677658081055, 1.4171178340911865, 1.4081975221633911, 1.3983343839645386, 1.3876433372497559, 1.3762296438217163, 1.3641862869262695, 1.3515992164611816, 1.3385448455810547]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 119 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3233914375305176 and val_loss is 3.263735055923462\n",
      "for 1 epochs, loss is 0.3616747558116913 and val_loss is 3.2547342777252197\n",
      "for 2 epochs, loss is 0.4029010534286499 and val_loss is 3.2492082118988037\n",
      "for 3 epochs, loss is 0.4393172264099121 and val_loss is 3.246300220489502\n",
      "for 4 epochs, loss is 0.4698232412338257 and val_loss is 3.245316982269287\n",
      "for 5 epochs, loss is 0.49372348189353943 and val_loss is 3.245730400085449\n",
      "for 6 epochs, loss is 0.5107178092002869 and val_loss is 3.2471582889556885\n",
      "for 7 epochs, loss is 0.5208597183227539 and val_loss is 3.249345302581787\n",
      "for 8 epochs, loss is 0.5244961977005005 and val_loss is 3.2521376609802246\n",
      "for 9 epochs, loss is 0.5221979022026062 and val_loss is 3.2554590702056885\n",
      "for 10 epochs, loss is 0.5146880745887756 and val_loss is 3.259289503097534\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.222), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4243), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.23), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4181), \"['a', 'x', 'a', 'x']\": np.float64(-3.6686), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4242), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.9131), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4296), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.9089), \"['a', 'x', 'a', 'y']\": np.float64(2.3516), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.2317), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4284), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.2369), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.424), \"['a', 'x', 'b', 'x']\": np.float64(-3.6818), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4189), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.9085), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4226), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.9054), \"['a', 'x', 'b', 'y']\": np.float64(2.3413), \"['a', 'x']\": np.float64(-6.057), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4235), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.9125), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4286), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.9085), \"['a', 'y', 'a', 'x']\": np.float64(-2.3502), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.9125), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.5851), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.916), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.5824), \"['a', 'y', 'a', 'y']\": np.float64(1.5068), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.43), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.9152), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4333), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.9124), \"['a', 'y', 'b', 'x']\": np.float64(-2.3591), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.909), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.582), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.9113), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.58), \"['a', 'y', 'b', 'y']\": np.float64(1.4999), \"['a', 'y']\": np.float64(3.8806), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.2363), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4335), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.2443), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4272), \"['b', 'x', 'a', 'x']\": np.float64(-3.6921), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4311), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.9175), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4365), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.9133), \"['b', 'x', 'a', 'y']\": np.float64(2.363), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.2432), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4357), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.2484), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4313), \"['b', 'x', 'b', 'x']\": np.float64(-3.7007), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4276), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.9141), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4314), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.911), \"['b', 'x', 'b', 'y']\": np.float64(2.3558), \"['b', 'x']\": np.float64(-6.091), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4145), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.9067), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4196), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.9028), \"['b', 'y', 'a', 'x']\": np.float64(-2.3354), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.9056), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.5806), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.909), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.5779), \"['b', 'y', 'a', 'y']\": np.float64(1.4953), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4193), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.9084), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4226), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.9056), \"['b', 'y', 'b', 'x']\": np.float64(-2.3415), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.9032), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.5783), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.9055), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.5763), \"['b', 'y', 'b', 'y']\": np.float64(1.4903), \"['b', 'y']\": np.float64(3.8537)}\n",
      "It s train loss bro [0.3233914375305176, 0.3616747558116913, 0.4029010534286499, 0.4393172264099121, 0.4698232412338257, 0.49372348189353943, 0.5107178092002869, 0.5208597183227539, 0.5244961977005005, 0.5221979022026062, 0.5146880745887756]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 120 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.5003673434257507 and val_loss is 3.263735055923462\n",
      "for 1 epochs, loss is 0.4873691201210022 and val_loss is 3.2687249183654785\n",
      "for 2 epochs, loss is 0.46920663118362427 and val_loss is 3.274299383163452\n",
      "for 3 epochs, loss is 0.449057012796402 and val_loss is 3.2804932594299316\n",
      "for 4 epochs, loss is 0.42761191725730896 and val_loss is 3.28733491897583\n",
      "for 5 epochs, loss is 0.4054716229438782 and val_loss is 3.2948386669158936\n",
      "for 6 epochs, loss is 0.3831396996974945 and val_loss is 3.303004503250122\n",
      "for 7 epochs, loss is 0.3610239028930664 and val_loss is 3.3118202686309814\n",
      "for 8 epochs, loss is 0.3394426703453064 and val_loss is 3.321256637573242\n",
      "for 9 epochs, loss is 0.31863316893577576 and val_loss is 3.3312788009643555\n",
      "for 10 epochs, loss is 0.29876282811164856 and val_loss is 3.341839075088501\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3202), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.368), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.3178), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.372), \"['a', 'x', 'a', 'x']\": np.float64(-5.7163), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3683), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4327), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3672), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4346), \"['a', 'x', 'a', 'y']\": np.float64(1.8101), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.3154), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3707), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3167), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3695), \"['a', 'x', 'b', 'x']\": np.float64(-5.7131), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.372), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4343), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3716), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4352), \"['a', 'x', 'b', 'y']\": np.float64(1.8153), \"['a', 'x']\": np.float64(-7.5634), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3628), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4315), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.362), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4328), \"['a', 'y', 'a', 'x']\": np.float64(-1.8031), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.431), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1363), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4306), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1369), \"['a', 'y', 'a', 'y']\": np.float64(0.5702), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3609), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4322), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3613), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4319), \"['a', 'y', 'b', 'x']\": np.float64(-1.8016), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4328), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.137), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4327), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1373), \"['a', 'y', 'b', 'y']\": np.float64(0.5727), \"['a', 'y']\": np.float64(2.3853), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3144), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3662), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.312), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3701), \"['b', 'x', 'a', 'x']\": np.float64(-5.7086), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3707), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4334), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3695), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4353), \"['b', 'x', 'a', 'y']\": np.float64(1.8132), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.3133), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.37), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3146), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3688), \"['b', 'x', 'b', 'x']\": np.float64(-5.7104), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3692), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4334), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3688), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4344), \"['b', 'x', 'b', 'y']\": np.float64(1.8116), \"['b', 'x']\": np.float64(-7.5576), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3689), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4335), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3682), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4347), \"['b', 'y', 'a', 'x']\": np.float64(-1.8113), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4334), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1371), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4331), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1377), \"['b', 'y', 'a', 'y']\": np.float64(0.5734), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3677), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4344), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3681), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.434), \"['b', 'y', 'b', 'x']\": np.float64(-1.8107), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4343), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1375), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4341), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1378), \"['b', 'y', 'b', 'y']\": np.float64(0.5746), \"['b', 'y']\": np.float64(2.3965)}\n",
      "It s train loss bro [0.5003673434257507, 0.4873691201210022, 0.46920663118362427, 0.449057012796402, 0.42761191725730896, 0.4054716229438782, 0.3831396996974945, 0.3610239028930664, 0.3394426703453064, 0.31863316893577576, 0.29876282811164856]\n",
      "% good predict : 60\n",
      "\u001b[0;34miteration 121 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.3517115116119385 and val_loss is 3.3426191806793213\n",
      "for 1 epochs, loss is 1.4332034587860107 and val_loss is 3.343104362487793\n",
      "for 2 epochs, loss is 1.4345614910125732 and val_loss is 3.343323230743408\n",
      "for 3 epochs, loss is 1.434004306793213 and val_loss is 3.343304395675659\n",
      "for 4 epochs, loss is 1.4317315816879272 and val_loss is 3.3430750370025635\n",
      "for 5 epochs, loss is 1.4279260635375977 and val_loss is 3.3426616191864014\n",
      "for 6 epochs, loss is 1.4227551221847534 and val_loss is 3.342087745666504\n",
      "for 7 epochs, loss is 1.4163708686828613 and val_loss is 3.3413753509521484\n",
      "for 8 epochs, loss is 1.4089117050170898 and val_loss is 3.340546131134033\n",
      "for 9 epochs, loss is 1.4005038738250732 and val_loss is 3.3396189212799072\n",
      "for 10 epochs, loss is 1.3912605047225952 and val_loss is 3.338611364364624\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.1201), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3911), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.1169), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3958), \"['a', 'x', 'a', 'x']\": np.float64(-5.5386), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3914), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4691), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3899), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4714), \"['a', 'x', 'a', 'y']\": np.float64(1.87), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.1143), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3936), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.1152), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3928), \"['a', 'x', 'b', 'x']\": np.float64(-5.5344), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3958), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4712), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3951), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4723), \"['a', 'x', 'b', 'y']\": np.float64(1.8763), \"['a', 'x']\": np.float64(-7.4455), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3853), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4677), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3842), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4693), \"['a', 'y', 'a', 'x']\": np.float64(-1.8622), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4671), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1575), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4666), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1583), \"['a', 'y', 'a', 'y']\": np.float64(0.6278), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3829), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4684), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3832), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4681), \"['a', 'y', 'b', 'x']\": np.float64(-1.8602), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4693), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1584), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4691), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1588), \"['a', 'y', 'b', 'y']\": np.float64(0.6309), \"['a', 'y']\": np.float64(2.5027), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.1127), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3886), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.1095), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3933), \"['b', 'x', 'a', 'x']\": np.float64(-5.5287), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3934), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4697), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3919), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4721), \"['b', 'x', 'a', 'y']\": np.float64(1.8727), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.111), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3925), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.112), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3917), \"['b', 'x', 'b', 'x']\": np.float64(-5.53), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3923), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.47), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3917), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4711), \"['b', 'x', 'b', 'y']\": np.float64(1.8717), \"['b', 'x']\": np.float64(-7.437), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3927), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4702), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3916), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4718), \"['b', 'y', 'a', 'x']\": np.float64(-1.8721), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4702), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1585), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4697), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1593), \"['b', 'y', 'a', 'y']\": np.float64(0.632), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3911), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4712), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3914), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4709), \"['b', 'y', 'b', 'x']\": np.float64(-1.8712), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4712), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1591), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.471), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1595), \"['b', 'y', 'b', 'y']\": np.float64(0.6335), \"['b', 'y']\": np.float64(2.5166)}\n",
      "It s train loss bro [1.3517115116119385, 1.4332034587860107, 1.4345614910125732, 1.434004306793213, 1.4317315816879272, 1.4279260635375977, 1.4227551221847534, 1.4163708686828613, 1.4089117050170898, 1.4005038738250732, 1.3912605047225952]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 122 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3716931641101837 and val_loss is 3.3309073448181152\n",
      "for 1 epochs, loss is 0.32542312145233154 and val_loss is 3.3258585929870605\n",
      "for 2 epochs, loss is 0.35152962803840637 and val_loss is 3.322859764099121\n",
      "for 3 epochs, loss is 0.3737235367298126 and val_loss is 3.321425199508667\n",
      "for 4 epochs, loss is 0.39164137840270996 and val_loss is 3.3211779594421387\n",
      "for 5 epochs, loss is 0.4051225185394287 and val_loss is 3.321843147277832\n",
      "for 6 epochs, loss is 0.41418787837028503 and val_loss is 3.323232889175415\n",
      "for 7 epochs, loss is 0.41900914907455444 and val_loss is 3.3252246379852295\n",
      "for 8 epochs, loss is 0.4198772609233856 and val_loss is 3.32775616645813\n",
      "for 9 epochs, loss is 0.4171697795391083 and val_loss is 3.3308043479919434\n",
      "for 10 epochs, loss is 0.41132089495658875 and val_loss is 3.334373712539673\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.9945), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4602), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.9904), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4657), \"['a', 'x', 'a', 'x']\": np.float64(-4.4776), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4605), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7116), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4581), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7149), \"['a', 'x', 'a', 'y']\": np.float64(2.1834), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.9883), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4608), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.9875), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4621), \"['a', 'x', 'b', 'x']\": np.float64(-4.4715), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4654), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.7151), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4642), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7167), \"['a', 'x', 'b', 'y']\": np.float64(2.1917), \"['a', 'x']\": np.float64(-6.6954), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4546), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7093), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4526), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.712), \"['a', 'y', 'a', 'x']\": np.float64(-2.175), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.7089), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3454), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7077), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.347), \"['a', 'y', 'a', 'y']\": np.float64(1.0597), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4512), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7094), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4508), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.71), \"['a', 'y', 'b', 'x']\": np.float64(-2.1714), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7118), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3474), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.7113), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3482), \"['a', 'y', 'b', 'y']\": np.float64(1.0646), \"['a', 'y']\": np.float64(3.2516), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.9867), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4564), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.9825), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4619), \"['b', 'x', 'a', 'x']\": np.float64(-4.4659), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4602), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7115), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4578), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7147), \"['b', 'x', 'a', 'y']\": np.float64(2.183), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.9838), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4586), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.983), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4599), \"['b', 'x', 'b', 'x']\": np.float64(-4.4647), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4609), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.713), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4598), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7146), \"['b', 'x', 'b', 'y']\": np.float64(2.185), \"['b', 'x']\": np.float64(-6.6824), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.462), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7129), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.46), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7156), \"['b', 'y', 'a', 'x']\": np.float64(-2.1862), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.7137), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3477), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7125), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3493), \"['b', 'y', 'a', 'y']\": np.float64(1.0669), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4599), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7136), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4595), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7143), \"['b', 'y', 'b', 'x']\": np.float64(-2.1845), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7149), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3489), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.7144), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3497), \"['b', 'y', 'b', 'y']\": np.float64(1.0693), \"['b', 'y']\": np.float64(3.2697)}\n",
      "It s train loss bro [0.3716931641101837, 0.32542312145233154, 0.35152962803840637, 0.3737235367298126, 0.39164137840270996, 0.4051225185394287, 0.41418787837028503, 0.41900914907455444, 0.4198772609233856, 0.4171697795391083, 0.41132089495658875]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 123 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.4157918691635132 and val_loss is 3.338531494140625\n",
      "for 1 epochs, loss is 0.3921545445919037 and val_loss is 3.343252420425415\n",
      "for 2 epochs, loss is 0.37975040078163147 and val_loss is 3.348564386367798\n",
      "for 3 epochs, loss is 0.36601361632347107 and val_loss is 3.354494094848633\n",
      "for 4 epochs, loss is 0.351338267326355 and val_loss is 3.3610565662384033\n",
      "for 5 epochs, loss is 0.33607593178749084 and val_loss is 3.3682591915130615\n",
      "for 6 epochs, loss is 0.3205307424068451 and val_loss is 3.376098394393921\n",
      "for 7 epochs, loss is 0.3049597144126892 and val_loss is 3.3845579624176025\n",
      "for 8 epochs, loss is 0.2895735204219818 and val_loss is 3.3936121463775635\n",
      "for 9 epochs, loss is 0.2745397388935089 and val_loss is 3.4032278060913086\n",
      "for 10 epochs, loss is 0.2599872052669525 and val_loss is 3.413363456726074\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.7836), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3073), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7829), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3092), \"['a', 'x', 'a', 'x']\": np.float64(-6.1167), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3071), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3572), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3071), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3578), \"['a', 'x', 'a', 'y']\": np.float64(1.6716), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.7824), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3086), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7826), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3082), \"['a', 'x', 'b', 'x']\": np.float64(-6.1158), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3091), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3578), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3091), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3581), \"['a', 'x', 'b', 'y']\": np.float64(1.674), \"['a', 'x']\": np.float64(-7.8213), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3052), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3567), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.305), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3572), \"['a', 'y', 'a', 'x']\": np.float64(-1.669), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3566), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0975), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3566), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0976), \"['a', 'y', 'a', 'y']\": np.float64(0.4561), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.305), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3571), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3051), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.357), \"['a', 'y', 'b', 'x']\": np.float64(-1.6689), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3573), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0977), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3573), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0977), \"['a', 'y', 'b', 'y']\": np.float64(0.4569), \"['a', 'y']\": np.float64(2.1343), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.7835), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3073), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7828), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3092), \"['b', 'x', 'a', 'x']\": np.float64(-6.1166), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3088), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3576), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3087), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3583), \"['b', 'x', 'a', 'y']\": np.float64(1.6737), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.7832), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3088), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7835), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3085), \"['b', 'x', 'b', 'x']\": np.float64(-6.1169), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3084), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3576), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3085), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.358), \"['b', 'x', 'b', 'y']\": np.float64(1.6732), \"['b', 'x']\": np.float64(-7.8221), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3075), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3573), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3073), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3578), \"['b', 'y', 'a', 'x']\": np.float64(-1.6719), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3573), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0976), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3573), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0978), \"['b', 'y', 'a', 'y']\": np.float64(0.457), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3074), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3577), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3075), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3576), \"['b', 'y', 'b', 'x']\": np.float64(-1.6719), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3577), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0978), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3577), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0978), \"['b', 'y', 'b', 'y']\": np.float64(0.4574), \"['b', 'y']\": np.float64(2.138)}\n",
      "It s train loss bro [0.4157918691635132, 0.3921545445919037, 0.37975040078163147, 0.36601361632347107, 0.351338267326355, 0.33607593178749084, 0.3205307424068451, 0.3049597144126892, 0.2895735204219818, 0.2745397388935089, 0.2599872052669525]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 124 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.25075238943099976 and val_loss is 3.4239566326141357\n",
      "for 1 epochs, loss is 0.23274193704128265 and val_loss is 3.4349730014801025\n",
      "for 2 epochs, loss is 0.22013945877552032 and val_loss is 3.4463577270507812\n",
      "for 3 epochs, loss is 0.2082248330116272 and val_loss is 3.4580585956573486\n",
      "for 4 epochs, loss is 0.19700397551059723 and val_loss is 3.4700229167938232\n",
      "for 5 epochs, loss is 0.18646961450576782 and val_loss is 3.482198715209961\n",
      "for 6 epochs, loss is 0.17660489678382874 and val_loss is 3.4945366382598877\n",
      "for 7 epochs, loss is 0.16738635301589966 and val_loss is 3.5069894790649414\n",
      "for 8 epochs, loss is 0.15878526866436005 and val_loss is 3.5195140838623047\n",
      "for 9 epochs, loss is 0.15076944231987 and val_loss is 3.532069206237793\n",
      "for 10 epochs, loss is 0.14330589771270752 and val_loss is 3.544619560241699\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.6466), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9443), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.646), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9453), \"['a', 'x', 'a', 'x']\": np.float64(-7.6142), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9437), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1346), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9443), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1343), \"['a', 'x', 'a', 'y']\": np.float64(1.0818), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.6464), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9451), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.645), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9453), \"['a', 'x', 'b', 'x']\": np.float64(-7.6136), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9448), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1346), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9452), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1344), \"['a', 'x', 'b', 'y']\": np.float64(1.0829), \"['a', 'x']\": np.float64(-8.7228), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9472), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1346), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9471), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1347), \"['a', 'y', 'a', 'x']\": np.float64(-1.0851), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.135), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0193), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1351), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0192), \"['a', 'y', 'a', 'y']\": np.float64(0.1547), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9479), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1348), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9477), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1348), \"['a', 'y', 'b', 'x']\": np.float64(-1.0858), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1347), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0192), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1348), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0192), \"['a', 'y', 'b', 'y']\": np.float64(0.1544), \"['a', 'y']\": np.float64(1.2439), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.6523), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9452), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.6518), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9461), \"['b', 'x', 'a', 'x']\": np.float64(-7.6209), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9453), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1348), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.946), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1345), \"['b', 'x', 'a', 'y']\": np.float64(1.0837), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.6514), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9458), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.6499), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.946), \"['b', 'x', 'b', 'x']\": np.float64(-7.6192), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9456), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1348), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.946), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1345), \"['b', 'x', 'b', 'y']\": np.float64(1.0839), \"['b', 'x']\": np.float64(-8.7298), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9446), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1342), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9445), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1343), \"['b', 'y', 'a', 'x']\": np.float64(-1.0821), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1345), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0192), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1346), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0191), \"['b', 'y', 'a', 'y']\": np.float64(0.1542), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9451), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1344), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9449), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1344), \"['b', 'y', 'b', 'x']\": np.float64(-1.0826), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1343), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0191), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1344), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0191), \"['b', 'y', 'b', 'y']\": np.float64(0.154), \"['b', 'y']\": np.float64(1.2403)}\n",
      "It s train loss bro [0.25075238943099976, 0.23274193704128265, 0.22013945877552032, 0.2082248330116272, 0.19700397551059723, 0.18646961450576782, 0.17660489678382874, 0.16738635301589966, 0.15878526866436005, 0.15076944231987, 0.14330589771270752]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 125 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.0273337364196777 and val_loss is 3.5451974868774414\n",
      "for 1 epochs, loss is 2.0850870609283447 and val_loss is 3.545125961303711\n",
      "for 2 epochs, loss is 2.0845251083374023 and val_loss is 3.5444726943969727\n",
      "for 3 epochs, loss is 2.0816891193389893 and val_loss is 3.543299436569214\n",
      "for 4 epochs, loss is 2.0768232345581055 and val_loss is 3.5416643619537354\n",
      "for 5 epochs, loss is 2.0701489448547363 and val_loss is 3.5396199226379395\n",
      "for 6 epochs, loss is 2.061868906021118 and val_loss is 3.5372142791748047\n",
      "for 7 epochs, loss is 2.0521657466888428 and val_loss is 3.5344910621643066\n",
      "for 8 epochs, loss is 2.0412046909332275 and val_loss is 3.5314900875091553\n",
      "for 9 epochs, loss is 2.0291357040405273 and val_loss is 3.528247594833374\n",
      "for 10 epochs, loss is 2.016094446182251 and val_loss is 3.5247974395751953\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4075), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9989), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.4075), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9994), \"['a', 'x', 'a', 'x']\": np.float64(-7.4302), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9981), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1562), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.999), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1558), \"['a', 'x', 'a', 'y']\": np.float64(1.1583), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.4089), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9988), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4069), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9996), \"['a', 'x', 'b', 'x']\": np.float64(-7.4303), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9988), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1562), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9994), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1559), \"['a', 'x', 'b', 'y']\": np.float64(1.1589), \"['a', 'x']\": np.float64(-8.6163), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0023), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1563), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0023), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1563), \"['a', 'y', 'a', 'x']\": np.float64(-1.1623), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1568), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0245), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1569), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0245), \"['a', 'y', 'a', 'y']\": np.float64(0.1819), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0033), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1564), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.003), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1565), \"['a', 'y', 'b', 'x']\": np.float64(-1.1632), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1563), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0244), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1564), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0244), \"['a', 'y', 'b', 'y']\": np.float64(0.1814), \"['a', 'y']\": np.float64(1.3488), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4155), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0002), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.4156), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0006), \"['b', 'x', 'a', 'x']\": np.float64(-7.4396), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9991), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1564), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9999), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1559), \"['b', 'x', 'a', 'y']\": np.float64(1.1594), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.415), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9997), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.413), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0005), \"['b', 'x', 'b', 'x']\": np.float64(-7.4373), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1564), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0006), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1561), \"['b', 'x', 'b', 'y']\": np.float64(1.1603), \"['b', 'x']\": np.float64(-8.6252), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9985), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1557), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9985), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1557), \"['b', 'y', 'a', 'x']\": np.float64(-1.1579), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.156), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0244), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1562), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0243), \"['b', 'y', 'a', 'y']\": np.float64(0.1811), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9993), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1557), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.999), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1559), \"['b', 'y', 'b', 'x']\": np.float64(-1.1586), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1557), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0244), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1558), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0243), \"['b', 'y', 'b', 'y']\": np.float64(0.1807), \"['b', 'y']\": np.float64(1.3435)}\n",
      "It s train loss bro [2.0273337364196777, 2.0850870609283447, 2.0845251083374023, 2.0816891193389893, 2.0768232345581055, 2.0701489448547363, 2.061868906021118, 2.0521657466888428, 2.0412046909332275, 2.0291357040405273, 2.016094446182251]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 126 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.15316423773765564 and val_loss is 3.4955599308013916\n",
      "for 1 epochs, loss is 0.16953617334365845 and val_loss is 3.471489191055298\n",
      "for 2 epochs, loss is 0.18916508555412292 and val_loss is 3.4518964290618896\n",
      "for 3 epochs, loss is 0.20753633975982666 and val_loss is 3.436169147491455\n",
      "for 4 epochs, loss is 0.22426702082157135 and val_loss is 3.4237654209136963\n",
      "for 5 epochs, loss is 0.2390664517879486 and val_loss is 3.4142203330993652\n",
      "for 6 epochs, loss is 0.25173628330230713 and val_loss is 3.4071333408355713\n",
      "for 7 epochs, loss is 0.2621660530567169 and val_loss is 3.4021689891815186\n",
      "for 8 epochs, loss is 0.2703244388103485 and val_loss is 3.3990466594696045\n",
      "for 9 epochs, loss is 0.27624788880348206 and val_loss is 3.397531747817993\n",
      "for 10 epochs, loss is 0.2800288796424866 and val_loss is 3.3974363803863525\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3073), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3691), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.3125), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3653), \"['a', 'x', 'a', 'x']\": np.float64(-5.7016), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3681), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4359), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3709), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4338), \"['a', 'x', 'a', 'y']\": np.float64(1.8123), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.3156), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3687), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3167), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3677), \"['a', 'x', 'b', 'x']\": np.float64(-5.7086), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3648), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4343), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3667), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4329), \"['a', 'x', 'b', 'y']\": np.float64(1.8072), \"['a', 'x']\": np.float64(-7.5474), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.372), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4361), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3736), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4349), \"['a', 'y', 'a', 'x']\": np.float64(-1.8161), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4368), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1391), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4377), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1385), \"['a', 'y', 'a', 'y']\": np.float64(0.5786), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3758), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4363), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3761), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.436), \"['a', 'y', 'b', 'x']\": np.float64(-1.8198), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4349), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1384), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4355), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1379), \"['a', 'y', 'b', 'y']\": np.float64(0.5759), \"['a', 'y']\": np.float64(2.4057), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.322), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3738), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.3273), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3699), \"['b', 'x', 'a', 'x']\": np.float64(-5.7211), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3698), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4364), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3725), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4344), \"['b', 'x', 'a', 'y']\": np.float64(1.8144), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.3261), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.372), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3272), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.371), \"['b', 'x', 'b', 'x']\": np.float64(-5.7225), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3692), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4357), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3711), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4343), \"['b', 'x', 'b', 'y']\": np.float64(1.8131), \"['b', 'x']\": np.float64(-7.5678), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3639), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4335), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3656), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4323), \"['b', 'y', 'a', 'x']\": np.float64(-1.8054), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4337), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1382), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4346), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1375), \"['b', 'y', 'a', 'y']\": np.float64(0.5745), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3668), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4335), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3671), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4331), \"['b', 'y', 'b', 'x']\": np.float64(-1.8079), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4325), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1376), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4331), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1372), \"['b', 'y', 'b', 'y']\": np.float64(0.5726), \"['b', 'y']\": np.float64(2.3907)}\n",
      "It s train loss bro [0.15316423773765564, 0.16953617334365845, 0.18916508555412292, 0.20753633975982666, 0.22426702082157135, 0.2390664517879486, 0.25173628330230713, 0.2621660530567169, 0.2703244388103485, 0.27624788880348206, 0.2800288796424866]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 127 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.309014230966568 and val_loss is 3.3987507820129395\n",
      "for 1 epochs, loss is 0.2817305624485016 and val_loss is 3.4011809825897217\n",
      "for 2 epochs, loss is 0.2800092399120331 and val_loss is 3.4046192169189453\n",
      "for 3 epochs, loss is 0.2768397033214569 and val_loss is 3.40897798538208\n",
      "for 4 epochs, loss is 0.2724260687828064 and val_loss is 3.4141790866851807\n",
      "for 5 epochs, loss is 0.266968309879303 and val_loss is 3.420156717300415\n",
      "for 6 epochs, loss is 0.2606584429740906 and val_loss is 3.4268500804901123\n",
      "for 7 epochs, loss is 0.2536756694316864 and val_loss is 3.4342026710510254\n",
      "for 8 epochs, loss is 0.24618424475193024 and val_loss is 3.442162275314331\n",
      "for 9 epochs, loss is 0.23833198845386505 and val_loss is 3.4506781101226807\n",
      "for 10 epochs, loss is 0.2302490621805191 and val_loss is 3.4597012996673584\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.1512), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2481), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.1546), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2456), \"['a', 'x', 'a', 'x']\": np.float64(-6.4238), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2471), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3031), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.249), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3017), \"['a', 'x', 'a', 'y']\": np.float64(1.5564), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.1575), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2471), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.157), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2471), \"['a', 'x', 'b', 'x']\": np.float64(-6.4281), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.245), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3023), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2463), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3013), \"['a', 'x', 'b', 'y']\": np.float64(1.5534), \"['a', 'x']\": np.float64(-8.0109), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2518), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3033), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2526), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3027), \"['a', 'y', 'a', 'x']\": np.float64(-1.561), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.304), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0739), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3045), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0735), \"['a', 'y', 'a', 'y']\": np.float64(0.3794), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2544), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3033), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2543), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3033), \"['a', 'y', 'b', 'x']\": np.float64(-1.5635), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3027), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0735), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.303), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0733), \"['a', 'y', 'b', 'y']\": np.float64(0.3777), \"['a', 'y']\": np.float64(1.9482), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.1643), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2513), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.1678), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2488), \"['b', 'x', 'a', 'x']\": np.float64(-6.4403), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2478), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3033), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2497), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3019), \"['b', 'x', 'a', 'y']\": np.float64(1.5572), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.1668), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2493), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.1663), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2494), \"['b', 'x', 'b', 'x']\": np.float64(-6.4397), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2482), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.303), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2495), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3021), \"['b', 'x', 'b', 'y']\": np.float64(1.5573), \"['b', 'x']\": np.float64(-8.0269), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2447), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3016), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2455), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.301), \"['b', 'y', 'a', 'x']\": np.float64(-1.5522), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3019), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0734), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3024), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.073), \"['b', 'y', 'a', 'y']\": np.float64(0.3768), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2467), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3014), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2466), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3015), \"['b', 'y', 'b', 'x']\": np.float64(-1.5538), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3011), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0731), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3014), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0729), \"['b', 'y', 'b', 'y']\": np.float64(0.3756), \"['b', 'y']\": np.float64(1.9366)}\n",
      "It s train loss bro [0.309014230966568, 0.2817305624485016, 0.2800092399120331, 0.2768397033214569, 0.2724260687828064, 0.266968309879303, 0.2606584429740906, 0.2536756694316864, 0.24618424475193024, 0.23833198845386505, 0.2302490621805191]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 128 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.23469188809394836 and val_loss is 3.4692487716674805\n",
      "for 1 epochs, loss is 0.21382570266723633 and val_loss is 3.4792017936706543\n",
      "for 2 epochs, loss is 0.20566204190254211 and val_loss is 3.4895122051239014\n",
      "for 3 epochs, loss is 0.1976233422756195 and val_loss is 3.500135898590088\n",
      "for 4 epochs, loss is 0.1897624135017395 and val_loss is 3.5110299587249756\n",
      "for 5 epochs, loss is 0.1821204125881195 and val_loss is 3.522152900695801\n",
      "for 6 epochs, loss is 0.17472831904888153 and val_loss is 3.533461809158325\n",
      "for 7 epochs, loss is 0.16760815680027008 and val_loss is 3.544919490814209\n",
      "for 8 epochs, loss is 0.16077470779418945 and val_loss is 3.5564894676208496\n",
      "for 9 epochs, loss is 0.15423627197742462 and val_loss is 3.5681378841400146\n",
      "for 10 epochs, loss is 0.14799627661705017 and val_loss is 3.579831838607788\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.5462), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9688), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.5466), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9686), \"['a', 'x', 'a', 'x']\": np.float64(-7.5366), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9679), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.144), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9689), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1433), \"['a', 'x', 'a', 'y']\": np.float64(1.1154), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.5482), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9683), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.5461), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9692), \"['a', 'x', 'b', 'x']\": np.float64(-7.537), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9679), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1438), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9686), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1433), \"['a', 'x', 'b', 'y']\": np.float64(1.1152), \"['a', 'x']\": np.float64(-8.6768), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9738), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1441), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9739), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1441), \"['a', 'y', 'a', 'x']\": np.float64(-1.1211), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1447), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0215), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1449), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0214), \"['a', 'y', 'a', 'y']\": np.float64(0.1668), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.975), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1442), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9747), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1443), \"['a', 'y', 'b', 'x']\": np.float64(-1.1223), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1441), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0214), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1442), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0213), \"['a', 'y', 'b', 'y']\": np.float64(0.166), \"['a', 'y']\": np.float64(1.2919), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.556), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9702), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.5564), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9701), \"['b', 'x', 'a', 'x']\": np.float64(-7.5479), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9686), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1441), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9696), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1434), \"['b', 'x', 'a', 'y']\": np.float64(1.1162), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.5555), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9694), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.5534), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9703), \"['b', 'x', 'b', 'x']\": np.float64(-7.5454), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9697), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1441), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9704), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1436), \"['b', 'x', 'b', 'y']\": np.float64(1.1172), \"['b', 'x']\": np.float64(-8.6876), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9681), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1433), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9682), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1432), \"['b', 'y', 'a', 'x']\": np.float64(-1.1146), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1437), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0214), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1439), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0213), \"['b', 'y', 'a', 'y']\": np.float64(0.1656), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.969), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1433), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9687), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1434), \"['b', 'y', 'b', 'x']\": np.float64(-1.1154), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1433), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0213), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1434), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0212), \"['b', 'y', 'b', 'y']\": np.float64(0.1651), \"['b', 'y']\": np.float64(1.2841)}\n",
      "It s train loss bro [0.23469188809394836, 0.21382570266723633, 0.20566204190254211, 0.1976233422756195, 0.1897624135017395, 0.1821204125881195, 0.17472831904888153, 0.16760815680027008, 0.16077470779418945, 0.15423627197742462, 0.14799627661705017]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 129 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1566305309534073 and val_loss is 3.591644048690796\n",
      "for 1 epochs, loss is 0.13639143109321594 and val_loss is 3.6034352779388428\n",
      "for 2 epochs, loss is 0.13101820647716522 and val_loss is 3.615182399749756\n",
      "for 3 epochs, loss is 0.1259254366159439 and val_loss is 3.626864433288574\n",
      "for 4 epochs, loss is 0.12110287696123123 and val_loss is 3.638462781906128\n",
      "for 5 epochs, loss is 0.11653962731361389 and val_loss is 3.6499581336975098\n",
      "for 6 epochs, loss is 0.11222369223833084 and val_loss is 3.6613380908966064\n",
      "for 7 epochs, loss is 0.10814318060874939 and val_loss is 3.6725881099700928\n",
      "for 8 epochs, loss is 0.10428592562675476 and val_loss is 3.6836986541748047\n",
      "for 9 epochs, loss is 0.1006396934390068 and val_loss is 3.6946589946746826\n",
      "for 10 epochs, loss is 0.0971929132938385 and val_loss is 3.705463171005249\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.5593), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7188), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.5581), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7196), \"['a', 'x', 'a', 'x']\": np.float64(-8.2957), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7183), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0688), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7188), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0684), \"['a', 'x', 'a', 'y']\": np.float64(0.7889), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.5552), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7226), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.5562), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7199), \"['a', 'x', 'b', 'x']\": np.float64(-8.2944), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7191), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0688), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7195), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0685), \"['a', 'x', 'b', 'y']\": np.float64(0.7897), \"['a', 'x']\": np.float64(-9.1039), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7238), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0688), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7237), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0689), \"['a', 'y', 'a', 'x']\": np.float64(-0.7943), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0692), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0066), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0693), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0066), \"['a', 'y', 'a', 'y']\": np.float64(0.076), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.724), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0692), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7241), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.069), \"['a', 'y', 'b', 'x']\": np.float64(-0.7949), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0689), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0066), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0689), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0066), \"['a', 'y', 'b', 'y']\": np.float64(0.0756), \"['a', 'y']\": np.float64(0.8724), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.5625), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7192), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.5613), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7199), \"['b', 'x', 'a', 'x']\": np.float64(-8.2993), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7227), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0692), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7232), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0688), \"['b', 'x', 'a', 'y']\": np.float64(0.7937), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.5606), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7231), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.5617), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7204), \"['b', 'x', 'b', 'x']\": np.float64(-8.3004), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7202), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0689), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7205), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0686), \"['b', 'x', 'b', 'y']\": np.float64(0.7908), \"['b', 'x']\": np.float64(-9.1113), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7195), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0684), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7194), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0685), \"['b', 'y', 'a', 'x']\": np.float64(-0.7896), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0687), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0066), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0688), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0065), \"['b', 'y', 'a', 'y']\": np.float64(0.0755), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7195), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0688), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7196), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0686), \"['b', 'y', 'b', 'x']\": np.float64(-0.79), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0685), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0065), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0685), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0065), \"['b', 'y', 'b', 'y']\": np.float64(0.0752), \"['b', 'y']\": np.float64(0.8671)}\n",
      "It s train loss bro [0.1566305309534073, 0.13639143109321594, 0.13101820647716522, 0.1259254366159439, 0.12110287696123123, 0.11653962731361389, 0.11222369223833084, 0.10814318060874939, 0.10428592562675476, 0.1006396934390068, 0.0971929132938385]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 130 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.10563818365335464 and val_loss is 3.7162139415740967\n",
      "for 1 epochs, loss is 0.0908316820859909 and val_loss is 3.726785898208618\n",
      "for 2 epochs, loss is 0.08789865672588348 and val_loss is 3.7371766567230225\n",
      "for 3 epochs, loss is 0.0851244106888771 and val_loss is 3.7473833560943604\n",
      "for 4 epochs, loss is 0.08249921351671219 and val_loss is 3.7574048042297363\n",
      "for 5 epochs, loss is 0.08001336455345154 and val_loss is 3.767242908477783\n",
      "for 6 epochs, loss is 0.07765810191631317 and val_loss is 3.7768964767456055\n",
      "for 7 epochs, loss is 0.07542508095502853 and val_loss is 3.786367893218994\n",
      "for 8 epochs, loss is 0.07330671697854996 and val_loss is 3.7956573963165283\n",
      "for 9 epochs, loss is 0.07129531353712082 and val_loss is 3.8047690391540527\n",
      "for 10 epochs, loss is 0.06938429176807404 and val_loss is 3.8137049674987793\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.1756), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5517), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.1752), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5514), \"['a', 'x', 'a', 'x']\": np.float64(-8.7417), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5514), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0374), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5517), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0372), \"['a', 'x', 'a', 'y']\": np.float64(0.5899), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.1633), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5641), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.1743), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5517), \"['a', 'x', 'b', 'x']\": np.float64(-8.7412), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5512), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0373), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5514), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0372), \"['a', 'x', 'b', 'y']\": np.float64(0.5896), \"['a', 'x']\": np.float64(-9.3469), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5547), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0374), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5547), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0374), \"['a', 'y', 'a', 'x']\": np.float64(-0.5931), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0376), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0025), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0376), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0025), \"['a', 'y', 'a', 'y']\": np.float64(0.0402), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5542), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0383), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5549), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0375), \"['a', 'y', 'b', 'x']\": np.float64(-0.5934), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0374), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0025), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0374), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0025), \"['a', 'y', 'b', 'y']\": np.float64(0.04), \"['a', 'y']\": np.float64(0.6345), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.1682), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5512), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.1678), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5509), \"['b', 'x', 'a', 'x']\": np.float64(-8.7338), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5642), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0383), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5645), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.038), \"['b', 'x', 'a', 'y']\": np.float64(0.6036), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.1673), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5644), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.1783), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.552), \"['b', 'x', 'b', 'x']\": np.float64(-8.7455), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5518), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0374), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.552), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0372), \"['b', 'x', 'b', 'y']\": np.float64(0.5902), \"['b', 'x']\": np.float64(-9.3521), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5515), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0372), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5514), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0372), \"['b', 'y', 'a', 'x']\": np.float64(-0.5896), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0373), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0025), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0373), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0025), \"['b', 'y', 'a', 'y']\": np.float64(0.0399), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5508), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0381), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5516), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0372), \"['b', 'y', 'b', 'x']\": np.float64(-0.5898), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0372), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0025), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0372), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0025), \"['b', 'y', 'b', 'y']\": np.float64(0.0398), \"['b', 'y']\": np.float64(0.6307)}\n",
      "It s train loss bro [0.10563818365335464, 0.0908316820859909, 0.08789865672588348, 0.0851244106888771, 0.08249921351671219, 0.08001336455345154, 0.07765810191631317, 0.07542508095502853, 0.07330671697854996, 0.07129531353712082, 0.06938429176807404]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 131 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.07301653176546097 and val_loss is 3.8225154876708984\n",
      "for 1 epochs, loss is 0.0658324584364891 and val_loss is 3.831150531768799\n",
      "for 2 epochs, loss is 0.06418099254369736 and val_loss is 3.8396151065826416\n",
      "for 3 epochs, loss is 0.06260764598846436 and val_loss is 3.8479132652282715\n",
      "for 4 epochs, loss is 0.06110728904604912 and val_loss is 3.856048345565796\n",
      "for 5 epochs, loss is 0.05967554450035095 and val_loss is 3.8640260696411133\n",
      "for 6 epochs, loss is 0.05830821394920349 and val_loss is 3.871847152709961\n",
      "for 7 epochs, loss is 0.05700116977095604 and val_loss is 3.879520893096924\n",
      "for 8 epochs, loss is 0.05575104430317879 and val_loss is 3.88704776763916\n",
      "for 9 epochs, loss is 0.054554104804992676 and val_loss is 3.894432544708252\n",
      "for 10 epochs, loss is 0.05340748652815819 and val_loss is 3.9016802310943604\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.5501), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4458), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5526), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4426), \"['a', 'x', 'a', 'x']\": np.float64(-9.0081), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4458), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0232), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.446), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0231), \"['a', 'x', 'a', 'y']\": np.float64(0.4697), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.526), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4724), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.5547), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4429), \"['a', 'x', 'b', 'x']\": np.float64(-9.0107), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4426), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.023), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4427), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0229), \"['a', 'x', 'b', 'y']\": np.float64(0.4663), \"['a', 'x']\": np.float64(-9.4905), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4452), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0232), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4453), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.023), \"['a', 'y', 'a', 'x']\": np.float64(-0.4691), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0232), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0012), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0232), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0012), \"['a', 'y', 'a', 'y']\": np.float64(0.0244), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.444), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0246), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4455), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0231), \"['a', 'y', 'b', 'x']\": np.float64(-0.4692), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.023), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0012), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.023), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0012), \"['a', 'y', 'b', 'y']\": np.float64(0.0243), \"['a', 'y']\": np.float64(0.4942), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.527), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4446), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5295), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4414), \"['b', 'x', 'a', 'x']\": np.float64(-8.9837), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4724), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0246), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4726), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0244), \"['b', 'x', 'a', 'y']\": np.float64(0.4977), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.5291), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4725), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.5579), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4431), \"['b', 'x', 'b', 'x']\": np.float64(-9.014), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.443), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.023), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4431), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0229), \"['b', 'x', 'b', 'y']\": np.float64(0.4667), \"['b', 'x']\": np.float64(-9.4944), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4427), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0231), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4428), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0229), \"['b', 'y', 'a', 'x']\": np.float64(-0.4664), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.023), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0012), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.023), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0012), \"['b', 'y', 'a', 'y']\": np.float64(0.0242), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4415), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0245), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4429), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0229), \"['b', 'y', 'b', 'x']\": np.float64(-0.4666), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0229), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0012), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0229), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0012), \"['b', 'y', 'b', 'y']\": np.float64(0.0241), \"['b', 'y']\": np.float64(0.4914)}\n",
      "It s train loss bro [0.07301653176546097, 0.0658324584364891, 0.06418099254369736, 0.06260764598846436, 0.06110728904604912, 0.05967554450035095, 0.05830821394920349, 0.05700116977095604, 0.05575104430317879, 0.054554104804992676, 0.05340748652815819]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 132 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 2.6728899478912354 and val_loss is 3.9018914699554443\n",
      "for 1 epochs, loss is 3.0100555419921875 and val_loss is 3.9012036323547363\n",
      "for 2 epochs, loss is 3.0064332485198975 and val_loss is 3.8997151851654053\n",
      "for 3 epochs, loss is 3.0002386569976807 and val_loss is 3.8975141048431396\n",
      "for 4 epochs, loss is 2.991753101348877 and val_loss is 3.894681930541992\n",
      "for 5 epochs, loss is 2.981231451034546 and val_loss is 3.8912909030914307\n",
      "for 6 epochs, loss is 2.968904495239258 and val_loss is 3.887406826019287\n",
      "for 7 epochs, loss is 2.954979658126831 and val_loss is 3.883087158203125\n",
      "for 8 epochs, loss is 2.9396443367004395 and val_loss is 3.878389358520508\n",
      "for 9 epochs, loss is 2.9230666160583496 and val_loss is 3.8733601570129395\n",
      "for 10 epochs, loss is 2.905398368835449 and val_loss is 3.8680419921875\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3732), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4971), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3751), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4945), \"['a', 'x', 'a', 'x']\": np.float64(-8.8832), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.497), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0294), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4972), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0293), \"['a', 'x', 'a', 'y']\": np.float64(0.5274), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3556), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5168), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3767), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4947), \"['a', 'x', 'b', 'x']\": np.float64(-8.8852), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4945), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0293), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4946), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0292), \"['a', 'x', 'b', 'y']\": np.float64(0.5246), \"['a', 'x']\": np.float64(-9.4243), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4967), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0295), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4969), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0293), \"['a', 'y', 'a', 'x']\": np.float64(-0.527), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0294), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0017), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0294), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0017), \"['a', 'y', 'a', 'y']\": np.float64(0.0312), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4958), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0307), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.497), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0294), \"['a', 'y', 'b', 'x']\": np.float64(-0.5272), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0293), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0017), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0293), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0017), \"['a', 'y', 'b', 'y']\": np.float64(0.0311), \"['a', 'y']\": np.float64(0.5591), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3569), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4961), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3587), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4935), \"['b', 'x', 'a', 'x']\": np.float64(-8.8659), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5169), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0306), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.517), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0305), \"['b', 'x', 'a', 'y']\": np.float64(0.5484), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3585), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.517), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3795), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4949), \"['b', 'x', 'b', 'x']\": np.float64(-8.8882), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4948), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0293), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4949), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0292), \"['b', 'x', 'b', 'y']\": np.float64(0.5249), \"['b', 'x']\": np.float64(-9.4278), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4947), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0294), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4948), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0292), \"['b', 'y', 'a', 'x']\": np.float64(-0.5249), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0293), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0017), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0293), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0017), \"['b', 'y', 'a', 'y']\": np.float64(0.0311), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4937), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0305), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4949), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0292), \"['b', 'y', 'b', 'x']\": np.float64(-0.525), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0292), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0017), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0292), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0017), \"['b', 'y', 'b', 'y']\": np.float64(0.031), \"['b', 'y']\": np.float64(0.5568)}\n",
      "It s train loss bro [2.6728899478912354, 3.0100555419921875, 3.0064332485198975, 3.0002386569976807, 2.991753101348877, 2.981231451034546, 2.968904495239258, 2.954979658126831, 2.9396443367004395, 2.9230666160583496, 2.905398368835449]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 133 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.6276004314422607 and val_loss is 3.862518310546875\n",
      "for 1 epochs, loss is 2.8648924827575684 and val_loss is 3.856765031814575\n",
      "for 2 epochs, loss is 2.845067262649536 and val_loss is 3.850816488265991\n",
      "for 3 epochs, loss is 2.8246071338653564 and val_loss is 3.8447024822235107\n",
      "for 4 epochs, loss is 2.803603172302246 and val_loss is 3.8384501934051514\n",
      "for 5 epochs, loss is 2.7821362018585205 and val_loss is 3.8320846557617188\n",
      "for 6 epochs, loss is 2.7602791786193848 and val_loss is 3.8256282806396484\n",
      "for 7 epochs, loss is 2.7380967140197754 and val_loss is 3.819101095199585\n",
      "for 8 epochs, loss is 2.7156472206115723 and val_loss is 3.8125202655792236\n",
      "for 9 epochs, loss is 2.6929821968078613 and val_loss is 3.805903673171997\n",
      "for 10 epochs, loss is 2.6701486110687256 and val_loss is 3.799262762069702\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.9758), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.609), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.9764), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6078), \"['a', 'x', 'a', 'x']\": np.float64(-8.5997), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.609), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0465), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6091), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0464), \"['a', 'x', 'a', 'y']\": np.float64(0.6567), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.9662), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6199), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.9769), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6079), \"['a', 'x', 'b', 'x']\": np.float64(-8.6003), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6078), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0464), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6079), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0463), \"['a', 'x', 'b', 'y']\": np.float64(0.6554), \"['a', 'x']\": np.float64(-9.2723), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6093), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0465), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6093), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0464), \"['a', 'y', 'a', 'x']\": np.float64(-0.6569), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0465), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0035), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0465), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0035), \"['a', 'y', 'a', 'y']\": np.float64(0.0501), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6086), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0474), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6094), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0464), \"['a', 'y', 'b', 'x']\": np.float64(-0.6571), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0464), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0035), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0464), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0035), \"['a', 'y', 'b', 'y']\": np.float64(0.0501), \"['a', 'y']\": np.float64(0.7084), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.968), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6084), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.9686), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6072), \"['b', 'x', 'a', 'x']\": np.float64(-8.5912), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.62), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0473), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6201), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0473), \"['b', 'x', 'a', 'y']\": np.float64(0.6686), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.9684), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6201), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.9792), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.608), \"['b', 'x', 'b', 'x']\": np.float64(-8.6028), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6079), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0464), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6081), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0463), \"['b', 'x', 'b', 'y']\": np.float64(0.6556), \"['b', 'x']\": np.float64(-9.2751), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6081), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0464), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6082), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0463), \"['b', 'y', 'a', 'x']\": np.float64(-0.6557), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0464), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0035), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0464), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0035), \"['b', 'y', 'a', 'y']\": np.float64(0.05), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6074), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0473), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6082), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0464), \"['b', 'y', 'b', 'x']\": np.float64(-0.6558), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0463), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0035), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0463), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0035), \"['b', 'y', 'b', 'y']\": np.float64(0.05), \"['b', 'y']\": np.float64(0.707)}\n",
      "It s train loss bro [2.6276004314422607, 2.8648924827575684, 2.845067262649536, 2.8246071338653564, 2.803603172302246, 2.7821362018585205, 2.7602791786193848, 2.7380967140197754, 2.7156472206115723, 2.6929821968078613, 2.6701486110687256]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 134 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.09997415542602539 and val_loss is 3.741466999053955\n",
      "for 1 epochs, loss is 0.09300354868173599 and val_loss is 3.6920132637023926\n",
      "for 2 epochs, loss is 0.11140341311693192 and val_loss is 3.6499252319335938\n",
      "for 3 epochs, loss is 0.13025319576263428 and val_loss is 3.614281177520752\n",
      "for 4 epochs, loss is 0.14904481172561646 and val_loss is 3.584228038787842\n",
      "for 5 epochs, loss is 0.16730144619941711 and val_loss is 3.558993101119995\n",
      "for 6 epochs, loss is 0.184600368142128 and val_loss is 3.5378804206848145\n",
      "for 7 epochs, loss is 0.20058874785900116 and val_loss is 3.520282030105591\n",
      "for 8 epochs, loss is 0.21499024331569672 and val_loss is 3.505666732788086\n",
      "for 9 epochs, loss is 0.22760716080665588 and val_loss is 3.4935784339904785\n",
      "for 10 epochs, loss is 0.23831631243228912 and val_loss is 3.483632802963257\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.7564), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3164), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7497), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3241), \"['a', 'x', 'a', 'x']\": np.float64(-6.0959), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3173), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3633), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3143), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3668), \"['a', 'x', 'a', 'y']\": np.float64(1.6871), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.745), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3199), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.7454), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3195), \"['a', 'x', 'b', 'x']\": np.float64(-6.0873), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3245), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.366), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3228), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3681), \"['a', 'x', 'b', 'y']\": np.float64(1.697), \"['a', 'x']\": np.float64(-7.8127), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3082), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3621), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3064), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3642), \"['a', 'y', 'a', 'x']\": np.float64(-1.6767), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.361), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0996), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3602), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1005), \"['a', 'y', 'a', 'y']\": np.float64(0.4624), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.304), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3627), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3041), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3626), \"['a', 'y', 'b', 'x']\": np.float64(-1.6728), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3643), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1007), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3639), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1012), \"['a', 'y', 'b', 'y']\": np.float64(0.4668), \"['a', 'y']\": np.float64(2.1474), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.739), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3116), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.7323), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3193), \"['b', 'x', 'a', 'x']\": np.float64(-6.0736), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3191), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3638), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3161), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3673), \"['b', 'x', 'a', 'y']\": np.float64(1.6894), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.7347), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.317), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.735), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3167), \"['b', 'x', 'b', 'x']\": np.float64(-6.074), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3182), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3643), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3165), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3663), \"['b', 'x', 'b', 'y']\": np.float64(1.689), \"['b', 'x']\": np.float64(-7.7917), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3228), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3661), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3209), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3682), \"['b', 'y', 'a', 'x']\": np.float64(-1.6953), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3658), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1009), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3649), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1018), \"['b', 'y', 'a', 'y']\": np.float64(0.4685), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3197), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3671), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3198), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.367), \"['b', 'y', 'b', 'x']\": np.float64(-1.6931), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3677), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1016), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3672), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1022), \"['b', 'y', 'b', 'y']\": np.float64(0.4711), \"['b', 'y']\": np.float64(2.1721)}\n",
      "It s train loss bro [0.09997415542602539, 0.09300354868173599, 0.11140341311693192, 0.13025319576263428, 0.14904481172561646, 0.16730144619941711, 0.184600368142128, 0.20058874785900116, 0.21499024331569672, 0.22760716080665588, 0.23831631243228912]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 135 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.3952593803405762 and val_loss is 3.4826629161834717\n",
      "for 1 epochs, loss is 1.5286097526550293 and val_loss is 3.4816651344299316\n",
      "for 2 epochs, loss is 1.523123025894165 and val_loss is 3.4806487560272217\n",
      "for 3 epochs, loss is 1.5165892839431763 and val_loss is 3.4796218872070312\n",
      "for 4 epochs, loss is 1.509125828742981 and val_loss is 3.4785914421081543\n",
      "for 5 epochs, loss is 1.5008386373519897 and val_loss is 3.4775664806365967\n",
      "for 6 epochs, loss is 1.4918230772018433 and val_loss is 3.4765520095825195\n",
      "for 7 epochs, loss is 1.4821654558181763 and val_loss is 3.475555896759033\n",
      "for 8 epochs, loss is 1.4719443321228027 and val_loss is 3.4745819568634033\n",
      "for 9 epochs, loss is 1.461228847503662 and val_loss is 3.4736363887786865\n",
      "for 10 epochs, loss is 1.4500826597213745 and val_loss is 3.4727234840393066\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3777), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3683), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.369), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3781), \"['a', 'x', 'a', 'x']\": np.float64(-5.7686), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3693), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4265), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3653), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4311), \"['a', 'x', 'a', 'y']\": np.float64(1.803), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.364), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3712), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3632), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3722), \"['a', 'x', 'b', 'x']\": np.float64(-5.7571), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3784), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4304), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3761), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.433), \"['a', 'x', 'b', 'y']\": np.float64(1.8159), \"['a', 'x']\": np.float64(-7.6014), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3588), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4247), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3561), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4277), \"['a', 'y', 'a', 'x']\": np.float64(-1.7905), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4235), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1319), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4223), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1333), \"['a', 'y', 'a', 'y']\": np.float64(0.5576), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3532), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4252), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3529), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4255), \"['a', 'y', 'b', 'x']\": np.float64(-1.7852), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4278), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1336), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4271), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1344), \"['a', 'y', 'b', 'y']\": np.float64(0.5636), \"['a', 'y']\": np.float64(2.3576), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3569), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3618), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.3482), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3715), \"['b', 'x', 'a', 'x']\": np.float64(-5.7411), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.37), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4267), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3659), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4313), \"['b', 'x', 'a', 'y']\": np.float64(1.8039), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.351), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3671), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3502), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3681), \"['b', 'x', 'b', 'x']\": np.float64(-5.74), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3702), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4278), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3679), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4305), \"['b', 'x', 'b', 'y']\": np.float64(1.8052), \"['b', 'x']\": np.float64(-7.5739), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3761), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4301), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3733), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4332), \"['b', 'y', 'a', 'x']\": np.float64(-1.8133), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.43), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1339), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4287), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1354), \"['b', 'y', 'a', 'y']\": np.float64(0.5662), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3722), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4311), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3719), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4315), \"['b', 'y', 'b', 'x']\": np.float64(-1.8102), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4324), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.135), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4317), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1358), \"['b', 'y', 'b', 'y']\": np.float64(0.5697), \"['b', 'y']\": np.float64(2.3889)}\n",
      "It s train loss bro [1.3952593803405762, 1.5286097526550293, 1.523123025894165, 1.5165892839431763, 1.509125828742981, 1.5008386373519897, 1.4918230772018433, 1.4821654558181763, 1.4719443321228027, 1.461228847503662, 1.4500826597213745]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 136 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.27760857343673706 and val_loss is 3.4658925533294678\n",
      "for 1 epochs, loss is 0.30863118171691895 and val_loss is 3.4613683223724365\n",
      "for 2 epochs, loss is 0.33889052271842957 and val_loss is 3.4585022926330566\n",
      "for 3 epochs, loss is 0.36597028374671936 and val_loss is 3.456746816635132\n",
      "for 4 epochs, loss is 0.38925808668136597 and val_loss is 3.4556612968444824\n",
      "for 5 epochs, loss is 0.4083501100540161 and val_loss is 3.4549078941345215\n",
      "for 6 epochs, loss is 0.4230462312698364 and val_loss is 3.4542481899261475\n",
      "for 7 epochs, loss is 0.433332234621048 and val_loss is 3.453523635864258\n",
      "for 8 epochs, loss is 0.4393533170223236 and val_loss is 3.4526538848876953\n",
      "for 9 epochs, loss is 0.4413819909095764 and val_loss is 3.451608180999756\n",
      "for 10 epochs, loss is 0.4397851526737213 and val_loss is 3.4504055976867676\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.7311), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4646), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.7156), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4812), \"['a', 'x', 'a', 'x']\": np.float64(-4.2137), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4649), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7851), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.456), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7946), \"['a', 'x', 'a', 'y']\": np.float64(2.2597), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.7107), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4616), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.7042), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4685), \"['a', 'x', 'b', 'x']\": np.float64(-4.1898), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.479), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.7966), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4747), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.8012), \"['a', 'x', 'b', 'y']\": np.float64(2.2852), \"['a', 'x']\": np.float64(-6.5011), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4517), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7785), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4435), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7873), \"['a', 'y', 'a', 'x']\": np.float64(-2.2398), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.7782), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.417), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7734), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.4221), \"['a', 'y', 'a', 'y']\": np.float64(1.2003), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4403), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7766), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4368), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7803), \"['a', 'y', 'b', 'x']\": np.float64(-2.2261), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7863), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.4235), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.784), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.4259), \"['a', 'y', 'b', 'y']\": np.float64(1.2149), \"['a', 'y']\": np.float64(3.4549), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.7029), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4495), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-2.6875), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4658), \"['b', 'x', 'a', 'x']\": np.float64(-4.1701), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4577), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7812), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4488), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7907), \"['b', 'x', 'a', 'y']\": np.float64(2.2485), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-2.6915), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4512), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-2.685), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4581), \"['b', 'x', 'b', 'x']\": np.float64(-4.1601), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4622), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.7875), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4579), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.792), \"['b', 'x', 'b', 'y']\": np.float64(2.2592), \"['b', 'x']\": np.float64(-6.4455), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4742), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7906), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4659), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7995), \"['b', 'y', 'a', 'x']\": np.float64(-2.2745), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.7942), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.4256), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7893), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.4307), \"['b', 'y', 'a', 'y']\": np.float64(1.225), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4673), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7911), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4638), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7949), \"['b', 'y', 'b', 'x']\": np.float64(-2.2679), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7974), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.4295), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.7951), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.4319), \"['b', 'y', 'b', 'y']\": np.float64(1.232), \"['b', 'y']\": np.float64(3.5143)}\n",
      "It s train loss bro [0.27760857343673706, 0.30863118171691895, 0.33889052271842957, 0.36597028374671936, 0.38925808668136597, 0.4083501100540161, 0.4230462312698364, 0.433332234621048, 0.4393533170223236, 0.4413819909095764, 0.4397851526737213]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 137 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.43432775139808655 and val_loss is 3.449305772781372\n",
      "for 1 epochs, loss is 0.4276316463947296 and val_loss is 3.4481441974639893\n",
      "for 2 epochs, loss is 0.417979896068573 and val_loss is 3.447002649307251\n",
      "for 3 epochs, loss is 0.40649423003196716 and val_loss is 3.4459681510925293\n",
      "for 4 epochs, loss is 0.3936053514480591 and val_loss is 3.4451282024383545\n",
      "for 5 epochs, loss is 0.3797059655189514 and val_loss is 3.4445650577545166\n",
      "for 6 epochs, loss is 0.36514371633529663 and val_loss is 3.4443557262420654\n",
      "for 7 epochs, loss is 0.35021936893463135 and val_loss is 3.444566488265991\n",
      "for 8 epochs, loss is 0.3351850211620331 and val_loss is 3.445251941680908\n",
      "for 9 epochs, loss is 0.320247620344162 and val_loss is 3.446460723876953\n",
      "for 10 epochs, loss is 0.305571585893631 and val_loss is 3.448225736618042\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.1784), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3896), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.1725), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3968), \"['a', 'x', 'a', 'x']\": np.float64(-5.5913), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3903), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4612), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3874), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4648), \"['a', 'x', 'a', 'y']\": np.float64(1.8595), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.1689), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.392), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.1686), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3924), \"['a', 'x', 'b', 'x']\": np.float64(-5.5834), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.397), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4644), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3954), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4663), \"['a', 'x', 'b', 'y']\": np.float64(1.8692), \"['a', 'x']\": np.float64(-7.482), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3824), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4597), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3804), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4621), \"['a', 'y', 'a', 'x']\": np.float64(-1.8498), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4588), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1522), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4579), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1534), \"['a', 'y', 'a', 'y']\": np.float64(0.6137), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3783), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4602), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3782), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4604), \"['a', 'y', 'b', 'x']\": np.float64(-1.846), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4622), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1536), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4617), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1543), \"['a', 'y', 'b', 'y']\": np.float64(0.6184), \"['a', 'y']\": np.float64(2.474), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.1649), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3851), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.159), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3923), \"['b', 'x', 'a', 'x']\": np.float64(-5.5732), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3914), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4616), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3884), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4651), \"['b', 'x', 'a', 'y']\": np.float64(1.8609), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.161), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3893), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.1607), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3898), \"['b', 'x', 'b', 'x']\": np.float64(-5.5728), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3912), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4624), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3897), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4644), \"['b', 'x', 'b', 'y']\": np.float64(1.8615), \"['b', 'x']\": np.float64(-7.4641), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3944), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4637), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3924), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4661), \"['b', 'y', 'a', 'x']\": np.float64(-1.8658), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4637), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1538), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4628), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.155), \"['b', 'y', 'a', 'y']\": np.float64(0.6202), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3916), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4646), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3915), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4648), \"['b', 'y', 'b', 'x']\": np.float64(-1.8637), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4655), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1547), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.465), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1554), \"['b', 'y', 'b', 'y']\": np.float64(0.6228), \"['b', 'y']\": np.float64(2.4965)}\n",
      "It s train loss bro [0.43432775139808655, 0.4276316463947296, 0.417979896068573, 0.40649423003196716, 0.3936053514480591, 0.3797059655189514, 0.36514371633529663, 0.35021936893463135, 0.3351850211620331, 0.320247620344162, 0.305571585893631]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 138 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3210453987121582 and val_loss is 3.4506995677948\n",
      "for 1 epochs, loss is 0.27742287516593933 and val_loss is 3.4537622928619385\n",
      "for 2 epochs, loss is 0.2641203701496124 and val_loss is 3.4574203491210938\n",
      "for 3 epochs, loss is 0.2514168620109558 and val_loss is 3.46167254447937\n",
      "for 4 epochs, loss is 0.23933522403240204 and val_loss is 3.46651029586792\n",
      "for 5 epochs, loss is 0.22788313031196594 and val_loss is 3.4719183444976807\n",
      "for 6 epochs, loss is 0.21705731749534607 and val_loss is 3.4778754711151123\n",
      "for 7 epochs, loss is 0.20684553682804108 and val_loss is 3.4843552112579346\n",
      "for 8 epochs, loss is 0.19722948968410492 and val_loss is 3.491328716278076\n",
      "for 9 epochs, loss is 0.18818625807762146 and val_loss is 3.4987614154815674\n",
      "for 10 epochs, loss is 0.1796901673078537 and val_loss is 3.506620168685913\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.9736), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0929), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9736), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0936), \"['a', 'x', 'a', 'x']\": np.float64(-7.0912), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0923), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2001), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0929), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2001), \"['a', 'x', 'a', 'y']\": np.float64(1.2973), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.974), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0939), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9734), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0932), \"['a', 'x', 'b', 'x']\": np.float64(-7.0912), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0932), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2002), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0936), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2001), \"['a', 'x', 'b', 'y']\": np.float64(1.2981), \"['a', 'x']\": np.float64(-8.4179), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0948), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2003), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0947), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2004), \"['a', 'y', 'a', 'x']\": np.float64(-1.2996), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2005), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0367), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2006), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0367), \"['a', 'y', 'a', 'y']\": np.float64(0.2381), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0954), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2006), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0953), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2005), \"['a', 'y', 'b', 'x']\": np.float64(-1.3003), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2004), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0367), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2005), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0367), \"['a', 'y', 'b', 'y']\": np.float64(0.238), \"['a', 'y']\": np.float64(1.5435), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.9785), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0937), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9785), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0944), \"['b', 'x', 'a', 'x']\": np.float64(-7.097), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0942), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2005), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0948), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2004), \"['b', 'x', 'a', 'y']\": np.float64(1.2996), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.9784), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0947), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9778), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.094), \"['b', 'x', 'b', 'x']\": np.float64(-7.0964), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0937), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2003), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0941), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2002), \"['b', 'x', 'b', 'y']\": np.float64(1.2987), \"['b', 'x']\": np.float64(-8.4243), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0936), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2001), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0936), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2002), \"['b', 'y', 'a', 'x']\": np.float64(-1.2982), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2002), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0367), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2003), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0367), \"['b', 'y', 'a', 'y']\": np.float64(0.2378), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0941), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2003), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.094), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2002), \"['b', 'y', 'b', 'x']\": np.float64(-1.2987), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2001), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0367), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2002), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0366), \"['b', 'y', 'b', 'y']\": np.float64(0.2377), \"['b', 'y']\": np.float64(1.5417)}\n",
      "It s train loss bro [0.3210453987121582, 0.27742287516593933, 0.2641203701496124, 0.2514168620109558, 0.23933522403240204, 0.22788313031196594, 0.21705731749534607, 0.20684553682804108, 0.19722948968410492, 0.18818625807762146, 0.1796901673078537]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 139 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.8447601795196533 and val_loss is 3.5069632530212402\n",
      "for 1 epochs, loss is 1.8706880807876587 and val_loss is 3.5068819522857666\n",
      "for 2 epochs, loss is 1.870776891708374 and val_loss is 3.506417751312256\n",
      "for 3 epochs, loss is 1.8691004514694214 and val_loss is 3.5056138038635254\n",
      "for 4 epochs, loss is 1.865845799446106 and val_loss is 3.5045042037963867\n",
      "for 5 epochs, loss is 1.8611823320388794 and val_loss is 3.5031254291534424\n",
      "for 6 epochs, loss is 1.8552632331848145 and val_loss is 3.501507043838501\n",
      "for 7 epochs, loss is 1.8482284545898438 and val_loss is 3.49967885017395\n",
      "for 8 epochs, loss is 1.8402035236358643 and val_loss is 3.4976677894592285\n",
      "for 9 epochs, loss is 1.8313026428222656 and val_loss is 3.495497465133667\n",
      "for 10 epochs, loss is 1.8216288089752197 and val_loss is 3.4931907653808594\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.7803), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1318), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.7808), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1321), \"['a', 'x', 'a', 'x']\": np.float64(-6.9372), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1312), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2219), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1319), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2217), \"['a', 'x', 'a', 'y']\": np.float64(1.3583), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.7817), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1324), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.7809), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.132), \"['a', 'x', 'b', 'x']\": np.float64(-6.9377), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1317), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2219), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1322), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2217), \"['a', 'x', 'b', 'y']\": np.float64(1.3587), \"['a', 'x']\": np.float64(-8.3255), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1343), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2221), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1344), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2222), \"['a', 'y', 'a', 'x']\": np.float64(-1.3613), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2224), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0436), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2225), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0436), \"['a', 'y', 'a', 'y']\": np.float64(0.267), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1352), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2223), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1351), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2223), \"['a', 'y', 'b', 'x']\": np.float64(-1.3622), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2222), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0436), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2223), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0435), \"['a', 'y', 'b', 'y']\": np.float64(0.2668), \"['a', 'y']\": np.float64(1.6346), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.7868), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1331), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.7873), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1334), \"['b', 'x', 'a', 'x']\": np.float64(-6.945), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1328), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2222), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1335), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.222), \"['b', 'x', 'a', 'y']\": np.float64(1.3602), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.787), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1334), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.7862), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1331), \"['b', 'x', 'b', 'x']\": np.float64(-6.944), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1326), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2221), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1331), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2219), \"['b', 'x', 'b', 'y']\": np.float64(1.3598), \"['b', 'x']\": np.float64(-8.3335), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1322), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2217), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1323), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2218), \"['b', 'y', 'a', 'x']\": np.float64(-1.3588), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2219), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0435), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.222), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0435), \"['b', 'y', 'a', 'y']\": np.float64(0.2664), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.133), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2219), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1328), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2218), \"['b', 'y', 'b', 'x']\": np.float64(-1.3595), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2217), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0435), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2218), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0434), \"['b', 'y', 'b', 'y']\": np.float64(0.2662), \"['b', 'y']\": np.float64(1.6314)}\n",
      "It s train loss bro [1.8447601795196533, 1.8706880807876587, 1.870776891708374, 1.8691004514694214, 1.865845799446106, 1.8611823320388794, 1.8552632331848145, 1.8482284545898438, 1.8402035236358643, 1.8313026428222656, 1.8216288089752197]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 140 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.21363422274589539 and val_loss is 3.4736502170562744\n",
      "for 1 epochs, loss is 0.20174874365329742 and val_loss is 3.457871675491333\n",
      "for 2 epochs, loss is 0.21896086633205414 and val_loss is 3.445342779159546\n",
      "for 3 epochs, loss is 0.2345532923936844 and val_loss is 3.4356205463409424\n",
      "for 4 epochs, loss is 0.24830852448940277 and val_loss is 3.4283194541931152\n",
      "for 5 epochs, loss is 0.2600851058959961 and val_loss is 3.4231116771698\n",
      "for 6 epochs, loss is 0.26981213688850403 and val_loss is 3.419720411300659\n",
      "for 7 epochs, loss is 0.27748140692710876 and val_loss is 3.4179134368896484\n",
      "for 8 epochs, loss is 0.283138245344162 and val_loss is 3.4174964427948\n",
      "for 9 epochs, loss is 0.2868712842464447 and val_loss is 3.4183082580566406\n",
      "for 10 epochs, loss is 0.2888031005859375 and val_loss is 3.420212745666504\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.2109), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3806), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2148), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3782), \"['a', 'x', 'a', 'x']\": np.float64(-5.6162), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3798), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4531), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.382), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4518), \"['a', 'x', 'a', 'y']\": np.float64(1.8413), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.2171), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3806), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2179), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3799), \"['a', 'x', 'b', 'x']\": np.float64(-5.6214), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3779), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4521), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3794), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4511), \"['a', 'x', 'b', 'y']\": np.float64(1.8382), \"['a', 'x']\": np.float64(-7.4904), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3835), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4536), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3848), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4528), \"['a', 'y', 'a', 'x']\": np.float64(-1.8452), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4541), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1491), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4548), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1487), \"['a', 'y', 'a', 'y']\": np.float64(0.6059), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3864), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4539), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3867), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4537), \"['a', 'y', 'b', 'x']\": np.float64(-1.8481), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4529), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1486), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4533), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1483), \"['a', 'y', 'b', 'y']\": np.float64(0.6041), \"['a', 'y']\": np.float64(2.4623), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.2225), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3844), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2264), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.382), \"['b', 'x', 'a', 'x']\": np.float64(-5.6316), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3816), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4537), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3838), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4524), \"['b', 'x', 'a', 'y']\": np.float64(1.8437), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.2256), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3834), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2264), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3827), \"['b', 'x', 'b', 'x']\": np.float64(-5.6327), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3813), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4532), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3828), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4522), \"['b', 'x', 'b', 'y']\": np.float64(1.8427), \"['b', 'x']\": np.float64(-7.507), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3779), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4518), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3792), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.451), \"['b', 'y', 'a', 'x']\": np.float64(-1.8377), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4518), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1484), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4525), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1479), \"['b', 'y', 'a', 'y']\": np.float64(0.6029), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3801), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4518), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3804), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4516), \"['b', 'y', 'b', 'x']\": np.float64(-1.8397), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.451), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.148), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4515), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1477), \"['b', 'y', 'b', 'y']\": np.float64(0.6017), \"['b', 'y']\": np.float64(2.4516)}\n",
      "It s train loss bro [0.21363422274589539, 0.20174874365329742, 0.21896086633205414, 0.2345532923936844, 0.24830852448940277, 0.2600851058959961, 0.26981213688850403, 0.27748140692710876, 0.283138245344162, 0.2868712842464447, 0.2888031005859375]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 141 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.30368268489837646 and val_loss is 3.4232144355773926\n",
      "for 1 epochs, loss is 0.287885844707489 and val_loss is 3.4270923137664795\n",
      "for 2 epochs, loss is 0.2853686213493347 and val_loss is 3.4317688941955566\n",
      "for 3 epochs, loss is 0.28170233964920044 and val_loss is 3.4371752738952637\n",
      "for 4 epochs, loss is 0.277057409286499 and val_loss is 3.4432532787323\n",
      "for 5 epochs, loss is 0.27159759402275085 and val_loss is 3.4499502182006836\n",
      "for 6 epochs, loss is 0.26547661423683167 and val_loss is 3.457218647003174\n",
      "for 7 epochs, loss is 0.25883564352989197 and val_loss is 3.465013265609741\n",
      "for 8 epochs, loss is 0.2518034875392914 and val_loss is 3.473292350769043\n",
      "for 9 epochs, loss is 0.24449467658996582 and val_loss is 3.4820165634155273\n",
      "for 10 epochs, loss is 0.23700971901416779 and val_loss is 3.491147756576538\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.0349), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2684), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0371), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2672), \"['a', 'x', 'a', 'x']\": np.float64(-6.3269), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2676), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3201), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2691), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3192), \"['a', 'x', 'a', 'y']\": np.float64(1.5939), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.0393), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2678), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.0387), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2681), \"['a', 'x', 'b', 'x']\": np.float64(-6.3297), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2667), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3196), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2677), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3189), \"['a', 'x', 'b', 'y']\": np.float64(1.5924), \"['a', 'x']\": np.float64(-7.9504), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2716), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3203), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2722), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.32), \"['a', 'y', 'a', 'x']\": np.float64(-1.5979), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3209), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.081), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3212), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0808), \"['a', 'y', 'a', 'y']\": np.float64(0.4035), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2737), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3204), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2735), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3205), \"['a', 'y', 'b', 'x']\": np.float64(-1.5998), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.32), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0807), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3203), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0806), \"['a', 'y', 'b', 'y']\": np.float64(0.4023), \"['a', 'y']\": np.float64(2.0092), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.0451), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.271), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0474), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2698), \"['b', 'x', 'a', 'x']\": np.float64(-6.3397), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2685), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3203), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.27), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3194), \"['b', 'x', 'a', 'y']\": np.float64(1.595), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.0467), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2697), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.046), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2699), \"['b', 'x', 'b', 'x']\": np.float64(-6.3389), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2691), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3201), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2701), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3195), \"['b', 'x', 'b', 'y']\": np.float64(1.5953), \"['b', 'x']\": np.float64(-7.9631), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2668), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3191), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2674), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3188), \"['b', 'y', 'a', 'x']\": np.float64(-1.5919), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3194), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0806), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3197), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0804), \"['b', 'y', 'a', 'y']\": np.float64(0.4016), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2683), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3191), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2682), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3192), \"['b', 'y', 'b', 'x']\": np.float64(-1.5931), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3188), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0804), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3191), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0803), \"['b', 'y', 'b', 'y']\": np.float64(0.4008), \"['b', 'y']\": np.float64(2.0011)}\n",
      "It s train loss bro [0.30368268489837646, 0.287885844707489, 0.2853686213493347, 0.28170233964920044, 0.277057409286499, 0.27159759402275085, 0.26547661423683167, 0.25883564352989197, 0.2518034875392914, 0.24449467658996582, 0.23700971901416779]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 142 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.23220235109329224 and val_loss is 3.500663995742798\n",
      "for 1 epochs, loss is 0.22186844050884247 and val_loss is 3.5105106830596924\n",
      "for 2 epochs, loss is 0.21434566378593445 and val_loss is 3.5206518173217773\n",
      "for 3 epochs, loss is 0.20691987872123718 and val_loss is 3.531053066253662\n",
      "for 4 epochs, loss is 0.19963325560092926 and val_loss is 3.541682004928589\n",
      "for 5 epochs, loss is 0.1925194412469864 and val_loss is 3.5525052547454834\n",
      "for 6 epochs, loss is 0.18560458719730377 and val_loss is 3.5634922981262207\n",
      "for 7 epochs, loss is 0.17890788614749908 and val_loss is 3.5746147632598877\n",
      "for 8 epochs, loss is 0.1724429726600647 and val_loss is 3.5858426094055176\n",
      "for 9 epochs, loss is 0.16621889173984528 and val_loss is 3.5971503257751465\n",
      "for 10 epochs, loss is 0.16024045646190643 and val_loss is 3.6085116863250732\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.3029), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0252), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.3028), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0256), \"['a', 'x', 'a', 'x']\": np.float64(-7.3489), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0245), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1673), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0253), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1668), \"['a', 'x', 'a', 'y']\": np.float64(1.1953), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.3045), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0245), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.3021), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0259), \"['a', 'x', 'b', 'x']\": np.float64(-7.3487), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.025), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1672), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0256), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1669), \"['a', 'x', 'b', 'y']\": np.float64(1.1958), \"['a', 'x']\": np.float64(-8.5684), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0294), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1674), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0294), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1675), \"['a', 'y', 'a', 'x']\": np.float64(-1.2002), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1679), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0274), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1681), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0273), \"['a', 'y', 'a', 'y']\": np.float64(0.196), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0305), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1675), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0301), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1677), \"['a', 'y', 'b', 'x']\": np.float64(-1.2012), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1675), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0273), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1676), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0273), \"['a', 'y', 'b', 'y']\": np.float64(0.1954), \"['a', 'y']\": np.float64(1.4004), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.3111), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0265), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.311), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.027), \"['b', 'x', 'a', 'x']\": np.float64(-7.3585), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0248), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1673), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0257), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1668), \"['b', 'x', 'a', 'y']\": np.float64(1.1958), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.3104), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0254), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.308), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0268), \"['b', 'x', 'b', 'x']\": np.float64(-7.3556), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0264), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1674), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0269), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1671), \"['b', 'x', 'b', 'y']\": np.float64(1.1974), \"['b', 'x']\": np.float64(-8.5772), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0254), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1668), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0254), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1669), \"['b', 'y', 'a', 'x']\": np.float64(-1.1955), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1671), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0273), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1673), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0272), \"['b', 'y', 'a', 'y']\": np.float64(0.195), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0262), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1668), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0258), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.167), \"['b', 'y', 'b', 'x']\": np.float64(-1.1962), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1668), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0272), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1669), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0272), \"['b', 'y', 'b', 'y']\": np.float64(0.1946), \"['b', 'y']\": np.float64(1.3947)}\n",
      "It s train loss bro [0.23220235109329224, 0.22186844050884247, 0.21434566378593445, 0.20691987872123718, 0.19963325560092926, 0.1925194412469864, 0.18560458719730377, 0.17890788614749908, 0.1724429726600647, 0.16621889173984528, 0.16024045646190643]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 143 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.15944766998291016 and val_loss is 3.619948387145996\n",
      "for 1 epochs, loss is 0.1490229368209839 and val_loss is 3.6313884258270264\n",
      "for 2 epochs, loss is 0.1437796950340271 and val_loss is 3.6428134441375732\n",
      "for 3 epochs, loss is 0.1387747973203659 and val_loss is 3.654205322265625\n",
      "for 4 epochs, loss is 0.1340014487504959 and val_loss is 3.665546178817749\n",
      "for 5 epochs, loss is 0.12945261597633362 and val_loss is 3.676820993423462\n",
      "for 6 epochs, loss is 0.12512032687664032 and val_loss is 3.6880180835723877\n",
      "for 7 epochs, loss is 0.12099577486515045 and val_loss is 3.6991233825683594\n",
      "for 8 epochs, loss is 0.11707049608230591 and val_loss is 3.7101268768310547\n",
      "for 9 epochs, loss is 0.11333528906106949 and val_loss is 3.7210187911987305\n",
      "for 10 epochs, loss is 0.10978135466575623 and val_loss is 3.731792688369751\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.2804), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7929), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.2786), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7944), \"['a', 'x', 'a', 'x']\": np.float64(-8.0904), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7923), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0868), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7928), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0865), \"['a', 'x', 'a', 'y']\": np.float64(0.8811), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.279), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7933), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.2763), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7944), \"['a', 'x', 'b', 'x']\": np.float64(-8.0884), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7939), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0869), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7942), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0867), \"['a', 'x', 'b', 'y']\": np.float64(0.8827), \"['a', 'x']\": np.float64(-8.9905), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7976), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0869), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7974), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.087), \"['a', 'y', 'a', 'x']\": np.float64(-0.8864), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0873), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0096), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0873), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0095), \"['a', 'y', 'a', 'y']\": np.float64(0.0971), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7982), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.087), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7979), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0871), \"['a', 'y', 'b', 'x']\": np.float64(-0.8869), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.087), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0095), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.087), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0095), \"['a', 'y', 'b', 'y']\": np.float64(0.0967), \"['a', 'y']\": np.float64(0.9857), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.286), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7935), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.2842), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.795), \"['b', 'x', 'a', 'x']\": np.float64(-8.0965), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7934), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0869), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7939), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0866), \"['b', 'x', 'a', 'y']\": np.float64(0.8824), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.2837), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7938), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.281), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7949), \"['b', 'x', 'b', 'x']\": np.float64(-8.0936), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7947), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0869), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.795), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0867), \"['b', 'x', 'b', 'y']\": np.float64(0.8837), \"['b', 'x']\": np.float64(-8.9969), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7943), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0865), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7941), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0867), \"['b', 'y', 'a', 'x']\": np.float64(-0.8826), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0868), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0095), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0869), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0095), \"['b', 'y', 'a', 'y']\": np.float64(0.0966), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7946), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0866), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7944), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0867), \"['b', 'y', 'b', 'x']\": np.float64(-0.883), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0866), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0095), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0867), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0095), \"['b', 'y', 'b', 'y']\": np.float64(0.0963), \"['b', 'y']\": np.float64(0.9815)}\n",
      "It s train loss bro [0.15944766998291016, 0.1490229368209839, 0.1437796950340271, 0.1387747973203659, 0.1340014487504959, 0.12945261597633362, 0.12512032687664032, 0.12099577486515045, 0.11707049608230591, 0.11333528906106949, 0.10978135466575623]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 144 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.12193475663661957 and val_loss is 3.7421696186065674\n",
      "for 1 epochs, loss is 0.10316034406423569 and val_loss is 3.7524471282958984\n",
      "for 2 epochs, loss is 0.10007920861244202 and val_loss is 3.7626161575317383\n",
      "for 3 epochs, loss is 0.09714758396148682 and val_loss is 3.772674560546875\n",
      "for 4 epochs, loss is 0.09435758739709854 and val_loss is 3.7826144695281982\n",
      "for 5 epochs, loss is 0.0917009711265564 and val_loss is 3.7924323081970215\n",
      "for 6 epochs, loss is 0.0891704261302948 and val_loss is 3.802124500274658\n",
      "for 7 epochs, loss is 0.08675883710384369 and val_loss is 3.811689615249634\n",
      "for 8 epochs, loss is 0.08445941656827927 and val_loss is 3.8211236000061035\n",
      "for 9 epochs, loss is 0.08226581662893295 and val_loss is 3.83042573928833\n",
      "for 10 epochs, loss is 0.08017192035913467 and val_loss is 3.8395936489105225\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.9233), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6227), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.9205), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6248), \"['a', 'x', 'a', 'x']\": np.float64(-8.5599), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6222), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0493), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6225), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.049), \"['a', 'x', 'a', 'y']\": np.float64(0.6727), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.9188), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6249), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.9172), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6248), \"['a', 'x', 'b', 'x']\": np.float64(-8.5569), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6244), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0494), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6246), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0492), \"['a', 'x', 'b', 'y']\": np.float64(0.675), \"['a', 'x']\": np.float64(-9.2478), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6279), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0493), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6277), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0495), \"['a', 'y', 'a', 'x']\": np.float64(-0.6784), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0497), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0039), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0497), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0039), \"['a', 'y', 'a', 'y']\": np.float64(0.0537), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6281), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0496), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.628), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0496), \"['a', 'y', 'b', 'x']\": np.float64(-0.6787), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0495), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0039), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0495), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0039), \"['a', 'y', 'b', 'y']\": np.float64(0.0535), \"['a', 'y']\": np.float64(0.7334), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.9261), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6229), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.9233), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.625), \"['b', 'x', 'a', 'x']\": np.float64(-8.563), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.625), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0495), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6253), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0493), \"['b', 'x', 'a', 'y']\": np.float64(0.6757), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.9229), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6252), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.9213), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6252), \"['b', 'x', 'b', 'x']\": np.float64(-8.5613), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.625), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0494), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6252), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0493), \"['b', 'x', 'b', 'y']\": np.float64(0.6757), \"['b', 'x']\": np.float64(-9.253), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6248), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0491), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6246), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0493), \"['b', 'y', 'a', 'x']\": np.float64(-0.675), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0494), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0039), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0494), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0039), \"['b', 'y', 'a', 'y']\": np.float64(0.0534), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6249), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0493), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6247), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0493), \"['b', 'y', 'b', 'x']\": np.float64(-0.6752), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0492), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0039), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0493), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0039), \"['b', 'y', 'b', 'y']\": np.float64(0.0532), \"['b', 'y']\": np.float64(0.7297)}\n",
      "It s train loss bro [0.12193475663661957, 0.10316034406423569, 0.10007920861244202, 0.09714758396148682, 0.09435758739709854, 0.0917009711265564, 0.0891704261302948, 0.08675883710384369, 0.08445941656827927, 0.08226581662893295, 0.08017192035913467]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 145 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.08531401306390762 and val_loss is 3.8486878871917725\n",
      "for 1 epochs, loss is 0.0762551799416542 and val_loss is 3.8576419353485107\n",
      "for 2 epochs, loss is 0.0744226798415184 and val_loss is 3.866455554962158\n",
      "for 3 epochs, loss is 0.07266984134912491 and val_loss is 3.875131845474243\n",
      "for 4 epochs, loss is 0.07099221646785736 and val_loss is 3.883671283721924\n",
      "for 5 epochs, loss is 0.06938529759645462 and val_loss is 3.8920745849609375\n",
      "for 6 epochs, loss is 0.0678454041481018 and val_loss is 3.9003446102142334\n",
      "for 7 epochs, loss is 0.06636862456798553 and val_loss is 3.9084830284118652\n",
      "for 8 epochs, loss is 0.06495154649019241 and val_loss is 3.9164915084838867\n",
      "for 9 epochs, loss is 0.06359074264764786 and val_loss is 3.9243717193603516\n",
      "for 10 epochs, loss is 0.06228308752179146 and val_loss is 3.9321277141571045\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3387), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5067), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3361), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5084), \"['a', 'x', 'a', 'x']\": np.float64(-8.8571), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5064), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.031), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5066), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0309), \"['a', 'x', 'a', 'y']\": np.float64(0.5382), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.331), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5123), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3333), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5085), \"['a', 'x', 'b', 'x']\": np.float64(-8.8544), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5081), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0311), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5083), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.031), \"['a', 'x', 'b', 'y']\": np.float64(0.54), \"['a', 'x']\": np.float64(-9.4077), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5111), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0311), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5109), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0312), \"['a', 'y', 'a', 'x']\": np.float64(-0.5429), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0313), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0019), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0313), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0019), \"['a', 'y', 'a', 'y']\": np.float64(0.0332), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.511), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0314), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5111), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0312), \"['a', 'y', 'b', 'x']\": np.float64(-0.5431), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0311), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0019), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0311), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0019), \"['a', 'y', 'b', 'y']\": np.float64(0.0331), \"['a', 'y']\": np.float64(0.577), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3372), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5066), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3347), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5083), \"['b', 'x', 'a', 'x']\": np.float64(-8.8555), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5123), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0314), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5125), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0312), \"['b', 'x', 'a', 'y']\": np.float64(0.5445), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3343), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5125), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3366), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5087), \"['b', 'x', 'b', 'x']\": np.float64(-8.8579), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5086), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0311), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5087), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.031), \"['b', 'x', 'b', 'y']\": np.float64(0.5405), \"['b', 'x']\": np.float64(-9.4118), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5085), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0309), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5083), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.031), \"['b', 'y', 'a', 'x']\": np.float64(-0.5401), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0311), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0019), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0311), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0019), \"['b', 'y', 'a', 'y']\": np.float64(0.033), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5083), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0313), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5084), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.031), \"['b', 'y', 'b', 'x']\": np.float64(-0.5402), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.031), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0019), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.031), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0019), \"['b', 'y', 'b', 'y']\": np.float64(0.0329), \"['b', 'y']\": np.float64(0.574)}\n",
      "It s train loss bro [0.08531401306390762, 0.0762551799416542, 0.0744226798415184, 0.07266984134912491, 0.07099221646785736, 0.06938529759645462, 0.0678454041481018, 0.06636862456798553, 0.06495154649019241, 0.06359074264764786, 0.06228308752179146]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 146 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.07755076885223389 and val_loss is 3.9395806789398193\n",
      "for 1 epochs, loss is 0.05980176851153374 and val_loss is 3.946934223175049\n",
      "for 2 epochs, loss is 0.05862439423799515 and val_loss is 3.954188108444214\n",
      "for 3 epochs, loss is 0.05749144405126572 and val_loss is 3.961343765258789\n",
      "for 4 epochs, loss is 0.05640069767832756 and val_loss is 3.9684035778045654\n",
      "for 5 epochs, loss is 0.055349480360746384 and val_loss is 3.9753658771514893\n",
      "for 6 epochs, loss is 0.05433610826730728 and val_loss is 3.982234477996826\n",
      "for 7 epochs, loss is 0.05335810035467148 and val_loss is 3.9890096187591553\n",
      "for 8 epochs, loss is 0.05241409316658974 and val_loss is 3.9956917762756348\n",
      "for 9 epochs, loss is 0.05150213837623596 and val_loss is 4.002284526824951\n",
      "for 10 epochs, loss is 0.050620634108781815 and val_loss is 4.008788108825684\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6223), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.425), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6204), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.426), \"['a', 'x', 'a', 'x']\": np.float64(-9.0573), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4248), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0211), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4249), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.021), \"['a', 'x', 'a', 'y']\": np.float64(0.4464), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.6114), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4342), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.6183), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4261), \"['a', 'x', 'b', 'x']\": np.float64(-9.0553), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4258), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0211), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4259), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.021), \"['a', 'x', 'b', 'y']\": np.float64(0.4475), \"['a', 'x']\": np.float64(-9.5142), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4283), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0211), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4282), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0212), \"['a', 'y', 'a', 'x']\": np.float64(-0.4499), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0212), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0011), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0213), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.001), \"['a', 'y', 'a', 'y']\": np.float64(0.0223), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.428), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0216), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4284), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0212), \"['a', 'y', 'b', 'x']\": np.float64(-0.4501), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0211), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.001), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0211), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.001), \"['a', 'y', 'b', 'y']\": np.float64(0.0222), \"['a', 'y']\": np.float64(0.4728), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6163), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4247), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6145), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4257), \"['b', 'x', 'a', 'x']\": np.float64(-9.051), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4342), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0215), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4344), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0214), \"['b', 'x', 'a', 'y']\": np.float64(0.4564), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.6142), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4343), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.6211), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4262), \"['b', 'x', 'b', 'x']\": np.float64(-9.0583), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4262), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0211), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4263), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.021), \"['b', 'x', 'b', 'y']\": np.float64(0.4479), \"['b', 'x']\": np.float64(-9.5176), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.426), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.021), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4259), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.021), \"['b', 'y', 'a', 'x']\": np.float64(-0.4475), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0211), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.001), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0211), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.001), \"['b', 'y', 'a', 'y']\": np.float64(0.0222), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4257), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0215), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.426), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0211), \"['b', 'y', 'b', 'x']\": np.float64(-0.4476), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.021), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.001), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.021), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.001), \"['b', 'y', 'b', 'y']\": np.float64(0.0221), \"['b', 'y']\": np.float64(0.4703)}\n",
      "It s train loss bro [0.07755076885223389, 0.05980176851153374, 0.05862439423799515, 0.05749144405126572, 0.05640069767832756, 0.055349480360746384, 0.05433610826730728, 0.05335810035467148, 0.05241409316658974, 0.05150213837623596, 0.050620634108781815]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 147 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.06156338006258011 and val_loss is 4.0153374671936035\n",
      "for 1 epochs, loss is 0.04893304780125618 and val_loss is 4.021789073944092\n",
      "for 2 epochs, loss is 0.04812527820467949 and val_loss is 4.028141975402832\n",
      "for 3 epochs, loss is 0.047343574464321136 and val_loss is 4.034401893615723\n",
      "for 4 epochs, loss is 0.046586863696575165 and val_loss is 4.0405707359313965\n",
      "for 5 epochs, loss is 0.04585348814725876 and val_loss is 4.046649932861328\n",
      "for 6 epochs, loss is 0.04514271020889282 and val_loss is 4.052643299102783\n",
      "for 7 epochs, loss is 0.044453203678131104 and val_loss is 4.058552265167236\n",
      "for 8 epochs, loss is 0.04378410056233406 and val_loss is 4.064378261566162\n",
      "for 9 epochs, loss is 0.04313453286886215 and val_loss is 4.07012414932251\n",
      "for 10 epochs, loss is 0.042503394186496735 and val_loss is 4.075794696807861\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8238), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3661), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.8237), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3655), \"['a', 'x', 'a', 'x']\": np.float64(-9.1987), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.366), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0152), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3661), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0151), \"['a', 'x', 'a', 'y']\": np.float64(0.3816), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.8092), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3806), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8233), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3656), \"['a', 'x', 'b', 'x']\": np.float64(-9.1985), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3654), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0152), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3655), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0151), \"['a', 'x', 'b', 'y']\": np.float64(0.381), \"['a', 'x']\": np.float64(-9.5895), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3674), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0152), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3674), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0152), \"['a', 'y', 'a', 'x']\": np.float64(-0.383), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0153), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0153), \"['a', 'y', 'a', 'y']\": np.float64(0.0159), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3669), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0159), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3675), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0152), \"['a', 'y', 'b', 'x']\": np.float64(-0.3831), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0152), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0152), \"['a', 'y', 'b', 'y']\": np.float64(0.0158), \"['a', 'y']\": np.float64(0.3993), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8118), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3656), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.8117), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.365), \"['b', 'x', 'a', 'x']\": np.float64(-9.1862), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3807), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0158), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3808), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0157), \"['b', 'x', 'a', 'y']\": np.float64(0.3969), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.8114), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3807), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8256), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3657), \"['b', 'x', 'b', 'x']\": np.float64(-9.2009), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3657), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0152), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3657), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0151), \"['b', 'x', 'b', 'y']\": np.float64(0.3813), \"['b', 'x']\": np.float64(-9.5922), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3655), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0152), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3655), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0151), \"['b', 'y', 'a', 'x']\": np.float64(-0.381), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0152), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0152), \"['b', 'y', 'a', 'y']\": np.float64(0.0158), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.365), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0158), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3656), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0151), \"['b', 'y', 'b', 'x']\": np.float64(-0.3811), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0151), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0151), \"['b', 'y', 'b', 'y']\": np.float64(0.0158), \"['b', 'y']\": np.float64(0.3973)}\n",
      "It s train loss bro [0.06156338006258011, 0.04893304780125618, 0.04812527820467949, 0.047343574464321136, 0.046586863696575165, 0.04585348814725876, 0.04514271020889282, 0.044453203678131104, 0.04378410056233406, 0.04313453286886215, 0.042503394186496735]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 148 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.04241679236292839 and val_loss is 4.081385612487793\n",
      "for 1 epochs, loss is 0.04129345715045929 and val_loss is 4.086903095245361\n",
      "for 2 epochs, loss is 0.04071323201060295 and val_loss is 4.092349529266357\n",
      "for 3 epochs, loss is 0.040148474276065826 and val_loss is 4.097726821899414\n",
      "for 4 epochs, loss is 0.03959863632917404 and val_loss is 4.103035926818848\n",
      "for 5 epochs, loss is 0.03906305506825447 and val_loss is 4.108278751373291\n",
      "for 6 epochs, loss is 0.03854118287563324 and val_loss is 4.113457679748535\n",
      "for 7 epochs, loss is 0.03803223371505737 and val_loss is 4.118573188781738\n",
      "for 8 epochs, loss is 0.03753599897027016 and val_loss is 4.123627662658691\n",
      "for 9 epochs, loss is 0.03705192729830742 and val_loss is 4.128623008728027\n",
      "for 10 epochs, loss is 0.03657934069633484 and val_loss is 4.133559226989746\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.9719), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3226), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9743), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3196), \"['a', 'x', 'a', 'x']\": np.float64(-9.3024), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3226), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0115), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3227), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0115), \"['a', 'x', 'a', 'y']\": np.float64(0.3345), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.9528), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3439), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.9765), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3197), \"['a', 'x', 'b', 'x']\": np.float64(-9.3049), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3196), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0114), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3197), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0114), \"['a', 'x', 'b', 'y']\": np.float64(0.3313), \"['a', 'x']\": np.float64(-9.645), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3211), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0115), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3212), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0114), \"['a', 'y', 'a', 'x']\": np.float64(-0.3329), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0115), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0115), \"['a', 'y', 'a', 'y']\": np.float64(0.0119), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3204), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0123), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3213), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0114), \"['a', 'y', 'b', 'x']\": np.float64(-0.333), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0114), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0114), \"['a', 'y', 'b', 'y']\": np.float64(0.0118), \"['a', 'y']\": np.float64(0.3452), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.9524), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3219), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9548), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3189), \"['b', 'x', 'a', 'x']\": np.float64(-9.2822), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3439), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0123), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.344), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0122), \"['b', 'x', 'a', 'y']\": np.float64(0.3566), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.9546), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.344), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.9784), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3198), \"['b', 'x', 'b', 'x']\": np.float64(-9.3068), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3198), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0114), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3198), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0114), \"['b', 'x', 'b', 'y']\": np.float64(0.3315), \"['b', 'x']\": np.float64(-9.6472), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3196), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0115), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3197), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0114), \"['b', 'y', 'a', 'x']\": np.float64(-0.3314), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0114), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0114), \"['b', 'y', 'a', 'y']\": np.float64(0.0118), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3189), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0123), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3197), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0114), \"['b', 'y', 'b', 'x']\": np.float64(-0.3314), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0114), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0114), \"['b', 'y', 'b', 'y']\": np.float64(0.0118), \"['b', 'y']\": np.float64(0.3435)}\n",
      "It s train loss bro [0.04241679236292839, 0.04129345715045929, 0.04071323201060295, 0.040148474276065826, 0.03959863632917404, 0.03906305506825447, 0.03854118287563324, 0.03803223371505737, 0.03753599897027016, 0.03705192729830742, 0.03657934069633484]\n",
      "% good predict : 0\n",
      "\u001b[0;34miteration 149 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 3.160137414932251 and val_loss is 4.133460998535156\n",
      "for 1 epochs, loss is 3.3680036067962646 and val_loss is 4.132404804229736\n",
      "for 2 epochs, loss is 3.363584280014038 and val_loss is 4.130495071411133\n",
      "for 3 epochs, loss is 3.3566699028015137 and val_loss is 4.127825736999512\n",
      "for 4 epochs, loss is 3.347536563873291 and val_loss is 4.124483108520508\n",
      "for 5 epochs, loss is 3.3364341259002686 and val_loss is 4.120543003082275\n",
      "for 6 epochs, loss is 3.323589563369751 and val_loss is 4.116075038909912\n",
      "for 7 epochs, loss is 3.3092057704925537 and val_loss is 4.11113977432251\n",
      "for 8 epochs, loss is 3.2934682369232178 and val_loss is 4.105797290802002\n",
      "for 9 epochs, loss is 3.2765419483184814 and val_loss is 4.100095748901367\n",
      "for 10 epochs, loss is 3.258575916290283 and val_loss is 4.094081878662109\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8383), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3627), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.8399), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3604), \"['a', 'x', 'a', 'x']\": np.float64(-9.2096), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3627), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0148), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3628), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0148), \"['a', 'x', 'a', 'y']\": np.float64(0.3779), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.8236), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3791), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8413), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3606), \"['a', 'x', 'b', 'x']\": np.float64(-9.2112), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3605), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0147), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3605), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0147), \"['a', 'x', 'b', 'y']\": np.float64(0.3756), \"['a', 'x']\": np.float64(-9.5964), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3619), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0148), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3619), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0148), \"['a', 'y', 'a', 'x']\": np.float64(-0.3771), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0148), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0148), \"['a', 'y', 'a', 'y']\": np.float64(0.0154), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3613), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0155), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.362), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0148), \"['a', 'y', 'b', 'x']\": np.float64(-0.3771), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0147), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0147), \"['a', 'y', 'b', 'y']\": np.float64(0.0154), \"['a', 'y']\": np.float64(0.3929), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.824), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3621), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.8255), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3599), \"['b', 'x', 'a', 'x']\": np.float64(-9.1946), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3791), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0155), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3792), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0154), \"['b', 'x', 'a', 'y']\": np.float64(0.395), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.8254), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3791), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8431), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3607), \"['b', 'x', 'b', 'x']\": np.float64(-9.2131), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3606), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0147), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3607), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0147), \"['b', 'x', 'b', 'y']\": np.float64(0.3757), \"['b', 'x']\": np.float64(-9.5985), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3605), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0148), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3606), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0147), \"['b', 'y', 'a', 'x']\": np.float64(-0.3757), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0147), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0147), \"['b', 'y', 'a', 'y']\": np.float64(0.0153), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3599), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0155), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3606), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0147), \"['b', 'y', 'b', 'x']\": np.float64(-0.3757), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0147), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0147), \"['b', 'y', 'b', 'y']\": np.float64(0.0153), \"['b', 'y']\": np.float64(0.3914)}\n",
      "It s train loss bro [3.160137414932251, 3.3680036067962646, 3.363584280014038, 3.3566699028015137, 3.347536563873291, 3.3364341259002686, 3.323589563369751, 3.3092057704925537, 3.2934682369232178, 3.2765419483184814, 3.258575916290283]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 150 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.05336126685142517 and val_loss is 4.04092264175415\n",
      "for 1 epochs, loss is 0.04857824370265007 and val_loss is 3.9942538738250732\n",
      "for 2 epochs, loss is 0.05629252642393112 and val_loss is 3.953394651412964\n",
      "for 3 epochs, loss is 0.0640161857008934 and val_loss is 3.917719841003418\n",
      "for 4 epochs, loss is 0.07160031050443649 and val_loss is 3.886662483215332\n",
      "for 5 epochs, loss is 0.0789140909910202 and val_loss is 3.8597075939178467\n",
      "for 6 epochs, loss is 0.08584792912006378 and val_loss is 3.8363964557647705\n",
      "for 7 epochs, loss is 0.09231497347354889 and val_loss is 3.816316843032837\n",
      "for 8 epochs, loss is 0.0982489362359047 and val_loss is 3.799100875854492\n",
      "for 9 epochs, loss is 0.10360486060380936 and val_loss is 3.7844252586364746\n",
      "for 10 epochs, loss is 0.10835488885641098 and val_loss is 3.7719993591308594\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.1323), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8364), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.1322), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8362), \"['a', 'x', 'a', 'x']\": np.float64(-7.9847), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8369), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0974), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8362), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0982), \"['a', 'x', 'a', 'y']\": np.float64(0.9363), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.1278), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8413), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.133), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8351), \"['a', 'x', 'b', 'x']\": np.float64(-7.9846), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8367), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0975), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8362), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.098), \"['a', 'x', 'b', 'y']\": np.float64(0.9362), \"['a', 'x']\": np.float64(-8.939), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8295), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0973), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8295), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0973), \"['a', 'y', 'a', 'x']\": np.float64(-0.9286), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0966), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0112), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0966), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0113), \"['a', 'y', 'a', 'y']\": np.float64(0.1081), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8283), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0978), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8289), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.097), \"['a', 'y', 'b', 'x']\": np.float64(-0.9279), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0974), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0113), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0974), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0114), \"['a', 'y', 'b', 'y']\": np.float64(0.109), \"['a', 'y']\": np.float64(1.039), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.1225), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8352), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.1224), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8351), \"['b', 'x', 'a', 'x']\": np.float64(-7.9737), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8412), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0979), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8405), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0987), \"['b', 'x', 'a', 'y']\": np.float64(0.9412), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.1234), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8408), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.1286), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8345), \"['b', 'x', 'b', 'x']\": np.float64(-7.9796), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8349), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0973), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8345), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0978), \"['b', 'x', 'b', 'y']\": np.float64(0.9342), \"['b', 'x']\": np.float64(-8.9322), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8359), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.098), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8359), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.098), \"['b', 'y', 'a', 'x']\": np.float64(-0.9358), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0975), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0113), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0974), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0114), \"['b', 'y', 'a', 'y']\": np.float64(0.109), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.835), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0986), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8356), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0978), \"['b', 'y', 'b', 'x']\": np.float64(-0.9353), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.098), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0114), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0979), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0115), \"['b', 'y', 'b', 'y']\": np.float64(0.1096), \"['b', 'y']\": np.float64(1.0471)}\n",
      "It s train loss bro [0.05336126685142517, 0.04857824370265007, 0.05629252642393112, 0.0640161857008934, 0.07160031050443649, 0.0789140909910202, 0.08584792912006378, 0.09231497347354889, 0.0982489362359047, 0.10360486060380936, 0.10835488885641098]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 151 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1173565462231636 and val_loss is 3.761610746383667\n",
      "for 1 epochs, loss is 0.11600274592638016 and val_loss is 3.7529900074005127\n",
      "for 2 epochs, loss is 0.11891578137874603 and val_loss is 3.7459423542022705\n",
      "for 3 epochs, loss is 0.12124956399202347 and val_loss is 3.7402915954589844\n",
      "for 4 epochs, loss is 0.12303309142589569 and val_loss is 3.7358875274658203\n",
      "for 5 epochs, loss is 0.12430077791213989 and val_loss is 3.732595205307007\n",
      "for 6 epochs, loss is 0.12509045004844666 and val_loss is 3.730294704437256\n",
      "for 7 epochs, loss is 0.1254420429468155 and val_loss is 3.7288825511932373\n",
      "for 8 epochs, loss is 0.1253965198993683 and val_loss is 3.7282660007476807\n",
      "for 9 epochs, loss is 0.12499430030584335 and val_loss is 3.7283637523651123\n",
      "for 10 epochs, loss is 0.12427572160959244 and val_loss is 3.7291038036346436\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.9025), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8933), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.9029), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8926), \"['a', 'x', 'a', 'x']\": np.float64(-7.8131), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8941), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1146), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8932), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1157), \"['a', 'x', 'a', 'y']\": np.float64(1.0112), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.8979), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.899), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.9047), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8912), \"['a', 'x', 'b', 'x']\": np.float64(-7.8136), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8933), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1147), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8927), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1154), \"['a', 'x', 'b', 'y']\": np.float64(1.0104), \"['a', 'x']\": np.float64(-8.8439), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8841), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1144), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8841), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1143), \"['a', 'y', 'a', 'x']\": np.float64(-1.0007), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1135), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0145), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1133), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0147), \"['a', 'y', 'a', 'y']\": np.float64(0.1283), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8825), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.115), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8833), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.114), \"['a', 'y', 'b', 'x']\": np.float64(-0.9996), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1145), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0147), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1144), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0148), \"['a', 'y', 'b', 'y']\": np.float64(0.1295), \"['a', 'y']\": np.float64(1.1317), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.8904), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8917), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.8908), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.891), \"['b', 'x', 'a', 'x']\": np.float64(-7.7994), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8989), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1153), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8979), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1164), \"['b', 'x', 'a', 'y']\": np.float64(1.0165), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.8921), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8983), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.8989), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8905), \"['b', 'x', 'b', 'x']\": np.float64(-7.8071), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8909), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1144), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8903), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1151), \"['b', 'x', 'b', 'y']\": np.float64(1.0077), \"['b', 'x']\": np.float64(-8.8349), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8923), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1155), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8923), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1154), \"['b', 'y', 'a', 'x']\": np.float64(-1.01), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1146), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0147), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1145), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0148), \"['b', 'y', 'a', 'y']\": np.float64(0.1296), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.891), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1161), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8919), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1151), \"['b', 'y', 'b', 'x']\": np.float64(-1.0093), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1153), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0148), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1153), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0149), \"['b', 'y', 'b', 'y']\": np.float64(0.1305), \"['b', 'y']\": np.float64(1.1423)}\n",
      "It s train loss bro [0.1173565462231636, 0.11600274592638016, 0.11891578137874603, 0.12124956399202347, 0.12303309142589569, 0.12430077791213989, 0.12509045004844666, 0.1254420429468155, 0.1253965198993683, 0.12499430030584335, 0.12427572160959244]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 152 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1361851692199707 and val_loss is 3.730494499206543\n",
      "for 1 epochs, loss is 0.12202782183885574 and val_loss is 3.7323999404907227\n",
      "for 2 epochs, loss is 0.1205710768699646 and val_loss is 3.734766721725464\n",
      "for 3 epochs, loss is 0.11894012987613678 and val_loss is 3.7375521659851074\n",
      "for 4 epochs, loss is 0.11716412007808685 and val_loss is 3.740713357925415\n",
      "for 5 epochs, loss is 0.11526980996131897 and val_loss is 3.744215726852417\n",
      "for 6 epochs, loss is 0.11328089982271194 and val_loss is 3.748023748397827\n",
      "for 7 epochs, loss is 0.11121901869773865 and val_loss is 3.752110481262207\n",
      "for 8 epochs, loss is 0.10910327732563019 and val_loss is 3.7564451694488525\n",
      "for 9 epochs, loss is 0.10695049166679382 and val_loss is 3.7610058784484863\n",
      "for 10 epochs, loss is 0.10477534681558609 and val_loss is 3.7657670974731445\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.3406), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7832), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.3435), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7796), \"['a', 'x', 'a', 'x']\": np.float64(-8.14), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7839), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0827), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7834), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0833), \"['a', 'x', 'a', 'y']\": np.float64(0.8685), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.3373), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7904), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.3474), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7788), \"['a', 'x', 'b', 'x']\": np.float64(-8.1433), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7803), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0823), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7799), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0828), \"['a', 'x', 'b', 'y']\": np.float64(0.8645), \"['a', 'x']\": np.float64(-9.0266), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.774), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0826), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7743), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0822), \"['a', 'y', 'a', 'x']\": np.float64(-0.8582), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0817), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0086), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0816), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0087), \"['a', 'y', 'a', 'y']\": np.float64(0.0905), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7728), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0832), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7738), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.082), \"['a', 'y', 'b', 'x']\": np.float64(-0.8576), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0824), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0087), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0823), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0087), \"['a', 'y', 'b', 'y']\": np.float64(0.0912), \"['a', 'y']\": np.float64(0.9508), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.3297), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.782), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.3327), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7785), \"['b', 'x', 'a', 'x']\": np.float64(-8.128), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7904), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0833), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7898), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.084), \"['b', 'x', 'a', 'y']\": np.float64(0.8756), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.3336), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.79), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.3437), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7784), \"['b', 'x', 'b', 'x']\": np.float64(-8.1392), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7787), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0822), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7783), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0826), \"['b', 'x', 'b', 'y']\": np.float64(0.8628), \"['b', 'x']\": np.float64(-9.0209), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7798), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0832), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7801), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0828), \"['b', 'y', 'a', 'x']\": np.float64(-0.8648), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0824), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0087), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0823), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0088), \"['b', 'y', 'a', 'y']\": np.float64(0.0912), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7788), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0839), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7799), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0827), \"['b', 'y', 'b', 'x']\": np.float64(-0.8644), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0828), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0087), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0828), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0088), \"['b', 'y', 'b', 'y']\": np.float64(0.0918), \"['b', 'y']\": np.float64(0.9581)}\n",
      "It s train loss bro [0.1361851692199707, 0.12202782183885574, 0.1205710768699646, 0.11894012987613678, 0.11716412007808685, 0.11526980996131897, 0.11328089982271194, 0.11121901869773865, 0.10910327732563019, 0.10695049166679382, 0.10477534681558609]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 153 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.11861640959978104 and val_loss is 3.770848035812378\n",
      "for 1 epochs, loss is 0.10039377957582474 and val_loss is 3.7760777473449707\n",
      "for 2 epochs, loss is 0.09820971637964249 and val_loss is 3.7814369201660156\n",
      "for 3 epochs, loss is 0.09604599326848984 and val_loss is 3.7869129180908203\n",
      "for 4 epochs, loss is 0.09390948712825775 and val_loss is 3.7924911975860596\n",
      "for 5 epochs, loss is 0.09180527180433273 and val_loss is 3.7981574535369873\n",
      "for 6 epochs, loss is 0.08973781764507294 and val_loss is 3.8038992881774902\n",
      "for 7 epochs, loss is 0.08771052211523056 and val_loss is 3.809706926345825\n",
      "for 8 epochs, loss is 0.08572626858949661 and val_loss is 3.8155689239501953\n",
      "for 9 epochs, loss is 0.08378686010837555 and val_loss is 3.8214757442474365\n",
      "for 10 epochs, loss is 0.08189420402050018 and val_loss is 3.827418565750122\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.8488), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6487), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.8553), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6411), \"['a', 'x', 'a', 'x']\": np.float64(-8.5118), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6494), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0527), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6491), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0531), \"['a', 'x', 'a', 'y']\": np.float64(0.7035), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.8456), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6596), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.8624), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6409), \"['a', 'x', 'b', 'x']\": np.float64(-8.5188), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6418), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0521), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6416), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0524), \"['a', 'x', 'b', 'y']\": np.float64(0.6952), \"['a', 'x']\": np.float64(-9.2308), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6379), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0527), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6385), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0521), \"['a', 'y', 'a', 'x']\": np.float64(-0.6918), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0518), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0042), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0518), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0042), \"['a', 'y', 'a', 'y']\": np.float64(0.0562), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6369), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0535), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6383), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.052), \"['a', 'y', 'b', 'x']\": np.float64(-0.6915), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0522), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0042), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0522), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0043), \"['a', 'y', 'b', 'y']\": np.float64(0.0566), \"['a', 'y']\": np.float64(0.7494), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.8366), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6477), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.8431), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6401), \"['b', 'x', 'a', 'x']\": np.float64(-8.4985), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6596), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0535), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6593), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0539), \"['b', 'x', 'a', 'y']\": np.float64(0.7145), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.8437), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6595), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.8604), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6407), \"['b', 'x', 'b', 'x']\": np.float64(-8.5167), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6408), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.052), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6406), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0523), \"['b', 'x', 'b', 'y']\": np.float64(0.6942), \"['b', 'x']\": np.float64(-9.2278), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6417), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.053), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6422), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0524), \"['b', 'y', 'a', 'x']\": np.float64(-0.6959), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0522), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0042), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0522), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0043), \"['b', 'y', 'a', 'y']\": np.float64(0.0565), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6407), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0539), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6421), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0523), \"['b', 'y', 'b', 'x']\": np.float64(-0.6957), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0524), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0043), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0524), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0043), \"['b', 'y', 'b', 'y']\": np.float64(0.0568), \"['b', 'y']\": np.float64(0.7539)}\n",
      "It s train loss bro [0.11861640959978104, 0.10039377957582474, 0.09820971637964249, 0.09604599326848984, 0.09390948712825775, 0.09180527180433273, 0.08973781764507294, 0.08771052211523056, 0.08572626858949661, 0.08378686010837555, 0.08189420402050018]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 154 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.08709827810525894 and val_loss is 3.833460569381714\n",
      "for 1 epochs, loss is 0.07825002819299698 and val_loss is 3.8395140171051025\n",
      "for 2 epochs, loss is 0.07649992406368256 and val_loss is 3.845574378967285\n",
      "for 3 epochs, loss is 0.07479829341173172 and val_loss is 3.8516345024108887\n",
      "for 4 epochs, loss is 0.07314474135637283 and val_loss is 3.8576881885528564\n",
      "for 5 epochs, loss is 0.07153850048780441 and val_loss is 3.8637309074401855\n",
      "for 6 epochs, loss is 0.06997880339622498 and val_loss is 3.869758367538452\n",
      "for 7 epochs, loss is 0.06846487522125244 and val_loss is 3.875765323638916\n",
      "for 8 epochs, loss is 0.06699569523334503 and val_loss is 3.881747245788574\n",
      "for 9 epochs, loss is 0.06557013094425201 and val_loss is 3.887702465057373\n",
      "for 10 epochs, loss is 0.06418691575527191 and val_loss is 3.893625020980835\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.254), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5376), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.2665), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5236), \"['a', 'x', 'a', 'x']\": np.float64(-8.8038), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5385), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0339), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5384), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0342), \"['a', 'x', 'a', 'y']\": np.float64(0.5734), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.2486), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5571), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.2794), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5238), \"['a', 'x', 'b', 'x']\": np.float64(-8.8172), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5244), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0331), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5243), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0332), \"['a', 'x', 'b', 'y']\": np.float64(0.5584), \"['a', 'x']\": np.float64(-9.3903), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5217), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.034), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5224), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0331), \"['a', 'y', 'a', 'x']\": np.float64(-0.5564), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0329), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0021), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0329), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0021), \"['a', 'y', 'a', 'y']\": np.float64(0.0351), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5204), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0351), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5223), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.033), \"['a', 'y', 'b', 'x']\": np.float64(-0.5562), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0332), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0021), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0332), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0021), \"['a', 'y', 'b', 'y']\": np.float64(0.0353), \"['a', 'y']\": np.float64(0.5925), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.2345), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5363), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.247), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5223), \"['b', 'x', 'a', 'x']\": np.float64(-8.7831), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5571), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0351), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5569), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0354), \"['b', 'x', 'a', 'y']\": np.float64(0.5932), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.2474), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.557), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.2783), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5237), \"['b', 'x', 'b', 'x']\": np.float64(-8.816), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5238), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.033), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5237), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0332), \"['b', 'x', 'b', 'y']\": np.float64(0.5577), \"['b', 'x']\": np.float64(-9.3886), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5243), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0342), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5251), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0333), \"['b', 'y', 'a', 'x']\": np.float64(-0.5593), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0331), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0021), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0331), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0021), \"['b', 'y', 'a', 'y']\": np.float64(0.0353), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5231), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0353), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5251), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0332), \"['b', 'y', 'b', 'x']\": np.float64(-0.5592), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0333), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0021), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0333), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0021), \"['b', 'y', 'b', 'y']\": np.float64(0.0354), \"['b', 'y']\": np.float64(0.5955)}\n",
      "It s train loss bro [0.08709827810525894, 0.07825002819299698, 0.07649992406368256, 0.07479829341173172, 0.07314474135637283, 0.07153850048780441, 0.06997880339622498, 0.06846487522125244, 0.06699569523334503, 0.06557013094425201, 0.06418691575527191]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 155 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.08327475935220718 and val_loss is 3.8996589183807373\n",
      "for 1 epochs, loss is 0.06151439622044563 and val_loss is 3.905642509460449\n",
      "for 2 epochs, loss is 0.06022624298930168 and val_loss is 3.9115748405456543\n",
      "for 3 epochs, loss is 0.05897868424654007 and val_loss is 3.917454242706299\n",
      "for 4 epochs, loss is 0.05777030065655708 and val_loss is 3.923279285430908\n",
      "for 5 epochs, loss is 0.056599654257297516 and val_loss is 3.9290499687194824\n",
      "for 6 epochs, loss is 0.05546519532799721 and val_loss is 3.9347667694091797\n",
      "for 7 epochs, loss is 0.05436546355485916 and val_loss is 3.940425395965576\n",
      "for 8 epochs, loss is 0.05329899117350578 and val_loss is 3.9460296630859375\n",
      "for 9 epochs, loss is 0.05226464569568634 and val_loss is 3.9515764713287354\n",
      "for 10 epochs, loss is 0.05126094073057175 and val_loss is 3.957066774368286\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.5492), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4566), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5732), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4308), \"['a', 'x', 'a', 'x']\": np.float64(-9.0163), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.458), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0229), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4579), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0231), \"['a', 'x', 'a', 'y']\": np.float64(0.4816), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.5369), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4947), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.5976), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4315), \"['a', 'x', 'b', 'x']\": np.float64(-9.0416), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4321), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0216), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.432), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0217), \"['a', 'x', 'b', 'y']\": np.float64(0.4543), \"['a', 'x']\": np.float64(-9.509), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4293), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0229), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4305), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0216), \"['a', 'y', 'a', 'x']\": np.float64(-0.4527), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0215), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0011), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0215), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0011), \"['a', 'y', 'a', 'y']\": np.float64(0.0226), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4273), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0248), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4304), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0216), \"['a', 'y', 'b', 'x']\": np.float64(-0.4526), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0217), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0011), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0217), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0011), \"['a', 'y', 'b', 'y']\": np.float64(0.0228), \"['a', 'y']\": np.float64(0.4761), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.5118), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4546), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5357), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4289), \"['b', 'x', 'a', 'x']\": np.float64(-8.9768), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4947), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0247), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4946), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0249), \"['b', 'x', 'a', 'y']\": np.float64(0.5202), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.536), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4946), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.5967), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4315), \"['b', 'x', 'b', 'x']\": np.float64(-9.0407), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4315), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0216), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4314), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0217), \"['b', 'x', 'b', 'y']\": np.float64(0.4538), \"['b', 'x']\": np.float64(-9.5076), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4316), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0231), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4328), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0217), \"['b', 'y', 'a', 'x']\": np.float64(-0.4551), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0217), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0011), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0216), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0011), \"['b', 'y', 'a', 'y']\": np.float64(0.0228), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4297), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0249), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4327), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0217), \"['b', 'y', 'b', 'x']\": np.float64(-0.4551), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0218), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0011), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0218), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0011), \"['b', 'y', 'b', 'y']\": np.float64(0.0229), \"['b', 'y']\": np.float64(0.4786)}\n",
      "It s train loss bro [0.08327475935220718, 0.06151439622044563, 0.06022624298930168, 0.05897868424654007, 0.05777030065655708, 0.056599654257297516, 0.05546519532799721, 0.05436546355485916, 0.05329899117350578, 0.05226464569568634, 0.05126094073057175]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 156 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.051860544830560684 and val_loss is 3.962517738342285\n",
      "for 1 epochs, loss is 0.049340032041072845 and val_loss is 3.967910051345825\n",
      "for 2 epochs, loss is 0.04842061549425125 and val_loss is 3.973243474960327\n",
      "for 3 epochs, loss is 0.047527290880680084 and val_loss is 3.9785189628601074\n",
      "for 4 epochs, loss is 0.046659111976623535 and val_loss is 3.983736276626587\n",
      "for 5 epochs, loss is 0.045815229415893555 and val_loss is 3.9888949394226074\n",
      "for 6 epochs, loss is 0.044994451105594635 and val_loss is 3.9939963817596436\n",
      "for 7 epochs, loss is 0.04419592767953873 and val_loss is 3.999041795730591\n",
      "for 8 epochs, loss is 0.04341890662908554 and val_loss is 4.004029750823975\n",
      "for 9 epochs, loss is 0.04266241565346718 and val_loss is 4.0089616775512695\n",
      "for 10 epochs, loss is 0.04192570224404335 and val_loss is 4.013838768005371\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.7471), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4055), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.7908), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3597), \"['a', 'x', 'a', 'x']\": np.float64(-9.1616), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4075), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0166), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4074), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0167), \"['a', 'x', 'a', 'y']\": np.float64(0.4247), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7184), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4797), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8352), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.361), \"['a', 'x', 'b', 'x']\": np.float64(-9.2075), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3616), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0147), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3615), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0148), \"['a', 'x', 'b', 'y']\": np.float64(0.3768), \"['a', 'x']\": np.float64(-9.5958), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3582), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0166), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.36), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0147), \"['a', 'y', 'a', 'x']\": np.float64(-0.3751), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0146), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0146), \"['a', 'y', 'a', 'y']\": np.float64(0.0153), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3551), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0195), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3599), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0147), \"['a', 'y', 'b', 'x']\": np.float64(-0.375), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0148), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0148), \"['a', 'y', 'b', 'y']\": np.float64(0.0154), \"['a', 'y']\": np.float64(0.3909), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6736), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4021), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.7171), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3567), \"['b', 'x', 'a', 'x']\": np.float64(-9.0847), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4797), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0195), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4796), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0197), \"['b', 'x', 'a', 'y']\": np.float64(0.4998), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7174), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4796), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8342), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.361), \"['b', 'x', 'b', 'x']\": np.float64(-9.2065), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.361), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0147), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.361), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0148), \"['b', 'x', 'b', 'y']\": np.float64(0.3762), \"['b', 'x']\": np.float64(-9.5944), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3604), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0167), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3622), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0148), \"['b', 'y', 'a', 'x']\": np.float64(-0.3775), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0147), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0147), \"['b', 'y', 'a', 'y']\": np.float64(0.0154), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3574), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0197), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3622), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0148), \"['b', 'y', 'b', 'x']\": np.float64(-0.3774), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0148), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0148), \"['b', 'y', 'b', 'y']\": np.float64(0.0155), \"['b', 'y']\": np.float64(0.3934)}\n",
      "It s train loss bro [0.051860544830560684, 0.049340032041072845, 0.04842061549425125, 0.047527290880680084, 0.046659111976623535, 0.045815229415893555, 0.044994451105594635, 0.04419592767953873, 0.04341890662908554, 0.04266241565346718, 0.04192570224404335]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 157 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.05074460059404373 and val_loss is 4.018775463104248\n",
      "for 1 epochs, loss is 0.040498036891222 and val_loss is 4.023646831512451\n",
      "for 2 epochs, loss is 0.03980680927634239 and val_loss is 4.028454780578613\n",
      "for 3 epochs, loss is 0.03913378715515137 and val_loss is 4.033200740814209\n",
      "for 4 epochs, loss is 0.03847820684313774 and val_loss is 4.037886619567871\n",
      "for 5 epochs, loss is 0.037839073687791824 and val_loss is 4.042513847351074\n",
      "for 6 epochs, loss is 0.0372161827981472 and val_loss is 4.047083377838135\n",
      "for 7 epochs, loss is 0.036608416587114334 and val_loss is 4.051597595214844\n",
      "for 8 epochs, loss is 0.036015577614307404 and val_loss is 4.056055545806885\n",
      "for 9 epochs, loss is 0.03543688356876373 and val_loss is 4.060461521148682\n",
      "for 10 epochs, loss is 0.03487178683280945 and val_loss is 4.064814567565918\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8632), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3826), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.941), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3032), \"['a', 'x', 'a', 'x']\": np.float64(-9.2542), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.386), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.013), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3859), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0132), \"['a', 'x', 'a', 'y']\": np.float64(0.3995), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7952), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5297), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.0197), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3054), \"['a', 'x', 'b', 'x']\": np.float64(-9.3353), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3059), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0103), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3058), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0104), \"['a', 'x', 'b', 'y']\": np.float64(0.3166), \"['a', 'x']\": np.float64(-9.6624), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3015), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.013), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3042), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0103), \"['a', 'y', 'a', 'x']\": np.float64(-0.3148), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0102), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0102), \"['a', 'y', 'a', 'y']\": np.float64(0.0106), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2965), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0179), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3041), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0103), \"['a', 'y', 'b', 'x']\": np.float64(-0.3147), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0104), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0104), \"['a', 'y', 'b', 'y']\": np.float64(0.0107), \"['a', 'y']\": np.float64(0.3258), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.7173), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3763), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.7938), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2982), \"['b', 'x', 'a', 'x']\": np.float64(-9.1018), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5297), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0178), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5296), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.018), \"['b', 'x', 'a', 'y']\": np.float64(0.5482), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7941), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5297), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.0187), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3053), \"['b', 'x', 'b', 'x']\": np.float64(-9.3342), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3054), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0103), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3053), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0104), \"['b', 'x', 'b', 'y']\": np.float64(0.316), \"['b', 'x']\": np.float64(-9.6608), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3038), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0131), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3065), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0104), \"['b', 'y', 'a', 'x']\": np.float64(-0.3172), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0103), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0103), \"['b', 'y', 'a', 'y']\": np.float64(0.0107), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2988), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.018), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3064), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0104), \"['b', 'y', 'b', 'x']\": np.float64(-0.3171), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0104), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0104), \"['b', 'y', 'b', 'y']\": np.float64(0.0108), \"['b', 'y']\": np.float64(0.3282)}\n",
      "It s train loss bro [0.05074460059404373, 0.040498036891222, 0.03980680927634239, 0.03913378715515137, 0.03847820684313774, 0.037839073687791824, 0.0372161827981472, 0.036608416587114334, 0.036015577614307404, 0.03543688356876373, 0.03487178683280945]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 158 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.03970152512192726 and val_loss is 4.069200038909912\n",
      "for 1 epochs, loss is 0.0337776243686676 and val_loss is 4.073526859283447\n",
      "for 2 epochs, loss is 0.03324791043996811 and val_loss is 4.077796459197998\n",
      "for 3 epochs, loss is 0.032730262726545334 and val_loss is 4.0820136070251465\n",
      "for 4 epochs, loss is 0.032224349677562714 and val_loss is 4.086176872253418\n",
      "for 5 epochs, loss is 0.031729843467473984 and val_loss is 4.090290546417236\n",
      "for 6 epochs, loss is 0.031246071681380272 and val_loss is 4.094354629516602\n",
      "for 7 epochs, loss is 0.03077293001115322 and val_loss is 4.098369121551514\n",
      "for 8 epochs, loss is 0.03030974231660366 and val_loss is 4.10233736038208\n",
      "for 9 epochs, loss is 0.029856406152248383 and val_loss is 4.106260299682617\n",
      "for 10 epochs, loss is 0.029412588104605675 and val_loss is 4.1101393699646\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8906), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3945), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.0268), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2574), \"['a', 'x', 'a', 'x']\": np.float64(-9.2933), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4006), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0113), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4005), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0115), \"['a', 'x', 'a', 'y']\": np.float64(0.4123), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.5828), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8332), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.1655), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2608), \"['a', 'x', 'b', 'x']\": np.float64(-9.4357), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2614), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0074), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2613), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0075), \"['a', 'x', 'b', 'y']\": np.float64(0.269), \"['a', 'x']\": np.float64(-9.7143), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2556), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0113), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2595), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0074), \"['a', 'y', 'a', 'x']\": np.float64(-0.2671), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0073), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0073), \"['a', 'y', 'a', 'y']\": np.float64(0.0076), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2429), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0236), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2594), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0074), \"['a', 'y', 'b', 'x']\": np.float64(-0.2671), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0074), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0074), \"['a', 'y', 'b', 'y']\": np.float64(0.0077), \"['a', 'y']\": np.float64(0.275), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.4519), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.375), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5814), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2447), \"['b', 'x', 'a', 'x']\": np.float64(-8.8347), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8331), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0236), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8329), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0239), \"['b', 'x', 'a', 'y']\": np.float64(0.8576), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.5817), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.833), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.1643), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2608), \"['b', 'x', 'b', 'x']\": np.float64(-9.4345), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2608), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0074), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2608), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0075), \"['b', 'x', 'b', 'y']\": np.float64(0.2685), \"['b', 'x']\": np.float64(-9.7126), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2579), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0114), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2618), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0075), \"['b', 'y', 'a', 'x']\": np.float64(-0.2696), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0074), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0074), \"['b', 'y', 'a', 'y']\": np.float64(0.0076), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2451), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0238), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2618), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0075), \"['b', 'y', 'b', 'x']\": np.float64(-0.2695), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0075), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0075), \"['b', 'y', 'b', 'y']\": np.float64(0.0077), \"['b', 'y']\": np.float64(0.2775)}\n",
      "It s train loss bro [0.03970152512192726, 0.0337776243686676, 0.03324791043996811, 0.032730262726545334, 0.032224349677562714, 0.031729843467473984, 0.031246071681380272, 0.03077293001115322, 0.03030974231660366, 0.029856406152248383, 0.029412588104605675]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 159 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.04012167453765869 and val_loss is 4.114115238189697\n",
      "for 1 epochs, loss is 0.028537798672914505 and val_loss is 4.11803674697876\n",
      "for 2 epochs, loss is 0.02810811437666416 and val_loss is 4.121903896331787\n",
      "for 3 epochs, loss is 0.027687985450029373 and val_loss is 4.125720977783203\n",
      "for 4 epochs, loss is 0.027277424931526184 and val_loss is 4.129489898681641\n",
      "for 5 epochs, loss is 0.026875630021095276 and val_loss is 4.133211612701416\n",
      "for 6 epochs, loss is 0.026482613757252693 and val_loss is 4.1368865966796875\n",
      "for 7 epochs, loss is 0.026097921654582024 and val_loss is 4.140519142150879\n",
      "for 8 epochs, loss is 0.025721097365021706 and val_loss is 4.14410924911499\n",
      "for 9 epochs, loss is 0.025352034717798233 and val_loss is 4.1476593017578125\n",
      "for 10 epochs, loss is 0.024990392848849297 and val_loss is 4.1511688232421875\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.7763), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4675), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.0272), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.218), \"['a', 'x', 'a', 'x']\": np.float64(-9.2535), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.481), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0115), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4808), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0117), \"['a', 'x', 'a', 'y']\": np.float64(0.4929), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.0063), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(4.0015), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.2857), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2237), \"['a', 'x', 'b', 'x']\": np.float64(-9.5181), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2243), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0054), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2242), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0054), \"['a', 'x', 'b', 'y']\": np.float64(0.2299), \"['a', 'x']\": np.float64(-9.7567), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2161), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0115), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2223), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0054), \"['a', 'y', 'a', 'x']\": np.float64(-0.2278), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0053), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0053), \"['a', 'y', 'a', 'y']\": np.float64(0.0054), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1198), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0957), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2222), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0054), \"['a', 'y', 'b', 'x']\": np.float64(-0.2278), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0054), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0054), \"['a', 'y', 'b', 'y']\": np.float64(0.0055), \"['a', 'y']\": np.float64(0.2335), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.8662), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.2592), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0054), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1209), \"['b', 'x', 'a', 'x']\": np.float64(-5.1309), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-4.0014), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0957), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-4.0002), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0973), \"['b', 'x', 'a', 'y']\": np.float64(4.101), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.0056), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(4.0009), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.2844), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2237), \"['b', 'x', 'b', 'x']\": np.float64(-9.5168), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2237), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0054), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2237), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0054), \"['b', 'x', 'b', 'y']\": np.float64(0.2293), \"['b', 'x']\": np.float64(-9.7549), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2184), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0116), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2246), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0054), \"['b', 'y', 'a', 'x']\": np.float64(-0.2303), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0054), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0054), \"['b', 'y', 'a', 'y']\": np.float64(0.0055), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.1211), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0968), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2246), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0054), \"['b', 'y', 'b', 'x']\": np.float64(-0.2302), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0054), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0054), \"['b', 'y', 'b', 'y']\": np.float64(0.0056), \"['b', 'y']\": np.float64(0.236)}\n",
      "It s train loss bro [0.04012167453765869, 0.028537798672914505, 0.02810811437666416, 0.027687985450029373, 0.027277424931526184, 0.026875630021095276, 0.026482613757252693, 0.026097921654582024, 0.025721097365021706, 0.025352034717798233, 0.024990392848849297]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 160 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.02726559340953827 and val_loss is 4.154672622680664\n",
      "for 1 epochs, loss is 0.024286730214953423 and val_loss is 4.158135890960693\n",
      "for 2 epochs, loss is 0.023944489657878876 and val_loss is 4.16156005859375\n",
      "for 3 epochs, loss is 0.02360888384282589 and val_loss is 4.164947986602783\n",
      "for 4 epochs, loss is 0.023279689252376556 and val_loss is 4.168299198150635\n",
      "for 5 epochs, loss is 0.022956792265176773 and val_loss is 4.1716156005859375\n",
      "for 6 epochs, loss is 0.02263985201716423 and val_loss is 4.17489767074585\n",
      "for 7 epochs, loss is 0.022328872233629227 and val_loss is 4.178147315979004\n",
      "for 8 epochs, loss is 0.022023510187864304 and val_loss is 4.181365489959717\n",
      "for 9 epochs, loss is 0.02172377146780491 and val_loss is 4.184552192687988\n",
      "for 10 epochs, loss is 0.021429192274808884 and val_loss is 4.187709331512451\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.0253), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8182), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6777), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1792), \"['a', 'x', 'a', 'x']\": np.float64(-8.8643), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8848), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0181), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8845), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0184), \"['a', 'x', 'a', 'y']\": np.float64(0.9037), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-0.3177), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(2.9145), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.3837), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1933), \"['a', 'x', 'b', 'x']\": np.float64(-9.5849), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1938), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.004), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1938), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.004), \"['a', 'x', 'b', 'y']\": np.float64(0.1979), \"['a', 'x']\": np.float64(-9.791), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1773), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0181), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1917), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.004), \"['a', 'y', 'a', 'x']\": np.float64(-0.1958), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0039), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0039), \"['a', 'y', 'a', 'y']\": np.float64(0.004), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0065), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0595), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1916), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0039), \"['a', 'y', 'b', 'x']\": np.float64(-0.1957), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.004), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.004), \"['a', 'y', 'b', 'y']\": np.float64(0.0041), \"['a', 'y']\": np.float64(0.2), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-0.2938), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.03), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-0.3177), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0066), \"['b', 'x', 'a', 'x']\": np.float64(-0.3245), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-2.9145), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0595), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-2.9135), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0607), \"['b', 'x', 'a', 'y']\": np.float64(2.9766), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-0.3177), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(2.9141), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.3823), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1932), \"['b', 'x', 'b', 'x']\": np.float64(-9.5836), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1933), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0039), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1932), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.004), \"['b', 'x', 'b', 'y']\": np.float64(0.1974), \"['b', 'x']\": np.float64(-9.7892), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1795), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0183), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1941), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.004), \"['b', 'y', 'a', 'x']\": np.float64(-0.1982), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.004), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.004), \"['b', 'y', 'a', 'y']\": np.float64(0.004), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0066), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0603), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.194), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.004), \"['b', 'y', 'b', 'x']\": np.float64(-0.1982), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.004), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.004), \"['b', 'y', 'b', 'y']\": np.float64(0.0041), \"['b', 'y']\": np.float64(0.2025)}\n",
      "It s train loss bro [0.02726559340953827, 0.024286730214953423, 0.023944489657878876, 0.02360888384282589, 0.023279689252376556, 0.022956792265176773, 0.02263985201716423, 0.022328872233629227, 0.022023510187864304, 0.02172377146780491, 0.021429192274808884]\n",
      "% good predict : 60\n",
      "\u001b[0;34miteration 161 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.02223280817270279 and val_loss is 4.190850734710693\n",
      "for 1 epochs, loss is 0.020854836329817772 and val_loss is 4.193963050842285\n",
      "for 2 epochs, loss is 0.02057483419775963 and val_loss is 4.197045803070068\n",
      "for 3 epochs, loss is 0.020299658179283142 and val_loss is 4.200100898742676\n",
      "for 4 epochs, loss is 0.02002931386232376 and val_loss is 4.203127861022949\n",
      "for 5 epochs, loss is 0.019763456657528877 and val_loss is 4.20612907409668\n",
      "for 6 epochs, loss is 0.019502202048897743 and val_loss is 4.209103584289551\n",
      "for 7 epochs, loss is 0.019245324656367302 and val_loss is 4.212054252624512\n",
      "for 8 epochs, loss is 0.0189927089959383 and val_loss is 4.214979648590088\n",
      "for 9 epochs, loss is 0.01874435879290104 and val_loss is 4.217881202697754\n",
      "for 10 epochs, loss is 0.018500041216611862 and val_loss is 4.22075891494751\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-2.7028), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(2.2142), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.0578), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0899), \"['a', 'x', 'a', 'x']\": np.float64(-5.1516), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-4.1442), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0727), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-4.1429), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0743), \"['a', 'x', 'a', 'y']\": np.float64(4.2204), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-0.0421), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0431), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.4651), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1678), \"['a', 'x', 'b', 'x']\": np.float64(-9.6404), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1683), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.003), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1682), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.003), \"['a', 'x', 'b', 'y']\": np.float64(0.1714), \"['a', 'x']\": np.float64(-9.8193), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.0888), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0727), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1661), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.003), \"['a', 'y', 'a', 'x']\": np.float64(-0.1692), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0029), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0029), \"['a', 'y', 'a', 'y']\": np.float64(0.003), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0183), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1661), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0029), \"['a', 'y', 'b', 'x']\": np.float64(-0.1692), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.003), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.003), \"['a', 'y', 'b', 'y']\": np.float64(0.003), \"['a', 'y']\": np.float64(0.1723), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-0.0225), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0184), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-0.0421), \"['b', 'x', 'a', 'x']\": np.float64(-0.0429), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0431), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0183), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0427), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0187), \"['b', 'x', 'a', 'y']\": np.float64(1.0622), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-0.0421), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0429), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.4638), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.1677), \"['b', 'x', 'b', 'x']\": np.float64(-9.639), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.1678), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0029), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.1677), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.003), \"['b', 'x', 'b', 'y']\": np.float64(0.1708), \"['b', 'x']\": np.float64(-9.8175), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.09), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0737), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.1684), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.003), \"['b', 'y', 'a', 'x']\": np.float64(-0.1716), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.003), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.003), \"['b', 'y', 'a', 'y']\": np.float64(0.003), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0186), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.1684), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.003), \"['b', 'y', 'b', 'x']\": np.float64(-0.1715), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.003), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.003), \"['b', 'y', 'b', 'y']\": np.float64(0.0031), \"['b', 'y']\": np.float64(0.1747)}\n",
      "It s train loss bro [0.02223280817270279, 0.020854836329817772, 0.02057483419775963, 0.020299658179283142, 0.02002931386232376, 0.019763456657528877, 0.019502202048897743, 0.019245324656367302, 0.0189927089959383, 0.01874435879290104, 0.018500041216611862]\n",
      "% good predict : 60\n",
      "\u001b[0;34miteration 162 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 4.013764381408691 and val_loss is 4.220051288604736\n",
      "for 1 epochs, loss is 4.058271408081055 and val_loss is 4.218510627746582\n",
      "for 2 epochs, loss is 4.048316478729248 and val_loss is 4.21625280380249\n",
      "for 3 epochs, loss is 4.034224033355713 and val_loss is 4.213371753692627\n",
      "for 4 epochs, loss is 4.016663551330566 and val_loss is 4.2099456787109375\n",
      "for 5 epochs, loss is 3.996183395385742 and val_loss is 4.2060465812683105\n",
      "for 6 epochs, loss is 3.9732446670532227 and val_loss is 4.201730728149414\n",
      "for 7 epochs, loss is 3.9482405185699463 and val_loss is 4.197052955627441\n",
      "for 8 epochs, loss is 3.9215087890625 and val_loss is 4.192058086395264\n",
      "for 9 epochs, loss is 3.8933403491973877 and val_loss is 4.186788558959961\n",
      "for 10 epochs, loss is 3.863987922668457 and val_loss is 4.181278705596924\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.518), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5938), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9175), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.1994), \"['a', 'x', 'a', 'x']\": np.float64(-9.1245), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6217), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0138), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6215), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.014), \"['a', 'x', 'a', 'y']\": np.float64(0.6361), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-1.4254), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(4.9409), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.336), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2083), \"['a', 'x', 'b', 'x']\": np.float64(-9.5524), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2088), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0046), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2087), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0047), \"['a', 'x', 'b', 'y']\": np.float64(0.2136), \"['a', 'x']\": np.float64(-9.7742), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1976), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0138), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2068), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0046), \"['a', 'y', 'a', 'x']\": np.float64(-0.2116), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0046), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0046), \"['a', 'y', 'a', 'y']\": np.float64(0.0047), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0316), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1094), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2068), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0046), \"['a', 'y', 'b', 'x']\": np.float64(-0.2116), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0047), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0047), \"['a', 'y', 'b', 'y']\": np.float64(0.0048), \"['a', 'y']\": np.float64(0.2165), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-1.3613), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.0949), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-1.4251), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.0319), \"['b', 'x', 'a', 'x']\": np.float64(-1.4582), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-4.9409), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1094), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-4.9394), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1113), \"['b', 'x', 'a', 'y']\": np.float64(5.0549), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-1.4252), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(4.9403), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.3349), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2083), \"['b', 'x', 'b', 'x']\": np.float64(-9.5513), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2083), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0046), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2082), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0047), \"['b', 'x', 'b', 'y']\": np.float64(0.2131), \"['b', 'x']\": np.float64(-9.7727), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.1996), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0139), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.209), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0047), \"['b', 'y', 'a', 'x']\": np.float64(-0.2139), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0046), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0046), \"['b', 'y', 'a', 'y']\": np.float64(0.0047), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.0319), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1106), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.209), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0047), \"['b', 'y', 'b', 'x']\": np.float64(-0.2138), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0047), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0047), \"['b', 'y', 'b', 'y']\": np.float64(0.0048), \"['b', 'y']\": np.float64(0.2188)}\n",
      "It s train loss bro [4.013764381408691, 4.058271408081055, 4.048316478729248, 4.034224033355713, 4.016663551330566, 3.996183395385742, 3.9732446670532227, 3.9482405185699463, 3.9215087890625, 3.8933403491973877, 3.863987922668457]\n",
      "% good predict : 60\n",
      "\u001b[0;34miteration 163 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 3.665252685546875 and val_loss is 4.175660133361816\n",
      "for 1 epochs, loss is 3.799447536468506 and val_loss is 4.169900417327881\n",
      "for 2 epochs, loss is 3.768021821975708 and val_loss is 4.1640238761901855\n",
      "for 3 epochs, loss is 3.7361061573028564 and val_loss is 4.158043384552002\n",
      "for 4 epochs, loss is 3.703824043273926 and val_loss is 4.151978492736816\n",
      "for 5 epochs, loss is 3.6712825298309326 and val_loss is 4.145841598510742\n",
      "for 6 epochs, loss is 3.6385748386383057 and val_loss is 4.139647483825684\n",
      "for 7 epochs, loss is 3.6057801246643066 and val_loss is 4.13340950012207\n",
      "for 8 epochs, loss is 3.572967052459717 and val_loss is 4.127137184143066\n",
      "for 9 epochs, loss is 3.5401949882507324 and val_loss is 4.120842456817627\n",
      "for 10 epochs, loss is 3.507513999938965 and val_loss is 4.114533424377441\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8279), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4082), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9473), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2877), \"['a', 'x', 'a', 'x']\": np.float64(-9.2441), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4138), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0132), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4137), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0134), \"['a', 'x', 'a', 'y']\": np.float64(0.4275), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7335), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6239), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.0688), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2911), \"['a', 'x', 'b', 'x']\": np.float64(-9.3692), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2916), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0093), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.2916), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0094), \"['a', 'x', 'b', 'y']\": np.float64(0.3013), \"['a', 'x']\": np.float64(-9.68), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2859), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0132), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2898), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0093), \"['a', 'y', 'a', 'x']\": np.float64(-0.2994), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0093), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0092), \"['a', 'y', 'a', 'y']\": np.float64(0.0096), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.279), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0199), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2897), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0093), \"['a', 'y', 'b', 'x']\": np.float64(-0.2993), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0094), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0094), \"['a', 'y', 'b', 'y']\": np.float64(0.0097), \"['a', 'y']\": np.float64(0.3093), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6153), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3984), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.7319), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.2808), \"['b', 'x', 'a', 'x']\": np.float64(-9.0216), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6239), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0199), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6237), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0202), \"['b', 'x', 'a', 'y']\": np.float64(0.6445), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7323), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6238), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.0676), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.2911), \"['b', 'x', 'b', 'x']\": np.float64(-9.3679), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.2911), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0093), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.291), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0094), \"['b', 'x', 'b', 'y']\": np.float64(0.3007), \"['b', 'x']\": np.float64(-9.6783), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.2883), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0133), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.2922), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0094), \"['b', 'y', 'a', 'x']\": np.float64(-0.3018), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0093), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0093), \"['b', 'y', 'a', 'y']\": np.float64(0.0096), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.2813), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0201), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.2921), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0094), \"['b', 'y', 'b', 'x']\": np.float64(-0.3018), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0094), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0094), \"['b', 'y', 'b', 'y']\": np.float64(0.0097), \"['b', 'y']\": np.float64(0.3118)}\n",
      "It s train loss bro [3.665252685546875, 3.799447536468506, 3.768021821975708, 3.7361061573028564, 3.703824043273926, 3.6712825298309326, 3.6385748386383057, 3.6057801246643066, 3.572967052459717, 3.5401949882507324, 3.507513999938965]\n",
      "% good predict : 70\n",
      "\u001b[0;34miteration 164 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 0.04596700891852379 and val_loss is 4.058595180511475\n",
      "for 1 epochs, loss is 0.04309263080358505 and val_loss is 4.0095109939575195\n",
      "for 2 epochs, loss is 0.05465151369571686 and val_loss is 3.966714859008789\n",
      "for 3 epochs, loss is 0.06675419211387634 and val_loss is 3.9295871257781982\n",
      "for 4 epochs, loss is 0.0789775475859642 and val_loss is 3.8975069522857666\n",
      "for 5 epochs, loss is 0.09094750136137009 and val_loss is 3.8698854446411133\n",
      "for 6 epochs, loss is 0.10235735028982162 and val_loss is 3.846177101135254\n",
      "for 7 epochs, loss is 0.11297249048948288 and val_loss is 3.8258962631225586\n",
      "for 8 epochs, loss is 0.12262655049562454 and val_loss is 3.8086109161376953\n",
      "for 9 epochs, loss is 0.13121238350868225 and val_loss is 3.793942928314209\n",
      "for 10 epochs, loss is 0.13867300748825073 and val_loss is 3.781564235687256\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4694), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9964), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.4692), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9966), \"['a', 'x', 'a', 'x']\": np.float64(-7.4818), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9969), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1528), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9962), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1537), \"['a', 'x', 'a', 'y']\": np.float64(1.1523), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.4673), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9985), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4701), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9954), \"['a', 'x', 'b', 'x']\": np.float64(-7.4816), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9971), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1529), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9966), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1535), \"['a', 'x', 'b', 'y']\": np.float64(1.1526), \"['a', 'x']\": np.float64(-8.6527), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9905), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1526), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9905), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1526), \"['a', 'y', 'a', 'x']\": np.float64(-1.1456), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1519), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0233), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1518), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0234), \"['a', 'y', 'a', 'y']\": np.float64(0.1756), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9895), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1528), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9899), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1523), \"['a', 'y', 'b', 'x']\": np.float64(-1.1447), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1528), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0234), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1527), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0235), \"['a', 'y', 'b', 'y']\": np.float64(0.1766), \"['a', 'y']\": np.float64(1.3241), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4622), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9953), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.462), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9955), \"['b', 'x', 'a', 'x']\": np.float64(-7.4735), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9983), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.153), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9975), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1539), \"['b', 'x', 'a', 'y']\": np.float64(1.1539), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.4631), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9979), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4658), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9947), \"['b', 'x', 'b', 'x']\": np.float64(-7.4767), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9951), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1526), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9946), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1532), \"['b', 'x', 'b', 'y']\": np.float64(1.1503), \"['b', 'x']\": np.float64(-8.6456), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.997), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1536), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.997), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1536), \"['b', 'y', 'a', 'x']\": np.float64(-1.1531), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.153), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0234), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1529), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0236), \"['b', 'y', 'a', 'y']\": np.float64(0.1768), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9963), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1538), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9967), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1533), \"['b', 'y', 'b', 'x']\": np.float64(-1.1525), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1535), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0235), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1535), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0236), \"['b', 'y', 'b', 'y']\": np.float64(0.1775), \"['b', 'y']\": np.float64(1.3329)}\n",
      "It s train loss bro [0.04596700891852379, 0.04309263080358505, 0.05465151369571686, 0.06675419211387634, 0.0789775475859642, 0.09094750136137009, 0.10235735028982162, 0.11297249048948288, 0.12262655049562454, 0.13121238350868225, 0.13867300748825073]\n",
      "% good predict : 70\n",
      "\u001b[0;34miteration 165 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.14607693254947662 and val_loss is 3.771209239959717\n",
      "for 1 epochs, loss is 0.150184765458107 and val_loss is 3.7626125812530518\n",
      "for 2 epochs, loss is 0.15428684651851654 and val_loss is 3.755563497543335\n",
      "for 3 epochs, loss is 0.15735088288784027 and val_loss is 3.7498793601989746\n",
      "for 4 epochs, loss is 0.15944360196590424 and val_loss is 3.745408058166504\n",
      "for 5 epochs, loss is 0.16063940525054932 and val_loss is 3.7420129776000977\n",
      "for 6 epochs, loss is 0.1610179841518402 and val_loss is 3.739581346511841\n",
      "for 7 epochs, loss is 0.1606612205505371 and val_loss is 3.73801326751709\n",
      "for 8 epochs, loss is 0.15965072810649872 and val_loss is 3.7372217178344727\n",
      "for 9 epochs, loss is 0.15806666016578674 and val_loss is 3.737133264541626\n",
      "for 10 epochs, loss is 0.15598604083061218 and val_loss is 3.7376809120178223\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.3119), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0302), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.3103), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0322), \"['a', 'x', 'a', 'x']\": np.float64(-7.3599), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0305), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1678), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0298), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1687), \"['a', 'x', 'a', 'y']\": np.float64(1.2012), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.3092), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0313), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.3096), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0308), \"['a', 'x', 'b', 'x']\": np.float64(-7.358), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0324), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1682), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.032), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1687), \"['a', 'x', 'b', 'y']\": np.float64(1.2035), \"['a', 'x']\": np.float64(-8.5819), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0243), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1672), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.024), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1675), \"['a', 'y', 'a', 'x']\": np.float64(-1.1944), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1668), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0272), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1667), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0273), \"['a', 'y', 'a', 'y']\": np.float64(0.1945), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0234), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1673), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0235), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1672), \"['a', 'y', 'b', 'x']\": np.float64(-1.1935), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1677), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0273), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1676), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0274), \"['a', 'y', 'b', 'y']\": np.float64(0.1955), \"['a', 'y']\": np.float64(1.3923), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.3058), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0292), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.3041), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0312), \"['b', 'x', 'a', 'x']\": np.float64(-7.3527), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.031), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1679), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0303), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1688), \"['b', 'x', 'a', 'y']\": np.float64(1.2019), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.3051), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0306), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.3055), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0301), \"['b', 'x', 'b', 'x']\": np.float64(-7.3532), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0304), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1679), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.03), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1684), \"['b', 'x', 'b', 'y']\": np.float64(1.2013), \"['b', 'x']\": np.float64(-8.575), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0307), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1682), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0304), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1685), \"['b', 'y', 'a', 'x']\": np.float64(-1.2018), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.168), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0274), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1679), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0275), \"['b', 'y', 'a', 'y']\": np.float64(0.1958), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0301), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1684), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0301), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1683), \"['b', 'y', 'b', 'x']\": np.float64(-1.2013), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1685), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0275), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1684), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0275), \"['b', 'y', 'b', 'y']\": np.float64(0.1964), \"['b', 'y']\": np.float64(1.401)}\n",
      "It s train loss bro [0.14607693254947662, 0.150184765458107, 0.15428684651851654, 0.15735088288784027, 0.15944360196590424, 0.16063940525054932, 0.1610179841518402, 0.1606612205505371, 0.15965072810649872, 0.15806666016578674, 0.15598604083061218]\n",
      "% good predict : 60\n",
      "\u001b[0;34miteration 166 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.20055490732192993 and val_loss is 3.7390599250793457\n",
      "for 1 epochs, loss is 0.1505201756954193 and val_loss is 3.740943670272827\n",
      "for 2 epochs, loss is 0.14727912843227386 and val_loss is 3.743290662765503\n",
      "for 3 epochs, loss is 0.14381593465805054 and val_loss is 3.7460601329803467\n",
      "for 4 epochs, loss is 0.14018197357654572 and val_loss is 3.7492170333862305\n",
      "for 5 epochs, loss is 0.13642346858978271 and val_loss is 3.752728223800659\n",
      "for 6 epochs, loss is 0.13258105516433716 and val_loss is 3.7565643787384033\n",
      "for 7 epochs, loss is 0.12869027256965637 and val_loss is 3.760697841644287\n",
      "for 8 epochs, loss is 0.12478198111057281 and val_loss is 3.7651007175445557\n",
      "for 9 epochs, loss is 0.1208825409412384 and val_loss is 3.7697503566741943\n",
      "for 10 epochs, loss is 0.11701428890228271 and val_loss is 3.774622917175293\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.122), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8367), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.1214), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8375), \"['a', 'x', 'a', 'x']\": np.float64(-7.9763), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8369), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0981), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8366), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0985), \"['a', 'x', 'a', 'y']\": np.float64(0.9371), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.1207), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8377), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.1214), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8367), \"['a', 'x', 'b', 'x']\": np.float64(-7.9756), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8376), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0983), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8375), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0985), \"['a', 'x', 'b', 'y']\": np.float64(0.938), \"['a', 'x']\": np.float64(-8.933), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8319), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0977), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8318), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0978), \"['a', 'y', 'a', 'x']\": np.float64(-0.9316), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0975), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0114), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0975), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0115), \"['a', 'y', 'a', 'y']\": np.float64(0.1092), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8315), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0978), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8316), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0977), \"['a', 'y', 'b', 'x']\": np.float64(-0.9313), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0979), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0115), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0979), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0115), \"['a', 'y', 'b', 'y']\": np.float64(0.1097), \"['a', 'y']\": np.float64(1.0432), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.119), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8364), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.1184), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8372), \"['b', 'x', 'a', 'x']\": np.float64(-7.9729), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8377), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0982), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8374), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0986), \"['b', 'x', 'a', 'y']\": np.float64(0.938), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.119), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8375), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.1197), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8365), \"['b', 'x', 'b', 'x']\": np.float64(-7.9737), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8366), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0981), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8365), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0984), \"['b', 'x', 'b', 'y']\": np.float64(0.9369), \"['b', 'x']\": np.float64(-8.9302), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8355), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0982), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8354), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0982), \"['b', 'y', 'a', 'x']\": np.float64(-0.9357), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.098), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0115), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.098), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0115), \"['b', 'y', 'a', 'y']\": np.float64(0.1098), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8352), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0983), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8353), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0981), \"['b', 'y', 'b', 'x']\": np.float64(-0.9355), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0982), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0115), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0982), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0115), \"['b', 'y', 'b', 'y']\": np.float64(0.11), \"['b', 'y']\": np.float64(1.0478)}\n",
      "It s train loss bro [0.20055490732192993, 0.1505201756954193, 0.14727912843227386, 0.14381593465805054, 0.14018197357654572, 0.13642346858978271, 0.13258105516433716, 0.12869027256965637, 0.12478198111057281, 0.1208825409412384, 0.11701428890228271]\n",
      "% good predict : 50\n",
      "\u001b[0;34miteration 167 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.12472063302993774 and val_loss is 3.7797985076904297\n",
      "for 1 epochs, loss is 0.10945038497447968 and val_loss is 3.785144090652466\n",
      "for 2 epochs, loss is 0.10578244179487228 and val_loss is 3.790642023086548\n",
      "for 3 epochs, loss is 0.10220184177160263 and val_loss is 3.796273708343506\n",
      "for 4 epochs, loss is 0.09871670603752136 and val_loss is 3.80202317237854\n",
      "for 5 epochs, loss is 0.09533274173736572 and val_loss is 3.8078746795654297\n",
      "for 6 epochs, loss is 0.09205406904220581 and val_loss is 3.8138132095336914\n",
      "for 7 epochs, loss is 0.088883176445961 and val_loss is 3.819826126098633\n",
      "for 8 epochs, loss is 0.08582167327404022 and val_loss is 3.825899362564087\n",
      "for 9 epochs, loss is 0.08286983519792557 and val_loss is 3.8320207595825195\n",
      "for 10 epochs, loss is 0.08002734184265137 and val_loss is 3.838179588317871\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.9306), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6222), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.931), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6217), \"['a', 'x', 'a', 'x']\": np.float64(-8.5687), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6222), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0487), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6221), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0488), \"['a', 'x', 'a', 'y']\": np.float64(0.6722), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.9303), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6231), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.9316), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6213), \"['a', 'x', 'b', 'x']\": np.float64(-8.5692), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6217), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0487), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6217), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0487), \"['a', 'x', 'b', 'y']\": np.float64(0.6717), \"['a', 'x']\": np.float64(-9.2582), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6178), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0485), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6179), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0484), \"['a', 'y', 'a', 'x']\": np.float64(-0.6675), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0483), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0038), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0483), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0038), \"['a', 'y', 'a', 'y']\": np.float64(0.0522), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6177), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0485), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6178), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0484), \"['a', 'y', 'b', 'x']\": np.float64(-0.6675), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0485), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0038), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0485), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0038), \"['a', 'y', 'b', 'y']\": np.float64(0.0524), \"['a', 'y']\": np.float64(0.7212), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.9297), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6221), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.9302), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6216), \"['b', 'x', 'a', 'x']\": np.float64(-8.5678), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6232), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0487), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6231), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0489), \"['b', 'x', 'a', 'y']\": np.float64(0.6732), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.9304), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6231), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.9317), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6213), \"['b', 'x', 'b', 'x']\": np.float64(-8.5693), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6213), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0486), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6213), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0487), \"['b', 'x', 'b', 'y']\": np.float64(0.6712), \"['b', 'x']\": np.float64(-9.258), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6193), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0486), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6194), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0485), \"['b', 'y', 'a', 'x']\": np.float64(-0.6692), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0485), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0038), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0485), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0038), \"['b', 'y', 'a', 'y']\": np.float64(0.0524), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6192), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0487), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6193), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0485), \"['b', 'y', 'b', 'x']\": np.float64(-0.6691), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0486), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0038), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0485), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0038), \"['b', 'y', 'b', 'y']\": np.float64(0.0525), \"['b', 'y']\": np.float64(0.7229)}\n",
      "It s train loss bro [0.12472063302993774, 0.10945038497447968, 0.10578244179487228, 0.10220184177160263, 0.09871670603752136, 0.09533274173736572, 0.09205406904220581, 0.088883176445961, 0.08582167327404022, 0.08286983519792557, 0.08002734184265137]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 168 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.08480368554592133 and val_loss is 3.8444926738739014\n",
      "for 1 epochs, loss is 0.07464983314275742 and val_loss is 3.850811243057251\n",
      "for 2 epochs, loss is 0.07211287319660187 and val_loss is 3.8571271896362305\n",
      "for 3 epochs, loss is 0.06967920064926147 and val_loss is 3.8634307384490967\n",
      "for 4 epochs, loss is 0.06734580546617508 and val_loss is 3.869718551635742\n",
      "for 5 epochs, loss is 0.0651097223162651 and val_loss is 3.8759820461273193\n",
      "for 6 epochs, loss is 0.06296724081039429 and val_loss is 3.8822152614593506\n",
      "for 7 epochs, loss is 0.060915056616067886 and val_loss is 3.888413906097412\n",
      "for 8 epochs, loss is 0.05894969031214714 and val_loss is 3.894573926925659\n",
      "for 9 epochs, loss is 0.05706760659813881 and val_loss is 3.9006893634796143\n",
      "for 10 epochs, loss is 0.05526556074619293 and val_loss is 3.9067578315734863\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.515), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4556), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5162), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.454), \"['a', 'x', 'a', 'x']\": np.float64(-8.9847), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4557), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0243), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4556), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0243), \"['a', 'x', 'a', 'y']\": np.float64(0.4807), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.5154), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4567), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.5176), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4538), \"['a', 'x', 'b', 'x']\": np.float64(-8.986), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4541), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0242), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4541), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0242), \"['a', 'x', 'b', 'y']\": np.float64(0.4791), \"['a', 'x']\": np.float64(-9.4802), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.451), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0241), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4511), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.024), \"['a', 'y', 'a', 'x']\": np.float64(-0.4759), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.024), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0013), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.024), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0013), \"['a', 'y', 'a', 'y']\": np.float64(0.0253), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.451), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0242), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4511), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.024), \"['a', 'y', 'b', 'x']\": np.float64(-0.4759), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0241), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0013), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0241), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0013), \"['a', 'y', 'b', 'y']\": np.float64(0.0254), \"['a', 'y']\": np.float64(0.5021), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.515), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4556), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5163), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.454), \"['b', 'x', 'a', 'x']\": np.float64(-8.9847), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4567), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0243), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4567), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0244), \"['b', 'x', 'a', 'y']\": np.float64(0.4819), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.5164), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4567), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.5185), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4539), \"['b', 'x', 'b', 'x']\": np.float64(-8.9869), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4539), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0242), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4539), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0242), \"['b', 'x', 'b', 'y']\": np.float64(0.4788), \"['b', 'x']\": np.float64(-9.4811), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4514), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0242), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4515), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0241), \"['b', 'y', 'a', 'x']\": np.float64(-0.4763), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.024), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0013), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.024), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0013), \"['b', 'y', 'a', 'y']\": np.float64(0.0254), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4514), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0242), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4515), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0241), \"['b', 'y', 'b', 'x']\": np.float64(-0.4763), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0241), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0013), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0241), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0013), \"['b', 'y', 'b', 'y']\": np.float64(0.0254), \"['b', 'y']\": np.float64(0.5025)}\n",
      "It s train loss bro [0.08480368554592133, 0.07464983314275742, 0.07211287319660187, 0.06967920064926147, 0.06734580546617508, 0.0651097223162651, 0.06296724081039429, 0.060915056616067886, 0.05894969031214714, 0.05706760659813881, 0.05526556074619293]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 169 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.06341107189655304 and val_loss is 3.912900447845459\n",
      "for 1 epochs, loss is 0.05187729746103287 and val_loss is 3.9189772605895996\n",
      "for 2 epochs, loss is 0.05028603971004486 and val_loss is 3.9249870777130127\n",
      "for 3 epochs, loss is 0.04876286908984184 and val_loss is 3.9309279918670654\n",
      "for 4 epochs, loss is 0.04730457440018654 and val_loss is 3.9367990493774414\n",
      "for 5 epochs, loss is 0.04590803012251854 and val_loss is 3.9426004886627197\n",
      "for 6 epochs, loss is 0.04457030072808266 and val_loss is 3.948330879211426\n",
      "for 7 epochs, loss is 0.04328843206167221 and val_loss is 3.953988552093506\n",
      "for 8 epochs, loss is 0.042060013860464096 and val_loss is 3.959575653076172\n",
      "for 9 epochs, loss is 0.04088215157389641 and val_loss is 3.9650917053222656\n",
      "for 10 epochs, loss is 0.039752621203660965 and val_loss is 3.9705355167388916\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.9019), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3407), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.904), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3381), \"['a', 'x', 'a', 'x']\": np.float64(-9.255), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3408), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0129), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3408), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.013), \"['a', 'x', 'a', 'y']\": np.float64(0.3542), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.9031), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.342), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.9061), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.338), \"['a', 'x', 'b', 'x']\": np.float64(-9.2572), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3382), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0128), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3382), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0129), \"['a', 'x', 'b', 'y']\": np.float64(0.3515), \"['a', 'x']\": np.float64(-9.6221), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3356), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0128), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3356), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0127), \"['a', 'y', 'a', 'x']\": np.float64(-0.3489), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0127), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0127), \"['a', 'y', 'a', 'y']\": np.float64(0.0132), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3355), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0129), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3356), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0127), \"['a', 'y', 'b', 'x']\": np.float64(-0.3489), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0128), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0128), \"['a', 'y', 'b', 'y']\": np.float64(0.0133), \"['a', 'y']\": np.float64(0.3626), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.9022), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3407), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9043), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3381), \"['b', 'x', 'a', 'x']\": np.float64(-9.2553), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.342), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.013), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.342), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.013), \"['b', 'x', 'a', 'y']\": np.float64(0.3555), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.9043), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3421), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.9074), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3381), \"['b', 'x', 'b', 'x']\": np.float64(-9.2585), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.338), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0128), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.338), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0128), \"['b', 'x', 'b', 'y']\": np.float64(0.3514), \"['b', 'x']\": np.float64(-9.6234), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3354), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0128), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3355), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0127), \"['b', 'y', 'a', 'x']\": np.float64(-0.3487), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0127), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0127), \"['b', 'y', 'a', 'y']\": np.float64(0.0132), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3354), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0129), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3355), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0127), \"['b', 'y', 'b', 'x']\": np.float64(-0.3487), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0127), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0127), \"['b', 'y', 'b', 'y']\": np.float64(0.0133), \"['b', 'y']\": np.float64(0.3625)}\n",
      "It s train loss bro [0.06341107189655304, 0.05187729746103287, 0.05028603971004486, 0.04876286908984184, 0.04730457440018654, 0.04590803012251854, 0.04457030072808266, 0.04328843206167221, 0.042060013860464096, 0.04088215157389641, 0.039752621203660965]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 170 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 3.222226142883301 and val_loss is 3.970228433609009\n",
      "for 1 epochs, loss is 3.3081202507019043 and val_loss is 3.969181776046753\n",
      "for 2 epochs, loss is 3.300257682800293 and val_loss is 3.9675023555755615\n",
      "for 3 epochs, loss is 3.2880218029022217 and val_loss is 3.9652793407440186\n",
      "for 4 epochs, loss is 3.272059202194214 and val_loss is 3.962590217590332\n",
      "for 5 epochs, loss is 3.252929210662842 and val_loss is 3.959501028060913\n",
      "for 6 epochs, loss is 3.23111891746521 and val_loss is 3.956069231033325\n",
      "for 7 epochs, loss is 3.2070536613464355 and val_loss is 3.952345132827759\n",
      "for 8 epochs, loss is 3.181104898452759 and val_loss is 3.948371410369873\n",
      "for 9 epochs, loss is 3.1535961627960205 and val_loss is 3.9441888332366943\n",
      "for 10 epochs, loss is 3.124810218811035 and val_loss is 3.9398300647735596\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6596), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4131), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6617), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4106), \"['a', 'x', 'a', 'x']\": np.float64(-9.0859), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4132), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0196), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4132), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0196), \"['a', 'x', 'a', 'y']\": np.float64(0.4335), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.6611), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4141), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.6638), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4106), \"['a', 'x', 'b', 'x']\": np.float64(-9.0882), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4106), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0195), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4107), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0195), \"['a', 'x', 'b', 'y']\": np.float64(0.4308), \"['a', 'x']\": np.float64(-9.5332), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4091), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0195), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4092), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0194), \"['a', 'y', 'a', 'x']\": np.float64(-0.4293), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0194), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0194), \"['a', 'y', 'a', 'y']\": np.float64(0.0204), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4091), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0196), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4093), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0194), \"['a', 'y', 'b', 'x']\": np.float64(-0.4293), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0194), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0194), \"['a', 'y', 'b', 'y']\": np.float64(0.0204), \"['a', 'y']\": np.float64(0.4503), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6615), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4132), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6637), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4107), \"['b', 'x', 'a', 'x']\": np.float64(-9.0879), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4142), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0197), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4142), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0197), \"['b', 'x', 'a', 'y']\": np.float64(0.4345), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.6636), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4142), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.6662), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4108), \"['b', 'x', 'b', 'x']\": np.float64(-9.0907), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4107), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0195), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4108), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0195), \"['b', 'x', 'b', 'y']\": np.float64(0.4309), \"['b', 'x']\": np.float64(-9.536), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4077), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0195), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4078), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0193), \"['b', 'y', 'a', 'x']\": np.float64(-0.4278), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0194), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0194), \"['b', 'y', 'a', 'y']\": np.float64(0.0203), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4077), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0195), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4078), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0193), \"['b', 'y', 'b', 'x']\": np.float64(-0.4278), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0193), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0193), \"['b', 'y', 'b', 'y']\": np.float64(0.0203), \"['b', 'y']\": np.float64(0.4488)}\n",
      "It s train loss bro [3.222226142883301, 3.3081202507019043, 3.300257682800293, 3.2880218029022217, 3.272059202194214, 3.252929210662842, 3.23111891746521, 3.2070536613464355, 3.181104898452759, 3.1535961627960205, 3.124810218811035]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 171 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.062318816781044006 and val_loss is 3.902114152908325\n",
      "for 1 epochs, loss is 0.06207416579127312 and val_loss is 3.8701181411743164\n",
      "for 2 epochs, loss is 0.07688304036855698 and val_loss is 3.843144178390503\n",
      "for 3 epochs, loss is 0.0918545350432396 and val_loss is 3.820526123046875\n",
      "for 4 epochs, loss is 0.10646887868642807 and val_loss is 3.8016626834869385\n",
      "for 5 epochs, loss is 0.12031257897615433 and val_loss is 3.786015033721924\n",
      "for 6 epochs, loss is 0.13308510184288025 and val_loss is 3.7731239795684814\n",
      "for 7 epochs, loss is 0.1445896327495575 and val_loss is 3.76259183883667\n",
      "for 8 epochs, loss is 0.15471585094928741 and val_loss is 3.7540862560272217\n",
      "for 9 epochs, loss is 0.1634213924407959 and val_loss is 3.7473244667053223\n",
      "for 10 epochs, loss is 0.17071564495563507 and val_loss is 3.7420716285705566\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.8921), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1182), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8961), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1144), \"['a', 'x', 'a', 'x']\": np.float64(-7.0276), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1182), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2122), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1191), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2115), \"['a', 'x', 'a', 'y']\": np.float64(1.3338), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.8966), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1188), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.8995), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1157), \"['a', 'x', 'b', 'x']\": np.float64(-7.0325), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1145), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2114), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1151), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2109), \"['a', 'x', 'b', 'y']\": np.float64(1.3292), \"['a', 'x']\": np.float64(-8.382), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1187), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2123), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1194), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2116), \"['a', 'y', 'a', 'x']\": np.float64(-1.3342), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2123), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0403), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2124), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0402), \"['a', 'y', 'a', 'y']\": np.float64(0.2532), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1196), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2124), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1202), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2118), \"['a', 'y', 'b', 'x']\": np.float64(-1.3353), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2116), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0401), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2117), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.04), \"['a', 'y', 'b', 'y']\": np.float64(0.2524), \"['a', 'y']\": np.float64(1.5915), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.8992), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1196), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9033), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1158), \"['b', 'x', 'a', 'x']\": np.float64(-7.0361), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1192), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2124), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1201), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2117), \"['b', 'x', 'a', 'y']\": np.float64(1.335), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.9026), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1199), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9055), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1169), \"['b', 'x', 'b', 'x']\": np.float64(-7.0396), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1163), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2117), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1169), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2112), \"['b', 'x', 'b', 'y']\": np.float64(1.3314), \"['b', 'x']\": np.float64(-8.3915), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1134), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2113), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1141), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2106), \"['b', 'y', 'a', 'x']\": np.float64(-1.3279), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2112), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0401), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2113), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0399), \"['b', 'y', 'a', 'y']\": np.float64(0.2519), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1141), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2114), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1146), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2108), \"['b', 'y', 'b', 'x']\": np.float64(-1.3287), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2107), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.04), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2108), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0399), \"['b', 'y', 'b', 'y']\": np.float64(0.2513), \"['b', 'y']\": np.float64(1.5838)}\n",
      "It s train loss bro [0.062318816781044006, 0.06207416579127312, 0.07688304036855698, 0.0918545350432396, 0.10646887868642807, 0.12031257897615433, 0.13308510184288025, 0.1445896327495575, 0.15471585094928741, 0.1634213924407959, 0.17071564495563507]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 172 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1791621297597885 and val_loss is 3.7381393909454346\n",
      "for 1 epochs, loss is 0.18128293752670288 and val_loss is 3.7353527545928955\n",
      "for 2 epochs, loss is 0.18471354246139526 and val_loss is 3.733572483062744\n",
      "for 3 epochs, loss is 0.1870347410440445 and val_loss is 3.732682943344116\n",
      "for 4 epochs, loss is 0.18834909796714783 and val_loss is 3.7325854301452637\n",
      "for 5 epochs, loss is 0.18876096606254578 and val_loss is 3.733194589614868\n",
      "for 6 epochs, loss is 0.18837259709835052 and val_loss is 3.7344415187835693\n",
      "for 7 epochs, loss is 0.18728287518024445 and val_loss is 3.73626446723938\n",
      "for 8 epochs, loss is 0.18558578193187714 and val_loss is 3.7386085987091064\n",
      "for 9 epochs, loss is 0.1833696961402893 and val_loss is 3.741429328918457\n",
      "for 10 epochs, loss is 0.1807161569595337 and val_loss is 3.744684934616089\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.8685), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1238), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8705), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.122), \"['a', 'x', 'a', 'x']\": np.float64(-7.0096), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1239), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2149), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1241), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2149), \"['a', 'x', 'a', 'y']\": np.float64(1.3423), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.8703), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1246), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.8723), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1223), \"['a', 'x', 'b', 'x']\": np.float64(-7.0119), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1222), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2146), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1223), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2145), \"['a', 'x', 'b', 'y']\": np.float64(1.3402), \"['a', 'x']\": np.float64(-8.3724), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1223), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2149), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1227), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2146), \"['a', 'y', 'a', 'x']\": np.float64(-1.3405), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2147), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0411), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2147), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0411), \"['a', 'y', 'a', 'y']\": np.float64(0.2564), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1225), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.215), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1229), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2146), \"['a', 'y', 'b', 'x']\": np.float64(-1.3408), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2147), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0411), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2147), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.041), \"['a', 'y', 'b', 'y']\": np.float64(0.2564), \"['a', 'y']\": np.float64(1.601), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.8704), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1241), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8723), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1224), \"['b', 'x', 'a', 'x']\": np.float64(-7.0117), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1247), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2151), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1249), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2151), \"['b', 'x', 'a', 'y']\": np.float64(1.3432), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.8722), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1249), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.8743), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1226), \"['b', 'x', 'b', 'x']\": np.float64(-7.0142), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1224), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2147), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1226), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2146), \"['b', 'x', 'b', 'y']\": np.float64(1.3405), \"['b', 'x']\": np.float64(-8.3753), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1216), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2148), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.122), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2144), \"['b', 'y', 'a', 'x']\": np.float64(-1.3397), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2145), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.041), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2146), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.041), \"['b', 'y', 'a', 'y']\": np.float64(0.2562), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1217), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2149), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1221), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2145), \"['b', 'y', 'b', 'x']\": np.float64(-1.3399), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2145), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.041), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2145), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.041), \"['b', 'y', 'b', 'y']\": np.float64(0.2561), \"['b', 'y']\": np.float64(1.5999)}\n",
      "It s train loss bro [0.1791621297597885, 0.18128293752670288, 0.18471354246139526, 0.1870347410440445, 0.18834909796714783, 0.18876096606254578, 0.18837259709835052, 0.18728287518024445, 0.18558578193187714, 0.1833696961402893, 0.1807161569595337]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 173 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.22773504257202148 and val_loss is 3.7486839294433594\n",
      "for 1 epochs, loss is 0.17435237765312195 and val_loss is 3.753014087677002\n",
      "for 2 epochs, loss is 0.1707761436700821 and val_loss is 3.7576472759246826\n",
      "for 3 epochs, loss is 0.16702529788017273 and val_loss is 3.7625598907470703\n",
      "for 4 epochs, loss is 0.16314761340618134 and val_loss is 3.7677290439605713\n",
      "for 5 epochs, loss is 0.15918530523777008 and val_loss is 3.7731308937072754\n",
      "for 6 epochs, loss is 0.15517446398735046 and val_loss is 3.7787463665008545\n",
      "for 7 epochs, loss is 0.15114635229110718 and val_loss is 3.784555196762085\n",
      "for 8 epochs, loss is 0.14712725579738617 and val_loss is 3.7905397415161133\n",
      "for 9 epochs, loss is 0.1431397795677185 and val_loss is 3.7966806888580322\n",
      "for 10 epochs, loss is 0.13920210301876068 and val_loss is 3.802963972091675\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.6622), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9497), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.6633), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9484), \"['a', 'x', 'a', 'x']\": np.float64(-7.6281), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9498), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1351), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9498), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1353), \"['a', 'x', 'a', 'y']\": np.float64(1.0874), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.6628), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9507), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.6645), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9483), \"['a', 'x', 'b', 'x']\": np.float64(-7.6294), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9486), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1349), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9486), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.135), \"['a', 'x', 'b', 'y']\": np.float64(1.0859), \"['a', 'x']\": np.float64(-8.7341), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.948), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1351), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9481), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.135), \"['a', 'y', 'a', 'x']\": np.float64(-1.0854), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1349), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0192), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1349), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0192), \"['a', 'y', 'a', 'y']\": np.float64(0.1544), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9479), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1352), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9481), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1349), \"['a', 'y', 'b', 'x']\": np.float64(-1.0854), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.135), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0192), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.135), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0192), \"['a', 'y', 'b', 'y']\": np.float64(0.1546), \"['a', 'y']\": np.float64(1.2426), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.6622), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9497), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.6633), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9484), \"['b', 'x', 'a', 'x']\": np.float64(-7.6282), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9507), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1352), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9507), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1354), \"['b', 'x', 'a', 'y']\": np.float64(1.0884), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.6635), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9508), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.6652), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9484), \"['b', 'x', 'b', 'x']\": np.float64(-7.6302), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9484), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1349), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9484), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.135), \"['b', 'x', 'b', 'y']\": np.float64(1.0857), \"['b', 'x']\": np.float64(-8.7348), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9486), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1352), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9488), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.135), \"['b', 'y', 'a', 'x']\": np.float64(-1.0861), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.135), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0192), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.135), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0192), \"['b', 'y', 'a', 'y']\": np.float64(0.1545), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9485), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1353), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9488), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.135), \"['b', 'y', 'b', 'x']\": np.float64(-1.0861), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.135), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0192), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.135), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0192), \"['b', 'y', 'b', 'y']\": np.float64(0.1546), \"['b', 'y']\": np.float64(1.2434)}\n",
      "It s train loss bro [0.22773504257202148, 0.17435237765312195, 0.1707761436700821, 0.16702529788017273, 0.16314761340618134, 0.15918530523777008, 0.15517446398735046, 0.15114635229110718, 0.14712725579738617, 0.1431397795677185, 0.13920210301876068]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 174 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.041560173034668 and val_loss is 3.803205728530884\n",
      "for 1 epochs, loss is 2.085967540740967 and val_loss is 3.803097724914551\n",
      "for 2 epochs, loss is 2.084547758102417 and val_loss is 3.8026773929595947\n",
      "for 3 epochs, loss is 2.0813586711883545 and val_loss is 3.801978826522827\n",
      "for 4 epochs, loss is 2.0765907764434814 and val_loss is 3.8010363578796387\n",
      "for 5 epochs, loss is 2.070415735244751 and val_loss is 3.7998783588409424\n",
      "for 6 epochs, loss is 2.062991142272949 and val_loss is 3.7985339164733887\n",
      "for 7 epochs, loss is 2.0544586181640625 and val_loss is 3.7970259189605713\n",
      "for 8 epochs, loss is 2.0449459552764893 and val_loss is 3.795379161834717\n",
      "for 9 epochs, loss is 2.034569501876831 and val_loss is 3.793613910675049\n",
      "for 10 epochs, loss is 2.0234341621398926 and val_loss is 3.7917490005493164\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4454), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0002), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.4464), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9991), \"['a', 'x', 'a', 'x']\": np.float64(-7.462), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0003), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1549), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0003), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1551), \"['a', 'x', 'a', 'y']\": np.float64(1.1579), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.4459), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0011), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4477), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9989), \"['a', 'x', 'b', 'x']\": np.float64(-7.4631), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9993), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1547), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9992), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1549), \"['a', 'x', 'b', 'y']\": np.float64(1.1566), \"['a', 'x']\": np.float64(-8.6388), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9979), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1548), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.998), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1547), \"['a', 'y', 'a', 'x']\": np.float64(-1.1553), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1545), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0239), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1545), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.024), \"['a', 'y', 'a', 'y']\": np.float64(0.1789), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9977), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.155), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.998), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1546), \"['a', 'y', 'b', 'x']\": np.float64(-1.1552), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1548), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.024), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1548), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.024), \"['a', 'y', 'b', 'y']\": np.float64(0.1791), \"['a', 'y']\": np.float64(1.3372), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.4448), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0001), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.4458), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.999), \"['b', 'x', 'a', 'x']\": np.float64(-7.4613), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0012), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.155), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0011), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1552), \"['b', 'x', 'a', 'y']\": np.float64(1.1588), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.4461), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0012), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.4478), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9989), \"['b', 'x', 'b', 'x']\": np.float64(-7.4633), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9989), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1547), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9989), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1548), \"['b', 'x', 'b', 'y']\": np.float64(1.1562), \"['b', 'x']\": np.float64(-8.6388), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9991), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.155), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9993), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1549), \"['b', 'y', 'a', 'x']\": np.float64(-1.1567), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1547), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.024), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1547), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.024), \"['b', 'y', 'a', 'y']\": np.float64(0.1791), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.999), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1552), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9993), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1548), \"['b', 'y', 'b', 'x']\": np.float64(-1.1567), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1549), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.024), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1549), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.024), \"['b', 'y', 'b', 'y']\": np.float64(0.1793), \"['b', 'y']\": np.float64(1.3389)}\n",
      "It s train loss bro [2.041560173034668, 2.085967540740967, 2.084547758102417, 2.0813586711883545, 2.0765907764434814, 2.070415735244751, 2.062991142272949, 2.0544586181640625, 2.0449459552764893, 2.034569501876831, 2.0234341621398926]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 175 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.15842710435390472 and val_loss is 3.776170492172241\n",
      "for 1 epochs, loss is 0.16309799253940582 and val_loss is 3.7635109424591064\n",
      "for 2 epochs, loss is 0.17906346917152405 and val_loss is 3.7533071041107178\n",
      "for 3 epochs, loss is 0.19385194778442383 and val_loss is 3.745159864425659\n",
      "for 4 epochs, loss is 0.20724289119243622 and val_loss is 3.738722324371338\n",
      "for 5 epochs, loss is 0.2190791368484497 and val_loss is 3.7337050437927246\n",
      "for 6 epochs, loss is 0.22926339507102966 and val_loss is 3.7298648357391357\n",
      "for 7 epochs, loss is 0.23775266110897064 and val_loss is 3.7270054817199707\n",
      "for 8 epochs, loss is 0.24455077946186066 and val_loss is 3.7249679565429688\n",
      "for 9 epochs, loss is 0.2497013956308365 and val_loss is 3.7236251831054688\n",
      "for 10 epochs, loss is 0.25327950716018677 and val_loss is 3.7228801250457764\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.6481), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3379), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.6469), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3394), \"['a', 'x', 'a', 'x']\": np.float64(-6.0021), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3383), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3846), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3373), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3857), \"['a', 'x', 'a', 'y']\": np.float64(1.7276), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.6461), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3385), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.6466), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3381), \"['a', 'x', 'b', 'x']\": np.float64(-6.0006), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3397), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3852), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3392), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3858), \"['a', 'x', 'b', 'y']\": np.float64(1.7296), \"['a', 'x']\": np.float64(-7.7505), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3324), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3835), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.332), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3839), \"['a', 'y', 'a', 'x']\": np.float64(-1.7205), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.383), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1101), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3828), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1104), \"['a', 'y', 'a', 'y']\": np.float64(0.4944), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3312), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3835), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3314), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3834), \"['a', 'y', 'b', 'x']\": np.float64(-1.7193), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3841), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1104), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.384), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1106), \"['a', 'y', 'b', 'y']\": np.float64(0.4959), \"['a', 'y']\": np.float64(2.221), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.643), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3364), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.6419), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.338), \"['b', 'x', 'a', 'x']\": np.float64(-5.9956), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3381), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3845), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3372), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3857), \"['b', 'x', 'a', 'y']\": np.float64(1.7273), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.6427), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3376), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.6433), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3371), \"['b', 'x', 'b', 'x']\": np.float64(-5.9963), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3375), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3846), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.337), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3852), \"['b', 'x', 'b', 'y']\": np.float64(1.7268), \"['b', 'x']\": np.float64(-7.7435), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3375), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.385), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3372), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3854), \"['b', 'y', 'a', 'x']\": np.float64(-1.7271), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3847), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1106), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3845), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1109), \"['b', 'y', 'a', 'y']\": np.float64(0.4966), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3368), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3851), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.337), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.385), \"['b', 'y', 'b', 'x']\": np.float64(-1.7265), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3853), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1108), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3851), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.111), \"['b', 'y', 'b', 'y']\": np.float64(0.4974), \"['b', 'y']\": np.float64(2.2298)}\n",
      "It s train loss bro [0.15842710435390472, 0.16309799253940582, 0.17906346917152405, 0.19385194778442383, 0.20724289119243622, 0.2190791368484497, 0.22926339507102966, 0.23775266110897064, 0.24455077946186066, 0.2497013956308365, 0.25327950716018677]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 176 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.30764150619506836 and val_loss is 3.7228400707244873\n",
      "for 1 epochs, loss is 0.25612160563468933 and val_loss is 3.723250389099121\n",
      "for 2 epochs, loss is 0.2556326985359192 and val_loss is 3.724073886871338\n",
      "for 3 epochs, loss is 0.2540523409843445 and val_loss is 3.725282669067383\n",
      "for 4 epochs, loss is 0.25151586532592773 and val_loss is 3.7268598079681396\n",
      "for 5 epochs, loss is 0.24815663695335388 and val_loss is 3.7287914752960205\n",
      "for 6 epochs, loss is 0.2441026121377945 and val_loss is 3.7310702800750732\n",
      "for 7 epochs, loss is 0.2394745647907257 and val_loss is 3.733690023422241\n",
      "for 8 epochs, loss is 0.23438413441181183 and val_loss is 3.736644744873047\n",
      "for 9 epochs, loss is 0.2289333939552307 and val_loss is 3.739931583404541\n",
      "for 10 epochs, loss is 0.22321422398090363 and val_loss is 3.7435455322265625\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.2084), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2494), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.2087), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2495), \"['a', 'x', 'a', 'x']\": np.float64(-6.4743), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2497), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2993), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2494), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2999), \"['a', 'x', 'a', 'y']\": np.float64(1.5531), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.2081), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2502), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.2093), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2489), \"['a', 'x', 'b', 'x']\": np.float64(-6.4746), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2497), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2994), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2496), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2997), \"['a', 'x', 'b', 'y']\": np.float64(1.5531), \"['a', 'x']\": np.float64(-8.0478), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2461), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2989), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2462), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2989), \"['a', 'y', 'a', 'x']\": np.float64(-1.549), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2985), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0715), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2985), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0716), \"['a', 'y', 'a', 'y']\": np.float64(0.371), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2457), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.299), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.246), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2987), \"['a', 'y', 'b', 'x']\": np.float64(-1.5486), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2991), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0716), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.299), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0717), \"['a', 'y', 'b', 'y']\": np.float64(0.3717), \"['a', 'y']\": np.float64(1.925), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.2065), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.249), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.2067), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.249), \"['b', 'x', 'a', 'x']\": np.float64(-6.4718), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2501), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2994), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2498), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2999), \"['b', 'x', 'a', 'y']\": np.float64(1.5536), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.2071), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.25), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.2083), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2487), \"['b', 'x', 'b', 'x']\": np.float64(-6.4733), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2488), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2992), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2486), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2995), \"['b', 'x', 'b', 'y']\": np.float64(1.552), \"['b', 'x']\": np.float64(-8.0456), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2486), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2995), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2487), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2995), \"['b', 'y', 'a', 'x']\": np.float64(-1.5521), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2992), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0717), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2991), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0718), \"['b', 'y', 'a', 'y']\": np.float64(0.3719), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2484), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2997), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2487), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2994), \"['b', 'y', 'b', 'x']\": np.float64(-1.5519), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2995), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0718), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2994), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0718), \"['b', 'y', 'b', 'y']\": np.float64(0.3722), \"['b', 'y']\": np.float64(1.929)}\n",
      "It s train loss bro [0.30764150619506836, 0.25612160563468933, 0.2556326985359192, 0.2540523409843445, 0.25151586532592773, 0.24815663695335388, 0.2441026121377945, 0.2394745647907257, 0.23438413441181183, 0.2289333939552307, 0.22321422398090363]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 177 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.21929581463336945 and val_loss is 3.74751877784729\n",
      "for 1 epochs, loss is 0.21129924058914185 and val_loss is 3.7518014907836914\n",
      "for 2 epochs, loss is 0.20523376762866974 and val_loss is 3.756385326385498\n",
      "for 3 epochs, loss is 0.19916370511054993 and val_loss is 3.7612624168395996\n",
      "for 4 epochs, loss is 0.19313304126262665 and val_loss is 3.7664196491241455\n",
      "for 5 epochs, loss is 0.18717849254608154 and val_loss is 3.7718465328216553\n",
      "for 6 epochs, loss is 0.18132978677749634 and val_loss is 3.77752947807312\n",
      "for 7 epochs, loss is 0.17561060190200806 and val_loss is 3.7834553718566895\n",
      "for 8 epochs, loss is 0.1700393408536911 and val_loss is 3.7896087169647217\n",
      "for 9 epochs, loss is 0.164629727602005 and val_loss is 3.7959749698638916\n",
      "for 10 epochs, loss is 0.15939177572727203 and val_loss is 3.8025386333465576\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.2915), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0343), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.2923), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0335), \"['a', 'x', 'a', 'x']\": np.float64(-7.3417), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0343), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.17), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0344), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1699), \"['a', 'x', 'a', 'y']\": np.float64(1.207), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.2922), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0347), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.2929), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0335), \"['a', 'x', 'b', 'x']\": np.float64(-7.3426), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0334), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1698), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0336), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1698), \"['a', 'x', 'b', 'y']\": np.float64(1.206), \"['a', 'x']\": np.float64(-8.5672), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0347), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1701), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0348), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.17), \"['a', 'y', 'a', 'x']\": np.float64(-1.2074), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.17), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0279), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1701), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0279), \"['a', 'y', 'a', 'y']\": np.float64(0.1984), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0349), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1702), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.035), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.17), \"['a', 'y', 'b', 'x']\": np.float64(-1.2077), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.17), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0279), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.17), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0279), \"['a', 'y', 'b', 'y']\": np.float64(0.1984), \"['a', 'y']\": np.float64(1.4091), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.2932), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0346), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.294), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0338), \"['b', 'x', 'a', 'x']\": np.float64(-7.3438), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0348), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1701), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.035), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.17), \"['b', 'x', 'a', 'y']\": np.float64(1.2076), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.2939), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.035), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.2946), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0338), \"['b', 'x', 'b', 'x']\": np.float64(-7.3446), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0337), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1699), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0338), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1698), \"['b', 'x', 'b', 'y']\": np.float64(1.2063), \"['b', 'x']\": np.float64(-8.5697), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.034), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.17), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0342), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1699), \"['b', 'y', 'a', 'x']\": np.float64(-1.2067), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1699), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0279), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1699), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0279), \"['b', 'y', 'a', 'y']\": np.float64(0.1983), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0342), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1701), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0343), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1699), \"['b', 'y', 'b', 'x']\": np.float64(-1.2068), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1699), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0279), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1699), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0279), \"['b', 'y', 'b', 'y']\": np.float64(0.1982), \"['b', 'y']\": np.float64(1.4081)}\n",
      "It s train loss bro [0.21929581463336945, 0.21129924058914185, 0.20523376762866974, 0.19916370511054993, 0.19313304126262665, 0.18717849254608154, 0.18132978677749634, 0.17561060190200806, 0.1700393408536911, 0.164629727602005, 0.15939177572727203]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 178 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.887197494506836 and val_loss is 3.8028275966644287\n",
      "for 1 epochs, loss is 1.9611351490020752 and val_loss is 3.802760124206543\n",
      "for 2 epochs, loss is 1.9604781866073608 and val_loss is 3.802375555038452\n",
      "for 3 epochs, loss is 1.9582301378250122 and val_loss is 3.8017072677612305\n",
      "for 4 epochs, loss is 1.954559564590454 and val_loss is 3.8007874488830566\n",
      "for 5 epochs, loss is 1.9496190547943115 and val_loss is 3.7996456623077393\n",
      "for 6 epochs, loss is 1.9435473680496216 and val_loss is 3.7983100414276123\n",
      "for 7 epochs, loss is 1.9364694356918335 and val_loss is 3.7968051433563232\n",
      "for 8 epochs, loss is 1.9284982681274414 and val_loss is 3.7951526641845703\n",
      "for 9 epochs, loss is 1.9197367429733276 and val_loss is 3.793374538421631\n",
      "for 10 epochs, loss is 1.9102764129638672 and val_loss is 3.7914905548095703\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.1007), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0759), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.1016), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0749), \"['a', 'x', 'a', 'x']\": np.float64(-7.1926), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0758), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1896), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.076), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1896), \"['a', 'x', 'a', 'y']\": np.float64(1.2684), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.1016), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0763), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.1025), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.075), \"['a', 'x', 'b', 'x']\": np.float64(-7.1937), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0749), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1895), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.075), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1894), \"['a', 'x', 'b', 'y']\": np.float64(1.2673), \"['a', 'x']\": np.float64(-8.4799), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.076), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1898), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0762), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1896), \"['a', 'y', 'a', 'x']\": np.float64(-1.2686), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1897), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0334), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1897), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0334), \"['a', 'y', 'a', 'y']\": np.float64(0.2236), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0762), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1898), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0764), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1896), \"['a', 'y', 'b', 'x']\": np.float64(-1.2689), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1896), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0334), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1897), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0334), \"['a', 'y', 'b', 'y']\": np.float64(0.2236), \"['a', 'y']\": np.float64(1.4957), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.1025), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0762), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.1034), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0752), \"['b', 'x', 'a', 'x']\": np.float64(-7.1947), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0764), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1897), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0766), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1897), \"['b', 'x', 'a', 'y']\": np.float64(1.2691), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.1033), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0766), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.1042), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0753), \"['b', 'x', 'b', 'x']\": np.float64(-7.1958), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0751), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1895), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0753), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1894), \"['b', 'x', 'b', 'y']\": np.float64(1.2676), \"['b', 'x']\": np.float64(-8.4825), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0753), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1896), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0755), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1895), \"['b', 'y', 'a', 'x']\": np.float64(-1.2678), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1895), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0334), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1896), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0334), \"['b', 'y', 'a', 'y']\": np.float64(0.2235), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0755), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1897), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0757), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1895), \"['b', 'y', 'b', 'x']\": np.float64(-1.268), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1895), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0334), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1895), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0334), \"['b', 'y', 'b', 'y']\": np.float64(0.2234), \"['b', 'y']\": np.float64(1.4947)}\n",
      "It s train loss bro [1.887197494506836, 1.9611351490020752, 1.9604781866073608, 1.9582301378250122, 1.954559564590454, 1.9496190547943115, 1.9435473680496216, 1.9364694356918335, 1.9284982681274414, 1.9197367429733276, 1.9102764129638672]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 179 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.17282293736934662 and val_loss is 3.7757492065429688\n",
      "for 1 epochs, loss is 0.18105041980743408 and val_loss is 3.7629754543304443\n",
      "for 2 epochs, loss is 0.1962437480688095 and val_loss is 3.752713203430176\n",
      "for 3 epochs, loss is 0.21013414859771729 and val_loss is 3.744570016860962\n",
      "for 4 epochs, loss is 0.222545325756073 and val_loss is 3.7382078170776367\n",
      "for 5 epochs, loss is 0.2333592176437378 and val_loss is 3.7333412170410156\n",
      "for 6 epochs, loss is 0.242511585354805 and val_loss is 3.729734182357788\n",
      "for 7 epochs, loss is 0.2499864548444748 and val_loss is 3.7271957397460938\n",
      "for 8 epochs, loss is 0.2558092176914215 and val_loss is 3.72556471824646\n",
      "for 9 epochs, loss is 0.2600390613079071 and val_loss is 3.724717140197754\n",
      "for 10 epochs, loss is 0.26276248693466187 and val_loss is 3.7245523929595947\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.5279), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3533), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.5292), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3524), \"['a', 'x', 'a', 'x']\": np.float64(-5.8971), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3534), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4042), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3536), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4043), \"['a', 'x', 'a', 'y']\": np.float64(1.7625), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.529), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.354), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.5305), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3525), \"['a', 'x', 'b', 'x']\": np.float64(-5.8987), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3526), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.404), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3528), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4039), \"['a', 'x', 'b', 'y']\": np.float64(1.7613), \"['a', 'x']\": np.float64(-7.6803), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3508), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4037), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3512), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4035), \"['a', 'y', 'a', 'x']\": np.float64(-1.7593), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4035), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1205), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4035), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1205), \"['a', 'y', 'a', 'y']\": np.float64(0.5254), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3509), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4039), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3513), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4034), \"['a', 'y', 'b', 'x']\": np.float64(-1.7594), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4036), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1205), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4036), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1205), \"['a', 'y', 'b', 'y']\": np.float64(0.5255), \"['a', 'y']\": np.float64(2.291), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.5285), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3535), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.5298), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3526), \"['b', 'x', 'a', 'x']\": np.float64(-5.8979), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3541), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4044), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3542), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4045), \"['b', 'x', 'a', 'y']\": np.float64(1.7633), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.5299), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3543), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.5315), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3528), \"['b', 'x', 'b', 'x']\": np.float64(-5.9), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3525), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.404), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3527), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4039), \"['b', 'x', 'b', 'y']\": np.float64(1.7613), \"['b', 'x']\": np.float64(-7.6817), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.351), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4038), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3514), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4035), \"['b', 'y', 'a', 'x']\": np.float64(-1.7595), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4036), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1205), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4036), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1206), \"['b', 'y', 'a', 'y']\": np.float64(0.5255), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3512), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.404), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3516), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4035), \"['b', 'y', 'b', 'x']\": np.float64(-1.7598), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4035), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1205), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4036), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1205), \"['b', 'y', 'b', 'y']\": np.float64(0.5255), \"['b', 'y']\": np.float64(2.2913)}\n",
      "It s train loss bro [0.17282293736934662, 0.18105041980743408, 0.1962437480688095, 0.21013414859771729, 0.222545325756073, 0.2333592176437378, 0.242511585354805, 0.2499864548444748, 0.2558092176914215, 0.2600390613079071, 0.26276248693466187]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 180 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.4545886516571045 and val_loss is 3.724459409713745\n",
      "for 1 epochs, loss is 1.4713411331176758 and val_loss is 3.724297285079956\n",
      "for 2 epochs, loss is 1.4686001539230347 and val_loss is 3.724073886871338\n",
      "for 3 epochs, loss is 1.4648635387420654 and val_loss is 3.72379994392395\n",
      "for 4 epochs, loss is 1.4602395296096802 and val_loss is 3.7234842777252197\n",
      "for 5 epochs, loss is 1.4548249244689941 and val_loss is 3.7231369018554688\n",
      "for 6 epochs, loss is 1.4487078189849854 and val_loss is 3.722764492034912\n",
      "for 7 epochs, loss is 1.4419671297073364 and val_loss is 3.722374439239502\n",
      "for 8 epochs, loss is 1.4346753358840942 and val_loss is 3.721975564956665\n",
      "for 9 epochs, loss is 1.4268970489501953 and val_loss is 3.7215733528137207\n",
      "for 10 epochs, loss is 1.4186903238296509 and val_loss is 3.7211740016937256\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.2698), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3857), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2707), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3852), \"['a', 'x', 'a', 'x']\": np.float64(-5.6711), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3859), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4495), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.386), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4497), \"['a', 'x', 'a', 'y']\": np.float64(1.8405), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.2705), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3865), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2719), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3852), \"['a', 'x', 'b', 'x']\": np.float64(-5.6723), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3854), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4494), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3855), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4494), \"['a', 'x', 'b', 'y']\": np.float64(1.8398), \"['a', 'x']\": np.float64(-7.5322), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3826), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4487), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3829), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4486), \"['a', 'y', 'a', 'x']\": np.float64(-1.8364), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4485), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1454), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4485), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1455), \"['a', 'y', 'a', 'y']\": np.float64(0.5956), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3826), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4489), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.383), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4484), \"['a', 'y', 'b', 'x']\": np.float64(-1.8364), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4487), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1455), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4487), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1455), \"['a', 'y', 'b', 'y']\": np.float64(0.5958), \"['a', 'y']\": np.float64(2.4387), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.2697), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3857), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2707), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3852), \"['b', 'x', 'a', 'x']\": np.float64(-5.671), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3864), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4496), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3865), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4499), \"['b', 'x', 'a', 'y']\": np.float64(1.8412), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.2709), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3866), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2724), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3853), \"['b', 'x', 'b', 'x']\": np.float64(-5.6729), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3851), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4493), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3852), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4493), \"['b', 'x', 'b', 'y']\": np.float64(1.8394), \"['b', 'x']\": np.float64(-7.5326), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3834), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.449), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3837), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4488), \"['b', 'y', 'a', 'x']\": np.float64(-1.8374), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4488), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1456), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4488), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1456), \"['b', 'y', 'a', 'y']\": np.float64(0.596), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3835), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4492), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3839), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4487), \"['b', 'y', 'b', 'x']\": np.float64(-1.8376), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4488), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1456), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4488), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1456), \"['b', 'y', 'b', 'y']\": np.float64(0.596), \"['b', 'y']\": np.float64(2.4401)}\n",
      "It s train loss bro [1.4545886516571045, 1.4713411331176758, 1.4686001539230347, 1.4648635387420654, 1.4602395296096802, 1.4548249244689941, 1.4487078189849854, 1.4419671297073364, 1.4346753358840942, 1.4268970489501953, 1.4186903238296509]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 181 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3241167962551117 and val_loss is 3.718639850616455\n",
      "for 1 epochs, loss is 0.3081139624118805 and val_loss is 3.7174041271209717\n",
      "for 2 epochs, loss is 0.33023664355278015 and val_loss is 3.7170462608337402\n",
      "for 3 epochs, loss is 0.3495394289493561 and val_loss is 3.71722674369812\n",
      "for 4 epochs, loss is 0.36579078435897827 and val_loss is 3.7176849842071533\n",
      "for 5 epochs, loss is 0.37887808680534363 and val_loss is 3.718233346939087\n",
      "for 6 epochs, loss is 0.38879451155662537 and val_loss is 3.718742847442627\n",
      "for 7 epochs, loss is 0.39562419056892395 and val_loss is 3.7191390991210938\n",
      "for 8 epochs, loss is 0.3995237350463867 and val_loss is 3.719390392303467\n",
      "for 9 epochs, loss is 0.40070584416389465 and val_loss is 3.719498634338379\n",
      "for 10 epochs, loss is 0.39942237734794617 and val_loss is 3.719491720199585\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.0505), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4698), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.0494), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4714), \"['a', 'x', 'a', 'x']\": np.float64(-4.5332), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4696), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7084), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4691), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7091), \"['a', 'x', 'a', 'y']\": np.float64(2.1842), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.0489), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4699), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.0489), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4701), \"['a', 'x', 'b', 'x']\": np.float64(-4.5316), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.471), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.7095), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4711), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7095), \"['a', 'x', 'b', 'y']\": np.float64(2.1866), \"['a', 'x']\": np.float64(-6.7367), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.464), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7054), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4635), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7062), \"['a', 'y', 'a', 'x']\": np.float64(-2.1756), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.7056), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.3401), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7053), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3405), \"['a', 'y', 'a', 'y']\": np.float64(1.0486), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4633), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7054), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4633), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7056), \"['a', 'y', 'b', 'x']\": np.float64(-2.1749), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7062), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3406), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.7063), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3406), \"['a', 'y', 'b', 'y']\": np.float64(1.0498), \"['a', 'y']\": np.float64(3.2335), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.0475), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4683), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.0463), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4699), \"['b', 'x', 'a', 'x']\": np.float64(-4.5287), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4691), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.7081), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4685), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.7088), \"['b', 'x', 'a', 'y']\": np.float64(2.1833), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.047), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.469), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.047), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4692), \"['b', 'x', 'b', 'x']\": np.float64(-4.5288), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.469), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.7085), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4691), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.7085), \"['b', 'x', 'b', 'y']\": np.float64(2.1837), \"['b', 'x']\": np.float64(-6.7311), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4671), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.7069), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4666), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.7077), \"['b', 'y', 'a', 'x']\": np.float64(-2.1803), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.7075), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.341), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.7073), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.3414), \"['b', 'y', 'a', 'y']\": np.float64(1.0516), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.467), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.7073), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.467), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.7074), \"['b', 'y', 'b', 'x']\": np.float64(-2.1804), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.7074), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.3412), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.7075), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.3412), \"['b', 'y', 'b', 'y']\": np.float64(1.0516), \"['b', 'y']\": np.float64(3.2409)}\n",
      "It s train loss bro [0.3241167962551117, 0.3081139624118805, 0.33023664355278015, 0.3495394289493561, 0.36579078435897827, 0.37887808680534363, 0.38879451155662537, 0.39562419056892395, 0.3995237350463867, 0.40070584416389465, 0.39942237734794617]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 182 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.4125266373157501 and val_loss is 3.7195563316345215\n",
      "for 1 epochs, loss is 0.39065831899642944 and val_loss is 3.719592571258545\n",
      "for 2 epochs, loss is 0.38374170660972595 and val_loss is 3.7196640968322754\n",
      "for 3 epochs, loss is 0.3754797875881195 and val_loss is 3.7198381423950195\n",
      "for 4 epochs, loss is 0.3661375343799591 and val_loss is 3.720177412033081\n",
      "for 5 epochs, loss is 0.35596024990081787 and val_loss is 3.7207460403442383\n",
      "for 6 epochs, loss is 0.34517034888267517 and val_loss is 3.7215981483459473\n",
      "for 7 epochs, loss is 0.3339654207229614 and val_loss is 3.722782611846924\n",
      "for 8 epochs, loss is 0.32251834869384766 and val_loss is 3.724341630935669\n",
      "for 9 epochs, loss is 0.31097686290740967 and val_loss is 3.726306676864624\n",
      "for 10 epochs, loss is 0.29946526885032654 and val_loss is 3.728705883026123\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.212), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3925), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2127), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3921), \"['a', 'x', 'a', 'x']\": np.float64(-5.6197), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3927), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4601), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3927), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4604), \"['a', 'x', 'a', 'y']\": np.float64(1.8579), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.2125), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3932), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2137), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.392), \"['a', 'x', 'b', 'x']\": np.float64(-5.6207), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3923), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.46), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3924), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4601), \"['a', 'x', 'b', 'y']\": np.float64(1.8574), \"['a', 'x']\": np.float64(-7.4979), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3896), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4594), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3898), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4593), \"['a', 'y', 'a', 'x']\": np.float64(-1.854), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4591), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1517), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4591), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1518), \"['a', 'y', 'a', 'y']\": np.float64(0.6125), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3894), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4595), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3899), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4591), \"['a', 'y', 'b', 'x']\": np.float64(-1.8539), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4594), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1518), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4594), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1518), \"['a', 'y', 'b', 'y']\": np.float64(0.6129), \"['a', 'y']\": np.float64(2.4733), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.2119), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3925), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2126), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3921), \"['b', 'x', 'a', 'x']\": np.float64(-5.6195), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3932), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4602), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3931), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4605), \"['b', 'x', 'a', 'y']\": np.float64(1.8585), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.2128), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3933), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2141), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3921), \"['b', 'x', 'b', 'x']\": np.float64(-5.6212), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.392), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4599), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3921), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.46), \"['b', 'x', 'b', 'y']\": np.float64(1.857), \"['b', 'x']\": np.float64(-7.4981), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3904), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4597), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3906), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4595), \"['b', 'y', 'a', 'x']\": np.float64(-1.8551), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4595), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1518), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4595), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1519), \"['b', 'y', 'a', 'y']\": np.float64(0.613), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3904), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4598), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3908), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4595), \"['b', 'y', 'b', 'x']\": np.float64(-1.8552), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4595), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1518), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4595), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1518), \"['b', 'y', 'b', 'y']\": np.float64(0.613), \"['b', 'y']\": np.float64(2.4748)}\n",
      "It s train loss bro [0.4125266373157501, 0.39065831899642944, 0.38374170660972595, 0.3754797875881195, 0.3661375343799591, 0.35596024990081787, 0.34517034888267517, 0.3339654207229614, 0.32251834869384766, 0.31097686290740967, 0.29946526885032654]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 183 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2889977693557739 and val_loss is 3.731550455093384\n",
      "for 1 epochs, loss is 0.2769293785095215 and val_loss is 3.734856367111206\n",
      "for 2 epochs, loss is 0.26605117321014404 and val_loss is 3.7386274337768555\n",
      "for 3 epochs, loss is 0.25550106167793274 and val_loss is 3.7428603172302246\n",
      "for 4 epochs, loss is 0.24531491100788116 and val_loss is 3.7475485801696777\n",
      "for 5 epochs, loss is 0.23551784455776215 and val_loss is 3.7526814937591553\n",
      "for 6 epochs, loss is 0.22612477838993073 and val_loss is 3.7582409381866455\n",
      "for 7 epochs, loss is 0.21714337170124054 and val_loss is 3.764209747314453\n",
      "for 8 epochs, loss is 0.20857451856136322 and val_loss is 3.7705647945404053\n",
      "for 9 epochs, loss is 0.2004145234823227 and val_loss is 3.777284860610962\n",
      "for 10 epochs, loss is 0.19265541434288025 and val_loss is 3.784344434738159\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.7342), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1509), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.7349), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1502), \"['a', 'x', 'a', 'x']\": np.float64(-6.901), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1507), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2311), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1511), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2308), \"['a', 'x', 'a', 'y']\": np.float64(1.3851), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.7352), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.151), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.7354), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1504), \"['a', 'x', 'b', 'x']\": np.float64(-6.9018), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.15), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2309), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1503), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2307), \"['a', 'x', 'b', 'y']\": np.float64(1.3842), \"['a', 'x']\": np.float64(-8.3052), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1525), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2313), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1527), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2312), \"['a', 'y', 'a', 'x']\": np.float64(-1.387), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2314), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0465), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2315), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0464), \"['a', 'y', 'a', 'y']\": np.float64(0.2786), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.153), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2314), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.153), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2313), \"['a', 'y', 'b', 'x']\": np.float64(-1.3876), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2312), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0464), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2312), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0464), \"['a', 'y', 'b', 'y']\": np.float64(0.2783), \"['a', 'y']\": np.float64(1.6696), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.7374), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1516), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.7381), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1508), \"['b', 'x', 'a', 'x']\": np.float64(-6.9048), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1512), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2312), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1517), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.231), \"['b', 'x', 'a', 'y']\": np.float64(1.3858), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.7379), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1516), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.7381), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1509), \"['b', 'x', 'b', 'x']\": np.float64(-6.9051), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1507), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.231), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.151), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2308), \"['b', 'x', 'b', 'y']\": np.float64(1.385), \"['b', 'x']\": np.float64(-8.3094), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1507), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.231), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1509), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2308), \"['b', 'y', 'a', 'x']\": np.float64(-1.3849), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.231), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0464), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2311), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0463), \"['b', 'y', 'a', 'y']\": np.float64(0.2781), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1511), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.231), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1511), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2309), \"['b', 'y', 'b', 'x']\": np.float64(-1.3852), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2308), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0463), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2309), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0463), \"['b', 'y', 'b', 'y']\": np.float64(0.2778), \"['b', 'y']\": np.float64(1.6669)}\n",
      "It s train loss bro [0.2889977693557739, 0.2769293785095215, 0.26605117321014404, 0.25550106167793274, 0.24531491100788116, 0.23551784455776215, 0.22612477838993073, 0.21714337170124054, 0.20857451856136322, 0.2004145234823227, 0.19265541434288025]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 184 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.65079665184021 and val_loss is 3.7847094535827637\n",
      "for 1 epochs, loss is 1.7921066284179688 and val_loss is 3.7847259044647217\n",
      "for 2 epochs, loss is 1.7923449277877808 and val_loss is 3.784426689147949\n",
      "for 3 epochs, loss is 1.7911124229431152 and val_loss is 3.7838475704193115\n",
      "for 4 epochs, loss is 1.7885633707046509 and val_loss is 3.7830188274383545\n",
      "for 5 epochs, loss is 1.7848377227783203 and val_loss is 3.7819674015045166\n",
      "for 6 epochs, loss is 1.7800620794296265 and val_loss is 3.780722141265869\n",
      "for 7 epochs, loss is 1.7743502855300903 and val_loss is 3.7793047428131104\n",
      "for 8 epochs, loss is 1.7678074836730957 and val_loss is 3.7777390480041504\n",
      "for 9 epochs, loss is 1.7605271339416504 and val_loss is 3.7760438919067383\n",
      "for 10 epochs, loss is 1.7525935173034668 and val_loss is 3.7742390632629395\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.5713), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1824), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.5723), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1815), \"['a', 'x', 'a', 'x']\": np.float64(-6.7697), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1822), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.251), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1827), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2507), \"['a', 'x', 'a', 'y']\": np.float64(1.4367), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.5726), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1825), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.573), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1818), \"['a', 'x', 'b', 'x']\": np.float64(-6.7709), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1813), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2508), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1816), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2506), \"['a', 'x', 'b', 'y']\": np.float64(1.4356), \"['a', 'x']\": np.float64(-8.2258), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1839), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2513), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1841), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2511), \"['a', 'y', 'a', 'x']\": np.float64(-1.4386), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2513), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0534), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2514), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0533), \"['a', 'y', 'a', 'y']\": np.float64(0.3055), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1845), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2514), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1846), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2512), \"['a', 'y', 'b', 'x']\": np.float64(-1.4392), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2511), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0533), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2511), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0533), \"['a', 'y', 'b', 'y']\": np.float64(0.3051), \"['a', 'y']\": np.float64(1.7484), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.5747), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1831), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.5757), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1822), \"['b', 'x', 'a', 'x']\": np.float64(-6.7738), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1828), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2511), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1833), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2509), \"['b', 'x', 'a', 'y']\": np.float64(1.4375), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.5754), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1832), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.5758), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1824), \"['b', 'x', 'b', 'x']\": np.float64(-6.7743), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.182), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2509), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1824), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2507), \"['b', 'x', 'b', 'y']\": np.float64(1.4365), \"['b', 'x']\": np.float64(-8.2304), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1819), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2508), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1821), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2506), \"['b', 'y', 'a', 'x']\": np.float64(-1.4362), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2509), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0533), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.251), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0532), \"['b', 'y', 'a', 'y']\": np.float64(0.3049), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1824), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2509), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1824), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2507), \"['b', 'y', 'b', 'x']\": np.float64(-1.4366), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2507), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0532), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2507), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0532), \"['b', 'y', 'b', 'y']\": np.float64(0.3046), \"['b', 'y']\": np.float64(1.7453)}\n",
      "It s train loss bro [1.65079665184021, 1.7921066284179688, 1.7923449277877808, 1.7911124229431152, 1.7885633707046509, 1.7848377227783203, 1.7800620794296265, 1.7743502855300903, 1.7678074836730957, 1.7605271339416504, 1.7525935173034668]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 185 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2172321230173111 and val_loss is 3.759427785873413\n",
      "for 1 epochs, loss is 0.2115134596824646 and val_loss is 3.7475030422210693\n",
      "for 2 epochs, loss is 0.22642090916633606 and val_loss is 3.738029718399048\n",
      "for 3 epochs, loss is 0.23973526060581207 and val_loss is 3.730635643005371\n",
      "for 4 epochs, loss is 0.2513343393802643 and val_loss is 3.725003957748413\n",
      "for 5 epochs, loss is 0.2611532509326935 and val_loss is 3.7208683490753174\n",
      "for 6 epochs, loss is 0.26917752623558044 and val_loss is 3.718008279800415\n",
      "for 7 epochs, loss is 0.27543655037879944 and val_loss is 3.716242551803589\n",
      "for 8 epochs, loss is 0.2799948453903198 and val_loss is 3.715423345565796\n",
      "for 9 epochs, loss is 0.2829451262950897 and val_loss is 3.7154314517974854\n",
      "for 10 epochs, loss is 0.28440049290657043 and val_loss is 3.716172456741333\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.2617), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3853), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2639), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3834), \"['a', 'x', 'a', 'x']\": np.float64(-5.6627), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3853), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4502), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3861), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4497), \"['a', 'x', 'a', 'y']\": np.float64(1.8408), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.2641), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.386), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2659), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3842), \"['a', 'x', 'b', 'x']\": np.float64(-5.6657), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3834), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4496), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.384), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4491), \"['a', 'x', 'b', 'y']\": np.float64(1.8382), \"['a', 'x']\": np.float64(-7.5243), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3852), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4503), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.386), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4497), \"['a', 'y', 'a', 'x']\": np.float64(-1.8407), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4502), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1463), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4505), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1462), \"['a', 'y', 'a', 'y']\": np.float64(0.5982), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3861), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4505), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3867), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.45), \"['a', 'y', 'b', 'x']\": np.float64(-1.8417), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4497), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1461), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4499), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.146), \"['a', 'y', 'b', 'y']\": np.float64(0.5976), \"['a', 'y']\": np.float64(2.4458), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.2653), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3865), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2676), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3846), \"['b', 'x', 'a', 'x']\": np.float64(-5.6676), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3864), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4506), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3872), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4501), \"['b', 'x', 'a', 'y']\": np.float64(1.8422), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.2673), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3871), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2691), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3853), \"['b', 'x', 'b', 'x']\": np.float64(-5.67), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3847), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.45), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3853), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4495), \"['b', 'x', 'b', 'y']\": np.float64(1.8398), \"['b', 'x']\": np.float64(-7.5305), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3829), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4495), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3837), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4489), \"['b', 'y', 'a', 'x']\": np.float64(-1.8376), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4494), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1461), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4497), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1459), \"['b', 'y', 'a', 'y']\": np.float64(0.5972), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3836), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4497), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3842), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4492), \"['b', 'y', 'b', 'x']\": np.float64(-1.8384), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.449), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1459), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4492), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1458), \"['b', 'y', 'b', 'y']\": np.float64(0.5966), \"['b', 'y']\": np.float64(2.4416)}\n",
      "It s train loss bro [0.2172321230173111, 0.2115134596824646, 0.22642090916633606, 0.23973526060581207, 0.2513343393802643, 0.2611532509326935, 0.26917752623558044, 0.27543655037879944, 0.2799948453903198, 0.2829451262950897, 0.28440049290657043]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 186 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2937386631965637 and val_loss is 3.717707633972168\n",
      "for 1 epochs, loss is 0.2834002673625946 and val_loss is 3.719822645187378\n",
      "for 2 epochs, loss is 0.28121381998062134 and val_loss is 3.722470760345459\n",
      "for 3 epochs, loss is 0.27806904911994934 and val_loss is 3.725613594055176\n",
      "for 4 epochs, loss is 0.2741013765335083 and val_loss is 3.729219436645508\n",
      "for 5 epochs, loss is 0.2694407105445862 and val_loss is 3.733262062072754\n",
      "for 6 epochs, loss is 0.2642086148262024 and val_loss is 3.7377192974090576\n",
      "for 7 epochs, loss is 0.25851738452911377 and val_loss is 3.742570638656616\n",
      "for 8 epochs, loss is 0.25246983766555786 and val_loss is 3.7477986812591553\n",
      "for 9 epochs, loss is 0.2461579591035843 and val_loss is 3.7533836364746094\n",
      "for 10 epochs, loss is 0.23966340720653534 and val_loss is 3.759310007095337\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.9725), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2876), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.9741), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2863), \"['a', 'x', 'a', 'x']\": np.float64(-6.2758), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2875), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3334), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.288), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3331), \"['a', 'x', 'a', 'y']\": np.float64(1.6251), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.9743), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.288), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.9754), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2868), \"['a', 'x', 'b', 'x']\": np.float64(-6.2778), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2862), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.333), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2866), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3328), \"['a', 'x', 'b', 'y']\": np.float64(1.6234), \"['a', 'x']\": np.float64(-7.9207), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2883), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3336), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2887), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3332), \"['a', 'y', 'a', 'x']\": np.float64(-1.6259), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3336), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0864), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3337), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0863), \"['a', 'y', 'a', 'y']\": np.float64(0.4211), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2889), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3337), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2892), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3334), \"['a', 'y', 'b', 'x']\": np.float64(-1.6266), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3333), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0863), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3334), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0862), \"['a', 'y', 'b', 'y']\": np.float64(0.4206), \"['a', 'y']\": np.float64(2.0523), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.9757), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2885), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.9773), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2871), \"['b', 'x', 'a', 'x']\": np.float64(-6.2799), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2882), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3336), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2888), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3333), \"['b', 'x', 'a', 'y']\": np.float64(1.626), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.977), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2887), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.9781), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2875), \"['b', 'x', 'b', 'x']\": np.float64(-6.2812), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2871), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3332), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2875), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.333), \"['b', 'x', 'b', 'y']\": np.float64(1.6245), \"['b', 'x']\": np.float64(-7.9255), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2864), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3331), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2868), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3328), \"['b', 'y', 'a', 'x']\": np.float64(-1.6236), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.333), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0862), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3332), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0862), \"['b', 'y', 'a', 'y']\": np.float64(0.4204), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2869), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3332), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2871), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3329), \"['b', 'y', 'b', 'x']\": np.float64(-1.6241), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3328), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0862), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3329), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0861), \"['b', 'y', 'b', 'y']\": np.float64(0.42), \"['b', 'y']\": np.float64(2.0492)}\n",
      "It s train loss bro [0.2937386631965637, 0.2834002673625946, 0.28121381998062134, 0.27806904911994934, 0.2741013765335083, 0.2694407105445862, 0.2642086148262024, 0.25851738452911377, 0.25246983766555786, 0.2461579591035843, 0.23966340720653534]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 187 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.23655344545841217 and val_loss is 3.76560378074646\n",
      "for 1 epochs, loss is 0.2264164388179779 and val_loss is 3.7721970081329346\n",
      "for 2 epochs, loss is 0.2197769582271576 and val_loss is 3.7790727615356445\n",
      "for 3 epochs, loss is 0.21318449079990387 and val_loss is 3.786212921142578\n",
      "for 4 epochs, loss is 0.2066766619682312 and val_loss is 3.7935993671417236\n",
      "for 5 epochs, loss is 0.20028358697891235 and val_loss is 3.8012115955352783\n",
      "for 6 epochs, loss is 0.1940302699804306 and val_loss is 3.8090319633483887\n",
      "for 7 epochs, loss is 0.187935471534729 and val_loss is 3.817042112350464\n",
      "for 8 epochs, loss is 0.18201398849487305 and val_loss is 3.8252241611480713\n",
      "for 9 epochs, loss is 0.1762763410806656 and val_loss is 3.8335580825805664\n",
      "for 10 epochs, loss is 0.17072951793670654 and val_loss is 3.8420283794403076\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.0929), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0782), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.093), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.078), \"['a', 'x', 'a', 'x']\": np.float64(-7.1857), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0779), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1909), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0783), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1907), \"['a', 'x', 'a', 'y']\": np.float64(1.2716), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.0933), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0782), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.0929), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0781), \"['a', 'x', 'b', 'x']\": np.float64(-7.1858), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0777), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1909), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.078), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1907), \"['a', 'x', 'b', 'y']\": np.float64(1.2713), \"['a', 'x']\": np.float64(-8.4745), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0804), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1912), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0804), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1911), \"['a', 'y', 'a', 'x']\": np.float64(-1.2742), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1913), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0339), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1914), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0339), \"['a', 'y', 'a', 'y']\": np.float64(0.2257), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0808), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1912), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0807), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1912), \"['a', 'y', 'b', 'x']\": np.float64(-1.2746), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1911), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0339), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1912), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0338), \"['a', 'y', 'b', 'y']\": np.float64(0.2255), \"['a', 'y']\": np.float64(1.5031), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.0957), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0787), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.0958), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0785), \"['b', 'x', 'a', 'x']\": np.float64(-7.189), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0783), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.191), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0787), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1908), \"['b', 'x', 'a', 'y']\": np.float64(1.2721), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.0956), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0786), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.0952), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0785), \"['b', 'x', 'b', 'x']\": np.float64(-7.1885), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0783), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.191), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0785), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1908), \"['b', 'x', 'b', 'y']\": np.float64(1.2719), \"['b', 'x']\": np.float64(-8.478), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0789), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1909), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0789), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1909), \"['b', 'y', 'a', 'x']\": np.float64(-1.2724), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.191), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0338), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1911), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0338), \"['b', 'y', 'a', 'y']\": np.float64(0.2253), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0792), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.191), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0791), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1909), \"['b', 'y', 'b', 'x']\": np.float64(-1.2727), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1909), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0338), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1909), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0338), \"['b', 'y', 'b', 'y']\": np.float64(0.2252), \"['b', 'y']\": np.float64(1.5009)}\n",
      "It s train loss bro [0.23655344545841217, 0.2264164388179779, 0.2197769582271576, 0.21318449079990387, 0.2066766619682312, 0.20028358697891235, 0.1940302699804306, 0.187935471534729, 0.18201398849487305, 0.1762763410806656, 0.17072951793670654]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 188 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.18659891188144684 and val_loss is 3.8507580757141113\n",
      "for 1 epochs, loss is 0.16020673513412476 and val_loss is 3.8595755100250244\n",
      "for 2 epochs, loss is 0.1552349030971527 and val_loss is 3.86846661567688\n",
      "for 3 epochs, loss is 0.1504603773355484 and val_loss is 3.8774161338806152\n",
      "for 4 epochs, loss is 0.14587944746017456 and val_loss is 3.8864119052886963\n",
      "for 5 epochs, loss is 0.14148813486099243 and val_loss is 3.895439386367798\n",
      "for 6 epochs, loss is 0.13728095591068268 and val_loss is 3.9044880867004395\n",
      "for 7 epochs, loss is 0.13325247168540955 and val_loss is 3.913545846939087\n",
      "for 8 epochs, loss is 0.12939615547657013 and val_loss is 3.9226014614105225\n",
      "for 9 epochs, loss is 0.12570585310459137 and val_loss is 3.9316463470458984\n",
      "for 10 epochs, loss is 0.12217500805854797 and val_loss is 3.9406683444976807\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.008), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8673), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.0066), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8682), \"['a', 'x', 'a', 'x']\": np.float64(-7.8879), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.867), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1076), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8672), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1074), \"['a', 'x', 'a', 'y']\": np.float64(0.9762), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.0069), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8671), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.005), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8682), \"['a', 'x', 'b', 'x']\": np.float64(-7.8863), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8679), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1077), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8681), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1075), \"['a', 'x', 'b', 'y']\": np.float64(0.9772), \"['a', 'x']\": np.float64(-8.8783), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8709), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1078), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8707), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1079), \"['a', 'y', 'a', 'x']\": np.float64(-0.9802), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.108), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0134), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.108), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0134), \"['a', 'y', 'a', 'y']\": np.float64(0.1216), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8712), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1078), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8709), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1079), \"['a', 'y', 'b', 'x']\": np.float64(-0.9805), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1079), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0134), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1079), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0134), \"['a', 'y', 'b', 'y']\": np.float64(0.1215), \"['a', 'y']\": np.float64(1.1038), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.0104), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8676), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.009), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8685), \"['b', 'x', 'a', 'x']\": np.float64(-7.8906), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8672), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1076), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8675), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1075), \"['b', 'x', 'a', 'y']\": np.float64(0.9765), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.0088), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8674), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.0069), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8684), \"['b', 'x', 'b', 'x']\": np.float64(-7.8885), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8683), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1077), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8685), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1076), \"['b', 'x', 'b', 'y']\": np.float64(0.9777), \"['b', 'x']\": np.float64(-8.881), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8696), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1076), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8694), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1077), \"['b', 'y', 'a', 'x']\": np.float64(-0.9787), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1078), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0134), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1079), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0134), \"['b', 'y', 'a', 'y']\": np.float64(0.1214), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8698), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1076), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8695), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1078), \"['b', 'y', 'b', 'x']\": np.float64(-0.9789), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1077), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0134), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1077), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0133), \"['b', 'y', 'b', 'y']\": np.float64(0.1213), \"['b', 'y']\": np.float64(1.102)}\n",
      "It s train loss bro [0.18659891188144684, 0.16020673513412476, 0.1552349030971527, 0.1504603773355484, 0.14587944746017456, 0.14148813486099243, 0.13728095591068268, 0.13325247168540955, 0.12939615547657013, 0.12570585310459137, 0.12217500805854797]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 189 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1436128318309784 and val_loss is 3.949786424636841\n",
      "for 1 epochs, loss is 0.1155296117067337 and val_loss is 3.9588565826416016\n",
      "for 2 epochs, loss is 0.11240705102682114 and val_loss is 3.9678709506988525\n",
      "for 3 epochs, loss is 0.10942217707633972 and val_loss is 3.9768261909484863\n",
      "for 4 epochs, loss is 0.1065681129693985 and val_loss is 3.9857170581817627\n",
      "for 5 epochs, loss is 0.10383827239274979 and val_loss is 3.994537830352783\n",
      "for 6 epochs, loss is 0.10122625529766083 and val_loss is 4.0032854080200195\n",
      "for 7 epochs, loss is 0.0987263172864914 and val_loss is 4.011956214904785\n",
      "for 8 epochs, loss is 0.09633227437734604 and val_loss is 4.020547866821289\n",
      "for 9 epochs, loss is 0.0940389484167099 and val_loss is 4.029056549072266\n",
      "for 10 epochs, loss is 0.09184083342552185 and val_loss is 4.037480354309082\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.6473), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7027), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.6448), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7044), \"['a', 'x', 'a', 'x']\": np.float64(-8.3604), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7024), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0648), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7025), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0647), \"['a', 'x', 'a', 'y']\": np.float64(0.7683), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.6449), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7026), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.6422), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7043), \"['a', 'x', 'b', 'x']\": np.float64(-8.3578), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7041), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0649), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7042), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0649), \"['a', 'x', 'b', 'y']\": np.float64(0.7701), \"['a', 'x']\": np.float64(-9.1401), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.707), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.065), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7068), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0651), \"['a', 'y', 'a', 'x']\": np.float64(-0.7729), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0652), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.006), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0652), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.006), \"['a', 'y', 'a', 'y']\": np.float64(0.0713), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7072), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.065), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7069), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0651), \"['a', 'y', 'b', 'x']\": np.float64(-0.7731), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0651), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.006), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0651), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.006), \"['a', 'y', 'b', 'y']\": np.float64(0.0712), \"['a', 'y']\": np.float64(0.8454), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.6491), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7029), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.6467), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7046), \"['b', 'x', 'a', 'x']\": np.float64(-8.3624), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7026), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0648), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7028), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0647), \"['b', 'x', 'a', 'y']\": np.float64(0.7686), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.6465), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7028), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.6438), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7044), \"['b', 'x', 'b', 'x']\": np.float64(-8.3595), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7043), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.065), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7044), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0649), \"['b', 'x', 'b', 'y']\": np.float64(0.7704), \"['b', 'x']\": np.float64(-9.1422), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7058), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0649), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7056), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.065), \"['b', 'y', 'a', 'x']\": np.float64(-0.7717), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0651), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.006), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0651), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.006), \"['b', 'y', 'a', 'y']\": np.float64(0.0712), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.706), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0649), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7057), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.065), \"['b', 'y', 'b', 'x']\": np.float64(-0.7718), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.065), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.006), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.065), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.006), \"['b', 'y', 'b', 'y']\": np.float64(0.0711), \"['b', 'y']\": np.float64(0.844)}\n",
      "It s train loss bro [0.1436128318309784, 0.1155296117067337, 0.11240705102682114, 0.10942217707633972, 0.1065681129693985, 0.10383827239274979, 0.10122625529766083, 0.0987263172864914, 0.09633227437734604, 0.0940389484167099, 0.09184083342552185]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 190 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.10676601529121399 and val_loss is 4.045994758605957\n",
      "for 1 epochs, loss is 0.08769982308149338 and val_loss is 4.054403305053711\n",
      "for 2 epochs, loss is 0.08574924618005753 and val_loss is 4.0627055168151855\n",
      "for 3 epochs, loss is 0.0838770791888237 and val_loss is 4.070903778076172\n",
      "for 4 epochs, loss is 0.08207914978265762 and val_loss is 4.078996181488037\n",
      "for 5 epochs, loss is 0.0803515762090683 and val_loss is 4.086987495422363\n",
      "for 6 epochs, loss is 0.07869065552949905 and val_loss is 4.094875335693359\n",
      "for 7 epochs, loss is 0.0770929753780365 and val_loss is 4.102662563323975\n",
      "for 8 epochs, loss is 0.07555507868528366 and val_loss is 4.110348224639893\n",
      "for 9 epochs, loss is 0.0740741491317749 and val_loss is 4.117936134338379\n",
      "for 10 epochs, loss is 0.07264711707830429 and val_loss is 4.12542724609375\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.0824), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5837), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.0797), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5854), \"['a', 'x', 'a', 'x']\": np.float64(-8.6748), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5834), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0423), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5835), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0422), \"['a', 'x', 'a', 'y']\": np.float64(0.6264), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.0794), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5839), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.0769), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5853), \"['a', 'x', 'b', 'x']\": np.float64(-8.6719), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5852), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0425), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5853), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0424), \"['a', 'x', 'b', 'y']\": np.float64(0.6283), \"['a', 'x']\": np.float64(-9.3105), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5879), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0425), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5877), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0426), \"['a', 'y', 'a', 'x']\": np.float64(-0.631), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0426), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0031), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0427), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0031), \"['a', 'y', 'a', 'y']\": np.float64(0.0458), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.588), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0425), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5878), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0426), \"['a', 'y', 'b', 'x']\": np.float64(-0.6311), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0426), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0031), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0426), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0031), \"['a', 'y', 'b', 'y']\": np.float64(0.0457), \"['a', 'y']\": np.float64(0.6775), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.0836), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5838), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.0809), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5855), \"['b', 'x', 'a', 'x']\": np.float64(-8.6761), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5839), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0424), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5841), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0423), \"['b', 'x', 'a', 'y']\": np.float64(0.627), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.0808), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.584), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.0782), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5854), \"['b', 'x', 'b', 'x']\": np.float64(-8.6733), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5854), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0425), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5855), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0424), \"['b', 'x', 'b', 'y']\": np.float64(0.6286), \"['b', 'x']\": np.float64(-9.3123), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5869), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0424), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5867), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0425), \"['b', 'y', 'a', 'x']\": np.float64(-0.6299), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0426), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0031), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0426), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0031), \"['b', 'y', 'a', 'y']\": np.float64(0.0457), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.587), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0424), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5868), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0425), \"['b', 'y', 'b', 'x']\": np.float64(-0.63), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0425), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0031), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0425), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0031), \"['b', 'y', 'b', 'y']\": np.float64(0.0456), \"['b', 'y']\": np.float64(0.6764)}\n",
      "It s train loss bro [0.10676601529121399, 0.08769982308149338, 0.08574924618005753, 0.0838770791888237, 0.08207914978265762, 0.0803515762090683, 0.07869065552949905, 0.0770929753780365, 0.07555507868528366, 0.0740741491317749, 0.07264711707830429]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 191 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.09176307916641235 and val_loss is 4.133016586303711\n",
      "for 1 epochs, loss is 0.06992767006158829 and val_loss is 4.140493392944336\n",
      "for 2 epochs, loss is 0.06863250583410263 and val_loss is 4.147858619689941\n",
      "for 3 epochs, loss is 0.06738314032554626 and val_loss is 4.155116081237793\n",
      "for 4 epochs, loss is 0.06617739796638489 and val_loss is 4.162267208099365\n",
      "for 5 epochs, loss is 0.06501299142837524 and val_loss is 4.169318199157715\n",
      "for 6 epochs, loss is 0.0638878121972084 and val_loss is 4.176269054412842\n",
      "for 7 epochs, loss is 0.06279988586902618 and val_loss is 4.1831231117248535\n",
      "for 8 epochs, loss is 0.06174764409661293 and val_loss is 4.189883708953857\n",
      "for 9 epochs, loss is 0.060729071497917175 and val_loss is 4.196552276611328\n",
      "for 10 epochs, loss is 0.059742700308561325 and val_loss is 4.203132629394531\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3893), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4967), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3871), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4979), \"['a', 'x', 'a', 'x']\": np.float64(-8.8934), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4965), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0295), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4966), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0295), \"['a', 'x', 'a', 'y']\": np.float64(0.5265), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3863), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4975), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3847), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4979), \"['a', 'x', 'b', 'x']\": np.float64(-8.891), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4977), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0296), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4978), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0295), \"['a', 'x', 'b', 'y']\": np.float64(0.5278), \"['a', 'x']\": np.float64(-9.4277), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5002), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0296), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5001), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0297), \"['a', 'y', 'a', 'x']\": np.float64(-0.5302), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0297), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0018), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0297), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0018), \"['a', 'y', 'a', 'y']\": np.float64(0.0315), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5002), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0297), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5001), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0297), \"['a', 'y', 'b', 'x']\": np.float64(-0.5303), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0297), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0018), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0297), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0018), \"['a', 'y', 'b', 'y']\": np.float64(0.0315), \"['a', 'y']\": np.float64(0.5623), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3898), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4967), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3876), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.498), \"['b', 'x', 'a', 'x']\": np.float64(-8.8938), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4975), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0296), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4976), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0295), \"['b', 'x', 'a', 'y']\": np.float64(0.5276), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3874), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4976), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3858), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4979), \"['b', 'x', 'b', 'x']\": np.float64(-8.8922), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4979), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0296), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.498), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0295), \"['b', 'x', 'b', 'y']\": np.float64(0.528), \"['b', 'x']\": np.float64(-9.4291), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4993), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0296), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4992), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0296), \"['b', 'y', 'a', 'x']\": np.float64(-0.5293), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0297), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0018), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0297), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0018), \"['b', 'y', 'a', 'y']\": np.float64(0.0315), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4993), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0296), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4992), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0296), \"['b', 'y', 'b', 'x']\": np.float64(-0.5294), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0296), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0018), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0296), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0018), \"['b', 'y', 'b', 'y']\": np.float64(0.0314), \"['b', 'y']\": np.float64(0.5613)}\n",
      "It s train loss bro [0.09176307916641235, 0.06992767006158829, 0.06863250583410263, 0.06738314032554626, 0.06617739796638489, 0.06501299142837524, 0.0638878121972084, 0.06279988586902618, 0.06174764409661293, 0.060729071497917175, 0.059742700308561325]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 192 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.067935511469841 and val_loss is 4.209705352783203\n",
      "for 1 epochs, loss is 0.057853445410728455 and val_loss is 4.2161865234375\n",
      "for 2 epochs, loss is 0.056948695331811905 and val_loss is 4.222579002380371\n",
      "for 3 epochs, loss is 0.05607141554355621 and val_loss is 4.228883743286133\n",
      "for 4 epochs, loss is 0.05522032454609871 and val_loss is 4.235104084014893\n",
      "for 5 epochs, loss is 0.054394252598285675 and val_loss is 4.241244316101074\n",
      "for 6 epochs, loss is 0.053591903299093246 and val_loss is 4.247304916381836\n",
      "for 7 epochs, loss is 0.05281231552362442 and val_loss is 4.253289222717285\n",
      "for 8 epochs, loss is 0.05205463990569115 and val_loss is 4.259199142456055\n",
      "for 9 epochs, loss is 0.05131767690181732 and val_loss is 4.265035629272461\n",
      "for 10 epochs, loss is 0.0506008006632328 and val_loss is 4.270803451538086\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6131), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.432), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.612), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.432), \"['a', 'x', 'a', 'x']\": np.float64(-9.0513), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4319), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0217), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.432), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0216), \"['a', 'x', 'a', 'y']\": np.float64(0.454), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.6104), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4337), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.6109), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.432), \"['a', 'x', 'b', 'x']\": np.float64(-9.0503), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4319), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0217), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.432), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0217), \"['a', 'x', 'b', 'y']\": np.float64(0.454), \"['a', 'x']\": np.float64(-9.5119), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4341), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0218), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.434), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0218), \"['a', 'y', 'a', 'x']\": np.float64(-0.4561), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0218), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0011), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0218), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0011), \"['a', 'y', 'a', 'y']\": np.float64(0.0229), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4341), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0219), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4341), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0218), \"['a', 'y', 'b', 'x']\": np.float64(-0.4562), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0217), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0011), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0218), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0011), \"['a', 'y', 'b', 'y']\": np.float64(0.0229), \"['a', 'y']\": np.float64(0.4795), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6125), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4319), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6115), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.432), \"['b', 'x', 'a', 'x']\": np.float64(-9.0507), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4337), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0218), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4338), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0217), \"['b', 'x', 'a', 'y']\": np.float64(0.4559), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.6113), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4337), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.6118), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4321), \"['b', 'x', 'b', 'x']\": np.float64(-9.0512), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.432), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0217), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4321), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0217), \"['b', 'x', 'b', 'y']\": np.float64(0.4541), \"['b', 'x']\": np.float64(-9.5131), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4333), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0217), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4332), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0217), \"['b', 'y', 'a', 'x']\": np.float64(-0.4553), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0218), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0011), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0218), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0011), \"['b', 'y', 'a', 'y']\": np.float64(0.0229), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4333), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0218), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4333), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0217), \"['b', 'y', 'b', 'x']\": np.float64(-0.4554), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0217), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0011), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0217), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0011), \"['b', 'y', 'b', 'y']\": np.float64(0.0228), \"['b', 'y']\": np.float64(0.4786)}\n",
      "It s train loss bro [0.067935511469841, 0.057853445410728455, 0.056948695331811905, 0.05607141554355621, 0.05522032454609871, 0.054394252598285675, 0.053591903299093246, 0.05281231552362442, 0.05205463990569115, 0.05131767690181732, 0.0506008006632328]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 193 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.05521615222096443 and val_loss is 4.276562690734863\n",
      "for 1 epochs, loss is 0.04922008886933327 and val_loss is 4.28225040435791\n",
      "for 2 epochs, loss is 0.04855530709028244 and val_loss is 4.287866115570068\n",
      "for 3 epochs, loss is 0.04790780693292618 and val_loss is 4.293413162231445\n",
      "for 4 epochs, loss is 0.04727683216333389 and val_loss is 4.298895359039307\n",
      "for 5 epochs, loss is 0.0466616153717041 and val_loss is 4.304312229156494\n",
      "for 6 epochs, loss is 0.04606172814965248 and val_loss is 4.309667110443115\n",
      "for 7 epochs, loss is 0.0454765185713768 and val_loss is 4.314960956573486\n",
      "for 8 epochs, loss is 0.044905442744493484 and val_loss is 4.32019567489624\n",
      "for 9 epochs, loss is 0.04434795305132866 and val_loss is 4.325372695922852\n",
      "for 10 epochs, loss is 0.0438033863902092 and val_loss is 4.330493927001953\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.7817), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3827), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.7827), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3807), \"['a', 'x', 'a', 'x']\": np.float64(-9.1699), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3827), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0166), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3828), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0166), \"['a', 'x', 'a', 'y']\": np.float64(0.3997), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7799), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3857), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.7837), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3808), \"['a', 'x', 'b', 'x']\": np.float64(-9.171), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3807), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0165), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3807), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0165), \"['a', 'x', 'b', 'y']\": np.float64(0.3975), \"['a', 'x']\": np.float64(-9.5753), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3826), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0167), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3826), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0166), \"['a', 'y', 'a', 'x']\": np.float64(-0.3995), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0166), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0166), \"['a', 'y', 'a', 'y']\": np.float64(0.0174), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3825), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0168), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3827), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0166), \"['a', 'y', 'b', 'x']\": np.float64(-0.3996), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0166), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0166), \"['a', 'y', 'b', 'y']\": np.float64(0.0173), \"['a', 'y']\": np.float64(0.4171), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.7798), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3827), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.7808), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3806), \"['b', 'x', 'a', 'x']\": np.float64(-9.1679), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3857), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0168), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3858), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0167), \"['b', 'x', 'a', 'y']\": np.float64(0.4028), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.7807), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3858), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.7845), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3808), \"['b', 'x', 'b', 'x']\": np.float64(-9.1719), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3808), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0165), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3808), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0165), \"['b', 'x', 'b', 'y']\": np.float64(0.3976), \"['b', 'x']\": np.float64(-9.5763), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3819), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0166), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.382), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0166), \"['b', 'y', 'a', 'x']\": np.float64(-0.3988), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0166), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0166), \"['b', 'y', 'a', 'y']\": np.float64(0.0173), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3818), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0168), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.382), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0166), \"['b', 'y', 'b', 'x']\": np.float64(-0.3988), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0165), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0165), \"['b', 'y', 'b', 'y']\": np.float64(0.0173), \"['b', 'y']\": np.float64(0.4164)}\n",
      "It s train loss bro [0.05521615222096443, 0.04922008886933327, 0.04855530709028244, 0.04790780693292618, 0.04727683216333389, 0.0466616153717041, 0.04606172814965248, 0.0454765185713768, 0.044905442744493484, 0.04434795305132866, 0.0438033863902092]\n",
      "% good predict : 0\n",
      "\u001b[0;34miteration 194 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.06415941566228867 and val_loss is 4.335800647735596\n",
      "for 1 epochs, loss is 0.04274009168148041 and val_loss is 4.341031551361084\n",
      "for 2 epochs, loss is 0.042221616953611374 and val_loss is 4.346187591552734\n",
      "for 3 epochs, loss is 0.04171591252088547 and val_loss is 4.351273536682129\n",
      "for 4 epochs, loss is 0.04122207686305046 and val_loss is 4.356292247772217\n",
      "for 5 epochs, loss is 0.04073978587985039 and val_loss is 4.3612446784973145\n",
      "for 6 epochs, loss is 0.04026859626173973 and val_loss is 4.366136074066162\n",
      "for 7 epochs, loss is 0.03980795294046402 and val_loss is 4.370967388153076\n",
      "for 8 epochs, loss is 0.03935764357447624 and val_loss is 4.3757405281066895\n",
      "for 9 epochs, loss is 0.03891710564494133 and val_loss is 4.380459308624268\n",
      "for 10 epochs, loss is 0.03848612308502197 and val_loss is 4.385126113891602\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.9133), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3444), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9176), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3391), \"['a', 'x', 'a', 'x']\": np.float64(-9.2625), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3445), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0131), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3445), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0131), \"['a', 'x', 'a', 'y']\": np.float64(0.3578), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.913), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3492), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.9218), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3393), \"['a', 'x', 'b', 'x']\": np.float64(-9.267), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3392), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0129), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3393), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0129), \"['a', 'x', 'b', 'y']\": np.float64(0.3524), \"['a', 'x']\": np.float64(-9.6254), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3408), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0132), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.341), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.013), \"['a', 'y', 'a', 'x']\": np.float64(-0.3542), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.013), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.013), \"['a', 'y', 'a', 'y']\": np.float64(0.0135), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3407), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0134), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3411), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.013), \"['a', 'y', 'b', 'x']\": np.float64(-0.3542), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.013), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.013), \"['a', 'y', 'b', 'y']\": np.float64(0.0135), \"['a', 'y']\": np.float64(0.3679), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.9095), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3442), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9138), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3389), \"['b', 'x', 'a', 'x']\": np.float64(-9.2585), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3492), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0133), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3493), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0133), \"['b', 'x', 'a', 'y']\": np.float64(0.3628), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.9137), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3493), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.9225), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3393), \"['b', 'x', 'b', 'x']\": np.float64(-9.2677), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3393), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0129), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3393), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0129), \"['b', 'x', 'b', 'y']\": np.float64(0.3525), \"['b', 'x']\": np.float64(-9.6263), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3402), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0131), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3404), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0129), \"['b', 'y', 'a', 'x']\": np.float64(-0.3536), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.013), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.013), \"['b', 'y', 'a', 'y']\": np.float64(0.0135), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3401), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0133), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3404), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0129), \"['b', 'y', 'b', 'x']\": np.float64(-0.3536), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0129), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0129), \"['b', 'y', 'b', 'y']\": np.float64(0.0134), \"['b', 'y']\": np.float64(0.3673)}\n",
      "It s train loss bro [0.06415941566228867, 0.04274009168148041, 0.042221616953611374, 0.04171591252088547, 0.04122207686305046, 0.04073978587985039, 0.04026859626173973, 0.03980795294046402, 0.03935764357447624, 0.03891710564494133, 0.03848612308502197]\n",
      "% good predict : 0\n",
      "\u001b[0;34miteration 195 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.03926778584718704 and val_loss is 4.389747619628906\n",
      "for 1 epochs, loss is 0.03765058144927025 and val_loss is 4.394318580627441\n",
      "for 2 epochs, loss is 0.03724558278918266 and val_loss is 4.398841857910156\n",
      "for 3 epochs, loss is 0.03684869408607483 and val_loss is 4.403317451477051\n",
      "for 4 epochs, loss is 0.03645992651581764 and val_loss is 4.407748699188232\n",
      "for 5 epochs, loss is 0.03607882186770439 and val_loss is 4.412134647369385\n",
      "for 6 epochs, loss is 0.035705167800188065 and val_loss is 4.416477680206299\n",
      "for 7 epochs, loss is 0.03533884882926941 and val_loss is 4.420779705047607\n",
      "for 8 epochs, loss is 0.03497942164540291 and val_loss is 4.425041198730469\n",
      "for 9 epochs, loss is 0.03462688624858856 and val_loss is 4.429262638092041\n",
      "for 10 epochs, loss is 0.03428091108798981 and val_loss is 4.433446884155273\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.0143), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3155), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.0235), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3053), \"['a', 'x', 'a', 'x']\": np.float64(-9.3341), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3158), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0107), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3158), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0107), \"['a', 'x', 'a', 'y']\": np.float64(0.3267), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.0161), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3232), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.0327), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3056), \"['a', 'x', 'b', 'x']\": np.float64(-9.3437), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3056), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0104), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3056), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0103), \"['a', 'x', 'b', 'y']\": np.float64(0.3161), \"['a', 'x']\": np.float64(-9.6653), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3069), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0107), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3072), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0104), \"['a', 'y', 'a', 'x']\": np.float64(-0.3178), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0104), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0104), \"['a', 'y', 'a', 'y']\": np.float64(0.0108), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3067), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.011), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3073), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0104), \"['a', 'y', 'b', 'x']\": np.float64(-0.3178), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0104), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0104), \"['a', 'y', 'b', 'y']\": np.float64(0.0107), \"['a', 'y']\": np.float64(0.3288), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.0075), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3152), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.0168), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.305), \"['b', 'x', 'a', 'x']\": np.float64(-9.3271), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3232), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.011), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3233), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0109), \"['b', 'x', 'a', 'y']\": np.float64(0.3344), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.0167), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3233), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.0334), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3057), \"['b', 'x', 'b', 'x']\": np.float64(-9.3443), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3056), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0104), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3057), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0103), \"['b', 'x', 'b', 'y']\": np.float64(0.3162), \"['b', 'x']\": np.float64(-9.666), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3064), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0107), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3067), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0104), \"['b', 'y', 'a', 'x']\": np.float64(-0.3173), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0104), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0104), \"['b', 'y', 'a', 'y']\": np.float64(0.0108), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3062), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.011), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3067), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0104), \"['b', 'y', 'b', 'x']\": np.float64(-0.3173), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0104), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0104), \"['b', 'y', 'b', 'y']\": np.float64(0.0107), \"['b', 'y']\": np.float64(0.3282)}\n",
      "It s train loss bro [0.03926778584718704, 0.03765058144927025, 0.03724558278918266, 0.03684869408607483, 0.03645992651581764, 0.03607882186770439, 0.035705167800188065, 0.03533884882926941, 0.03497942164540291, 0.03462688624858856, 0.03428091108798981]\n",
      "% good predict : 0\n",
      "\u001b[0;34miteration 196 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict x \u001b[0m\n",
      "for 0 epochs, loss is 3.2283987998962402 and val_loss is 4.432930946350098\n",
      "for 1 epochs, loss is 3.4180264472961426 and val_loss is 4.431522369384766\n",
      "for 2 epochs, loss is 3.414170980453491 and val_loss is 4.429315090179443\n",
      "for 3 epochs, loss is 3.4081718921661377 and val_loss is 4.426394939422607\n",
      "for 4 epochs, loss is 3.4002633094787598 and val_loss is 4.422842502593994\n",
      "for 5 epochs, loss is 3.3906571865081787 and val_loss is 4.418726444244385\n",
      "for 6 epochs, loss is 3.3795456886291504 and val_loss is 4.414112567901611\n",
      "for 7 epochs, loss is 3.3671019077301025 and val_loss is 4.409056663513184\n",
      "for 8 epochs, loss is 3.3534815311431885 and val_loss is 4.403613090515137\n",
      "for 9 epochs, loss is 3.338825225830078 and val_loss is 4.397828102111816\n",
      "for 10 epochs, loss is 3.3232593536376953 and val_loss is 4.391744136810303\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.9128), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3448), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9174), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3392), \"['a', 'x', 'a', 'x']\": np.float64(-9.2624), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3449), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0131), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.345), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0131), \"['a', 'x', 'a', 'y']\": np.float64(0.3583), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.9129), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3495), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.922), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3394), \"['a', 'x', 'b', 'x']\": np.float64(-9.2672), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3393), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0129), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3394), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0129), \"['a', 'x', 'b', 'y']\": np.float64(0.3525), \"['a', 'x']\": np.float64(-9.6256), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3407), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0132), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3409), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.013), \"['a', 'y', 'a', 'x']\": np.float64(-0.3541), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.013), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.013), \"['a', 'y', 'a', 'y']\": np.float64(0.0135), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3406), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0134), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3409), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.013), \"['a', 'y', 'b', 'x']\": np.float64(-0.3541), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.013), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.013), \"['a', 'y', 'b', 'y']\": np.float64(0.0135), \"['a', 'y']\": np.float64(0.3678), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.9089), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3446), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.9135), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.339), \"['b', 'x', 'a', 'x']\": np.float64(-9.2583), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3495), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0133), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3496), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0133), \"['b', 'x', 'a', 'y']\": np.float64(0.3631), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.9134), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3495), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.9226), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3394), \"['b', 'x', 'b', 'x']\": np.float64(-9.2677), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3394), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0129), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3394), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0129), \"['b', 'x', 'b', 'y']\": np.float64(0.3525), \"['b', 'x']\": np.float64(-9.6263), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3403), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0132), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3405), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.013), \"['b', 'y', 'a', 'x']\": np.float64(-0.3537), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.013), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.013), \"['b', 'y', 'a', 'y']\": np.float64(0.0135), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3402), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0133), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3405), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.013), \"['b', 'y', 'b', 'x']\": np.float64(-0.3537), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0129), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0129), \"['b', 'y', 'b', 'y']\": np.float64(0.0134), \"['b', 'y']\": np.float64(0.3674)}\n",
      "It s train loss bro [3.2283987998962402, 3.4180264472961426, 3.414170980453491, 3.4081718921661377, 3.4002633094787598, 3.3906571865081787, 3.3795456886291504, 3.3671019077301025, 3.3534815311431885, 3.338825225830078, 3.3232593536376953]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 197 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 3.096583127975464 and val_loss is 4.3854594230651855\n",
      "for 1 epochs, loss is 3.2887680530548096 and val_loss is 4.3789544105529785\n",
      "for 2 epochs, loss is 3.271298408508301 and val_loss is 4.372262001037598\n",
      "for 3 epochs, loss is 3.253300666809082 and val_loss is 4.365411281585693\n",
      "for 4 epochs, loss is 3.2348508834838867 and val_loss is 4.358424186706543\n",
      "for 5 epochs, loss is 3.2160141468048096 and val_loss is 4.351324081420898\n",
      "for 6 epochs, loss is 3.1968510150909424 and val_loss is 4.3441338539123535\n",
      "for 7 epochs, loss is 3.1774141788482666 and val_loss is 4.336870193481445\n",
      "for 8 epochs, loss is 3.157752275466919 and val_loss is 4.329549789428711\n",
      "for 9 epochs, loss is 3.1379072666168213 and val_loss is 4.322187900543213\n",
      "for 10 epochs, loss is 3.1179158687591553 and val_loss is 4.314799785614014\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6837), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4121), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6842), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4107), \"['a', 'x', 'a', 'x']\": np.float64(-9.1016), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4121), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0195), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4122), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0195), \"['a', 'x', 'a', 'y']\": np.float64(0.432), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.6823), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4142), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.6847), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4107), \"['a', 'x', 'b', 'x']\": np.float64(-9.1022), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4107), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0194), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4107), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0194), \"['a', 'x', 'b', 'y']\": np.float64(0.4304), \"['a', 'x']\": np.float64(-9.5396), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.412), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0196), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.412), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0195), \"['a', 'y', 'a', 'x']\": np.float64(-0.4318), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0195), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0195), \"['a', 'y', 'a', 'y']\": np.float64(0.0204), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4119), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0196), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.412), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0195), \"['a', 'y', 'b', 'x']\": np.float64(-0.4318), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0195), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0195), \"['a', 'y', 'b', 'y']\": np.float64(0.0204), \"['a', 'y']\": np.float64(0.4526), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.6821), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.412), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.6826), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4106), \"['b', 'x', 'a', 'x']\": np.float64(-9.0999), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4142), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0196), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4142), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0196), \"['b', 'x', 'a', 'y']\": np.float64(0.4341), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.6826), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4142), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.685), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4107), \"['b', 'x', 'b', 'x']\": np.float64(-9.1025), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4107), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0194), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4107), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0194), \"['b', 'x', 'b', 'y']\": np.float64(0.4304), \"['b', 'x']\": np.float64(-9.54), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4119), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0195), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4119), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0195), \"['b', 'y', 'a', 'x']\": np.float64(-0.4317), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0195), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0195), \"['b', 'y', 'a', 'y']\": np.float64(0.0204), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4118), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0196), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4119), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0195), \"['b', 'y', 'b', 'x']\": np.float64(-0.4317), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0195), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0195), \"['b', 'y', 'b', 'y']\": np.float64(0.0204), \"['b', 'y']\": np.float64(0.4525)}\n",
      "It s train loss bro [3.096583127975464, 3.2887680530548096, 3.271298408508301, 3.253300666809082, 3.2348508834838867, 3.2160141468048096, 3.1968510150909424, 3.1774141788482666, 3.157752275466919, 3.1379072666168213, 3.1179158687591553]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 198 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.05690702795982361 and val_loss is 4.249286651611328\n",
      "for 1 epochs, loss is 0.05633534491062164 and val_loss is 4.191565036773682\n",
      "for 2 epochs, loss is 0.06599138677120209 and val_loss is 4.1408257484436035\n",
      "for 3 epochs, loss is 0.0757533609867096 and val_loss is 4.096317768096924\n",
      "for 4 epochs, loss is 0.08542171865701675 and val_loss is 4.057348251342773\n",
      "for 5 epochs, loss is 0.09481926262378693 and val_loss is 4.023285865783691\n",
      "for 6 epochs, loss is 0.10379539430141449 and val_loss is 3.993558883666992\n",
      "for 7 epochs, loss is 0.11222763359546661 and val_loss is 3.9676513671875\n",
      "for 8 epochs, loss is 0.12002229690551758 and val_loss is 3.9451043605804443\n",
      "for 9 epochs, loss is 0.1271125227212906 and val_loss is 3.925507068634033\n",
      "for 10 epochs, loss is 0.1334550827741623 and val_loss is 3.9084999561309814\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.5835), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9722), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.5815), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9741), \"['a', 'x', 'a', 'x']\": np.float64(-7.5694), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9725), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1432), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9717), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1441), \"['a', 'x', 'a', 'y']\": np.float64(1.1178), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.5807), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9729), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.5804), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9729), \"['a', 'x', 'b', 'x']\": np.float64(-7.5672), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9743), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1436), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9738), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1442), \"['a', 'x', 'b', 'y']\": np.float64(1.12), \"['a', 'x']\": np.float64(-8.703), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9665), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1427), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9662), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.143), \"['a', 'y', 'a', 'x']\": np.float64(-1.1113), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1424), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.021), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1423), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0211), \"['a', 'y', 'a', 'y']\": np.float64(0.1637), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9656), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1428), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9656), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1428), \"['a', 'y', 'b', 'x']\": np.float64(-1.1104), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1432), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0211), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1432), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0212), \"['a', 'y', 'b', 'y']\": np.float64(0.1647), \"['a', 'y']\": np.float64(1.2773), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.5777), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9714), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.5757), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9733), \"['b', 'x', 'a', 'x']\": np.float64(-7.5627), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9727), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1433), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.972), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1441), \"['b', 'x', 'a', 'y']\": np.float64(1.1181), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.5765), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9723), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.5762), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9723), \"['b', 'x', 'b', 'x']\": np.float64(-7.5624), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9726), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1433), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9721), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1439), \"['b', 'x', 'b', 'y']\": np.float64(1.118), \"['b', 'x']\": np.float64(-8.6964), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9721), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1436), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9718), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1438), \"['b', 'y', 'a', 'x']\": np.float64(-1.1177), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1433), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0211), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1432), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0212), \"['b', 'y', 'a', 'y']\": np.float64(0.1647), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9714), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1436), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9714), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1436), \"['b', 'y', 'b', 'x']\": np.float64(-1.1171), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1439), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0212), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1438), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0213), \"['b', 'y', 'b', 'y']\": np.float64(0.1654), \"['b', 'y']\": np.float64(1.2848)}\n",
      "It s train loss bro [0.05690702795982361, 0.05633534491062164, 0.06599138677120209, 0.0757533609867096, 0.08542171865701675, 0.09481926262378693, 0.10379539430141449, 0.11222763359546661, 0.12002229690551758, 0.1271125227212906, 0.1334550827741623]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 199 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.048372507095337 and val_loss is 3.9068045616149902\n",
      "for 1 epochs, loss is 2.0490095615386963 and val_loss is 3.9051241874694824\n",
      "for 2 epochs, loss is 2.0429868698120117 and val_loss is 3.9034628868103027\n",
      "for 3 epochs, loss is 2.036020517349243 and val_loss is 3.901824951171875\n",
      "for 4 epochs, loss is 2.028214693069458 and val_loss is 3.9002161026000977\n",
      "for 5 epochs, loss is 2.019662618637085 and val_loss is 3.898639678955078\n",
      "for 6 epochs, loss is 2.010448455810547 and val_loss is 3.8970980644226074\n",
      "for 7 epochs, loss is 2.0006487369537354 and val_loss is 3.895596742630005\n",
      "for 8 epochs, loss is 1.9903309345245361 and val_loss is 3.8941383361816406\n",
      "for 9 epochs, loss is 1.9795562028884888 and val_loss is 3.8927252292633057\n",
      "for 10 epochs, loss is 1.9683804512023926 and val_loss is 3.8913612365722656\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.2903), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0392), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.2876), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0419), \"['a', 'x', 'a', 'x']\": np.float64(-7.3435), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0396), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1713), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0385), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1725), \"['a', 'x', 'a', 'y']\": np.float64(1.2133), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.2867), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0399), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.2861), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0403), \"['a', 'x', 'b', 'x']\": np.float64(-7.3404), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0421), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1718), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0414), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1726), \"['a', 'x', 'b', 'y']\": np.float64(1.2163), \"['a', 'x']\": np.float64(-8.573), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0323), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1706), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0319), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.171), \"['a', 'y', 'a', 'x']\": np.float64(-1.2052), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1701), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.028), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.17), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0282), \"['a', 'y', 'a', 'y']\": np.float64(0.1986), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0311), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1706), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.031), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1706), \"['a', 'y', 'b', 'x']\": np.float64(-1.2039), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1713), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0282), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1712), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0284), \"['a', 'y', 'b', 'y']\": np.float64(0.1999), \"['a', 'y']\": np.float64(1.4065), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.283), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.038), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.2804), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0407), \"['b', 'x', 'a', 'x']\": np.float64(-7.335), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0396), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1713), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0386), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1725), \"['b', 'x', 'a', 'y']\": np.float64(1.2133), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.2815), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.039), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.2808), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0394), \"['b', 'x', 'b', 'x']\": np.float64(-7.3343), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0399), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1715), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0392), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1722), \"['b', 'x', 'b', 'y']\": np.float64(1.2137), \"['b', 'x']\": np.float64(-8.5645), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0391), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1717), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0387), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1721), \"['b', 'y', 'a', 'x']\": np.float64(-1.2131), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1714), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0282), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1712), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0284), \"['b', 'y', 'a', 'y']\": np.float64(0.2), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0383), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1717), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0382), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1718), \"['b', 'y', 'b', 'x']\": np.float64(-1.2123), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1721), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0284), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.172), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0285), \"['b', 'y', 'b', 'y']\": np.float64(0.2009), \"['b', 'y']\": np.float64(1.4159)}\n",
      "It s train loss bro [2.048372507095337, 2.0490095615386963, 2.0429868698120117, 2.036020517349243, 2.028214693069458, 2.019662618637085, 2.010448455810547, 2.0006487369537354, 1.9903309345245361, 1.9795562028884888, 1.9683804512023926]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 200 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1803833693265915 and val_loss is 3.879952907562256\n",
      "for 1 epochs, loss is 0.17220857739448547 and val_loss is 3.8705837726593018\n",
      "for 2 epochs, loss is 0.18906256556510925 and val_loss is 3.8628861904144287\n",
      "for 3 epochs, loss is 0.20485766232013702 and val_loss is 3.856527090072632\n",
      "for 4 epochs, loss is 0.21933285892009735 and val_loss is 3.851217269897461\n",
      "for 5 epochs, loss is 0.23228788375854492 and val_loss is 3.8467071056365967\n",
      "for 6 epochs, loss is 0.24358445405960083 and val_loss is 3.842792272567749\n",
      "for 7 epochs, loss is 0.2531431019306183 and val_loss is 3.8393054008483887\n",
      "for 8 epochs, loss is 0.26093921065330505 and val_loss is 3.8361146450042725\n",
      "for 9 epochs, loss is 0.2669958770275116 and val_loss is 3.833122968673706\n",
      "for 10 epochs, loss is 0.27137625217437744 and val_loss is 3.830258846282959\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.4044), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.375), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.396), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3837), \"['a', 'x', 'a', 'x']\": np.float64(-5.7928), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3748), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4294), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3717), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4327), \"['a', 'x', 'a', 'y']\": np.float64(1.8084), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.3945), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.374), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3899), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3787), \"['a', 'x', 'b', 'x']\": np.float64(-5.7817), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.383), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4326), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3812), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4344), \"['a', 'x', 'b', 'y']\": np.float64(1.8198), \"['a', 'x']\": np.float64(-7.6187), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3625), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4254), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3599), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.428), \"['a', 'y', 'a', 'x']\": np.float64(-1.792), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4255), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1329), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4245), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1339), \"['a', 'y', 'a', 'y']\": np.float64(0.5597), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3589), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4249), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3575), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4263), \"['a', 'y', 'b', 'x']\": np.float64(-1.7879), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4286), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1341), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.428), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1346), \"['a', 'y', 'b', 'y']\": np.float64(0.5639), \"['a', 'y']\": np.float64(2.3571), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3895), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3703), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.3811), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.379), \"['b', 'x', 'a', 'x']\": np.float64(-5.7731), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3722), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4286), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3691), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4318), \"['b', 'x', 'a', 'y']\": np.float64(1.805), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.3833), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3705), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3787), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3752), \"['b', 'x', 'b', 'x']\": np.float64(-5.767), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3765), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4306), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3747), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4324), \"['b', 'x', 'b', 'y']\": np.float64(1.8112), \"['b', 'x']\": np.float64(-7.5954), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3761), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4296), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3735), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4323), \"['b', 'y', 'a', 'x']\": np.float64(-1.8098), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4304), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1344), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4294), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1354), \"['b', 'y', 'a', 'y']\": np.float64(0.5661), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3738), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4295), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3724), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.431), \"['b', 'y', 'b', 'x']\": np.float64(-1.8075), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4321), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1352), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4315), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1357), \"['b', 'y', 'b', 'y']\": np.float64(0.5685), \"['b', 'y']\": np.float64(2.3814)}\n",
      "It s train loss bro [0.1803833693265915, 0.17220857739448547, 0.18906256556510925, 0.20485766232013702, 0.21933285892009735, 0.23228788375854492, 0.24358445405960083, 0.2531431019306183, 0.26093921065330505, 0.2669958770275116, 0.27137625217437744]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 201 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2940412163734436 and val_loss is 3.8274893760681152\n",
      "for 1 epochs, loss is 0.27549484372138977 and val_loss is 3.824775457382202\n",
      "for 2 epochs, loss is 0.2754894196987152 and val_loss is 3.822108268737793\n",
      "for 3 epochs, loss is 0.2743039131164551 and val_loss is 3.819491386413574\n",
      "for 4 epochs, loss is 0.2720857560634613 and val_loss is 3.816941022872925\n",
      "for 5 epochs, loss is 0.2689802348613739 and val_loss is 3.814479112625122\n",
      "for 6 epochs, loss is 0.2651274502277374 and val_loss is 3.8121337890625\n",
      "for 7 epochs, loss is 0.2606583535671234 and val_loss is 3.8099350929260254\n",
      "for 8 epochs, loss is 0.2556939125061035 and val_loss is 3.807917833328247\n",
      "for 9 epochs, loss is 0.25034299492836 and val_loss is 3.806114673614502\n",
      "for 10 epochs, loss is 0.24470265209674835 and val_loss is 3.804558753967285\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.8881), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3051), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.884), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3093), \"['a', 'x', 'a', 'x']\": np.float64(-6.2079), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3054), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3481), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3036), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.35), \"['a', 'x', 'a', 'y']\": np.float64(1.6574), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.8829), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3051), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.8813), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3067), \"['a', 'x', 'b', 'x']\": np.float64(-6.2027), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3094), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3494), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3084), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3506), \"['a', 'x', 'b', 'y']\": np.float64(1.6629), \"['a', 'x']\": np.float64(-7.8841), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2964), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3461), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2953), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3473), \"['a', 'y', 'a', 'x']\": np.float64(-1.6465), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3458), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0922), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3453), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0927), \"['a', 'y', 'a', 'y']\": np.float64(0.439), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2944), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.346), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.294), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3464), \"['a', 'y', 'b', 'x']\": np.float64(-1.6442), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3476), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0928), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3473), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0931), \"['a', 'y', 'b', 'y']\": np.float64(0.4414), \"['a', 'y']\": np.float64(2.0905), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.8794), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3027), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.8753), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.307), \"['b', 'x', 'a', 'x']\": np.float64(-6.1968), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3045), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3478), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3027), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3497), \"['b', 'x', 'a', 'y']\": np.float64(1.6563), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.8767), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3035), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.8751), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.305), \"['b', 'x', 'b', 'x']\": np.float64(-6.1948), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3058), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3485), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3048), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3496), \"['b', 'x', 'b', 'y']\": np.float64(1.6583), \"['b', 'x']\": np.float64(-7.8717), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3045), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3483), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3034), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3494), \"['b', 'y', 'a', 'x']\": np.float64(-1.6567), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3482), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0928), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3477), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0934), \"['b', 'y', 'a', 'y']\": np.float64(0.4421), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3032), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3483), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3027), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3487), \"['b', 'y', 'b', 'x']\": np.float64(-1.6554), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3493), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0932), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.349), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0935), \"['b', 'y', 'b', 'y']\": np.float64(0.4436), \"['b', 'y']\": np.float64(2.1039)}\n",
      "It s train loss bro [0.2940412163734436, 0.27549484372138977, 0.2754894196987152, 0.2743039131164551, 0.2720857560634613, 0.2689802348613739, 0.2651274502277374, 0.2606583535671234, 0.2556939125061035, 0.25034299492836, 0.24470265209674835]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 202 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2451857179403305 and val_loss is 3.8033196926116943\n",
      "for 1 epochs, loss is 0.23289428651332855 and val_loss is 3.8023855686187744\n",
      "for 2 epochs, loss is 0.22686025500297546 and val_loss is 3.8017818927764893\n",
      "for 3 epochs, loss is 0.22080795466899872 and val_loss is 3.801532030105591\n",
      "for 4 epochs, loss is 0.21478168666362762 and val_loss is 3.801654815673828\n",
      "for 5 epochs, loss is 0.20881688594818115 and val_loss is 3.8021657466888428\n",
      "for 6 epochs, loss is 0.2029426097869873 and val_loss is 3.803077459335327\n",
      "for 7 epochs, loss is 0.19718173146247864 and val_loss is 3.804396390914917\n",
      "for 8 epochs, loss is 0.19155186414718628 and val_loss is 3.8061270713806152\n",
      "for 9 epochs, loss is 0.18606649339199066 and val_loss is 3.80826997756958\n",
      "for 10 epochs, loss is 0.18073485791683197 and val_loss is 3.8108205795288086\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.9046), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1183), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9035), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1192), \"['a', 'x', 'a', 'x']\": np.float64(-7.0388), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1185), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2115), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.118), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2121), \"['a', 'x', 'a', 'y']\": np.float64(1.3331), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.9031), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1187), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.903), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1183), \"['a', 'x', 'b', 'x']\": np.float64(-7.0374), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1194), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2117), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1191), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2121), \"['a', 'x', 'b', 'y']\": np.float64(1.3342), \"['a', 'x']\": np.float64(-8.3907), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1153), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2112), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1151), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2114), \"['a', 'y', 'a', 'x']\": np.float64(-1.3295), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2109), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0399), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2108), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.04), \"['a', 'y', 'a', 'y']\": np.float64(0.2514), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1147), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2112), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1147), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2112), \"['a', 'y', 'b', 'x']\": np.float64(-1.3289), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2115), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.04), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2115), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0401), \"['a', 'y', 'b', 'y']\": np.float64(0.2521), \"['a', 'y']\": np.float64(1.5846), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.9019), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1178), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9008), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1187), \"['b', 'x', 'a', 'x']\": np.float64(-7.0355), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1186), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2115), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1181), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2121), \"['b', 'x', 'a', 'y']\": np.float64(1.3333), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.9014), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1183), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9013), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.118), \"['b', 'x', 'b', 'x']\": np.float64(-7.0354), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1183), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2115), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.118), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2119), \"['b', 'x', 'b', 'y']\": np.float64(1.3329), \"['b', 'x']\": np.float64(-8.3874), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1182), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2118), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.118), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.212), \"['b', 'y', 'a', 'x']\": np.float64(-1.333), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2115), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.04), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2115), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0401), \"['b', 'y', 'a', 'y']\": np.float64(0.2521), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1179), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2118), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1178), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2118), \"['b', 'y', 'b', 'x']\": np.float64(-1.3327), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2119), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0401), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2119), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0402), \"['b', 'y', 'b', 'y']\": np.float64(0.2526), \"['b', 'y']\": np.float64(1.5889)}\n",
      "It s train loss bro [0.2451857179403305, 0.23289428651332855, 0.22686025500297546, 0.22080795466899872, 0.21478168666362762, 0.20881688594818115, 0.2029426097869873, 0.19718173146247864, 0.19155186414718628, 0.18606649339199066, 0.18073485791683197]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 203 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.20457275211811066 and val_loss is 3.8139452934265137\n",
      "for 1 epochs, loss is 0.17052306234836578 and val_loss is 3.817448616027832\n",
      "for 2 epochs, loss is 0.16565389931201935 and val_loss is 3.8213181495666504\n",
      "for 3 epochs, loss is 0.16095618903636932 and val_loss is 3.8255438804626465\n",
      "for 4 epochs, loss is 0.15642859041690826 and val_loss is 3.830110549926758\n",
      "for 5 epochs, loss is 0.15206831693649292 and val_loss is 3.835000514984131\n",
      "for 6 epochs, loss is 0.14787177741527557 and val_loss is 3.840196132659912\n",
      "for 7 epochs, loss is 0.14383523166179657 and val_loss is 3.845679998397827\n",
      "for 8 epochs, loss is 0.13995419442653656 and val_loss is 3.851431131362915\n",
      "for 9 epochs, loss is 0.13622386753559113 and val_loss is 3.8574304580688477\n",
      "for 10 epochs, loss is 0.1326390951871872 and val_loss is 3.863656759262085\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.7871), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9201), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.7857), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9209), \"['a', 'x', 'a', 'x']\": np.float64(-7.7221), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9199), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1247), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9199), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1248), \"['a', 'x', 'a', 'y']\": np.float64(1.0468), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.7856), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9201), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.7845), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9204), \"['a', 'x', 'b', 'x']\": np.float64(-7.7205), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9207), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1248), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9207), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1249), \"['a', 'x', 'b', 'y']\": np.float64(1.0477), \"['a', 'x']\": np.float64(-8.7858), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.921), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1248), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9208), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.125), \"['a', 'y', 'a', 'x']\": np.float64(-1.0478), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1248), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0169), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1248), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0169), \"['a', 'y', 'a', 'y']\": np.float64(0.1421), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9209), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1249), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9208), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1249), \"['a', 'y', 'b', 'x']\": np.float64(-1.0478), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.125), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0169), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.125), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.017), \"['a', 'y', 'b', 'y']\": np.float64(0.1422), \"['a', 'y']\": np.float64(1.1924), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.7868), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.92), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.7854), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9208), \"['b', 'x', 'a', 'x']\": np.float64(-7.7217), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9201), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1247), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9201), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1249), \"['b', 'x', 'a', 'y']\": np.float64(1.047), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.7855), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9201), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.7844), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9204), \"['b', 'x', 'b', 'x']\": np.float64(-7.7203), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9204), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1248), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9204), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1249), \"['b', 'x', 'b', 'y']\": np.float64(1.0473), \"['b', 'x']\": np.float64(-8.7854), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9219), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.125), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9217), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1251), \"['b', 'y', 'a', 'x']\": np.float64(-1.0489), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.125), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0169), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.125), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.017), \"['b', 'y', 'a', 'y']\": np.float64(0.1422), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9219), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.125), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9217), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.125), \"['b', 'y', 'b', 'x']\": np.float64(-1.0489), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.125), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.017), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.125), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.017), \"['b', 'y', 'b', 'y']\": np.float64(0.1423), \"['b', 'y']\": np.float64(1.1935)}\n",
      "It s train loss bro [0.20457275211811066, 0.17052306234836578, 0.16565389931201935, 0.16095618903636932, 0.15642859041690826, 0.15206831693649292, 0.14787177741527557, 0.14383523166179657, 0.13995419442653656, 0.13622386753559113, 0.1326390951871872]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 204 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1335020214319229 and val_loss is 3.870150566101074\n",
      "for 1 epochs, loss is 0.12588612735271454 and val_loss is 3.8768253326416016\n",
      "for 2 epochs, loss is 0.12270763516426086 and val_loss is 3.8836615085601807\n",
      "for 3 epochs, loss is 0.11965398490428925 and val_loss is 3.890639543533325\n",
      "for 4 epochs, loss is 0.11672050505876541 and val_loss is 3.8977441787719727\n",
      "for 5 epochs, loss is 0.11390168964862823 and val_loss is 3.904954195022583\n",
      "for 6 epochs, loss is 0.111192986369133 and val_loss is 3.9122555255889893\n",
      "for 7 epochs, loss is 0.1085895523428917 and val_loss is 3.9196317195892334\n",
      "for 8 epochs, loss is 0.10608664155006409 and val_loss is 3.9270687103271484\n",
      "for 9 epochs, loss is 0.10367998480796814 and val_loss is 3.934551477432251\n",
      "for 10 epochs, loss is 0.1013651117682457 and val_loss is 3.942068338394165\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.4291), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7591), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.4271), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7602), \"['a', 'x', 'a', 'x']\": np.float64(-8.2012), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7589), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0776), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7589), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0777), \"['a', 'x', 'a', 'y']\": np.float64(0.838), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.427), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7591), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.4251), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7599), \"['a', 'x', 'b', 'x']\": np.float64(-8.1989), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.76), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0778), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7601), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0778), \"['a', 'x', 'b', 'y']\": np.float64(0.8392), \"['a', 'x']\": np.float64(-9.0534), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7614), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0778), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7612), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0779), \"['a', 'y', 'a', 'x']\": np.float64(-0.8405), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0779), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.008), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0779), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.008), \"['a', 'y', 'a', 'y']\": np.float64(0.086), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7614), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0778), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7612), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0779), \"['a', 'y', 'b', 'x']\": np.float64(-0.8406), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0779), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.008), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0779), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.008), \"['a', 'y', 'b', 'y']\": np.float64(0.086), \"['a', 'y']\": np.float64(0.9281), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.4293), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7591), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.4273), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7603), \"['b', 'x', 'a', 'x']\": np.float64(-8.2014), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7591), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0777), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7592), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0777), \"['b', 'x', 'a', 'y']\": np.float64(0.8382), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.4274), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7591), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.4254), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7599), \"['b', 'x', 'b', 'x']\": np.float64(-8.1993), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7599), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0777), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7599), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0777), \"['b', 'x', 'b', 'y']\": np.float64(0.8391), \"['b', 'x']\": np.float64(-9.0538), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7617), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0778), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7615), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0779), \"['b', 'y', 'a', 'x']\": np.float64(-0.8408), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0779), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.008), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0779), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.008), \"['b', 'y', 'a', 'y']\": np.float64(0.086), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7617), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0779), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7615), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0779), \"['b', 'y', 'b', 'x']\": np.float64(-0.8409), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0779), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.008), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0779), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.008), \"['b', 'y', 'b', 'y']\": np.float64(0.086), \"['b', 'y']\": np.float64(0.9285)}\n",
      "It s train loss bro [0.1335020214319229, 0.12588612735271454, 0.12270763516426086, 0.11965398490428925, 0.11672050505876541, 0.11390168964862823, 0.111192986369133, 0.1085895523428917, 0.10608664155006409, 0.10367998480796814, 0.1013651117682457]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 205 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.10010293871164322 and val_loss is 3.949613332748413\n",
      "for 1 epochs, loss is 0.09699451178312302 and val_loss is 3.957167863845825\n",
      "for 2 epochs, loss is 0.09493093192577362 and val_loss is 3.9647223949432373\n",
      "for 3 epochs, loss is 0.0929436981678009 and val_loss is 3.9722673892974854\n",
      "for 4 epochs, loss is 0.09102891385555267 and val_loss is 3.9797937870025635\n",
      "for 5 epochs, loss is 0.089183509349823 and val_loss is 3.987295627593994\n",
      "for 6 epochs, loss is 0.08740417659282684 and val_loss is 3.994764566421509\n",
      "for 7 epochs, loss is 0.08568786084651947 and val_loss is 4.002194404602051\n",
      "for 8 epochs, loss is 0.084031842648983 and val_loss is 4.009580135345459\n",
      "for 9 epochs, loss is 0.08243291079998016 and val_loss is 4.016916751861572\n",
      "for 10 epochs, loss is 0.08088891953229904 and val_loss is 4.0242018699646\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.8825), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6381), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.8805), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6391), \"['a', 'x', 'a', 'x']\": np.float64(-8.5316), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6379), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0517), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.638), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0517), \"['a', 'x', 'a', 'y']\": np.float64(0.6907), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.8803), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6383), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.8785), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6388), \"['a', 'x', 'b', 'x']\": np.float64(-8.5295), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6389), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0518), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6389), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0518), \"['a', 'x', 'b', 'y']\": np.float64(0.6917), \"['a', 'x']\": np.float64(-9.2342), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6403), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0518), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6401), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0519), \"['a', 'y', 'a', 'x']\": np.float64(-0.693), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0519), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0042), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0519), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0042), \"['a', 'y', 'a', 'y']\": np.float64(0.0562), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6403), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0519), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6402), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0519), \"['a', 'y', 'b', 'x']\": np.float64(-0.6931), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0519), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0042), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0519), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0042), \"['a', 'y', 'b', 'y']\": np.float64(0.0562), \"['a', 'y']\": np.float64(0.7503), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.8827), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6381), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.8807), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6391), \"['b', 'x', 'a', 'x']\": np.float64(-8.5319), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6383), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0518), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6384), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0518), \"['b', 'x', 'a', 'y']\": np.float64(0.6911), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.8807), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6383), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.8789), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6388), \"['b', 'x', 'b', 'x']\": np.float64(-8.5299), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6388), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0518), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6388), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0518), \"['b', 'x', 'b', 'y']\": np.float64(0.6916), \"['b', 'x']\": np.float64(-9.2347), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6404), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0518), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6403), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0519), \"['b', 'y', 'a', 'x']\": np.float64(-0.6932), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0519), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0042), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0519), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0042), \"['b', 'y', 'a', 'y']\": np.float64(0.0562), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6405), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0519), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.6403), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0519), \"['b', 'y', 'b', 'x']\": np.float64(-0.6932), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0519), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0042), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0519), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0042), \"['b', 'y', 'b', 'y']\": np.float64(0.0562), \"['b', 'y']\": np.float64(0.7505)}\n",
      "It s train loss bro [0.10010293871164322, 0.09699451178312302, 0.09493093192577362, 0.0929436981678009, 0.09102891385555267, 0.089183509349823, 0.08740417659282684, 0.08568786084651947, 0.084031842648983, 0.08243291079998016, 0.08088891953229904]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 206 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.0828484371304512 and val_loss is 4.031466484069824\n",
      "for 1 epochs, loss is 0.07795400172472 and val_loss is 4.038667678833008\n",
      "for 2 epochs, loss is 0.07655856013298035 and val_loss is 4.045801639556885\n",
      "for 3 epochs, loss is 0.07520893216133118 and val_loss is 4.052868843078613\n",
      "for 4 epochs, loss is 0.07390255481004715 and val_loss is 4.059864521026611\n",
      "for 5 epochs, loss is 0.07263802736997604 and val_loss is 4.0667901039123535\n",
      "for 6 epochs, loss is 0.0714130848646164 and val_loss is 4.073642730712891\n",
      "for 7 epochs, loss is 0.07022608816623688 and val_loss is 4.080422401428223\n",
      "for 8 epochs, loss is 0.06907550245523453 and val_loss is 4.087129592895508\n",
      "for 9 epochs, loss is 0.06795945763587952 and val_loss is 4.093761920928955\n",
      "for 10 epochs, loss is 0.0668768435716629 and val_loss is 4.100320339202881\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.2091), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5476), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.2077), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5478), \"['a', 'x', 'a', 'x']\": np.float64(-8.766), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5474), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0365), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5475), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0365), \"['a', 'x', 'a', 'y']\": np.float64(0.5847), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.2072), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5481), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.2062), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5476), \"['a', 'x', 'b', 'x']\": np.float64(-8.7645), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5477), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0366), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5478), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0365), \"['a', 'x', 'b', 'y']\": np.float64(0.585), \"['a', 'x']\": np.float64(-9.3608), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5489), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0366), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5488), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0366), \"['a', 'y', 'a', 'x']\": np.float64(-0.5862), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0366), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0024), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0366), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0024), \"['a', 'y', 'a', 'y']\": np.float64(0.0391), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.549), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0367), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5489), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0366), \"['a', 'y', 'b', 'x']\": np.float64(-0.5862), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0366), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0024), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0366), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0024), \"['a', 'y', 'b', 'y']\": np.float64(0.0391), \"['a', 'y']\": np.float64(0.6261), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.209), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5476), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.2076), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5478), \"['b', 'x', 'a', 'x']\": np.float64(-8.766), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5481), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0366), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5481), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0366), \"['b', 'x', 'a', 'y']\": np.float64(0.5854), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.2076), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5481), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.2066), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5477), \"['b', 'x', 'b', 'x']\": np.float64(-8.7649), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5476), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0365), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5477), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0365), \"['b', 'x', 'b', 'y']\": np.float64(0.5849), \"['b', 'x']\": np.float64(-9.3612), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5491), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0366), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.549), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0366), \"['b', 'y', 'a', 'x']\": np.float64(-0.5863), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0366), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0024), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0366), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0024), \"['b', 'y', 'a', 'y']\": np.float64(0.0391), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5491), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0367), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.549), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0366), \"['b', 'y', 'b', 'x']\": np.float64(-0.5864), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0366), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0024), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0366), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0024), \"['b', 'y', 'b', 'y']\": np.float64(0.0391), \"['b', 'y']\": np.float64(0.6262)}\n",
      "It s train loss bro [0.0828484371304512, 0.07795400172472, 0.07655856013298035, 0.07520893216133118, 0.07390255481004715, 0.07263802736997604, 0.0714130848646164, 0.07022608816623688, 0.06907550245523453, 0.06795945763587952, 0.0668768435716629]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 207 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.06596728414297104 and val_loss is 4.106811046600342\n",
      "for 1 epochs, loss is 0.06480508297681808 and val_loss is 4.113226413726807\n",
      "for 2 epochs, loss is 0.06381377577781677 and val_loss is 4.119569301605225\n",
      "for 3 epochs, loss is 0.06285037845373154 and val_loss is 4.125838756561279\n",
      "for 4 epochs, loss is 0.06191360577940941 and val_loss is 4.132036209106445\n",
      "for 5 epochs, loss is 0.061002764850854874 and val_loss is 4.138161659240723\n",
      "for 6 epochs, loss is 0.060116466134786606 and val_loss is 4.144216537475586\n",
      "for 7 epochs, loss is 0.0592537559568882 and val_loss is 4.150201797485352\n",
      "for 8 epochs, loss is 0.058413807302713394 and val_loss is 4.156118392944336\n",
      "for 9 epochs, loss is 0.057595882564783096 and val_loss is 4.161966323852539\n",
      "for 10 epochs, loss is 0.05679868906736374 and val_loss is 4.167747974395752\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.4521), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4785), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.4518), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4776), \"['a', 'x', 'a', 'x']\": np.float64(-8.9387), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4785), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.027), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4785), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.027), \"['a', 'x', 'a', 'y']\": np.float64(0.5061), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.451), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4795), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.4514), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4775), \"['a', 'x', 'b', 'x']\": np.float64(-8.9383), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4776), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.027), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4776), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.027), \"['a', 'x', 'b', 'y']\": np.float64(0.5051), \"['a', 'x']\": np.float64(-9.4533), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4786), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0271), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4786), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.027), \"['a', 'y', 'a', 'x']\": np.float64(-0.5062), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.027), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0015), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.027), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0015), \"['a', 'y', 'a', 'y']\": np.float64(0.0286), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4786), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0272), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4786), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.027), \"['a', 'y', 'b', 'x']\": np.float64(-0.5062), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.027), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0015), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.027), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0015), \"['a', 'y', 'b', 'y']\": np.float64(0.0286), \"['a', 'y']\": np.float64(0.5354), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.4517), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4785), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.4513), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4776), \"['b', 'x', 'a', 'x']\": np.float64(-8.9382), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4795), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0271), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4795), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0271), \"['b', 'x', 'a', 'y']\": np.float64(0.5071), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.4513), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4795), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.4518), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4775), \"['b', 'x', 'b', 'x']\": np.float64(-8.9387), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4775), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.027), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4775), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.027), \"['b', 'x', 'b', 'y']\": np.float64(0.505), \"['b', 'x']\": np.float64(-9.4537), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4787), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0271), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4787), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0271), \"['b', 'y', 'a', 'x']\": np.float64(-0.5063), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.027), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0015), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.027), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0015), \"['b', 'y', 'a', 'y']\": np.float64(0.0286), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4787), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0272), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4787), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.027), \"['b', 'y', 'b', 'x']\": np.float64(-0.5063), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.027), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0015), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.027), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0015), \"['b', 'y', 'b', 'y']\": np.float64(0.0286), \"['b', 'y']\": np.float64(0.5354)}\n",
      "It s train loss bro [0.06596728414297104, 0.06480508297681808, 0.06381377577781677, 0.06285037845373154, 0.06191360577940941, 0.061002764850854874, 0.060116466134786606, 0.0592537559568882, 0.058413807302713394, 0.057595882564783096, 0.05679868906736374]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 208 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.837353229522705 and val_loss is 4.167666435241699\n",
      "for 1 epochs, loss is 2.928615093231201 and val_loss is 4.16694974899292\n",
      "for 2 epochs, loss is 2.926004409790039 and val_loss is 4.1656646728515625\n",
      "for 3 epochs, loss is 2.921659469604492 and val_loss is 4.163873195648193\n",
      "for 4 epochs, loss is 2.915769577026367 and val_loss is 4.1616291999816895\n",
      "for 5 epochs, loss is 2.908503532409668 and val_loss is 4.158985614776611\n",
      "for 6 epochs, loss is 2.900017023086548 and val_loss is 4.155983924865723\n",
      "for 7 epochs, loss is 2.8904480934143066 and val_loss is 4.152667999267578\n",
      "for 8 epochs, loss is 2.8799214363098145 and val_loss is 4.14907169342041\n",
      "for 9 epochs, loss is 2.868551015853882 and val_loss is 4.145229816436768\n",
      "for 10 epochs, loss is 2.8564367294311523 and val_loss is 4.141173839569092\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3244), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.515), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3235), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5148), \"['a', 'x', 'a', 'x']\": np.float64(-8.8481), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5149), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0319), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.515), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0318), \"['a', 'x', 'a', 'y']\": np.float64(0.5474), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3229), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5157), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3225), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5147), \"['a', 'x', 'b', 'x']\": np.float64(-8.8471), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5147), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0318), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5148), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0318), \"['a', 'x', 'b', 'y']\": np.float64(0.5472), \"['a', 'x']\": np.float64(-9.4047), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5161), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0319), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5161), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0319), \"['a', 'y', 'a', 'x']\": np.float64(-0.5486), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0319), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.002), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0319), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.002), \"['a', 'y', 'a', 'y']\": np.float64(0.0339), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5161), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.032), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5161), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0319), \"['a', 'y', 'b', 'x']\": np.float64(-0.5486), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0319), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.002), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0319), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.002), \"['a', 'y', 'b', 'y']\": np.float64(0.0339), \"['a', 'y']\": np.float64(0.5832), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3246), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.515), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3236), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5148), \"['b', 'x', 'a', 'x']\": np.float64(-8.8483), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5157), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0319), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5157), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0319), \"['b', 'x', 'a', 'y']\": np.float64(0.5482), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3236), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5157), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3232), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5147), \"['b', 'x', 'b', 'x']\": np.float64(-8.8478), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5147), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0318), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5148), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0318), \"['b', 'x', 'b', 'y']\": np.float64(0.5472), \"['b', 'x']\": np.float64(-9.4055), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5159), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0319), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5159), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0319), \"['b', 'y', 'a', 'x']\": np.float64(-0.5484), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0319), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.002), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0319), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.002), \"['b', 'y', 'a', 'y']\": np.float64(0.0339), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.516), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.032), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5159), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0319), \"['b', 'y', 'b', 'x']\": np.float64(-0.5484), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0319), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.002), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0319), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.002), \"['b', 'y', 'b', 'y']\": np.float64(0.0339), \"['b', 'y']\": np.float64(0.583)}\n",
      "It s train loss bro [2.837353229522705, 2.928615093231201, 2.926004409790039, 2.921659469604492, 2.915769577026367, 2.908503532409668, 2.900017023086548, 2.8904480934143066, 2.8799214363098145, 2.868551015853882, 2.8564367294311523]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 209 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.0615164153277874 and val_loss is 4.1053948402404785\n",
      "for 1 epochs, loss is 0.06872455030679703 and val_loss is 4.0741095542907715\n",
      "for 2 epochs, loss is 0.0760435238480568 and val_loss is 4.046854496002197\n",
      "for 3 epochs, loss is 0.08307182788848877 and val_loss is 4.023207664489746\n",
      "for 4 epochs, loss is 0.08972267061471939 and val_loss is 4.002791881561279\n",
      "for 5 epochs, loss is 0.09592881053686142 and val_loss is 3.9852688312530518\n",
      "for 6 epochs, loss is 0.10164105147123337 and val_loss is 3.97033429145813\n",
      "for 7 epochs, loss is 0.10682675987482071 and val_loss is 3.957716703414917\n",
      "for 8 epochs, loss is 0.11146793514490128 and val_loss is 3.947174549102783\n",
      "for 9 epochs, loss is 0.11555913835763931 and val_loss is 3.938488721847534\n",
      "for 10 epochs, loss is 0.11910533159971237 and val_loss is 3.931466579437256\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.9373), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8842), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.9371), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8842), \"['a', 'x', 'a', 'x']\": np.float64(-7.8351), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.884), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1128), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8843), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1127), \"['a', 'x', 'a', 'y']\": np.float64(0.9986), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.9374), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8841), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.9366), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8842), \"['a', 'x', 'b', 'x']\": np.float64(-7.8349), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.884), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1128), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8842), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1127), \"['a', 'x', 'b', 'y']\": np.float64(0.9986), \"['a', 'x']\": np.float64(-8.8491), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8866), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.113), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8866), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.113), \"['a', 'y', 'a', 'x']\": np.float64(-1.0014), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1131), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0144), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1132), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0144), \"['a', 'y', 'a', 'y']\": np.float64(0.1278), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8869), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.113), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8869), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1131), \"['a', 'y', 'b', 'x']\": np.float64(-1.0017), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.113), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0144), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.113), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0144), \"['a', 'y', 'b', 'y']\": np.float64(0.1276), \"['a', 'y']\": np.float64(1.1313), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.9402), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8846), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.94), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8845), \"['b', 'x', 'a', 'x']\": np.float64(-7.8385), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8842), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1129), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8845), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1127), \"['b', 'x', 'a', 'y']\": np.float64(0.9989), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.9398), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8844), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.9391), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8846), \"['b', 'x', 'b', 'x']\": np.float64(-7.8377), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8844), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1129), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8846), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1127), \"['b', 'x', 'b', 'y']\": np.float64(0.9991), \"['b', 'x']\": np.float64(-8.8526), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8848), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1128), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8848), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1128), \"['b', 'y', 'a', 'x']\": np.float64(-0.9993), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1129), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0144), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1129), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0144), \"['b', 'y', 'a', 'y']\": np.float64(0.1275), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8851), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1128), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.885), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1128), \"['b', 'y', 'b', 'x']\": np.float64(-0.9996), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1127), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0144), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1128), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0144), \"['b', 'y', 'b', 'y']\": np.float64(0.1274), \"['b', 'y']\": np.float64(1.1289)}\n",
      "It s train loss bro [0.0615164153277874, 0.06872455030679703, 0.0760435238480568, 0.08307182788848877, 0.08972267061471939, 0.09592881053686142, 0.10164105147123337, 0.10682675987482071, 0.11146793514490128, 0.11555913835763931, 0.11910533159971237]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 210 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.12265311926603317 and val_loss is 3.9259400367736816\n",
      "for 1 epochs, loss is 0.12462317943572998 and val_loss is 3.9217491149902344\n",
      "for 2 epochs, loss is 0.12664009630680084 and val_loss is 3.9187560081481934\n",
      "for 3 epochs, loss is 0.1281995177268982 and val_loss is 3.916836738586426\n",
      "for 4 epochs, loss is 0.12933236360549927 and val_loss is 3.9158809185028076\n",
      "for 5 epochs, loss is 0.13007181882858276 and val_loss is 3.91579008102417\n",
      "for 6 epochs, loss is 0.1304505467414856 and val_loss is 3.9164748191833496\n",
      "for 7 epochs, loss is 0.13050201535224915 and val_loss is 3.9178547859191895\n",
      "for 8 epochs, loss is 0.1302582174539566 and val_loss is 3.919858932495117\n",
      "for 9 epochs, loss is 0.12975063920021057 and val_loss is 3.922422409057617\n",
      "for 10 epochs, loss is 0.1290089637041092 and val_loss is 3.9254863262176514\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.8131), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9149), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.8129), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9149), \"['a', 'x', 'a', 'x']\": np.float64(-7.7418), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9147), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1229), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9149), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1229), \"['a', 'x', 'a', 'y']\": np.float64(1.0396), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.8131), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9148), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.8126), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9149), \"['a', 'x', 'b', 'x']\": np.float64(-7.7416), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9148), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1229), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9149), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1228), \"['a', 'x', 'b', 'y']\": np.float64(1.0396), \"['a', 'x']\": np.float64(-8.7971), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.916), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.123), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.916), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.123), \"['a', 'y', 'a', 'x']\": np.float64(-1.0409), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1231), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0165), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1231), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0165), \"['a', 'y', 'a', 'y']\": np.float64(0.1399), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9162), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.123), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9161), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.123), \"['a', 'y', 'b', 'x']\": np.float64(-1.0411), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.123), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0165), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.123), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0165), \"['a', 'y', 'b', 'y']\": np.float64(0.1398), \"['a', 'y']\": np.float64(1.1829), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.8151), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9151), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.815), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9152), \"['b', 'x', 'a', 'x']\": np.float64(-7.7441), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.915), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1229), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9151), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1229), \"['b', 'x', 'a', 'y']\": np.float64(1.0398), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.8148), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9151), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.8144), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9151), \"['b', 'x', 'b', 'x']\": np.float64(-7.7436), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.915), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1229), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9152), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1229), \"['b', 'x', 'b', 'y']\": np.float64(1.0399), \"['b', 'x']\": np.float64(-8.7996), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.915), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1229), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.915), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1229), \"['b', 'y', 'a', 'x']\": np.float64(-1.0397), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1229), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0165), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1229), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0165), \"['b', 'y', 'a', 'y']\": np.float64(0.1397), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9151), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1229), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9151), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1229), \"['b', 'y', 'b', 'x']\": np.float64(-1.0399), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1229), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0165), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1229), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0165), \"['b', 'y', 'b', 'y']\": np.float64(0.1396), \"['b', 'y']\": np.float64(1.1816)}\n",
      "It s train loss bro [0.12265311926603317, 0.12462317943572998, 0.12664009630680084, 0.1281995177268982, 0.12933236360549927, 0.13007181882858276, 0.1304505467414856, 0.13050201535224915, 0.1302582174539566, 0.12975063920021057, 0.1290089637041092]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 211 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1734801083803177 and val_loss is 3.928879499435425\n",
      "for 1 epochs, loss is 0.12688510119915009 and val_loss is 3.932691812515259\n",
      "for 2 epochs, loss is 0.12555928528308868 and val_loss is 3.9368841648101807\n",
      "for 3 epochs, loss is 0.12410599738359451 and val_loss is 3.9414217472076416\n",
      "for 4 epochs, loss is 0.12254545092582703 and val_loss is 3.946270704269409\n",
      "for 5 epochs, loss is 0.1208958551287651 and val_loss is 3.9514029026031494\n",
      "for 6 epochs, loss is 0.11917411535978317 and val_loss is 3.956785202026367\n",
      "for 7 epochs, loss is 0.11739460378885269 and val_loss is 3.962392807006836\n",
      "for 8 epochs, loss is 0.11557114124298096 and val_loss is 3.9682016372680664\n",
      "for 9 epochs, loss is 0.11371519416570663 and val_loss is 3.9741873741149902\n",
      "for 10 epochs, loss is 0.11183756589889526 and val_loss is 3.980328321456909\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.1929), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8213), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.192), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8221), \"['a', 'x', 'a', 'x']\": np.float64(-8.0271), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8212), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0938), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8213), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0939), \"['a', 'x', 'a', 'y']\": np.float64(0.9166), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.1921), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8213), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.1911), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8219), \"['a', 'x', 'b', 'x']\": np.float64(-8.0261), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.822), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0939), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.822), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0939), \"['a', 'x', 'b', 'y']\": np.float64(0.9174), \"['a', 'x']\": np.float64(-8.958), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8223), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0939), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8222), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.094), \"['a', 'y', 'a', 'x']\": np.float64(-0.9176), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0939), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0107), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.094), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0107), \"['a', 'y', 'a', 'y']\": np.float64(0.1049), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8223), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0939), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8222), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.094), \"['a', 'y', 'b', 'x']\": np.float64(-0.9177), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.094), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0107), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.094), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0107), \"['a', 'y', 'b', 'y']\": np.float64(0.1049), \"['a', 'y']\": np.float64(1.0242), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.1939), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8215), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.193), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8222), \"['b', 'x', 'a', 'x']\": np.float64(-8.0282), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8214), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0939), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8214), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0939), \"['b', 'x', 'a', 'y']\": np.float64(0.9168), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.193), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8214), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.192), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.822), \"['b', 'x', 'b', 'x']\": np.float64(-8.0271), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.822), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0939), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8221), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0939), \"['b', 'x', 'b', 'y']\": np.float64(0.9175), \"['b', 'x']\": np.float64(-8.9592), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8221), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0939), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.822), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.094), \"['b', 'y', 'a', 'x']\": np.float64(-0.9174), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0939), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0107), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0939), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0107), \"['b', 'y', 'a', 'y']\": np.float64(0.1048), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8222), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0939), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.822), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.094), \"['b', 'y', 'b', 'x']\": np.float64(-0.9175), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0939), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0107), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.094), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0107), \"['b', 'y', 'b', 'y']\": np.float64(0.1049), \"['b', 'y']\": np.float64(1.024)}\n",
      "It s train loss bro [0.1734801083803177, 0.12688510119915009, 0.12555928528308868, 0.12410599738359451, 0.12254545092582703, 0.1208958551287651, 0.11917411535978317, 0.11739460378885269, 0.11557114124298096, 0.11371519416570663, 0.11183756589889526]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 212 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.11136097460985184 and val_loss is 3.9866127967834473\n",
      "for 1 epochs, loss is 0.10805287957191467 and val_loss is 3.9930121898651123\n",
      "for 2 epochs, loss is 0.10616104304790497 and val_loss is 3.9995110034942627\n",
      "for 3 epochs, loss is 0.10427786409854889 and val_loss is 4.006091117858887\n",
      "for 4 epochs, loss is 0.10240835696458817 and val_loss is 4.012739658355713\n",
      "for 5 epochs, loss is 0.1005571186542511 and val_loss is 4.019440650939941\n",
      "for 6 epochs, loss is 0.09872782975435257 and val_loss is 4.026184558868408\n",
      "for 7 epochs, loss is 0.09692364186048508 and val_loss is 4.032957553863525\n",
      "for 8 epochs, loss is 0.09514719247817993 and val_loss is 4.039750576019287\n",
      "for 9 epochs, loss is 0.0934002697467804 and val_loss is 4.046553611755371\n",
      "for 10 epochs, loss is 0.09168509393930435 and val_loss is 4.05335807800293\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.6351), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7066), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.6337), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7078), \"['a', 'x', 'a', 'x']\": np.float64(-8.3531), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7066), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0654), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7065), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0656), \"['a', 'x', 'a', 'y']\": np.float64(0.7731), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.6336), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7067), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.6324), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7075), \"['a', 'x', 'b', 'x']\": np.float64(-8.3515), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7078), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0655), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7077), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0656), \"['a', 'x', 'b', 'y']\": np.float64(0.7744), \"['a', 'x']\": np.float64(-9.1386), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7069), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0654), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7068), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0655), \"['a', 'y', 'a', 'x']\": np.float64(-0.7734), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0654), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0061), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0654), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0061), \"['a', 'y', 'a', 'y']\": np.float64(0.0716), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7069), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0654), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7068), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0655), \"['a', 'y', 'b', 'x']\": np.float64(-0.7734), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0656), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0061), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0656), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0061), \"['a', 'y', 'b', 'y']\": np.float64(0.0718), \"['a', 'y']\": np.float64(0.8462), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.6348), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7066), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.6334), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7078), \"['b', 'x', 'a', 'x']\": np.float64(-8.3528), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7067), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0654), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7066), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0656), \"['b', 'x', 'a', 'y']\": np.float64(0.7732), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.6335), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7066), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.6323), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7075), \"['b', 'x', 'b', 'x']\": np.float64(-8.3514), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7076), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0655), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7075), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0656), \"['b', 'x', 'b', 'y']\": np.float64(0.7742), \"['b', 'x']\": np.float64(-9.1384), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7078), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0655), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7076), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0656), \"['b', 'y', 'a', 'x']\": np.float64(-0.7743), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0655), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0061), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0655), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0061), \"['b', 'y', 'a', 'y']\": np.float64(0.0717), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7077), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0655), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7076), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0656), \"['b', 'y', 'b', 'x']\": np.float64(-0.7743), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0656), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0061), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0656), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0061), \"['b', 'y', 'b', 'y']\": np.float64(0.0718), \"['b', 'y']\": np.float64(0.8472)}\n",
      "It s train loss bro [0.11136097460985184, 0.10805287957191467, 0.10616104304790497, 0.10427786409854889, 0.10240835696458817, 0.1005571186542511, 0.09872782975435257, 0.09692364186048508, 0.09514719247817993, 0.0934002697467804, 0.09168509393930435]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 213 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.466355800628662 and val_loss is 4.053490161895752\n",
      "for 1 epochs, loss is 2.4691240787506104 and val_loss is 4.053120136260986\n",
      "for 2 epochs, loss is 2.467175006866455 and val_loss is 4.052302837371826\n",
      "for 3 epochs, loss is 2.463653802871704 and val_loss is 4.051083087921143\n",
      "for 4 epochs, loss is 2.458730697631836 and val_loss is 4.049506664276123\n",
      "for 5 epochs, loss is 2.452561378479004 and val_loss is 4.047615051269531\n",
      "for 6 epochs, loss is 2.445286273956299 and val_loss is 4.045441627502441\n",
      "for 7 epochs, loss is 2.437032461166382 and val_loss is 4.0430216789245605\n",
      "for 8 epochs, loss is 2.427914619445801 and val_loss is 4.0403852462768555\n",
      "for 9 epochs, loss is 2.4180352687835693 and val_loss is 4.037559509277344\n",
      "for 10 epochs, loss is 2.4074883460998535 and val_loss is 4.034569263458252\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.4741), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7495), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.4728), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7507), \"['a', 'x', 'a', 'x']\": np.float64(-8.2353), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7494), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0752), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7493), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0753), \"['a', 'x', 'a', 'y']\": np.float64(0.8258), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.4727), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7495), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.4715), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7503), \"['a', 'x', 'b', 'x']\": np.float64(-8.2338), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7506), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0753), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7506), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0754), \"['a', 'x', 'b', 'y']\": np.float64(0.8271), \"['a', 'x']\": np.float64(-9.074), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7496), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0752), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7495), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0753), \"['a', 'y', 'a', 'x']\": np.float64(-0.826), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0752), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0075), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0752), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0076), \"['a', 'y', 'a', 'y']\": np.float64(0.0829), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7496), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0752), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7494), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0753), \"['a', 'y', 'b', 'x']\": np.float64(-0.8259), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0753), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0076), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0753), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0076), \"['a', 'y', 'b', 'y']\": np.float64(0.083), \"['a', 'y']\": np.float64(0.9102), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.4738), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7494), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.4724), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7506), \"['b', 'x', 'a', 'x']\": np.float64(-8.2349), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7495), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0752), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7495), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0753), \"['b', 'x', 'a', 'y']\": np.float64(0.8259), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.4725), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7495), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.4713), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7503), \"['b', 'x', 'b', 'x']\": np.float64(-8.2336), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7504), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0753), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7503), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0754), \"['b', 'x', 'b', 'y']\": np.float64(0.8269), \"['b', 'x']\": np.float64(-9.0737), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7506), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0753), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7504), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0754), \"['b', 'y', 'a', 'x']\": np.float64(-0.827), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0753), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0076), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0753), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0076), \"['b', 'y', 'a', 'y']\": np.float64(0.083), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7505), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0753), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7504), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0754), \"['b', 'y', 'b', 'x']\": np.float64(-0.8269), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0754), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0076), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0754), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0076), \"['b', 'y', 'b', 'y']\": np.float64(0.0831), \"['b', 'y']\": np.float64(0.9113)}\n",
      "It s train loss bro [2.466355800628662, 2.4691240787506104, 2.467175006866455, 2.463653802871704, 2.458730697631836, 2.452561378479004, 2.445286273956299, 2.437032461166382, 2.427914619445801, 2.4180352687835693, 2.4074883460998535]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 214 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1005011647939682 and val_loss is 4.0085954666137695\n",
      "for 1 epochs, loss is 0.10748636722564697 and val_loss is 3.986299753189087\n",
      "for 2 epochs, loss is 0.11736704409122467 and val_loss is 3.9672489166259766\n",
      "for 3 epochs, loss is 0.12661142647266388 and val_loss is 3.951059579849243\n",
      "for 4 epochs, loss is 0.13512514531612396 and val_loss is 3.937389612197876\n",
      "for 5 epochs, loss is 0.14284241199493408 and val_loss is 3.925938129425049\n",
      "for 6 epochs, loss is 0.1497240513563156 and val_loss is 3.9164397716522217\n",
      "for 7 epochs, loss is 0.15575286746025085 and val_loss is 3.908663034439087\n",
      "for 8 epochs, loss is 0.16093102097511292 and val_loss is 3.9024038314819336\n",
      "for 9 epochs, loss is 0.165275439620018 and val_loss is 3.8974883556365967\n",
      "for 10 epochs, loss is 0.16881579160690308 and val_loss is 3.8937621116638184\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.9765), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1045), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.976), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.105), \"['a', 'x', 'a', 'x']\": np.float64(-7.0952), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1046), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.204), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1044), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2043), \"['a', 'x', 'a', 'y']\": np.float64(1.3113), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.9759), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1047), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9756), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1047), \"['a', 'x', 'b', 'x']\": np.float64(-7.0945), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.105), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2041), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1049), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2043), \"['a', 'x', 'b', 'y']\": np.float64(1.3119), \"['a', 'x']\": np.float64(-8.4232), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1034), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2039), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1033), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.204), \"['a', 'y', 'a', 'x']\": np.float64(-1.3099), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2038), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0376), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2038), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0377), \"['a', 'y', 'a', 'y']\": np.float64(0.242), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1032), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2039), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1032), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2039), \"['a', 'y', 'b', 'x']\": np.float64(-1.3097), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2041), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0377), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2041), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0377), \"['a', 'y', 'b', 'y']\": np.float64(0.2423), \"['a', 'y']\": np.float64(1.5551), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.976), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1045), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9755), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.105), \"['b', 'x', 'a', 'x']\": np.float64(-7.0946), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1047), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2041), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1046), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2044), \"['b', 'x', 'a', 'y']\": np.float64(1.3115), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.9757), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1046), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9754), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1046), \"['b', 'x', 'b', 'x']\": np.float64(-7.0943), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1047), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2041), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1046), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2043), \"['b', 'x', 'b', 'y']\": np.float64(1.3115), \"['b', 'x']\": np.float64(-8.4226), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1044), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2041), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1043), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2042), \"['b', 'y', 'a', 'x']\": np.float64(-1.3111), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.204), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0377), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.204), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0377), \"['b', 'y', 'a', 'y']\": np.float64(0.2422), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1043), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2041), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1043), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2041), \"['b', 'y', 'b', 'x']\": np.float64(-1.311), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2042), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0377), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2042), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0378), \"['b', 'y', 'b', 'y']\": np.float64(0.2424), \"['b', 'y']\": np.float64(1.5565)}\n",
      "It s train loss bro [0.1005011647939682, 0.10748636722564697, 0.11736704409122467, 0.12661142647266388, 0.13512514531612396, 0.14284241199493408, 0.1497240513563156, 0.15575286746025085, 0.16093102097511292, 0.165275439620018, 0.16881579160690308]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 215 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.19171848893165588 and val_loss is 3.8912835121154785\n",
      "for 1 epochs, loss is 0.17364336550235748 and val_loss is 3.889727830886841\n",
      "for 2 epochs, loss is 0.17502707242965698 and val_loss is 3.8889970779418945\n",
      "for 3 epochs, loss is 0.1757953017950058 and val_loss is 3.889007568359375\n",
      "for 4 epochs, loss is 0.1760028600692749 and val_loss is 3.8896865844726562\n",
      "for 5 epochs, loss is 0.17570430040359497 and val_loss is 3.890970230102539\n",
      "for 6 epochs, loss is 0.1749529093503952 and val_loss is 3.892803907394409\n",
      "for 7 epochs, loss is 0.17380087077617645 and val_loss is 3.895139694213867\n",
      "for 8 epochs, loss is 0.17229758203029633 and val_loss is 3.8979358673095703\n",
      "for 9 epochs, loss is 0.17048949003219604 and val_loss is 3.9011552333831787\n",
      "for 10 epochs, loss is 0.16842009127140045 and val_loss is 3.904764413833618\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.0742), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0839), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.0743), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0836), \"['a', 'x', 'a', 'x']\": np.float64(-7.1718), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0839), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1933), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0839), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1934), \"['a', 'x', 'a', 'y']\": np.float64(1.2798), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.0742), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0841), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.0744), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0835), \"['a', 'x', 'b', 'x']\": np.float64(-7.1719), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0836), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1933), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0837), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1933), \"['a', 'x', 'b', 'y']\": np.float64(1.2795), \"['a', 'x']\": np.float64(-8.4678), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0837), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1934), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0838), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1933), \"['a', 'y', 'a', 'x']\": np.float64(-1.2796), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1933), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0345), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1933), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0345), \"['a', 'y', 'a', 'y']\": np.float64(0.2282), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0838), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1934), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0838), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1933), \"['a', 'y', 'b', 'x']\": np.float64(-1.2796), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1934), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0345), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1934), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0345), \"['a', 'y', 'b', 'y']\": np.float64(0.2283), \"['a', 'y']\": np.float64(1.5109), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.0748), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.084), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.0749), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0838), \"['b', 'x', 'a', 'x']\": np.float64(-7.1726), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0841), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1934), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0842), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1935), \"['b', 'x', 'a', 'y']\": np.float64(1.2801), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.075), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0842), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.0751), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0836), \"['b', 'x', 'b', 'x']\": np.float64(-7.1728), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0836), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1933), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0836), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1933), \"['b', 'x', 'b', 'y']\": np.float64(1.2794), \"['b', 'x']\": np.float64(-8.4688), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.0837), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1934), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0837), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1933), \"['b', 'y', 'a', 'x']\": np.float64(-1.2795), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1933), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0345), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1933), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0345), \"['b', 'y', 'a', 'y']\": np.float64(0.2282), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0837), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1934), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0838), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1933), \"['b', 'y', 'b', 'x']\": np.float64(-1.2796), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1933), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0345), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1933), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0345), \"['b', 'y', 'b', 'y']\": np.float64(0.2283), \"['b', 'y']\": np.float64(1.5108)}\n",
      "It s train loss bro [0.19171848893165588, 0.17364336550235748, 0.17502707242965698, 0.1757953017950058, 0.1760028600692749, 0.17570430040359497, 0.1749529093503952, 0.17380087077617645, 0.17229758203029633, 0.17048949003219604, 0.16842009127140045]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 216 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.8823119401931763 and val_loss is 3.904858112335205\n",
      "for 1 epochs, loss is 1.889685034751892 and val_loss is 3.9047088623046875\n",
      "for 2 epochs, loss is 1.8881746530532837 and val_loss is 3.904344320297241\n",
      "for 3 epochs, loss is 1.8854808807373047 and val_loss is 3.9037885665893555\n",
      "for 4 epochs, loss is 1.8817291259765625 and val_loss is 3.903064489364624\n",
      "for 5 epochs, loss is 1.8770338296890259 and val_loss is 3.902193307876587\n",
      "for 6 epochs, loss is 1.8714985847473145 and val_loss is 3.9011943340301514\n",
      "for 7 epochs, loss is 1.86521577835083 and val_loss is 3.900085926055908\n",
      "for 8 epochs, loss is 1.8582711219787598 and val_loss is 3.8988840579986572\n",
      "for 9 epochs, loss is 1.8507399559020996 and val_loss is 3.8976049423217773\n",
      "for 10 epochs, loss is 1.8426915407180786 and val_loss is 3.896261215209961\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.8889), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1226), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8891), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1223), \"['a', 'x', 'a', 'x']\": np.float64(-7.0253), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1226), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2139), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1227), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.214), \"['a', 'x', 'a', 'y']\": np.float64(1.3393), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.8891), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1228), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.8894), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1222), \"['a', 'x', 'b', 'x']\": np.float64(-7.0256), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1223), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2138), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1223), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2139), \"['a', 'x', 'b', 'y']\": np.float64(1.3389), \"['a', 'x']\": np.float64(-8.3811), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1221), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2139), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1222), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2139), \"['a', 'y', 'a', 'x']\": np.float64(-1.3387), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2138), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0407), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2138), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0408), \"['a', 'y', 'a', 'y']\": np.float64(0.2551), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1222), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.214), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1222), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2138), \"['a', 'y', 'b', 'x']\": np.float64(-1.3388), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2139), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0408), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2139), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0408), \"['a', 'y', 'b', 'y']\": np.float64(0.2552), \"['a', 'y']\": np.float64(1.5971), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.8896), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1228), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8898), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1224), \"['b', 'x', 'a', 'x']\": np.float64(-7.0262), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1229), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.214), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1229), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.214), \"['b', 'x', 'a', 'y']\": np.float64(1.3396), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.8899), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.123), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.8901), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1223), \"['b', 'x', 'b', 'x']\": np.float64(-7.0265), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1223), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2138), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1223), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2139), \"['b', 'x', 'b', 'y']\": np.float64(1.3389), \"['b', 'x']\": np.float64(-8.3822), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1221), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2139), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1221), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2138), \"['b', 'y', 'a', 'x']\": np.float64(-1.3386), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2138), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0407), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2138), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0408), \"['b', 'y', 'a', 'y']\": np.float64(0.2551), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1221), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2139), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1222), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2138), \"['b', 'y', 'b', 'x']\": np.float64(-1.3387), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2138), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0407), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2139), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0408), \"['b', 'y', 'b', 'y']\": np.float64(0.2551), \"['b', 'y']\": np.float64(1.597)}\n",
      "It s train loss bro [1.8823119401931763, 1.889685034751892, 1.8881746530532837, 1.8854808807373047, 1.8817291259765625, 1.8770338296890259, 1.8714985847473145, 1.86521577835083, 1.8582711219787598, 1.8507399559020996, 1.8426915407180786]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 217 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.18348033726215363 and val_loss is 3.88523530960083\n",
      "for 1 epochs, loss is 0.1912374347448349 and val_loss is 3.876420736312866\n",
      "for 2 epochs, loss is 0.2047591656446457 and val_loss is 3.869441270828247\n",
      "for 3 epochs, loss is 0.21697086095809937 and val_loss is 3.863976240158081\n",
      "for 4 epochs, loss is 0.22777415812015533 and val_loss is 3.859754800796509\n",
      "for 5 epochs, loss is 0.2371142953634262 and val_loss is 3.8565523624420166\n",
      "for 6 epochs, loss is 0.24497617781162262 and val_loss is 3.854187250137329\n",
      "for 7 epochs, loss is 0.2513773739337921 and val_loss is 3.852508306503296\n",
      "for 8 epochs, loss is 0.2563629448413849 and val_loss is 3.8513998985290527\n",
      "for 9 epochs, loss is 0.2600005567073822 and val_loss is 3.85077166557312\n",
      "for 10 epochs, loss is 0.2623734772205353 and val_loss is 3.850554943084717\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.5358), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3549), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.5361), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3549), \"['a', 'x', 'a', 'x']\": np.float64(-5.9042), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.355), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4046), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3548), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4049), \"['a', 'x', 'a', 'y']\": np.float64(1.7637), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.536), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3552), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.5365), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3547), \"['a', 'x', 'b', 'x']\": np.float64(-5.9046), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3549), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4046), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3549), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4048), \"['a', 'x', 'b', 'y']\": np.float64(1.7636), \"['a', 'x']\": np.float64(-7.6855), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3515), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4037), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3516), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4037), \"['a', 'y', 'a', 'x']\": np.float64(-1.7592), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4036), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1205), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4036), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1206), \"['a', 'y', 'a', 'y']\": np.float64(0.5253), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3513), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4037), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3515), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4036), \"['a', 'y', 'b', 'x']\": np.float64(-1.7591), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4039), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1206), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4039), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1207), \"['a', 'y', 'b', 'y']\": np.float64(0.5257), \"['a', 'y']\": np.float64(2.2899), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.5359), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3549), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.5362), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3549), \"['b', 'x', 'a', 'x']\": np.float64(-5.9043), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3552), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4047), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3551), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.405), \"['b', 'x', 'a', 'y']\": np.float64(1.764), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.5363), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3553), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.5368), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3548), \"['b', 'x', 'b', 'x']\": np.float64(-5.905), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3548), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4046), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3547), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4047), \"['b', 'x', 'b', 'y']\": np.float64(1.7634), \"['b', 'x']\": np.float64(-7.6859), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3519), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4038), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.352), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4038), \"['b', 'y', 'a', 'x']\": np.float64(-1.7597), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4038), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1206), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4037), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1207), \"['b', 'y', 'a', 'y']\": np.float64(0.5256), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3518), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4039), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.352), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4037), \"['b', 'y', 'b', 'x']\": np.float64(-1.7597), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4039), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1206), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4039), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1207), \"['b', 'y', 'b', 'y']\": np.float64(0.5257), \"['b', 'y']\": np.float64(2.2905)}\n",
      "It s train loss bro [0.18348033726215363, 0.1912374347448349, 0.2047591656446457, 0.21697086095809937, 0.22777415812015533, 0.2371142953634262, 0.24497617781162262, 0.2513773739337921, 0.2563629448413849, 0.2600005567073822, 0.2623734772205353]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 218 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.4732311964035034 and val_loss is 3.8505518436431885\n",
      "for 1 epochs, loss is 1.4709851741790771 and val_loss is 3.8505795001983643\n",
      "for 2 epochs, loss is 1.4686681032180786 and val_loss is 3.850635528564453\n",
      "for 3 epochs, loss is 1.4655357599258423 and val_loss is 3.8507206439971924\n",
      "for 4 epochs, loss is 1.4616750478744507 and val_loss is 3.8508362770080566\n",
      "for 5 epochs, loss is 1.457163691520691 and val_loss is 3.850982427597046\n",
      "for 6 epochs, loss is 1.4520727396011353 and val_loss is 3.851161003112793\n",
      "for 7 epochs, loss is 1.4464654922485352 and val_loss is 3.8513729572296143\n",
      "for 8 epochs, loss is 1.4403996467590332 and val_loss is 3.8516218662261963\n",
      "for 9 epochs, loss is 1.4339269399642944 and val_loss is 3.8519067764282227\n",
      "for 10 epochs, loss is 1.427093744277954 and val_loss is 3.852231740951538\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3212), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3824), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.3212), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3827), \"['a', 'x', 'a', 'x']\": np.float64(-5.7167), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3824), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4422), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3822), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4426), \"['a', 'x', 'a', 'y']\": np.float64(1.8289), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.321), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3827), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3213), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3824), \"['a', 'x', 'b', 'x']\": np.float64(-5.7168), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3827), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4423), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3826), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4425), \"['a', 'x', 'b', 'y']\": np.float64(1.8292), \"['a', 'x']\": np.float64(-7.563), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3783), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4409), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3783), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.441), \"['a', 'y', 'a', 'x']\": np.float64(-1.8235), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4409), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.141), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4408), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1411), \"['a', 'y', 'a', 'y']\": np.float64(0.5832), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.378), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4409), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3781), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4409), \"['a', 'y', 'b', 'x']\": np.float64(-1.8232), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4413), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1412), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4412), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1412), \"['a', 'y', 'b', 'y']\": np.float64(0.5838), \"['a', 'y']\": np.float64(2.4123), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3208), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3823), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.3208), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3826), \"['b', 'x', 'a', 'x']\": np.float64(-5.7163), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3827), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4422), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3824), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4426), \"['b', 'x', 'a', 'y']\": np.float64(1.8291), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.321), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3826), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3213), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3824), \"['b', 'x', 'b', 'x']\": np.float64(-5.7167), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3824), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4422), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3823), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4424), \"['b', 'x', 'b', 'y']\": np.float64(1.8288), \"['b', 'x']\": np.float64(-7.5627), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3791), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4412), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3791), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4413), \"['b', 'y', 'a', 'x']\": np.float64(-1.8245), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4412), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1411), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4411), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1412), \"['b', 'y', 'a', 'y']\": np.float64(0.5837), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3789), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4412), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.379), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4412), \"['b', 'y', 'b', 'x']\": np.float64(-1.8243), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4414), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1412), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4413), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1413), \"['b', 'y', 'b', 'y']\": np.float64(0.5839), \"['b', 'y']\": np.float64(2.4137)}\n",
      "It s train loss bro [1.4732311964035034, 1.4709851741790771, 1.4686681032180786, 1.4655357599258423, 1.4616750478744507, 1.457163691520691, 1.4520727396011353, 1.4464654922485352, 1.4403996467590332, 1.4339269399642944, 1.427093744277954]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 219 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.29819127917289734 and val_loss is 3.855808973312378\n",
      "for 1 epochs, loss is 0.3000124990940094 and val_loss is 3.85941743850708\n",
      "for 2 epochs, loss is 0.3181177079677582 and val_loss is 3.8628246784210205\n",
      "for 3 epochs, loss is 0.333873987197876 and val_loss is 3.865851402282715\n",
      "for 4 epochs, loss is 0.3471464514732361 and val_loss is 3.8683695793151855\n",
      "for 5 epochs, loss is 0.3578818738460541 and val_loss is 3.870300054550171\n",
      "for 6 epochs, loss is 0.3661006689071655 and val_loss is 3.8716039657592773\n",
      "for 7 epochs, loss is 0.37188515067100525 and val_loss is 3.8722751140594482\n",
      "for 8 epochs, loss is 0.37536799907684326 and val_loss is 3.8723385334014893\n",
      "for 9 epochs, loss is 0.3767198324203491 and val_loss is 3.871840000152588\n",
      "for 10 epochs, loss is 0.3761378228664398 and val_loss is 3.8708457946777344\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.2627), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4676), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.2611), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4696), \"['a', 'x', 'a', 'x']\": np.float64(-4.7413), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4672), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.6607), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.4664), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6615), \"['a', 'x', 'a', 'y']\": np.float64(2.1327), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.2607), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4673), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.2598), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4684), \"['a', 'x', 'b', 'x']\": np.float64(-4.7389), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.469), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6618), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4688), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.6621), \"['a', 'x', 'b', 'y']\": np.float64(2.1356), \"['a', 'x']\": np.float64(-6.8899), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4607), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.6571), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.46), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.658), \"['a', 'y', 'a', 'x']\": np.float64(-2.1227), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.6576), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.2961), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.6572), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.2965), \"['a', 'y', 'a', 'y']\": np.float64(0.9559), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4599), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.6569), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4595), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6574), \"['a', 'y', 'b', 'x']\": np.float64(-2.1217), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.6584), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.2966), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.6583), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.2967), \"['a', 'y', 'b', 'y']\": np.float64(0.9571), \"['a', 'y']\": np.float64(3.0856), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-3.2603), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.4665), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-3.2587), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.4685), \"['b', 'x', 'a', 'x']\": np.float64(-4.7378), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.4667), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.6604), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.466), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.6613), \"['b', 'x', 'a', 'y']\": np.float64(2.132), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-3.2591), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.4666), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-3.2582), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.4677), \"['b', 'x', 'b', 'x']\": np.float64(-4.7365), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.4676), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.6612), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.4674), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.6615), \"['b', 'x', 'b', 'y']\": np.float64(2.1336), \"['b', 'x']\": np.float64(-6.8856), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.4631), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.6581), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.4624), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.659), \"['b', 'y', 'a', 'x']\": np.float64(-2.1262), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.659), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.2967), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.6586), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.2971), \"['b', 'y', 'a', 'y']\": np.float64(0.9579), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.4627), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.6582), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.4623), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.6587), \"['b', 'y', 'b', 'x']\": np.float64(-2.1258), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.6592), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.297), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.6591), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.2971), \"['b', 'y', 'b', 'y']\": np.float64(0.9583), \"['b', 'y']\": np.float64(3.0911)}\n",
      "It s train loss bro [0.29819127917289734, 0.3000124990940094, 0.3181177079677582, 0.333873987197876, 0.3471464514732361, 0.3578818738460541, 0.3661006689071655, 0.37188515067100525, 0.37536799907684326, 0.3767198324203491, 0.3761378228664398]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 220 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.45333102345466614 and val_loss is 3.8695614337921143\n",
      "for 1 epochs, loss is 0.3700527548789978 and val_loss is 3.8679351806640625\n",
      "for 2 epochs, loss is 0.36498764157295227 and val_loss is 3.8660671710968018\n",
      "for 3 epochs, loss is 0.35884854197502136 and val_loss is 3.8640551567077637\n",
      "for 4 epochs, loss is 0.35183238983154297 and val_loss is 3.8619983196258545\n",
      "for 5 epochs, loss is 0.34412023425102234 and val_loss is 3.8599894046783447\n",
      "for 6 epochs, loss is 0.3358750641345978 and val_loss is 3.8581199645996094\n",
      "for 7 epochs, loss is 0.3272421956062317 and val_loss is 3.8564720153808594\n",
      "for 8 epochs, loss is 0.3183477818965912 and val_loss is 3.8551218509674072\n",
      "for 9 epochs, loss is 0.30930066108703613 and val_loss is 3.8541369438171387\n",
      "for 10 epochs, loss is 0.3001929819583893 and val_loss is 3.8535776138305664\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.1755), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3987), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.1758), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3987), \"['a', 'x', 'a', 'x']\": np.float64(-5.5871), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3988), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4684), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3987), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4687), \"['a', 'x', 'a', 'y']\": np.float64(1.8716), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.1757), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.399), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.1762), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3986), \"['a', 'x', 'b', 'x']\": np.float64(-5.5875), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3987), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4685), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3987), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4686), \"['a', 'x', 'b', 'y']\": np.float64(1.8715), \"['a', 'x']\": np.float64(-7.4759), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.396), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4676), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3961), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4676), \"['a', 'y', 'a', 'x']\": np.float64(-1.8679), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4675), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1566), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4675), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1567), \"['a', 'y', 'a', 'y']\": np.float64(0.6256), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3959), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4676), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.396), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4675), \"['a', 'y', 'b', 'x']\": np.float64(-1.8678), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4678), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1567), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4678), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1567), \"['a', 'y', 'b', 'y']\": np.float64(0.6259), \"['a', 'y']\": np.float64(2.4993), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.1758), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3988), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.1761), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3988), \"['b', 'x', 'a', 'x']\": np.float64(-5.5875), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.399), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4685), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3989), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4688), \"['b', 'x', 'a', 'y']\": np.float64(1.8719), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.1762), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3991), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.1767), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3987), \"['b', 'x', 'b', 'x']\": np.float64(-5.5881), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3986), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4684), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3986), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4686), \"['b', 'x', 'b', 'y']\": np.float64(1.8714), \"['b', 'x']\": np.float64(-7.4765), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3962), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4677), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3963), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4677), \"['b', 'y', 'a', 'x']\": np.float64(-1.8682), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4676), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1566), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4676), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1567), \"['b', 'y', 'a', 'y']\": np.float64(0.6257), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3961), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4677), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3963), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4676), \"['b', 'y', 'b', 'x']\": np.float64(-1.8682), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4677), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1567), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4677), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1567), \"['b', 'y', 'b', 'y']\": np.float64(0.6259), \"['b', 'y']\": np.float64(2.4997)}\n",
      "It s train loss bro [0.45333102345466614, 0.3700527548789978, 0.36498764157295227, 0.35884854197502136, 0.35183238983154297, 0.34412023425102234, 0.3358750641345978, 0.3272421956062317, 0.3183477818965912, 0.30930066108703613, 0.3001929819583893]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 221 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2934218943119049 and val_loss is 3.8535611629486084\n",
      "for 1 epochs, loss is 0.28211113810539246 and val_loss is 3.8540542125701904\n",
      "for 2 epochs, loss is 0.2732475697994232 and val_loss is 3.855087995529175\n",
      "for 3 epochs, loss is 0.2645527124404907 and val_loss is 3.8566880226135254\n",
      "for 4 epochs, loss is 0.256058931350708 and val_loss is 3.85886812210083\n",
      "for 5 epochs, loss is 0.2477903813123703 and val_loss is 3.8616385459899902\n",
      "for 6 epochs, loss is 0.23976509273052216 and val_loss is 3.865000009536743\n",
      "for 7 epochs, loss is 0.23199555277824402 and val_loss is 3.8689463138580322\n",
      "for 8 epochs, loss is 0.22448942065238953 and val_loss is 3.8734688758850098\n",
      "for 9 epochs, loss is 0.2172510176897049 and val_loss is 3.878549098968506\n",
      "for 10 epochs, loss is 0.21028108894824982 and val_loss is 3.884167194366455\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.4285), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2112), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.4282), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2114), \"['a', 'x', 'a', 'x']\": np.float64(-6.6534), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.211), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2703), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2112), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2703), \"['a', 'x', 'a', 'y']\": np.float64(1.4845), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.4284), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2111), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.4279), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2113), \"['a', 'x', 'b', 'x']\": np.float64(-6.653), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2112), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2704), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2114), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2703), \"['a', 'x', 'b', 'y']\": np.float64(1.4847), \"['a', 'x']\": np.float64(-8.1547), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2131), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2707), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.213), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2707), \"['a', 'y', 'a', 'x']\": np.float64(-1.4868), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2708), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0604), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2708), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0604), \"['a', 'y', 'a', 'y']\": np.float64(0.3319), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2133), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2707), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2132), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2707), \"['a', 'y', 'b', 'x']\": np.float64(-1.4871), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2707), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0604), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2707), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0604), \"['a', 'y', 'b', 'y']\": np.float64(0.3318), \"['a', 'y']\": np.float64(1.8226), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.4298), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2115), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.4295), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2117), \"['b', 'x', 'a', 'x']\": np.float64(-6.6549), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2112), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2704), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2114), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2703), \"['b', 'x', 'a', 'y']\": np.float64(1.4847), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.4294), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2113), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.429), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2115), \"['b', 'x', 'b', 'x']\": np.float64(-6.6544), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2114), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2704), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2116), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2703), \"['b', 'x', 'b', 'y']\": np.float64(1.4849), \"['b', 'x']\": np.float64(-8.1563), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2125), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2705), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2124), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2706), \"['b', 'y', 'a', 'x']\": np.float64(-1.4861), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2706), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0604), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2707), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0604), \"['b', 'y', 'a', 'y']\": np.float64(0.3317), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2127), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2706), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2126), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2706), \"['b', 'y', 'b', 'x']\": np.float64(-1.4863), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2705), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0604), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2706), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0604), \"['b', 'y', 'b', 'y']\": np.float64(0.3316), \"['b', 'y']\": np.float64(1.8217)}\n",
      "It s train loss bro [0.2934218943119049, 0.28211113810539246, 0.2732475697994232, 0.2645527124404907, 0.256058931350708, 0.2477903813123703, 0.23976509273052216, 0.23199555277824402, 0.22448942065238953, 0.2172510176897049, 0.21028108894824982]\n",
      "% good predict : 40\n",
      "\u001b[0;34miteration 222 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.581056833267212 and val_loss is 3.884456157684326\n",
      "for 1 epochs, loss is 1.704607367515564 and val_loss is 3.88443660736084\n",
      "for 2 epochs, loss is 1.7049545049667358 and val_loss is 3.8841419219970703\n",
      "for 3 epochs, loss is 1.7041149139404297 and val_loss is 3.8835983276367188\n",
      "for 4 epochs, loss is 1.702211618423462 and val_loss is 3.8828327655792236\n",
      "for 5 epochs, loss is 1.6993565559387207 and val_loss is 3.881868600845337\n",
      "for 6 epochs, loss is 1.6956512928009033 and val_loss is 3.880729913711548\n",
      "for 7 epochs, loss is 1.6911875009536743 and val_loss is 3.879436731338501\n",
      "for 8 epochs, loss is 1.6860486268997192 and val_loss is 3.8780081272125244\n",
      "for 9 epochs, loss is 1.6803096532821655 and val_loss is 3.8764615058898926\n",
      "for 10 epochs, loss is 1.6740384101867676 and val_loss is 3.8748133182525635\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.297), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2346), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.297), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2346), \"['a', 'x', 'a', 'x']\": np.float64(-6.5454), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2344), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2879), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2347), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2877), \"['a', 'x', 'a', 'y']\": np.float64(1.5256), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.2972), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2345), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.2969), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2346), \"['a', 'x', 'b', 'x']\": np.float64(-6.5454), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2344), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2878), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2346), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2877), \"['a', 'x', 'b', 'y']\": np.float64(1.5256), \"['a', 'x']\": np.float64(-8.0881), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2367), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2882), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2367), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2882), \"['a', 'y', 'a', 'x']\": np.float64(-1.5282), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2884), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0673), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2884), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0672), \"['a', 'y', 'a', 'y']\": np.float64(0.3564), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2371), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2883), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.237), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2883), \"['a', 'y', 'b', 'x']\": np.float64(-1.5286), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2882), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0672), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2883), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0672), \"['a', 'y', 'b', 'y']\": np.float64(0.3562), \"['a', 'y']\": np.float64(1.8887), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.2987), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.235), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.2987), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.235), \"['b', 'x', 'a', 'x']\": np.float64(-6.5475), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2346), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2879), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.235), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2878), \"['b', 'x', 'a', 'y']\": np.float64(1.5259), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.2986), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2349), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.2983), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2349), \"['b', 'x', 'b', 'x']\": np.float64(-6.5471), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2347), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2879), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.235), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2878), \"['b', 'x', 'b', 'y']\": np.float64(1.526), \"['b', 'x']\": np.float64(-8.0903), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2358), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.288), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2358), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.288), \"['b', 'y', 'a', 'x']\": np.float64(-1.527), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2881), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0672), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2882), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0672), \"['b', 'y', 'a', 'y']\": np.float64(0.3561), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2361), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2881), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.236), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2881), \"['b', 'y', 'b', 'x']\": np.float64(-1.5273), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.288), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0672), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.288), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0671), \"['b', 'y', 'b', 'y']\": np.float64(0.3559), \"['b', 'y']\": np.float64(1.8872)}\n",
      "It s train loss bro [1.581056833267212, 1.704607367515564, 1.7049545049667358, 1.7041149139404297, 1.702211618423462, 1.6993565559387207, 1.6956512928009033, 1.6911875009536743, 1.6860486268997192, 1.6803096532821655, 1.6740384101867676]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 223 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.29201680421829224 and val_loss is 3.8619699478149414\n",
      "for 1 epochs, loss is 0.22589796781539917 and val_loss is 3.8516955375671387\n",
      "for 2 epochs, loss is 0.23840628564357758 and val_loss is 3.8436367511749268\n",
      "for 3 epochs, loss is 0.2493785321712494 and val_loss is 3.8374886512756348\n",
      "for 4 epochs, loss is 0.2587730586528778 and val_loss is 3.8329885005950928\n",
      "for 5 epochs, loss is 0.26658833026885986 and val_loss is 3.829914093017578\n",
      "for 6 epochs, loss is 0.2728549838066101 and val_loss is 3.828077793121338\n",
      "for 7 epochs, loss is 0.2776312232017517 and val_loss is 3.827319622039795\n",
      "for 8 epochs, loss is 0.2809962034225464 and val_loss is 3.8275070190429688\n",
      "for 9 epochs, loss is 0.28304463624954224 and val_loss is 3.828528642654419\n",
      "for 10 epochs, loss is 0.28388190269470215 and val_loss is 3.8302927017211914\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.2733), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3854), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2752), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3836), \"['a', 'x', 'a', 'x']\": np.float64(-5.6722), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3852), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4493), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3861), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4485), \"['a', 'x', 'a', 'y']\": np.float64(1.839), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.2756), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3858), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2767), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3845), \"['a', 'x', 'b', 'x']\": np.float64(-5.6747), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3835), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4486), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3842), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4481), \"['a', 'x', 'b', 'y']\": np.float64(1.8366), \"['a', 'x']\": np.float64(-7.5291), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3874), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4498), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.388), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4492), \"['a', 'y', 'a', 'x']\": np.float64(-1.8416), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4499), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1459), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4502), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1457), \"['a', 'y', 'a', 'y']\": np.float64(0.5973), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3885), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.45), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3888), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4496), \"['a', 'y', 'b', 'x']\": np.float64(-1.8428), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4492), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1457), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4494), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1455), \"['a', 'y', 'b', 'y']\": np.float64(0.5963), \"['a', 'y']\": np.float64(2.4449), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.2771), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3866), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.2789), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3849), \"['b', 'x', 'a', 'x']\": np.float64(-5.6772), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.386), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4495), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.387), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4488), \"['b', 'x', 'a', 'y']\": np.float64(1.8401), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.2786), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3867), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.2797), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3855), \"['b', 'x', 'b', 'x']\": np.float64(-5.6787), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3849), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.449), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3856), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4485), \"['b', 'x', 'b', 'y']\": np.float64(1.8384), \"['b', 'x']\": np.float64(-7.535), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3847), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4489), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3853), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4483), \"['b', 'y', 'a', 'x']\": np.float64(-1.838), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4489), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1456), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4492), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1454), \"['b', 'y', 'a', 'y']\": np.float64(0.5959), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3855), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4491), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3858), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4486), \"['b', 'y', 'b', 'x']\": np.float64(-1.8389), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4484), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1454), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4486), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1452), \"['b', 'y', 'b', 'y']\": np.float64(0.5952), \"['b', 'y']\": np.float64(2.4398)}\n",
      "It s train loss bro [0.29201680421829224, 0.22589796781539917, 0.23840628564357758, 0.2493785321712494, 0.2587730586528778, 0.26658833026885986, 0.2728549838066101, 0.2776312232017517, 0.2809962034225464, 0.28304463624954224, 0.28388190269470215]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 224 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.31091684103012085 and val_loss is 3.8329758644104004\n",
      "for 1 epochs, loss is 0.2824022173881531 and val_loss is 3.8362324237823486\n",
      "for 2 epochs, loss is 0.2803103029727936 and val_loss is 3.84000825881958\n",
      "for 3 epochs, loss is 0.27745530009269714 and val_loss is 3.8442602157592773\n",
      "for 4 epochs, loss is 0.2739436626434326 and val_loss is 3.8489508628845215\n",
      "for 5 epochs, loss is 0.2698756456375122 and val_loss is 3.854050636291504\n",
      "for 6 epochs, loss is 0.2653449773788452 and val_loss is 3.859531879425049\n",
      "for 7 epochs, loss is 0.26043686270713806 and val_loss is 3.8653714656829834\n",
      "for 8 epochs, loss is 0.255229651927948 and val_loss is 3.8715481758117676\n",
      "for 9 epochs, loss is 0.24979305267333984 and val_loss is 3.8780441284179688\n",
      "for 10 epochs, loss is 0.2441892772912979 and val_loss is 3.8848416805267334\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.8918), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3025), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.8924), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3019), \"['a', 'x', 'a', 'x']\": np.float64(-6.2073), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3023), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3469), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3027), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3466), \"['a', 'x', 'a', 'y']\": np.float64(1.6527), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.8926), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3025), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.8927), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3021), \"['a', 'x', 'b', 'x']\": np.float64(-6.208), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3018), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3467), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3021), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3465), \"['a', 'x', 'b', 'y']\": np.float64(1.652), \"['a', 'x']\": np.float64(-7.8765), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3043), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3473), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3044), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3471), \"['a', 'y', 'a', 'x']\": np.float64(-1.655), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3474), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0925), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3475), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0925), \"['a', 'y', 'a', 'y']\": np.float64(0.4409), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3048), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3474), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3048), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3473), \"['a', 'y', 'b', 'x']\": np.float64(-1.6556), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3471), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0924), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3472), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0924), \"['a', 'y', 'b', 'y']\": np.float64(0.4405), \"['a', 'y']\": np.float64(2.1004), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.8939), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.303), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.8944), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3025), \"['b', 'x', 'a', 'x']\": np.float64(-6.2099), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3026), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.347), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3031), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3467), \"['b', 'x', 'a', 'y']\": np.float64(1.6532), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.8942), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3029), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.8944), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3026), \"['b', 'x', 'b', 'x']\": np.float64(-6.2101), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3023), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3468), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3026), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3466), \"['b', 'x', 'b', 'y']\": np.float64(1.6527), \"['b', 'x']\": np.float64(-7.8795), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.303), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3469), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3031), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3468), \"['b', 'y', 'a', 'x']\": np.float64(-1.6533), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.347), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0924), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3471), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0923), \"['b', 'y', 'a', 'y']\": np.float64(0.4403), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3033), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.347), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3034), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3469), \"['b', 'y', 'b', 'x']\": np.float64(-1.6537), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3468), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0924), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3468), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0923), \"['b', 'y', 'b', 'y']\": np.float64(0.4401), \"['b', 'y']\": np.float64(2.0982)}\n",
      "It s train loss bro [0.31091684103012085, 0.2824022173881531, 0.2803103029727936, 0.27745530009269714, 0.2739436626434326, 0.2698756456375122, 0.2653449773788452, 0.26043686270713806, 0.255229651927948, 0.24979305267333984, 0.2441892772912979]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 225 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2386937439441681 and val_loss is 3.8919267654418945\n",
      "for 1 epochs, loss is 0.23269404470920563 and val_loss is 3.8992815017700195\n",
      "for 2 epochs, loss is 0.22689151763916016 and val_loss is 3.9068894386291504\n",
      "for 3 epochs, loss is 0.22110117971897125 and val_loss is 3.914736747741699\n",
      "for 4 epochs, loss is 0.21535304188728333 and val_loss is 3.922809362411499\n",
      "for 5 epochs, loss is 0.209672212600708 and val_loss is 3.9310903549194336\n",
      "for 6 epochs, loss is 0.20407946407794952 and val_loss is 3.939568042755127\n",
      "for 7 epochs, loss is 0.19859161972999573 and val_loss is 3.9482266902923584\n",
      "for 8 epochs, loss is 0.19322216510772705 and val_loss is 3.9570512771606445\n",
      "for 9 epochs, loss is 0.18798169493675232 and val_loss is 3.9660284519195557\n",
      "for 10 epochs, loss is 0.18287795782089233 and val_loss is 3.975144624710083\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.8659), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1283), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8649), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.129), \"['a', 'x', 'a', 'x']\": np.float64(-7.0061), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.128), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2172), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1282), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2171), \"['a', 'x', 'a', 'y']\": np.float64(1.3476), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.865), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1281), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.8639), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1288), \"['a', 'x', 'b', 'x']\": np.float64(-7.0049), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1288), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2173), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1289), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2173), \"['a', 'x', 'b', 'y']\": np.float64(1.3485), \"['a', 'x']\": np.float64(-8.3678), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1307), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2175), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1305), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2176), \"['a', 'y', 'a', 'x']\": np.float64(-1.3505), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2176), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0419), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2177), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0419), \"['a', 'y', 'a', 'y']\": np.float64(0.26), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1309), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2175), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1307), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2176), \"['a', 'y', 'b', 'x']\": np.float64(-1.3507), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2176), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0419), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2176), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0419), \"['a', 'y', 'b', 'y']\": np.float64(0.26), \"['a', 'y']\": np.float64(1.6134), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.8665), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1284), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8655), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1291), \"['b', 'x', 'a', 'x']\": np.float64(-7.0067), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1281), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2172), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1282), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2172), \"['b', 'x', 'a', 'y']\": np.float64(1.3477), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.8655), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1282), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.8644), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1288), \"['b', 'x', 'b', 'x']\": np.float64(-7.0054), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1288), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2173), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1289), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2173), \"['b', 'x', 'b', 'y']\": np.float64(1.3485), \"['b', 'x']\": np.float64(-8.3685), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1306), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2175), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1304), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2176), \"['b', 'y', 'a', 'x']\": np.float64(-1.3504), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2176), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0419), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2176), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0419), \"['b', 'y', 'a', 'y']\": np.float64(0.26), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1307), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2175), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1305), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2176), \"['b', 'y', 'b', 'x']\": np.float64(-1.3505), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2176), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0419), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2176), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0419), \"['b', 'y', 'b', 'y']\": np.float64(0.2599), \"['b', 'y']\": np.float64(1.6132)}\n",
      "It s train loss bro [0.2386937439441681, 0.23269404470920563, 0.22689151763916016, 0.22110117971897125, 0.21535304188728333, 0.209672212600708, 0.20407946407794952, 0.19859161972999573, 0.19322216510772705, 0.18798169493675232, 0.18287795782089233]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 226 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.22955965995788574 and val_loss is 3.984846353530884\n",
      "for 1 epochs, loss is 0.17307353019714355 and val_loss is 3.994612693786621\n",
      "for 2 epochs, loss is 0.16838251054286957 and val_loss is 4.004436492919922\n",
      "for 3 epochs, loss is 0.16384448111057281 and val_loss is 4.014308452606201\n",
      "for 4 epochs, loss is 0.15945833921432495 and val_loss is 4.024219512939453\n",
      "for 5 epochs, loss is 0.1552228480577469 and val_loss is 4.0341620445251465\n",
      "for 6 epochs, loss is 0.15113569796085358 and val_loss is 4.044126510620117\n",
      "for 7 epochs, loss is 0.14719362556934357 and val_loss is 4.054104328155518\n",
      "for 8 epochs, loss is 0.1433935910463333 and val_loss is 4.0640869140625\n",
      "for 9 epochs, loss is 0.13973155617713928 and val_loss is 4.074066162109375\n",
      "for 10 epochs, loss is 0.13620367646217346 and val_loss is 4.084035873413086\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.7171), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.941), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.7149), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9427), \"['a', 'x', 'a', 'x']\": np.float64(-7.6681), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9407), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1321), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9408), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.132), \"['a', 'x', 'a', 'y']\": np.float64(1.0742), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.715), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9407), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.7129), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9423), \"['a', 'x', 'b', 'x']\": np.float64(-7.6657), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9423), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1323), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9424), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1322), \"['a', 'x', 'b', 'y']\": np.float64(1.0761), \"['a', 'x']\": np.float64(-8.7538), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9446), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1323), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9443), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1326), \"['a', 'y', 'a', 'x']\": np.float64(-1.0783), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1325), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0186), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1326), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0186), \"['a', 'y', 'a', 'y']\": np.float64(0.1514), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9447), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1323), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9444), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1326), \"['a', 'y', 'b', 'x']\": np.float64(-1.0784), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1325), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0186), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1325), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0186), \"['a', 'y', 'b', 'y']\": np.float64(0.1513), \"['a', 'y']\": np.float64(1.2314), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.7171), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.941), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.7149), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9427), \"['b', 'x', 'a', 'x']\": np.float64(-7.6681), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9407), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1321), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9408), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.132), \"['b', 'x', 'a', 'y']\": np.float64(1.0742), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.715), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9407), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.7129), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9423), \"['b', 'x', 'b', 'x']\": np.float64(-7.6657), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9422), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1323), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9423), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1322), \"['b', 'x', 'b', 'y']\": np.float64(1.076), \"['b', 'x']\": np.float64(-8.7538), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9448), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1324), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9445), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1326), \"['b', 'y', 'a', 'x']\": np.float64(-1.0786), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1326), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0186), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1326), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0186), \"['b', 'y', 'a', 'y']\": np.float64(0.1514), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9449), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1324), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9446), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1326), \"['b', 'y', 'b', 'x']\": np.float64(-1.0787), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1326), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0186), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1326), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0186), \"['b', 'y', 'b', 'y']\": np.float64(0.1514), \"['b', 'y']\": np.float64(1.2317)}\n",
      "It s train loss bro [0.22955965995788574, 0.17307353019714355, 0.16838251054286957, 0.16384448111057281, 0.15945833921432495, 0.1552228480577469, 0.15113569796085358, 0.14719362556934357, 0.1433935910463333, 0.13973155617713928, 0.13620367646217346]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 227 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.090066432952881 and val_loss is 4.0844340324401855\n",
      "for 1 epochs, loss is 2.0971059799194336 and val_loss is 4.084266662597656\n",
      "for 2 epochs, loss is 2.096376657485962 and val_loss is 4.08359432220459\n",
      "for 3 epochs, loss is 2.0943267345428467 and val_loss is 4.082468509674072\n",
      "for 4 epochs, loss is 2.0910959243774414 and val_loss is 4.080939292907715\n",
      "for 5 epochs, loss is 2.08681058883667 and val_loss is 4.079051494598389\n",
      "for 6 epochs, loss is 2.081585168838501 and val_loss is 4.076846122741699\n",
      "for 7 epochs, loss is 2.0755233764648438 and val_loss is 4.074359893798828\n",
      "for 8 epochs, loss is 2.0687191486358643 and val_loss is 4.071629524230957\n",
      "for 9 epochs, loss is 2.0612568855285645 and val_loss is 4.068684101104736\n",
      "for 10 epochs, loss is 2.053213119506836 and val_loss is 4.065553188323975\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.5664), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9765), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.5644), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9781), \"['a', 'x', 'a', 'x']\": np.float64(-7.5533), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9762), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1454), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9762), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1454), \"['a', 'x', 'a', 'y']\": np.float64(1.1232), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.5644), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9762), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.5624), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9776), \"['a', 'x', 'b', 'x']\": np.float64(-7.5509), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9778), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1456), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9779), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1456), \"['a', 'x', 'b', 'y']\": np.float64(1.1251), \"['a', 'x']\": np.float64(-8.6884), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9793), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1456), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.979), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1459), \"['a', 'y', 'a', 'x']\": np.float64(-1.1265), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1458), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0217), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1458), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0217), \"['a', 'y', 'a', 'y']\": np.float64(0.1678), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9794), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1456), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9791), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1459), \"['a', 'y', 'b', 'x']\": np.float64(-1.1265), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1459), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0217), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1459), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0217), \"['a', 'y', 'b', 'y']\": np.float64(0.1678), \"['a', 'y']\": np.float64(1.2962), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.566), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9764), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.5639), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.978), \"['b', 'x', 'a', 'x']\": np.float64(-7.5527), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9762), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1454), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9762), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1454), \"['b', 'x', 'a', 'y']\": np.float64(1.1232), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.564), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9762), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.5621), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9776), \"['b', 'x', 'b', 'x']\": np.float64(-7.5505), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9776), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1456), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9776), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1456), \"['b', 'x', 'b', 'y']\": np.float64(1.1248), \"['b', 'x']\": np.float64(-8.6878), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9801), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1457), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9798), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.146), \"['b', 'y', 'a', 'x']\": np.float64(-1.1273), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1459), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0217), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1459), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0217), \"['b', 'y', 'a', 'y']\": np.float64(0.1679), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9801), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1458), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9798), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.146), \"['b', 'y', 'b', 'x']\": np.float64(-1.1274), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1459), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0217), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1459), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0217), \"['b', 'y', 'b', 'y']\": np.float64(0.1679), \"['b', 'y']\": np.float64(1.2971)}\n",
      "It s train loss bro [2.090066432952881, 2.0971059799194336, 2.096376657485962, 2.0943267345428467, 2.0910959243774414, 2.08681058883667, 2.081585168838501, 2.0755233764648438, 2.0687191486358643, 2.0612568855285645, 2.053213119506836]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 228 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.19064699113368988 and val_loss is 4.037371635437012\n",
      "for 1 epochs, loss is 0.15185633301734924 and val_loss is 4.013306140899658\n",
      "for 2 epochs, loss is 0.1625872701406479 and val_loss is 3.9928643703460693\n",
      "for 3 epochs, loss is 0.1723679155111313 and val_loss is 3.975609302520752\n",
      "for 4 epochs, loss is 0.18113137781620026 and val_loss is 3.9611523151397705\n",
      "for 5 epochs, loss is 0.18884120881557465 and val_loss is 3.9491500854492188\n",
      "for 6 epochs, loss is 0.1954868584871292 and val_loss is 3.9393036365509033\n",
      "for 7 epochs, loss is 0.20107975602149963 and val_loss is 3.9313502311706543\n",
      "for 8 epochs, loss is 0.20564933121204376 and val_loss is 3.925065040588379\n",
      "for 9 epochs, loss is 0.20923908054828644 and val_loss is 3.9202497005462646\n",
      "for 10 epochs, loss is 0.2119031697511673 and val_loss is 3.9167377948760986\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.2688), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2422), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.2661), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2447), \"['a', 'x', 'a', 'x']\": np.float64(-6.5236), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2423), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2927), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2414), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2937), \"['a', 'x', 'a', 'y']\": np.float64(1.5381), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.2656), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2421), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.264), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2435), \"['a', 'x', 'b', 'x']\": np.float64(-6.5203), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2447), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2934), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2441), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2941), \"['a', 'x', 'b', 'y']\": np.float64(1.5412), \"['a', 'x']\": np.float64(-8.0774), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2404), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2924), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2397), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.293), \"['a', 'y', 'a', 'x']\": np.float64(-1.5358), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2923), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0689), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2921), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0691), \"['a', 'y', 'a', 'y']\": np.float64(0.3619), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2394), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2924), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.239), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2927), \"['a', 'y', 'b', 'x']\": np.float64(-1.5347), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2932), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0691), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2931), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0693), \"['a', 'y', 'b', 'y']\": np.float64(0.3631), \"['a', 'y']\": np.float64(1.9015), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.2647), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2412), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.262), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2438), \"['b', 'x', 'a', 'x']\": np.float64(-6.5186), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2419), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2926), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2411), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2936), \"['b', 'x', 'a', 'y']\": np.float64(1.5376), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.2626), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2413), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.2609), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2428), \"['b', 'x', 'b', 'x']\": np.float64(-6.5165), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2433), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2931), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2427), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2937), \"['b', 'x', 'b', 'y']\": np.float64(1.5394), \"['b', 'x']\": np.float64(-8.0719), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.244), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2933), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2434), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2939), \"['b', 'y', 'a', 'x']\": np.float64(-1.5403), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2933), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0691), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2931), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0693), \"['b', 'y', 'a', 'y']\": np.float64(0.3631), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2433), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2933), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2429), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2936), \"['b', 'y', 'b', 'x']\": np.float64(-1.5396), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2939), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0693), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2938), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0694), \"['b', 'y', 'b', 'y']\": np.float64(0.3639), \"['b', 'y']\": np.float64(1.9072)}\n",
      "It s train loss bro [0.19064699113368988, 0.15185633301734924, 0.1625872701406479, 0.1723679155111313, 0.18113137781620026, 0.18884120881557465, 0.1954868584871292, 0.20107975602149963, 0.20564933121204376, 0.20923908054828644, 0.2119031697511673]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 229 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.21361389756202698 and val_loss is 3.9143869876861572\n",
      "for 1 epochs, loss is 0.2147105187177658 and val_loss is 3.913067102432251\n",
      "for 2 epochs, loss is 0.21499024331569672 and val_loss is 3.9126737117767334\n",
      "for 3 epochs, loss is 0.21461357176303864 and val_loss is 3.9131150245666504\n",
      "for 4 epochs, loss is 0.21365049481391907 and val_loss is 3.91430926322937\n",
      "for 5 epochs, loss is 0.2121688425540924 and val_loss is 3.9161903858184814\n",
      "for 6 epochs, loss is 0.21023394167423248 and val_loss is 3.9186999797821045\n",
      "for 7 epochs, loss is 0.2079067975282669 and val_loss is 3.9217848777770996\n",
      "for 8 epochs, loss is 0.20524512231349945 and val_loss is 3.925400733947754\n",
      "for 9 epochs, loss is 0.20230169594287872 and val_loss is 3.929506778717041\n",
      "for 10 epochs, loss is 0.19912512600421906 and val_loss is 3.9340667724609375\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.5594), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.189), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.5571), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1911), \"['a', 'x', 'a', 'x']\": np.float64(-6.761), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1891), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2542), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1885), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2549), \"['a', 'x', 'a', 'y']\": np.float64(1.446), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.5568), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.189), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.5552), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1902), \"['a', 'x', 'b', 'x']\": np.float64(-6.7582), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1911), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2547), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1907), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2552), \"['a', 'x', 'b', 'y']\": np.float64(1.4486), \"['a', 'x']\": np.float64(-8.2224), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1881), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2541), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1877), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2546), \"['a', 'y', 'a', 'x']\": np.float64(-1.445), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.254), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0543), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2539), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0545), \"['a', 'y', 'a', 'y']\": np.float64(0.3089), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1875), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2541), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1871), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2543), \"['a', 'y', 'b', 'x']\": np.float64(-1.4442), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2547), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0545), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2546), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0546), \"['a', 'y', 'b', 'y']\": np.float64(0.3098), \"['a', 'y']\": np.float64(1.7572), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.5561), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1883), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.5538), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1904), \"['b', 'x', 'a', 'x']\": np.float64(-6.7571), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1888), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2542), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1882), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2549), \"['b', 'x', 'a', 'y']\": np.float64(1.4458), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.5543), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1884), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.5528), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1896), \"['b', 'x', 'b', 'x']\": np.float64(-6.7552), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.19), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2545), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1896), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.255), \"['b', 'x', 'b', 'y']\": np.float64(1.4473), \"['b', 'x']\": np.float64(-8.2181), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1911), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2547), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1906), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2552), \"['b', 'y', 'a', 'x']\": np.float64(-1.4485), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2547), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0545), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2546), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0546), \"['b', 'y', 'a', 'y']\": np.float64(0.3098), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1906), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2547), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1903), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.255), \"['b', 'y', 'b', 'x']\": np.float64(-1.448), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2552), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0546), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2551), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0547), \"['b', 'y', 'b', 'y']\": np.float64(0.3104), \"['b', 'y']\": np.float64(1.7617)}\n",
      "It s train loss bro [0.21361389756202698, 0.2147105187177658, 0.21499024331569672, 0.21461357176303864, 0.21365049481391907, 0.2121688425540924, 0.21023394167423248, 0.2079067975282669, 0.20524512231349945, 0.20230169594287872, 0.19912512600421906]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 230 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.20014454424381256 and val_loss is 3.9390811920166016\n",
      "for 1 epochs, loss is 0.19224661588668823 and val_loss is 3.944484233856201\n",
      "for 2 epochs, loss is 0.1886192262172699 and val_loss is 3.950248956680298\n",
      "for 3 epochs, loss is 0.1849086731672287 and val_loss is 3.9563491344451904\n",
      "for 4 epochs, loss is 0.18114231526851654 and val_loss is 3.9627606868743896\n",
      "for 5 epochs, loss is 0.17734436690807343 and val_loss is 3.9694604873657227\n",
      "for 6 epochs, loss is 0.17353583872318268 and val_loss is 3.976426601409912\n",
      "for 7 epochs, loss is 0.16973458230495453 and val_loss is 3.9836385250091553\n",
      "for 8 epochs, loss is 0.16595597565174103 and val_loss is 3.991074800491333\n",
      "for 9 epochs, loss is 0.1622130125761032 and val_loss is 3.998715877532959\n",
      "for 10 epochs, loss is 0.158516526222229 and val_loss is 4.006542682647705\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.2842), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0402), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.2823), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0419), \"['a', 'x', 'a', 'x']\": np.float64(-7.336), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0401), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1722), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0399), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1725), \"['a', 'x', 'a', 'y']\": np.float64(1.2143), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.2821), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0402), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.2806), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.0413), \"['a', 'x', 'b', 'x']\": np.float64(-7.3337), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0418), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1725), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.0416), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1727), \"['a', 'x', 'b', 'y']\": np.float64(1.2163), \"['a', 'x']\": np.float64(-8.5638), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.041), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1723), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0407), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1726), \"['a', 'y', 'a', 'x']\": np.float64(-1.2153), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1723), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0285), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1723), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0286), \"['a', 'y', 'a', 'y']\": np.float64(0.2012), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0408), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1723), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0405), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1725), \"['a', 'y', 'b', 'x']\": np.float64(-1.215), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1726), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0286), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1726), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0286), \"['a', 'y', 'b', 'y']\": np.float64(0.2015), \"['a', 'y']\": np.float64(1.4188), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.282), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.0399), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.2801), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.0415), \"['b', 'x', 'a', 'x']\": np.float64(-7.3335), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.0401), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1722), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.0398), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1725), \"['b', 'x', 'a', 'y']\": np.float64(1.2142), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.2804), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.0399), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.2789), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.041), \"['b', 'x', 'b', 'x']\": np.float64(-7.3317), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.0411), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1724), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.041), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1726), \"['b', 'x', 'b', 'y']\": np.float64(1.2155), \"['b', 'x']\": np.float64(-8.5612), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.043), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1727), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.0427), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1729), \"['b', 'y', 'a', 'x']\": np.float64(-1.2176), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1727), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0286), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1727), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0286), \"['b', 'y', 'a', 'y']\": np.float64(0.2016), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.0428), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1727), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.0426), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1729), \"['b', 'y', 'b', 'x']\": np.float64(-1.2174), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1729), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0286), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1729), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0287), \"['b', 'y', 'b', 'y']\": np.float64(0.2019), \"['b', 'y']\": np.float64(1.4215)}\n",
      "It s train loss bro [0.20014454424381256, 0.19224661588668823, 0.1886192262172699, 0.1849086731672287, 0.18114231526851654, 0.17734436690807343, 0.17353583872318268, 0.16973458230495453, 0.16595597565174103, 0.1622130125761032, 0.158516526222229]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 231 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.15492312610149384 and val_loss is 4.014508247375488\n",
      "for 1 epochs, loss is 0.15129832923412323 and val_loss is 4.022624492645264\n",
      "for 2 epochs, loss is 0.1477896124124527 and val_loss is 4.030875205993652\n",
      "for 3 epochs, loss is 0.1443541944026947 and val_loss is 4.039242267608643\n",
      "for 4 epochs, loss is 0.14099565148353577 and val_loss is 4.047709941864014\n",
      "for 5 epochs, loss is 0.13771659135818481 and val_loss is 4.056262016296387\n",
      "for 6 epochs, loss is 0.13451853394508362 and val_loss is 4.064884185791016\n",
      "for 7 epochs, loss is 0.1314026415348053 and val_loss is 4.0735602378845215\n",
      "for 8 epochs, loss is 0.1283694952726364 and val_loss is 4.082278251647949\n",
      "for 9 epochs, loss is 0.12541860342025757 and val_loss is 4.091023921966553\n",
      "for 10 epochs, loss is 0.12254966795444489 and val_loss is 4.099786758422852\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.9823), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8772), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.9805), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8786), \"['a', 'x', 'a', 'x']\": np.float64(-7.8695), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8771), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1102), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.877), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1104), \"['a', 'x', 'a', 'y']\": np.float64(0.9886), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.9803), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8772), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.9789), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8782), \"['a', 'x', 'b', 'x']\": np.float64(-7.8675), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8785), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1104), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8785), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1105), \"['a', 'x', 'b', 'y']\": np.float64(0.9903), \"['a', 'x']\": np.float64(-8.8695), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8786), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1104), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8784), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1106), \"['a', 'y', 'a', 'x']\": np.float64(-0.9902), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1104), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0139), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1104), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0139), \"['a', 'y', 'a', 'y']\": np.float64(0.1245), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8785), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1104), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8783), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1105), \"['a', 'y', 'b', 'x']\": np.float64(-0.9902), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1106), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0139), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1106), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0139), \"['a', 'y', 'b', 'y']\": np.float64(0.1246), \"['a', 'y']\": np.float64(1.1162), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.9807), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.877), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.9789), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8784), \"['b', 'x', 'a', 'x']\": np.float64(-7.8678), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8771), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1103), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.877), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1104), \"['b', 'x', 'a', 'y']\": np.float64(0.9887), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.9791), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.877), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.9776), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.878), \"['b', 'x', 'b', 'x']\": np.float64(-7.8661), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8781), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1104), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8781), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1105), \"['b', 'x', 'b', 'y']\": np.float64(0.9899), \"['b', 'x']\": np.float64(-8.8678), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8801), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1106), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8799), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1107), \"['b', 'y', 'a', 'x']\": np.float64(-0.9919), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1106), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0139), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1106), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0139), \"['b', 'y', 'a', 'y']\": np.float64(0.1247), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.88), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1106), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8798), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1107), \"['b', 'y', 'b', 'x']\": np.float64(-0.9919), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1107), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0139), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1107), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0139), \"['b', 'y', 'b', 'y']\": np.float64(0.1248), \"['b', 'y']\": np.float64(1.1181)}\n",
      "It s train loss bro [0.15492312610149384, 0.15129832923412323, 0.1477896124124527, 0.1443541944026947, 0.14099565148353577, 0.13771659135818481, 0.13451853394508362, 0.1314026415348053, 0.1283694952726364, 0.12541860342025757, 0.12254966795444489]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 232 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.14643049240112305 and val_loss is 4.108792781829834\n",
      "for 1 epochs, loss is 0.11702542752027512 and val_loss is 4.117769241333008\n",
      "for 2 epochs, loss is 0.1143709048628807 and val_loss is 4.126707553863525\n",
      "for 3 epochs, loss is 0.11179620772600174 and val_loss is 4.135602951049805\n",
      "for 4 epochs, loss is 0.1092991828918457 and val_loss is 4.14444637298584\n",
      "for 5 epochs, loss is 0.1068776473402977 and val_loss is 4.153231620788574\n",
      "for 6 epochs, loss is 0.10452916473150253 and val_loss is 4.161953926086426\n",
      "for 7 epochs, loss is 0.10225167125463486 and val_loss is 4.170608043670654\n",
      "for 8 epochs, loss is 0.10004264861345291 and val_loss is 4.179187774658203\n",
      "for 9 epochs, loss is 0.09789997339248657 and val_loss is 4.187690734863281\n",
      "for 10 epochs, loss is 0.09582114964723587 and val_loss is 4.196111679077148\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.5475), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7329), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.5461), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.734), \"['a', 'x', 'a', 'x']\": np.float64(-8.2889), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7328), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0712), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7328), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0713), \"['a', 'x', 'a', 'y']\": np.float64(0.8049), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.5459), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7329), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.5448), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7336), \"['a', 'x', 'b', 'x']\": np.float64(-8.2873), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7339), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0713), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7339), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0714), \"['a', 'x', 'b', 'y']\": np.float64(0.8061), \"['a', 'x']\": np.float64(-9.1032), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7339), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0713), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7337), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0714), \"['a', 'y', 'a', 'x']\": np.float64(-0.806), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0713), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0069), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0713), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0069), \"['a', 'y', 'a', 'y']\": np.float64(0.0783), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7338), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0713), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7337), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0713), \"['a', 'y', 'b', 'x']\": np.float64(-0.8059), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0714), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0069), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0714), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0069), \"['a', 'y', 'b', 'y']\": np.float64(0.0784), \"['a', 'y']\": np.float64(0.8852), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.5462), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7328), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.5448), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7338), \"['b', 'x', 'a', 'x']\": np.float64(-8.2875), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7329), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0712), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7329), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0713), \"['b', 'x', 'a', 'y']\": np.float64(0.805), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.5449), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7329), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.5438), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7335), \"['b', 'x', 'b', 'x']\": np.float64(-8.2863), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7336), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0713), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7336), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0713), \"['b', 'x', 'b', 'y']\": np.float64(0.8057), \"['b', 'x']\": np.float64(-9.1019), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7351), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0714), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7349), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0715), \"['b', 'y', 'a', 'x']\": np.float64(-0.8073), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0714), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0069), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0714), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0069), \"['b', 'y', 'a', 'y']\": np.float64(0.0784), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.735), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0714), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7349), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0715), \"['b', 'y', 'b', 'x']\": np.float64(-0.8073), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0715), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0069), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0715), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0069), \"['b', 'y', 'b', 'y']\": np.float64(0.0785), \"['b', 'y']\": np.float64(0.8867)}\n",
      "It s train loss bro [0.14643049240112305, 0.11702542752027512, 0.1143709048628807, 0.11179620772600174, 0.1092991828918457, 0.1068776473402977, 0.10452916473150253, 0.10225167125463486, 0.10004264861345291, 0.09789997339248657, 0.09582114964723587]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 233 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.09468307346105576 and val_loss is 4.20443058013916\n",
      "for 1 epochs, loss is 0.09184801578521729 and val_loss is 4.212662696838379\n",
      "for 2 epochs, loss is 0.08994931727647781 and val_loss is 4.22080659866333\n",
      "for 3 epochs, loss is 0.08810552954673767 and val_loss is 4.22885799407959\n",
      "for 4 epochs, loss is 0.08631464093923569 and val_loss is 4.236815452575684\n",
      "for 5 epochs, loss is 0.08457477390766144 and val_loss is 4.244678497314453\n",
      "for 6 epochs, loss is 0.08288377523422241 and val_loss is 4.252443790435791\n",
      "for 7 epochs, loss is 0.08123991638422012 and val_loss is 4.260110855102539\n",
      "for 8 epochs, loss is 0.07964123785495758 and val_loss is 4.267678737640381\n",
      "for 9 epochs, loss is 0.07808620482683182 and val_loss is 4.275147438049316\n",
      "for 10 epochs, loss is 0.07657290995121002 and val_loss is 4.282515048980713\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.9824), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.6151), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.9815), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6155), \"['a', 'x', 'a', 'x']\": np.float64(-8.6047), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.615), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0474), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.615), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0475), \"['a', 'x', 'a', 'y']\": np.float64(0.663), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.9814), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6152), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.9809), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6153), \"['a', 'x', 'b', 'x']\": np.float64(-8.6039), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6155), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0474), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6155), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0475), \"['a', 'x', 'b', 'y']\": np.float64(0.6635), \"['a', 'x']\": np.float64(-9.2757), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6151), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0474), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.615), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0474), \"['a', 'y', 'a', 'x']\": np.float64(-0.6631), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0474), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0036), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0474), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0037), \"['a', 'y', 'a', 'y']\": np.float64(0.0511), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.615), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0474), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.615), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0474), \"['a', 'y', 'b', 'x']\": np.float64(-0.663), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0475), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0037), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0475), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0037), \"['a', 'y', 'b', 'y']\": np.float64(0.0512), \"['a', 'y']\": np.float64(0.7148), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.9812), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.615), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.9804), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.6154), \"['b', 'x', 'a', 'x']\": np.float64(-8.6035), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.6152), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0474), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.6152), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0475), \"['b', 'x', 'a', 'y']\": np.float64(0.6632), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.9806), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.6152), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.9801), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.6152), \"['b', 'x', 'b', 'x']\": np.float64(-8.603), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.6153), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0474), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.6152), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0475), \"['b', 'x', 'b', 'y']\": np.float64(0.6633), \"['b', 'x']\": np.float64(-9.2746), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.6161), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0475), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.6161), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0475), \"['b', 'y', 'a', 'x']\": np.float64(-0.6642), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0475), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0037), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0475), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0037), \"['b', 'y', 'a', 'y']\": np.float64(0.0512), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.6161), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0475), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.616), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0475), \"['b', 'y', 'b', 'x']\": np.float64(-0.6641), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0475), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0037), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0475), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0037), \"['b', 'y', 'b', 'y']\": np.float64(0.0512), \"['b', 'y']\": np.float64(0.716)}\n",
      "It s train loss bro [0.09468307346105576, 0.09184801578521729, 0.08994931727647781, 0.08810552954673767, 0.08631464093923569, 0.08457477390766144, 0.08288377523422241, 0.08123991638422012, 0.07964123785495758, 0.07808620482683182, 0.07657290995121002]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 234 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.07623684406280518 and val_loss is 4.2897772789001465\n",
      "for 1 epochs, loss is 0.07366514950990677 and val_loss is 4.296939849853516\n",
      "for 2 epochs, loss is 0.0722678080201149 and val_loss is 4.304001331329346\n",
      "for 3 epochs, loss is 0.07090605050325394 and val_loss is 4.3109636306762695\n",
      "for 4 epochs, loss is 0.06957879662513733 and val_loss is 4.317826271057129\n",
      "for 5 epochs, loss is 0.06828474253416061 and val_loss is 4.324588298797607\n",
      "for 6 epochs, loss is 0.06702245026826859 and val_loss is 4.331253528594971\n",
      "for 7 epochs, loss is 0.0657908245921135 and val_loss is 4.337820053100586\n",
      "for 8 epochs, loss is 0.0645887479186058 and val_loss is 4.344289779663086\n",
      "for 9 epochs, loss is 0.06341510266065598 and val_loss is 4.350662708282471\n",
      "for 10 epochs, loss is 0.06226874887943268 and val_loss is 4.356940746307373\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3224), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.5191), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3224), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5187), \"['a', 'x', 'a', 'x']\": np.float64(-8.8479), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5191), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0323), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5191), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0324), \"['a', 'x', 'a', 'y']\": np.float64(0.5519), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3222), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5193), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.3226), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5186), \"['a', 'x', 'b', 'x']\": np.float64(-8.8479), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5187), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0323), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5187), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0324), \"['a', 'x', 'b', 'y']\": np.float64(0.5515), \"['a', 'x']\": np.float64(-9.4065), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.518), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0323), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.518), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0323), \"['a', 'y', 'a', 'x']\": np.float64(-0.5507), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0322), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.002), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0322), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.002), \"['a', 'y', 'a', 'y']\": np.float64(0.0343), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5179), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0323), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5179), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0323), \"['a', 'y', 'b', 'x']\": np.float64(-0.5506), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0323), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.002), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0323), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.002), \"['a', 'y', 'b', 'y']\": np.float64(0.0344), \"['a', 'y']\": np.float64(0.5854), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.3215), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.519), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.3215), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.5187), \"['b', 'x', 'a', 'x']\": np.float64(-8.8469), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.5193), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0323), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.5193), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0324), \"['b', 'x', 'a', 'y']\": np.float64(0.5521), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.3216), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.5193), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.322), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.5185), \"['b', 'x', 'b', 'x']\": np.float64(-8.8473), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.5186), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0323), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.5185), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0323), \"['b', 'x', 'b', 'y']\": np.float64(0.5513), \"['b', 'x']\": np.float64(-9.4057), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.5188), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0324), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.5188), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0323), \"['b', 'y', 'a', 'x']\": np.float64(-0.5515), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0323), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.002), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0323), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.002), \"['b', 'y', 'a', 'y']\": np.float64(0.0343), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.5187), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0324), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.5187), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0323), \"['b', 'y', 'b', 'x']\": np.float64(-0.5515), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0324), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.002), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0324), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.002), \"['b', 'y', 'b', 'y']\": np.float64(0.0344), \"['b', 'y']\": np.float64(0.5863)}\n",
      "It s train loss bro [0.07623684406280518, 0.07366514950990677, 0.0722678080201149, 0.07090605050325394, 0.06957879662513733, 0.06828474253416061, 0.06702245026826859, 0.0657908245921135, 0.0645887479186058, 0.06341510266065598, 0.06226874887943268]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 235 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.07162106037139893 and val_loss is 4.363256931304932\n",
      "for 1 epochs, loss is 0.060052815824747086 and val_loss is 4.369467735290527\n",
      "for 2 epochs, loss is 0.05898183211684227 and val_loss is 4.375573635101318\n",
      "for 3 epochs, loss is 0.05793478712439537 and val_loss is 4.38157844543457\n",
      "for 4 epochs, loss is 0.05691108480095863 and val_loss is 4.387482643127441\n",
      "for 5 epochs, loss is 0.05590955168008804 and val_loss is 4.393290042877197\n",
      "for 6 epochs, loss is 0.0549296997487545 and val_loss is 4.3990020751953125\n",
      "for 7 epochs, loss is 0.05397068336606026 and val_loss is 4.40462064743042\n",
      "for 8 epochs, loss is 0.053031884133815765 and val_loss is 4.410148620605469\n",
      "for 9 epochs, loss is 0.052112236618995667 and val_loss is 4.415587425231934\n",
      "for 10 epochs, loss is 0.051211562007665634 and val_loss is 4.420938014984131\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.5958), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4396), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5967), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4384), \"['a', 'x', 'a', 'x']\": np.float64(-9.041), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4397), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0224), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4396), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0225), \"['a', 'x', 'a', 'y']\": np.float64(0.4624), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.5965), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4399), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.5977), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4383), \"['a', 'x', 'b', 'x']\": np.float64(-9.0419), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4384), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0223), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4384), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0224), \"['a', 'x', 'b', 'y']\": np.float64(0.4611), \"['a', 'x']\": np.float64(-9.5092), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4373), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0224), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.4374), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0223), \"['a', 'y', 'a', 'x']\": np.float64(-0.46), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0223), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0011), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0223), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0011), \"['a', 'y', 'a', 'y']\": np.float64(0.0234), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4373), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0224), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4373), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0223), \"['a', 'y', 'b', 'x']\": np.float64(-0.4599), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0224), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0011), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0224), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0011), \"['a', 'y', 'b', 'y']\": np.float64(0.0235), \"['a', 'y']\": np.float64(0.4837), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.595), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.4396), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.5959), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.4383), \"['b', 'x', 'a', 'x']\": np.float64(-9.0402), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.4399), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0224), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.4398), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0225), \"['b', 'x', 'a', 'y']\": np.float64(0.4626), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.596), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.4399), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.5973), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.4383), \"['b', 'x', 'b', 'x']\": np.float64(-9.0415), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.4383), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0223), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.4382), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0224), \"['b', 'x', 'b', 'y']\": np.float64(0.4609), \"['b', 'x']\": np.float64(-9.5087), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.4379), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0224), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.438), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0223), \"['b', 'y', 'a', 'x']\": np.float64(-0.4606), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0223), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0011), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0223), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0011), \"['b', 'y', 'a', 'y']\": np.float64(0.0235), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.4379), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0224), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.4379), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0223), \"['b', 'y', 'b', 'x']\": np.float64(-0.4606), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0224), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0011), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0224), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0011), \"['b', 'y', 'b', 'y']\": np.float64(0.0235), \"['b', 'y']\": np.float64(0.4844)}\n",
      "It s train loss bro [0.07162106037139893, 0.060052815824747086, 0.05898183211684227, 0.05793478712439537, 0.05691108480095863, 0.05590955168008804, 0.0549296997487545, 0.05397068336606026, 0.053031884133815765, 0.052112236618995667, 0.051211562007665634]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 236 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.058611128479242325 and val_loss is 4.426328182220459\n",
      "for 1 epochs, loss is 0.04946155101060867 and val_loss is 4.43162202835083\n",
      "for 2 epochs, loss is 0.048611514270305634 and val_loss is 4.436821937561035\n",
      "for 3 epochs, loss is 0.047778256237506866 and val_loss is 4.441931247711182\n",
      "for 4 epochs, loss is 0.046961478888988495 and val_loss is 4.446951389312744\n",
      "for 5 epochs, loss is 0.04616042599081993 and val_loss is 4.451884746551514\n",
      "for 6 epochs, loss is 0.0453747920691967 and val_loss is 4.456735610961914\n",
      "for 7 epochs, loss is 0.04460404813289642 and val_loss is 4.461504936218262\n",
      "for 8 epochs, loss is 0.04384754225611687 and val_loss is 4.4661946296691895\n",
      "for 9 epochs, loss is 0.04310530424118042 and val_loss is 4.470807075500488\n",
      "for 10 epochs, loss is 0.0423765704035759 and val_loss is 4.475343227386475\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.821), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3726), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.8228), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3704), \"['a', 'x', 'a', 'x']\": np.float64(-9.1986), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3727), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0156), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3726), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0157), \"['a', 'x', 'a', 'y']\": np.float64(0.3886), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.8226), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3729), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8248), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3704), \"['a', 'x', 'b', 'x']\": np.float64(-9.2005), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3705), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0155), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3705), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0156), \"['a', 'x', 'b', 'y']\": np.float64(0.3863), \"['a', 'x']\": np.float64(-9.5924), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3693), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0156), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3693), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0155), \"['a', 'y', 'a', 'x']\": np.float64(-0.3851), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0155), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0155), \"['a', 'y', 'a', 'y']\": np.float64(0.0162), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3692), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0156), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3693), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0155), \"['a', 'y', 'b', 'x']\": np.float64(-0.385), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0156), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0156), \"['a', 'y', 'b', 'y']\": np.float64(0.0163), \"['a', 'y']\": np.float64(0.4015), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8205), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3726), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.8223), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3704), \"['b', 'x', 'a', 'x']\": np.float64(-9.1981), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3729), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0156), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3728), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0157), \"['b', 'x', 'a', 'y']\": np.float64(0.3888), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.8224), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3729), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8246), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3704), \"['b', 'x', 'b', 'x']\": np.float64(-9.2003), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3704), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0155), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3703), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0156), \"['b', 'x', 'b', 'y']\": np.float64(0.3862), \"['b', 'x']\": np.float64(-9.5921), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3696), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0156), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3697), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0155), \"['b', 'y', 'a', 'x']\": np.float64(-0.3854), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0155), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0155), \"['b', 'y', 'a', 'y']\": np.float64(0.0162), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3695), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0156), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3696), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0155), \"['b', 'y', 'b', 'x']\": np.float64(-0.3854), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0156), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0156), \"['b', 'y', 'b', 'y']\": np.float64(0.0162), \"['b', 'y']\": np.float64(0.4018)}\n",
      "It s train loss bro [0.058611128479242325, 0.04946155101060867, 0.048611514270305634, 0.047778256237506866, 0.046961478888988495, 0.04616042599081993, 0.0453747920691967, 0.04460404813289642, 0.04384754225611687, 0.04310530424118042, 0.0423765704035759]\n",
      "% good predict : 0\n",
      "\u001b[0;34miteration 237 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.05853130668401718 and val_loss is 4.48001766204834\n",
      "for 1 epochs, loss is 0.040937650948762894 and val_loss is 4.4845991134643555\n",
      "for 2 epochs, loss is 0.040229205042123795 and val_loss is 4.489091396331787\n",
      "for 3 epochs, loss is 0.039534926414489746 and val_loss is 4.493496894836426\n",
      "for 4 epochs, loss is 0.03885461390018463 and val_loss is 4.497821807861328\n",
      "for 5 epochs, loss is 0.03818760812282562 and val_loss is 4.502065658569336\n",
      "for 6 epochs, loss is 0.03753358870744705 and val_loss is 4.5062336921691895\n",
      "for 7 epochs, loss is 0.036892011761665344 and val_loss is 4.510326862335205\n",
      "for 8 epochs, loss is 0.03626232221722603 and val_loss is 4.514349460601807\n",
      "for 9 epochs, loss is 0.035644546151161194 and val_loss is 4.518301486968994\n",
      "for 10 epochs, loss is 0.03503812476992607 and val_loss is 4.522186279296875\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.0129), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3144), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.0156), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3114), \"['a', 'x', 'a', 'x']\": np.float64(-9.3318), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3145), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0109), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3145), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.011), \"['a', 'x', 'a', 'y']\": np.float64(0.3256), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.0155), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3147), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.0184), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3114), \"['a', 'x', 'b', 'x']\": np.float64(-9.3347), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3115), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0108), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3114), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0108), \"['a', 'x', 'b', 'y']\": np.float64(0.3224), \"['a', 'x']\": np.float64(-9.6621), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3102), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0108), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3102), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0107), \"['a', 'y', 'a', 'x']\": np.float64(-0.3211), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0107), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0107), \"['a', 'y', 'a', 'y']\": np.float64(0.0111), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3101), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0108), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3102), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0107), \"['a', 'y', 'b', 'x']\": np.float64(-0.321), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0108), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0108), \"['a', 'y', 'b', 'y']\": np.float64(0.0112), \"['a', 'y']\": np.float64(0.3324), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-9.0127), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3144), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-9.0155), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3114), \"['b', 'x', 'a', 'x']\": np.float64(-9.3317), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3147), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0109), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3146), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.011), \"['b', 'x', 'a', 'y']\": np.float64(0.3258), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-9.0156), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3147), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-9.0185), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3114), \"['b', 'x', 'b', 'x']\": np.float64(-9.3348), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3114), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0108), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3113), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0108), \"['b', 'x', 'b', 'y']\": np.float64(0.3223), \"['b', 'x']\": np.float64(-9.6621), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3102), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0108), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3103), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0107), \"['b', 'y', 'a', 'x']\": np.float64(-0.3211), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0107), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0107), \"['b', 'y', 'a', 'y']\": np.float64(0.0111), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3101), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0108), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3102), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0107), \"['b', 'y', 'b', 'x']\": np.float64(-0.3211), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0108), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0108), \"['b', 'y', 'b', 'y']\": np.float64(0.0112), \"['b', 'y']\": np.float64(0.3324)}\n",
      "It s train loss bro [0.05853130668401718, 0.040937650948762894, 0.040229205042123795, 0.039534926414489746, 0.03885461390018463, 0.03818760812282562, 0.03753358870744705, 0.036892011761665344, 0.03626232221722603, 0.035644546151161194, 0.03503812476992607]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 238 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.9873318672180176 and val_loss is 4.521757125854492\n",
      "for 1 epochs, loss is 3.400156259536743 and val_loss is 4.520704746246338\n",
      "for 2 epochs, loss is 3.3949623107910156 and val_loss is 4.51909065246582\n",
      "for 3 epochs, loss is 3.386488199234009 and val_loss is 4.516973972320557\n",
      "for 4 epochs, loss is 3.3752715587615967 and val_loss is 4.514410495758057\n",
      "for 5 epochs, loss is 3.3617489337921143 and val_loss is 4.51144552230835\n",
      "for 6 epochs, loss is 3.346287488937378 and val_loss is 4.508125305175781\n",
      "for 7 epochs, loss is 3.329197883605957 and val_loss is 4.504488945007324\n",
      "for 8 epochs, loss is 3.3107495307922363 and val_loss is 4.500573635101318\n",
      "for 9 epochs, loss is 3.2911722660064697 and val_loss is 4.496410369873047\n",
      "for 10 epochs, loss is 3.270667552947998 and val_loss is 4.492031097412109\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8597), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3612), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.8625), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3581), \"['a', 'x', 'a', 'x']\": np.float64(-9.2258), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3613), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0146), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3613), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0147), \"['a', 'x', 'a', 'y']\": np.float64(0.3761), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.8624), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3615), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8654), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3581), \"['a', 'x', 'b', 'x']\": np.float64(-9.2287), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3582), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0145), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3582), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0145), \"['a', 'x', 'b', 'y']\": np.float64(0.3729), \"['a', 'x']\": np.float64(-9.6069), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.3572), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0146), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3573), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0144), \"['a', 'y', 'a', 'x']\": np.float64(-0.3719), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0144), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0144), \"['a', 'y', 'a', 'y']\": np.float64(0.015), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3571), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0146), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.3572), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0144), \"['a', 'y', 'b', 'x']\": np.float64(-0.3718), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0145), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0145), \"['a', 'y', 'b', 'y']\": np.float64(0.0151), \"['a', 'y']\": np.float64(0.3872), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-8.8598), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.3612), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-8.8626), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.3581), \"['b', 'x', 'a', 'x']\": np.float64(-9.2259), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.3615), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0146), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.3614), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0147), \"['b', 'x', 'a', 'y']\": np.float64(0.3763), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-8.8627), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.3615), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-8.8656), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.3582), \"['b', 'x', 'b', 'x']\": np.float64(-9.229), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.3582), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0145), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.3581), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0145), \"['b', 'x', 'b', 'y']\": np.float64(0.3728), \"['b', 'x']\": np.float64(-9.6072), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.357), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0146), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.3571), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0144), \"['b', 'y', 'a', 'x']\": np.float64(-0.3717), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0144), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0144), \"['b', 'y', 'a', 'y']\": np.float64(0.015), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.3569), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0146), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.357), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0144), \"['b', 'y', 'b', 'x']\": np.float64(-0.3717), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0145), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0145), \"['b', 'y', 'b', 'y']\": np.float64(0.0151), \"['b', 'y']\": np.float64(0.387)}\n",
      "It s train loss bro [2.9873318672180176, 3.400156259536743, 3.3949623107910156, 3.386488199234009, 3.3752715587615967, 3.3617489337921143, 3.346287488937378, 3.329197883605957, 3.3107495307922363, 3.2911722660064697, 3.270667552947998]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 239 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.052548471838235855 and val_loss is 4.453369617462158\n",
      "for 1 epochs, loss is 0.04834451898932457 and val_loss is 4.419547080993652\n",
      "for 2 epochs, loss is 0.05640723183751106 and val_loss is 4.3899993896484375\n",
      "for 3 epochs, loss is 0.0641314685344696 and val_loss is 4.3642258644104\n",
      "for 4 epochs, loss is 0.07135847210884094 and val_loss is 4.341792106628418\n",
      "for 5 epochs, loss is 0.07798630744218826 and val_loss is 4.322315692901611\n",
      "for 6 epochs, loss is 0.08395961672067642 and val_loss is 4.305459976196289\n",
      "for 7 epochs, loss is 0.08925863355398178 and val_loss is 4.2909321784973145\n",
      "for 8 epochs, loss is 0.09388919174671173 and val_loss is 4.278469562530518\n",
      "for 9 epochs, loss is 0.0978742465376854 and val_loss is 4.267845153808594\n",
      "for 10 epochs, loss is 0.10124790668487549 and val_loss is 4.258855819702148\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.3204), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7937), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.3209), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7929), \"['a', 'x', 'a', 'x']\": np.float64(-8.1219), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7937), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0861), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7938), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.086), \"['a', 'x', 'a', 'y']\": np.float64(0.8806), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.3209), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7937), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.3213), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.793), \"['a', 'x', 'b', 'x']\": np.float64(-8.1224), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7929), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.086), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.793), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0859), \"['a', 'x', 'b', 'y']\": np.float64(0.8798), \"['a', 'x']\": np.float64(-9.0112), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7946), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0862), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7946), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0861), \"['a', 'y', 'a', 'x']\": np.float64(-0.8816), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0862), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0093), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0862), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0093), \"['a', 'y', 'a', 'y']\": np.float64(0.0956), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7947), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0862), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7948), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0861), \"['a', 'y', 'b', 'x']\": np.float64(-0.8817), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0861), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0093), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0861), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0093), \"['a', 'y', 'b', 'y']\": np.float64(0.0955), \"['a', 'y']\": np.float64(0.9782), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.3212), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7938), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.3217), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.793), \"['b', 'x', 'a', 'x']\": np.float64(-8.1228), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7937), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0861), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7938), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.086), \"['b', 'x', 'a', 'y']\": np.float64(0.8807), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.3216), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7938), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.322), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7931), \"['b', 'x', 'b', 'x']\": np.float64(-8.1232), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.793), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.086), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7931), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0859), \"['b', 'x', 'b', 'y']\": np.float64(0.8799), \"['b', 'x']\": np.float64(-9.0121), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7939), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0861), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.794), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.086), \"['b', 'y', 'a', 'x']\": np.float64(-0.8809), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0861), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0093), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0861), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0093), \"['b', 'y', 'a', 'y']\": np.float64(0.0955), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.794), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0861), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7941), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.086), \"['b', 'y', 'b', 'x']\": np.float64(-0.881), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.086), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0093), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.086), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0093), \"['b', 'y', 'b', 'y']\": np.float64(0.0954), \"['b', 'y']\": np.float64(0.9774)}\n",
      "It s train loss bro [0.052548471838235855, 0.04834451898932457, 0.05640723183751106, 0.0641314685344696, 0.07135847210884094, 0.07798630744218826, 0.08395961672067642, 0.08925863355398178, 0.09388919174671173, 0.0978742465376854, 0.10124790668487549]\n",
      "% good predict : 10\n",
      "\u001b[0;34miteration 240 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.12573108077049255 and val_loss is 4.251495838165283\n",
      "for 1 epochs, loss is 0.1062970757484436 and val_loss is 4.245413303375244\n",
      "for 2 epochs, loss is 0.10806144028902054 and val_loss is 4.24047327041626\n",
      "for 3 epochs, loss is 0.10938616842031479 and val_loss is 4.236551284790039\n",
      "for 4 epochs, loss is 0.1103130504488945 and val_loss is 4.233538627624512\n",
      "for 5 epochs, loss is 0.11088169366121292 and val_loss is 4.231339454650879\n",
      "for 6 epochs, loss is 0.11112920194864273 and val_loss is 4.2298665046691895\n",
      "for 7 epochs, loss is 0.11109058558940887 and val_loss is 4.229045391082764\n",
      "for 8 epochs, loss is 0.11079782992601395 and val_loss is 4.22880744934082\n",
      "for 9 epochs, loss is 0.11028048396110535 and val_loss is 4.229092597961426\n",
      "for 10 epochs, loss is 0.10956599563360214 and val_loss is 4.2298479080200195\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.2179), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8198), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.2166), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8206), \"['a', 'x', 'a', 'x']\": np.float64(-8.0459), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8197), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0931), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8196), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0932), \"['a', 'x', 'a', 'y']\": np.float64(0.9138), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.2164), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8198), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.2157), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8202), \"['a', 'x', 'b', 'x']\": np.float64(-8.0445), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8206), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0932), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8205), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0933), \"['a', 'x', 'b', 'y']\": np.float64(0.9148), \"['a', 'x']\": np.float64(-8.9688), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8206), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0932), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8204), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0933), \"['a', 'y', 'a', 'x']\": np.float64(-0.9147), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0932), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0106), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0932), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0106), \"['a', 'y', 'a', 'y']\": np.float64(0.1039), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8204), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0932), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8204), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0932), \"['a', 'y', 'b', 'x']\": np.float64(-0.9146), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0933), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0106), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0933), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0106), \"['a', 'y', 'b', 'y']\": np.float64(0.104), \"['a', 'y']\": np.float64(1.0197), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.216), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8195), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.2148), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8204), \"['b', 'x', 'a', 'x']\": np.float64(-8.0438), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8198), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0931), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8197), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0932), \"['b', 'x', 'a', 'y']\": np.float64(0.9138), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.2151), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8197), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.2144), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.82), \"['b', 'x', 'b', 'x']\": np.float64(-8.043), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.82), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0931), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.82), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.0932), \"['b', 'x', 'b', 'y']\": np.float64(0.9142), \"['b', 'x']\": np.float64(-8.9669), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8222), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0934), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8221), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0935), \"['b', 'y', 'a', 'x']\": np.float64(-0.9165), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0934), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0106), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0934), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0106), \"['b', 'y', 'a', 'y']\": np.float64(0.1041), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8221), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0934), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8221), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0934), \"['b', 'y', 'b', 'x']\": np.float64(-0.9165), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0934), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0106), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0934), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0106), \"['b', 'y', 'b', 'y']\": np.float64(0.1042), \"['b', 'y']\": np.float64(1.0217)}\n",
      "It s train loss bro [0.12573108077049255, 0.1062970757484436, 0.10806144028902054, 0.10938616842031479, 0.1103130504488945, 0.11088169366121292, 0.11112920194864273, 0.11109058558940887, 0.11079782992601395, 0.11028048396110535, 0.10956599563360214]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 241 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.8232370615005493 and val_loss is 4.229734897613525\n",
      "for 1 epochs, loss is 2.2829999923706055 and val_loss is 4.2293219566345215\n",
      "for 2 epochs, loss is 2.2806570529937744 and val_loss is 4.228647708892822\n",
      "for 3 epochs, loss is 2.276851177215576 and val_loss is 4.227744102478027\n",
      "for 4 epochs, loss is 2.271740436553955 and val_loss is 4.2266387939453125\n",
      "for 5 epochs, loss is 2.265467643737793 and val_loss is 4.225362300872803\n",
      "for 6 epochs, loss is 2.2581655979156494 and val_loss is 4.223938465118408\n",
      "for 7 epochs, loss is 2.249952554702759 and val_loss is 4.222390651702881\n",
      "for 8 epochs, loss is 2.240936756134033 and val_loss is 4.220738887786865\n",
      "for 9 epochs, loss is 2.231215476989746 and val_loss is 4.219003677368164\n",
      "for 10 epochs, loss is 2.220876693725586 and val_loss is 4.217199325561523\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.0287), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8681), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.028), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8685), \"['a', 'x', 'a', 'x']\": np.float64(-7.9052), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8682), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1071), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8681), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1072), \"['a', 'x', 'a', 'y']\": np.float64(0.9764), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.0278), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8683), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.0278), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8679), \"['a', 'x', 'b', 'x']\": np.float64(-7.9044), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8686), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1071), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8685), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1072), \"['a', 'x', 'b', 'y']\": np.float64(0.9768), \"['a', 'x']\": np.float64(-8.891), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8674), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1071), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8674), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1072), \"['a', 'y', 'a', 'x']\": np.float64(-0.9756), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.107), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0132), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.107), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0132), \"['a', 'y', 'a', 'y']\": np.float64(0.1203), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8673), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1072), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8673), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1071), \"['a', 'y', 'b', 'x']\": np.float64(-0.9755), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1071), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0132), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1071), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0132), \"['a', 'y', 'b', 'y']\": np.float64(0.1205), \"['a', 'y']\": np.float64(1.0972), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.0267), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.8679), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.026), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.8682), \"['b', 'x', 'a', 'x']\": np.float64(-7.9029), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.8682), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1071), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.8681), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1072), \"['b', 'x', 'a', 'y']\": np.float64(0.9764), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.0265), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.8681), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.0265), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.8677), \"['b', 'x', 'b', 'x']\": np.float64(-7.9029), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.8678), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.107), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.8678), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1071), \"['b', 'x', 'b', 'y']\": np.float64(0.976), \"['b', 'x']\": np.float64(-8.8888), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.8693), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1074), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.8692), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1074), \"['b', 'y', 'a', 'x']\": np.float64(-0.9777), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1072), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0132), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1072), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0132), \"['b', 'y', 'a', 'y']\": np.float64(0.1206), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.8692), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1074), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.8692), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1073), \"['b', 'y', 'b', 'x']\": np.float64(-0.9776), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1073), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0132), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1073), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0133), \"['b', 'y', 'b', 'y']\": np.float64(0.1207), \"['b', 'y']\": np.float64(1.0995)}\n",
      "It s train loss bro [1.8232370615005493, 2.2829999923706055, 2.2806570529937744, 2.276851177215576, 2.271740436553955, 2.265467643737793, 2.2581655979156494, 2.249952554702759, 2.240936756134033, 2.231215476989746, 2.220876693725586]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 242 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.12289319932460785 and val_loss is 4.201647758483887\n",
      "for 1 epochs, loss is 0.12981733679771423 and val_loss is 4.188343048095703\n",
      "for 2 epochs, loss is 0.1414564698934555 and val_loss is 4.176958084106445\n",
      "for 3 epochs, loss is 0.15223632752895355 and val_loss is 4.167200565338135\n",
      "for 4 epochs, loss is 0.16204605996608734 and val_loss is 4.15880823135376\n",
      "for 5 epochs, loss is 0.17081232368946075 and val_loss is 4.151551723480225\n",
      "for 6 epochs, loss is 0.1784946173429489 and val_loss is 4.145235061645508\n",
      "for 7 epochs, loss is 0.18508106470108032 and val_loss is 4.139687538146973\n",
      "for 8 epochs, loss is 0.19058334827423096 and val_loss is 4.134768962860107\n",
      "for 9 epochs, loss is 0.1950330138206482 and val_loss is 4.130362033843994\n",
      "for 10 epochs, loss is 0.19847755134105682 and val_loss is 4.126370429992676\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.4669), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2122), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.4672), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2119), \"['a', 'x', 'a', 'x']\": np.float64(-6.6883), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2128), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2681), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2124), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2685), \"['a', 'x', 'a', 'y']\": np.float64(1.483), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.4672), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2123), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.4686), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2109), \"['a', 'x', 'b', 'x']\": np.float64(-6.6886), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2125), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2681), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2124), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2683), \"['a', 'x', 'b', 'y']\": np.float64(1.4827), \"['a', 'x']\": np.float64(-8.1825), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2051), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2672), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2052), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2672), \"['a', 'y', 'a', 'x']\": np.float64(-1.4743), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2666), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0589), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2665), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.059), \"['a', 'y', 'a', 'y']\": np.float64(0.326), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2048), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2671), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2051), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2668), \"['a', 'y', 'b', 'x']\": np.float64(-1.4739), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.267), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.059), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2669), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0591), \"['a', 'y', 'b', 'y']\": np.float64(0.3264), \"['a', 'y']\": np.float64(1.8028), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.4668), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2122), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.467), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2119), \"['b', 'x', 'a', 'x']\": np.float64(-6.6881), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2128), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2681), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2124), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2685), \"['b', 'x', 'a', 'y']\": np.float64(1.483), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.4681), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2125), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.4695), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2111), \"['b', 'x', 'b', 'x']\": np.float64(-6.6898), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2113), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2679), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2112), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.268), \"['b', 'x', 'b', 'y']\": np.float64(1.4813), \"['b', 'x']\": np.float64(-8.1823), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2052), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2672), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2053), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2672), \"['b', 'y', 'a', 'x']\": np.float64(-1.4745), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2666), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.059), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2666), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.059), \"['b', 'y', 'a', 'y']\": np.float64(0.326), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2051), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2672), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2054), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2669), \"['b', 'y', 'b', 'x']\": np.float64(-1.4743), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2668), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.059), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2668), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.059), \"['b', 'y', 'b', 'y']\": np.float64(0.3262), \"['b', 'y']\": np.float64(1.8031)}\n",
      "It s train loss bro [0.12289319932460785, 0.12981733679771423, 0.1414564698934555, 0.15223632752895355, 0.16204605996608734, 0.17081232368946075, 0.1784946173429489, 0.18508106470108032, 0.19058334827423096, 0.1950330138206482, 0.19847755134105682]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 243 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.22292819619178772 and val_loss is 4.122838497161865\n",
      "for 1 epochs, loss is 0.20258395373821259 and val_loss is 4.119576454162598\n",
      "for 2 epochs, loss is 0.20338870584964752 and val_loss is 4.1165385246276855\n",
      "for 3 epochs, loss is 0.20346738398075104 and val_loss is 4.113694667816162\n",
      "for 4 epochs, loss is 0.20289726555347443 and val_loss is 4.111023426055908\n",
      "for 5 epochs, loss is 0.2017545849084854 and val_loss is 4.1085124015808105\n",
      "for 6 epochs, loss is 0.20011311769485474 and val_loss is 4.106154918670654\n",
      "for 7 epochs, loss is 0.1980423778295517 and val_loss is 4.103951454162598\n",
      "for 8 epochs, loss is 0.19560782611370087 and val_loss is 4.1019062995910645\n",
      "for 9 epochs, loss is 0.19286954402923584 and val_loss is 4.100025653839111\n",
      "for 10 epochs, loss is 0.1898820996284485 and val_loss is 4.098318099975586\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.7056), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1662), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.7069), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1647), \"['a', 'x', 'a', 'x']\": np.float64(-6.8815), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1667), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2378), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1664), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2381), \"['a', 'x', 'a', 'y']\": np.float64(1.4065), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.7072), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1663), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.7092), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1641), \"['a', 'x', 'b', 'x']\": np.float64(-6.8831), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1653), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2375), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1652), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2376), \"['a', 'x', 'b', 'y']\": np.float64(1.4048), \"['a', 'x']\": np.float64(-8.2998), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1588), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2369), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1591), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2366), \"['a', 'y', 'a', 'x']\": np.float64(-1.3977), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2363), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0482), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2362), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0482), \"['a', 'y', 'a', 'y']\": np.float64(0.2849), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1586), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2368), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.159), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2363), \"['a', 'y', 'b', 'x']\": np.float64(-1.3973), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2366), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0482), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2366), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0482), \"['a', 'y', 'b', 'y']\": np.float64(0.2852), \"['a', 'y']\": np.float64(1.685), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.7066), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1664), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.708), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.165), \"['b', 'x', 'a', 'x']\": np.float64(-6.8827), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1667), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2378), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1664), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2381), \"['b', 'x', 'a', 'y']\": np.float64(1.4065), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.7088), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1666), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.7108), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1644), \"['b', 'x', 'b', 'x']\": np.float64(-6.8852), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1645), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2373), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1644), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2375), \"['b', 'x', 'b', 'y']\": np.float64(1.4039), \"['b', 'x']\": np.float64(-8.301), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1579), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2367), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1582), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2364), \"['b', 'y', 'a', 'x']\": np.float64(-1.3966), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2361), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0481), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.236), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0482), \"['b', 'y', 'a', 'y']\": np.float64(0.2846), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1579), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2366), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1583), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2362), \"['b', 'y', 'b', 'x']\": np.float64(-1.3964), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2362), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0481), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2362), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0482), \"['b', 'y', 'b', 'y']\": np.float64(0.2848), \"['b', 'y']\": np.float64(1.6836)}\n",
      "It s train loss bro [0.22292819619178772, 0.20258395373821259, 0.20338870584964752, 0.20346738398075104, 0.20289726555347443, 0.2017545849084854, 0.20011311769485474, 0.1980423778295517, 0.19560782611370087, 0.19286954402923584, 0.1898820996284485]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 244 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.5779635906219482 and val_loss is 4.098246097564697\n",
      "for 1 epochs, loss is 1.7784650325775146 and val_loss is 4.098264694213867\n",
      "for 2 epochs, loss is 1.7774137258529663 and val_loss is 4.098366737365723\n",
      "for 3 epochs, loss is 1.7752021551132202 and val_loss is 4.098546028137207\n",
      "for 4 epochs, loss is 1.7719507217407227 and val_loss is 4.098799228668213\n",
      "for 5 epochs, loss is 1.7677690982818604 and val_loss is 4.099120616912842\n",
      "for 6 epochs, loss is 1.7627559900283813 and val_loss is 4.099507808685303\n",
      "for 7 epochs, loss is 1.7569999694824219 and val_loss is 4.099958896636963\n",
      "for 8 epochs, loss is 1.7505817413330078 and val_loss is 4.100472927093506\n",
      "for 9 epochs, loss is 1.7435729503631592 and val_loss is 4.101047039031982\n",
      "for 10 epochs, loss is 1.7360389232635498 and val_loss is 4.101680755615234\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.5286), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2005), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.5303), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1988), \"['a', 'x', 'a', 'x']\": np.float64(-6.7386), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.201), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2601), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2008), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2603), \"['a', 'x', 'a', 'y']\": np.float64(1.4632), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.5306), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2005), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.5328), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1982), \"['a', 'x', 'b', 'x']\": np.float64(-6.7406), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1993), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2597), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1993), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2597), \"['a', 'x', 'b', 'y']\": np.float64(1.4611), \"['a', 'x']\": np.float64(-8.2135), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1928), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.259), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1931), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2586), \"['a', 'y', 'a', 'x']\": np.float64(-1.4538), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2584), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.056), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2584), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.056), \"['a', 'y', 'a', 'y']\": np.float64(0.3149), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1927), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2589), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1932), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2584), \"['a', 'y', 'b', 'x']\": np.float64(-1.4537), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2586), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.056), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2586), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.056), \"['a', 'y', 'b', 'y']\": np.float64(0.3151), \"['a', 'y']\": np.float64(1.7713), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.5305), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2008), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.5321), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1992), \"['b', 'x', 'a', 'x']\": np.float64(-6.7409), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.201), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2601), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2009), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2603), \"['b', 'x', 'a', 'y']\": np.float64(1.4632), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.5329), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.201), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.5351), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1987), \"['b', 'x', 'b', 'x']\": np.float64(-6.7435), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1987), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2596), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1987), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2596), \"['b', 'x', 'b', 'y']\": np.float64(1.4604), \"['b', 'x']\": np.float64(-8.2157), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1912), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2586), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1916), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2583), \"['b', 'y', 'a', 'x']\": np.float64(-1.4519), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2581), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0559), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.258), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0559), \"['b', 'y', 'a', 'y']\": np.float64(0.3144), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1913), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2586), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1917), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2581), \"['b', 'y', 'b', 'x']\": np.float64(-1.4519), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2581), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0559), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2581), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0559), \"['b', 'y', 'b', 'y']\": np.float64(0.3144), \"['b', 'y']\": np.float64(1.7689)}\n",
      "It s train loss bro [1.5779635906219482, 1.7784650325775146, 1.7774137258529663, 1.7752021551132202, 1.7719507217407227, 1.7677690982818604, 1.7627559900283813, 1.7569999694824219, 1.7505817413330078, 1.7435729503631592, 1.7360389232635498]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 245 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3250623047351837 and val_loss is 4.108521461486816\n",
      "for 1 epochs, loss is 0.2126224935054779 and val_loss is 4.114585876464844\n",
      "for 2 epochs, loss is 0.22655397653579712 and val_loss is 4.119868278503418\n",
      "for 3 epochs, loss is 0.23897765576839447 and val_loss is 4.124375343322754\n",
      "for 4 epochs, loss is 0.24977605044841766 and val_loss is 4.128122329711914\n",
      "for 5 epochs, loss is 0.25888580083847046 and val_loss is 4.131135940551758\n",
      "for 6 epochs, loss is 0.2662923336029053 and val_loss is 4.133452892303467\n",
      "for 7 epochs, loss is 0.2720244526863098 and val_loss is 4.13511848449707\n",
      "for 8 epochs, loss is 0.27614760398864746 and val_loss is 4.1361870765686035\n",
      "for 9 epochs, loss is 0.2787562310695648 and val_loss is 4.136716842651367\n",
      "for 10 epochs, loss is 0.2799665331840515 and val_loss is 4.136774063110352\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3226), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3878), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.327), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3835), \"['a', 'x', 'a', 'x']\": np.float64(-5.7187), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3879), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4455), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3892), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4442), \"['a', 'x', 'a', 'y']\": np.float64(1.836), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.3278), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3884), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3313), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3849), \"['a', 'x', 'b', 'x']\": np.float64(-5.7245), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3838), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4439), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3848), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4428), \"['a', 'x', 'b', 'y']\": np.float64(1.8303), \"['a', 'x']\": np.float64(-7.5656), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3823), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4438), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3837), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4424), \"['a', 'y', 'a', 'x']\": np.float64(-1.8288), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4437), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1424), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4442), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.142), \"['a', 'y', 'a', 'y']\": np.float64(0.587), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3839), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.444), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3851), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4429), \"['a', 'y', 'b', 'x']\": np.float64(-1.8306), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4425), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1419), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4428), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1416), \"['a', 'y', 'b', 'y']\": np.float64(0.5852), \"['a', 'y']\": np.float64(2.4193), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.3297), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3901), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.3341), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3857), \"['b', 'x', 'a', 'x']\": np.float64(-5.7281), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3891), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.4459), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3905), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.4446), \"['b', 'x', 'a', 'y']\": np.float64(1.8377), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.334), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3904), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.3376), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3869), \"['b', 'x', 'b', 'x']\": np.float64(-5.7327), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3858), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.4445), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3869), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.4434), \"['b', 'x', 'b', 'y']\": np.float64(1.833), \"['b', 'x']\": np.float64(-7.5767), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3761), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.4418), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3775), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.4404), \"['b', 'y', 'a', 'x']\": np.float64(-1.8205), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.4414), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1417), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.4419), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1413), \"['b', 'y', 'a', 'y']\": np.float64(0.584), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3774), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.4419), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3786), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.4408), \"['b', 'y', 'b', 'x']\": np.float64(-1.822), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.4404), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1413), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.4408), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1409), \"['b', 'y', 'b', 'y']\": np.float64(0.5825), \"['b', 'y']\": np.float64(2.408)}\n",
      "It s train loss bro [0.3250623047351837, 0.2126224935054779, 0.22655397653579712, 0.23897765576839447, 0.24977605044841766, 0.25888580083847046, 0.2662923336029053, 0.2720244526863098, 0.27614760398864746, 0.2787562310695648, 0.2799665331840515]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 246 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.27857109904289246 and val_loss is 4.136505126953125\n",
      "for 1 epochs, loss is 0.27875253558158875 and val_loss is 4.135896682739258\n",
      "for 2 epochs, loss is 0.2766045033931732 and val_loss is 4.135020732879639\n",
      "for 3 epochs, loss is 0.27360424399375916 and val_loss is 4.133948802947998\n",
      "for 4 epochs, loss is 0.2698853015899658 and val_loss is 4.132753372192383\n",
      "for 5 epochs, loss is 0.2655728757381439 and val_loss is 4.131505489349365\n",
      "for 6 epochs, loss is 0.2607816159725189 and val_loss is 4.130272388458252\n",
      "for 7 epochs, loss is 0.25561508536338806 and val_loss is 4.129116535186768\n",
      "for 8 epochs, loss is 0.25016507506370544 and val_loss is 4.128098964691162\n",
      "for 9 epochs, loss is 0.24451148509979248 and val_loss is 4.127274513244629\n",
      "for 10 epochs, loss is 0.23872323334217072 and val_loss is 4.126693248748779\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.9749), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2951), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.9771), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2928), \"['a', 'x', 'a', 'x']\": np.float64(-6.279), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2951), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3371), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2955), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3367), \"['a', 'x', 'a', 'y']\": np.float64(1.6345), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.9775), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2952), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.9791), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2935), \"['a', 'x', 'b', 'x']\": np.float64(-6.2818), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.293), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3364), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2933), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3361), \"['a', 'x', 'b', 'y']\": np.float64(1.6317), \"['a', 'x']\": np.float64(-7.925), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2931), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3366), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2937), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.336), \"['a', 'y', 'a', 'x']\": np.float64(-1.6321), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3365), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0876), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3367), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0875), \"['a', 'y', 'a', 'y']\": np.float64(0.4247), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2936), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3366), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2941), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3362), \"['a', 'y', 'b', 'x']\": np.float64(-1.6326), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3362), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0875), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3363), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0874), \"['a', 'y', 'b', 'y']\": np.float64(0.4243), \"['a', 'y']\": np.float64(2.0598), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.9777), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2958), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.9798), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2935), \"['b', 'x', 'a', 'x']\": np.float64(-6.2825), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2954), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3371), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2958), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3367), \"['b', 'x', 'a', 'y']\": np.float64(1.6348), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.9797), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2958), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.9814), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2941), \"['b', 'x', 'b', 'x']\": np.float64(-6.2846), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2937), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3366), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.294), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3363), \"['b', 'x', 'b', 'y']\": np.float64(1.6326), \"['b', 'x']\": np.float64(-7.9288), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2906), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.336), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2912), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3354), \"['b', 'y', 'a', 'x']\": np.float64(-1.6289), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3358), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0874), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3359), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0873), \"['b', 'y', 'a', 'y']\": np.float64(0.4238), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.291), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.336), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2915), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3355), \"['b', 'y', 'b', 'x']\": np.float64(-1.6293), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3355), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0873), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3356), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0872), \"['b', 'y', 'b', 'y']\": np.float64(0.4234), \"['b', 'y']\": np.float64(2.0557)}\n",
      "It s train loss bro [0.27857109904289246, 0.27875253558158875, 0.2766045033931732, 0.27360424399375916, 0.2698853015899658, 0.2655728757381439, 0.2607816159725189, 0.25561508536338806, 0.25016507506370544, 0.24451148509979248, 0.23872323334217072]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 247 action \u001b[0;31m a \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.23294906318187714 and val_loss is 4.126410961151123\n",
      "for 1 epochs, loss is 0.22697170078754425 and val_loss is 4.126453876495361\n",
      "for 2 epochs, loss is 0.2210969775915146 and val_loss is 4.126855373382568\n",
      "for 3 epochs, loss is 0.21526673436164856 and val_loss is 4.127645015716553\n",
      "for 4 epochs, loss is 0.2095077782869339 and val_loss is 4.128843307495117\n",
      "for 5 epochs, loss is 0.20384061336517334 and val_loss is 4.130467891693115\n",
      "for 6 epochs, loss is 0.19828173518180847 and val_loss is 4.1325297355651855\n",
      "for 7 epochs, loss is 0.19284290075302124 and val_loss is 4.135035514831543\n",
      "for 8 epochs, loss is 0.18753303587436676 and val_loss is 4.137986660003662\n",
      "for 9 epochs, loss is 0.1823585480451584 and val_loss is 4.1413798332214355\n",
      "for 10 epochs, loss is 0.1773228943347931 and val_loss is 4.145209789276123\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.9618), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1114), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9602), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1127), \"['a', 'x', 'a', 'x']\": np.float64(-7.0826), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1112), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2073), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1113), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2073), \"['a', 'x', 'a', 'y']\": np.float64(1.3203), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.9601), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1111), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9587), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1123), \"['a', 'x', 'b', 'x']\": np.float64(-7.0808), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1126), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2076), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1126), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2075), \"['a', 'x', 'b', 'y']\": np.float64(1.3219), \"['a', 'x']\": np.float64(-8.4142), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1133), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2075), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.113), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2078), \"['a', 'y', 'a', 'x']\": np.float64(-1.3226), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2077), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0387), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2077), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0387), \"['a', 'y', 'a', 'y']\": np.float64(0.2467), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1134), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2076), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1131), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2078), \"['a', 'y', 'b', 'x']\": np.float64(-1.3227), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2076), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0387), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2076), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0387), \"['a', 'y', 'b', 'y']\": np.float64(0.2467), \"['a', 'y']\": np.float64(1.5715), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.96), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1111), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9585), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1124), \"['b', 'x', 'a', 'x']\": np.float64(-7.0806), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.111), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2073), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1111), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2072), \"['b', 'x', 'a', 'y']\": np.float64(1.32), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.9586), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1109), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9572), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.112), \"['b', 'x', 'b', 'x']\": np.float64(-7.0789), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1121), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2075), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1122), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2074), \"['b', 'x', 'b', 'y']\": np.float64(1.3214), \"['b', 'x']\": np.float64(-8.4119), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1147), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2078), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1144), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2081), \"['b', 'y', 'a', 'x']\": np.float64(-1.3243), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2079), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0388), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.208), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0388), \"['b', 'y', 'a', 'y']\": np.float64(0.2471), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1148), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2078), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1145), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.208), \"['b', 'y', 'b', 'x']\": np.float64(-1.3244), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2079), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0388), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2079), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0388), \"['b', 'y', 'b', 'y']\": np.float64(0.247), \"['b', 'y']\": np.float64(1.5735)}\n",
      "It s train loss bro [0.23294906318187714, 0.22697170078754425, 0.2210969775915146, 0.21526673436164856, 0.2095077782869339, 0.20384061336517334, 0.19828173518180847, 0.19284290075302124, 0.18753303587436676, 0.1823585480451584, 0.1773228943347931]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 248 action \u001b[0;31m b \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 1.5803142786026 and val_loss is 4.1453776359558105\n",
      "for 1 epochs, loss is 1.852427363395691 and val_loss is 4.14522123336792\n",
      "for 2 epochs, loss is 1.8521080017089844 and val_loss is 4.144777774810791\n",
      "for 3 epochs, loss is 1.8505339622497559 and val_loss is 4.144076824188232\n",
      "for 4 epochs, loss is 1.8478388786315918 and val_loss is 4.143152713775635\n",
      "for 5 epochs, loss is 1.8441435098648071 and val_loss is 4.142030715942383\n",
      "for 6 epochs, loss is 1.8395576477050781 and val_loss is 4.140737056732178\n",
      "for 7 epochs, loss is 1.8341821432113647 and val_loss is 4.1392951011657715\n",
      "for 8 epochs, loss is 1.82810640335083 and val_loss is 4.1377272605896\n",
      "for 9 epochs, loss is 1.8214126825332642 and val_loss is 4.136053085327148\n",
      "for 10 epochs, loss is 1.8141751289367676 and val_loss is 4.1342902183532715\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.8091), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1428), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8074), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1442), \"['a', 'x', 'a', 'x']\": np.float64(-6.9614), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1427), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2249), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1426), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.225), \"['a', 'x', 'a', 'y']\": np.float64(1.3695), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.8073), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1426), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.806), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1437), \"['a', 'x', 'b', 'x']\": np.float64(-6.9594), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1441), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2252), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1441), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2253), \"['a', 'x', 'b', 'y']\": np.float64(1.3712), \"['a', 'x']\": np.float64(-8.3423), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.144), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2251), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1437), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2253), \"['a', 'y', 'a', 'x']\": np.float64(-1.371), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2251), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0443), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2251), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0443), \"['a', 'y', 'a', 'y']\": np.float64(0.2698), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1439), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2251), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1437), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2253), \"['a', 'y', 'b', 'x']\": np.float64(-1.3709), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2253), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0443), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2253), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0444), \"['a', 'y', 'b', 'y']\": np.float64(0.27), \"['a', 'y']\": np.float64(1.6431), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.807), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1424), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.8053), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1438), \"['b', 'x', 'a', 'x']\": np.float64(-6.9589), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1424), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2248), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1423), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.225), \"['b', 'x', 'a', 'y']\": np.float64(1.3691), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.8055), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1422), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.8042), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1433), \"['b', 'x', 'b', 'x']\": np.float64(-6.9573), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1435), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2251), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1434), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2252), \"['b', 'x', 'b', 'y']\": np.float64(1.3705), \"['b', 'x']\": np.float64(-8.3395), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1458), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2254), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1454), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2257), \"['b', 'y', 'a', 'x']\": np.float64(-1.373), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2255), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0444), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2255), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0444), \"['b', 'y', 'a', 'y']\": np.float64(0.2703), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1457), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2254), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1454), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2256), \"['b', 'y', 'b', 'x']\": np.float64(-1.373), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2256), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0444), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2256), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0444), \"['b', 'y', 'b', 'y']\": np.float64(0.2704), \"['b', 'y']\": np.float64(1.6456)}\n",
      "It s train loss bro [1.5803142786026, 1.852427363395691, 1.8521080017089844, 1.8505339622497559, 1.8478388786315918, 1.8441435098648071, 1.8395576477050781, 1.8341821432113647, 1.82810640335083, 1.8214126825332642, 1.8141751289367676]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 249 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.1824323534965515 and val_loss is 4.119395732879639\n",
      "for 1 epochs, loss is 0.19467581808567047 and val_loss is 4.107002258300781\n",
      "for 2 epochs, loss is 0.20694391429424286 and val_loss is 4.096698760986328\n",
      "for 3 epochs, loss is 0.21781973540782928 and val_loss is 4.088129043579102\n",
      "for 4 epochs, loss is 0.2272680699825287 and val_loss is 4.080984592437744\n",
      "for 5 epochs, loss is 0.2352883666753769 and val_loss is 4.075006484985352\n",
      "for 6 epochs, loss is 0.241908460855484 and val_loss is 4.069977760314941\n",
      "for 7 epochs, loss is 0.24717842042446136 and val_loss is 4.065718650817871\n",
      "for 8 epochs, loss is 0.25116613507270813 and val_loss is 4.062087059020996\n",
      "for 9 epochs, loss is 0.2539522051811218 and val_loss is 4.058965682983398\n",
      "for 10 epochs, loss is 0.25562652945518494 and val_loss is 4.056266784667969\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.6345), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.3456), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.6326), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3473), \"['a', 'x', 'a', 'x']\": np.float64(-5.9898), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3458), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3904), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3449), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3913), \"['a', 'x', 'a', 'y']\": np.float64(1.7391), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.6325), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3451), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.6316), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3458), \"['a', 'x', 'b', 'x']\": np.float64(-5.9874), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.3475), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.391), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.347), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3915), \"['a', 'x', 'b', 'y']\": np.float64(1.7414), \"['a', 'x']\": np.float64(-7.7415), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3419), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3896), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3414), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3901), \"['a', 'y', 'a', 'x']\": np.float64(-1.7344), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3894), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1129), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3891), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1132), \"['a', 'y', 'a', 'y']\": np.float64(0.5031), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.341), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3894), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3408), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3896), \"['a', 'y', 'b', 'x']\": np.float64(-1.7333), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3902), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1132), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3901), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1134), \"['a', 'y', 'b', 'y']\": np.float64(0.5043), \"['a', 'y']\": np.float64(2.2412), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-4.6325), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.345), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-4.6306), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.3468), \"['b', 'x', 'a', 'x']\": np.float64(-5.9872), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.3453), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3903), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.3444), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3912), \"['b', 'x', 'a', 'y']\": np.float64(1.7384), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-4.6315), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.3448), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-4.6307), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.3456), \"['b', 'x', 'b', 'x']\": np.float64(-5.9861), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.346), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3905), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.3455), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3911), \"['b', 'x', 'b', 'y']\": np.float64(1.7394), \"['b', 'x']\": np.float64(-7.7383), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.3435), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3901), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.3429), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3906), \"['b', 'y', 'a', 'x']\": np.float64(-1.7364), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3899), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.1131), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3896), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.1134), \"['b', 'y', 'a', 'y']\": np.float64(0.5038), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.3429), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3899), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.3427), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3901), \"['b', 'y', 'b', 'x']\": np.float64(-1.7357), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3904), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.1133), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.3902), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.1134), \"['b', 'y', 'b', 'y']\": np.float64(0.5045), \"['b', 'y']\": np.float64(2.2439)}\n",
      "It s train loss bro [0.1824323534965515, 0.19467581808567047, 0.20694391429424286, 0.21781973540782928, 0.2272680699825287, 0.2352883666753769, 0.241908460855484, 0.24717842042446136, 0.25116613507270813, 0.2539522051811218, 0.25562652945518494]\n",
      "% good predict : 30\n",
      "\u001b[0;34miteration 250 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.3155054748058319 and val_loss is 4.0540080070495605\n",
      "for 1 epochs, loss is 0.25593897700309753 and val_loss is 4.052046298980713\n",
      "for 2 epochs, loss is 0.2547842264175415 and val_loss is 4.050353050231934\n",
      "for 3 epochs, loss is 0.2529170513153076 and val_loss is 4.048904895782471\n",
      "for 4 epochs, loss is 0.2504301369190216 and val_loss is 4.0476908683776855\n",
      "for 5 epochs, loss is 0.24741114675998688 and val_loss is 4.04670524597168\n",
      "for 6 epochs, loss is 0.24394235014915466 and val_loss is 4.045949935913086\n",
      "for 7 epochs, loss is 0.24009950459003448 and val_loss is 4.045425891876221\n",
      "for 8 epochs, loss is 0.23595179617404938 and val_loss is 4.045138835906982\n",
      "for 9 epochs, loss is 0.23156139254570007 and val_loss is 4.045094966888428\n",
      "for 10 epochs, loss is 0.22698424756526947 and val_loss is 4.0453009605407715\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.1307), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2689), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.13), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2694), \"['a', 'x', 'a', 'x']\": np.float64(-6.41), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2692), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3134), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2687), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.314), \"['a', 'x', 'a', 'y']\": np.float64(1.5853), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.1301), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2686), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.13), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2684), \"['a', 'x', 'b', 'x']\": np.float64(-6.4091), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2697), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3136), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2694), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3139), \"['a', 'x', 'b', 'y']\": np.float64(1.5859), \"['a', 'x']\": np.float64(-8.0082), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.2653), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3129), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2651), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.313), \"['a', 'y', 'a', 'x']\": np.float64(-1.5807), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3125), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0772), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3124), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0773), \"['a', 'y', 'a', 'y']\": np.float64(0.3904), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2647), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.3127), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2647), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3127), \"['a', 'y', 'b', 'x']\": np.float64(-1.58), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3131), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0773), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.313), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0774), \"['a', 'y', 'b', 'y']\": np.float64(0.3911), \"['a', 'y']\": np.float64(1.9744), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.1297), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.2687), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.129), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.2692), \"['b', 'x', 'a', 'x']\": np.float64(-6.4087), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.2688), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.3133), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.2682), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.3139), \"['b', 'x', 'a', 'y']\": np.float64(1.5847), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.1298), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.2685), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.1297), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.2683), \"['b', 'x', 'b', 'x']\": np.float64(-6.4087), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.2686), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.3133), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.2683), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.3137), \"['b', 'x', 'b', 'y']\": np.float64(1.5845), \"['b', 'x']\": np.float64(-8.0065), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.266), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.3131), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.2658), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.3132), \"['b', 'y', 'a', 'x']\": np.float64(-1.5816), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.3127), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0772), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.3126), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0774), \"['b', 'y', 'a', 'y']\": np.float64(0.3906), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.2656), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.313), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.2656), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.3129), \"['b', 'y', 'b', 'x']\": np.float64(-1.5812), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.3131), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0773), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.313), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0774), \"['b', 'y', 'b', 'y']\": np.float64(0.391), \"['b', 'y']\": np.float64(1.9754)}\n",
      "It s train loss bro [0.3155054748058319, 0.25593897700309753, 0.2547842264175415, 0.2529170513153076, 0.2504301369190216, 0.24741114675998688, 0.24394235014915466, 0.24009950459003448, 0.23595179617404938, 0.23156139254570007, 0.22698424756526947]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 251 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2255604863166809 and val_loss is 4.045806407928467\n",
      "for 1 epochs, loss is 0.21746981143951416 and val_loss is 4.046570301055908\n",
      "for 2 epochs, loss is 0.21261246502399445 and val_loss is 4.047598838806152\n",
      "for 3 epochs, loss is 0.20773035287857056 and val_loss is 4.048895359039307\n",
      "for 4 epochs, loss is 0.2028510421514511 and val_loss is 4.050462245941162\n",
      "for 5 epochs, loss is 0.19799816608428955 and val_loss is 4.052300453186035\n",
      "for 6 epochs, loss is 0.19319140911102295 and val_loss is 4.054408550262451\n",
      "for 7 epochs, loss is 0.18844684958457947 and val_loss is 4.056784629821777\n",
      "for 8 epochs, loss is 0.18377767503261566 and val_loss is 4.059423923492432\n",
      "for 9 epochs, loss is 0.17919442057609558 and val_loss is 4.062320232391357\n",
      "for 10 epochs, loss is 0.17470519244670868 and val_loss is 4.065467357635498\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.9955), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1049), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9958), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.104), \"['a', 'x', 'a', 'x']\": np.float64(-7.1109), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1051), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2032), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1049), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2035), \"['a', 'x', 'a', 'y']\": np.float64(1.3104), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.9961), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1046), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9967), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1035), \"['a', 'x', 'b', 'x']\": np.float64(-7.1113), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1043), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2031), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1042), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2032), \"['a', 'x', 'b', 'y']\": np.float64(1.3094), \"['a', 'x']\": np.float64(-8.4338), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1021), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2031), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1022), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.2029), \"['a', 'y', 'a', 'x']\": np.float64(-1.3071), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2027), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0373), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2027), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0373), \"['a', 'y', 'a', 'y']\": np.float64(0.2404), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1019), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.203), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.102), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2028), \"['a', 'y', 'b', 'x']\": np.float64(-1.3069), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.203), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0373), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.203), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0374), \"['a', 'y', 'b', 'y']\": np.float64(0.2407), \"['a', 'y']\": np.float64(1.5499), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-5.9949), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(1.1048), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-5.9953), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(1.1039), \"['b', 'x', 'a', 'x']\": np.float64(-7.1102), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-1.1046), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.2031), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-1.1044), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.2034), \"['b', 'x', 'a', 'y']\": np.float64(1.3098), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-5.9958), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(1.1045), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-5.9964), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(1.1035), \"['b', 'x', 'b', 'x']\": np.float64(-7.1109), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-1.1036), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.2029), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-1.1035), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.2031), \"['b', 'x', 'b', 'y']\": np.float64(1.3086), \"['b', 'x']\": np.float64(-8.4326), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-1.1024), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.2032), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-1.1025), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.203), \"['b', 'y', 'a', 'x']\": np.float64(-1.3075), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.2028), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0373), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.2027), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0373), \"['b', 'y', 'a', 'y']\": np.float64(0.2404), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-1.1023), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.2031), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-1.1024), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.2029), \"['b', 'y', 'b', 'x']\": np.float64(-1.3073), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.2029), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0373), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.2029), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0373), \"['b', 'y', 'b', 'y']\": np.float64(0.2406), \"['b', 'y']\": np.float64(1.5503)}\n",
      "It s train loss bro [0.2255604863166809, 0.21746981143951416, 0.21261246502399445, 0.20773035287857056, 0.2028510421514511, 0.19799816608428955, 0.19319140911102295, 0.18844684958457947, 0.18377767503261566, 0.17919442057609558, 0.17470519244670868]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 252 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.2872255742549896 and val_loss is 4.068207740783691\n",
      "for 1 epochs, loss is 0.16601838171482086 and val_loss is 4.071271896362305\n",
      "for 2 epochs, loss is 0.16184400022029877 and val_loss is 4.074665546417236\n",
      "for 3 epochs, loss is 0.1577916145324707 and val_loss is 4.078383922576904\n",
      "for 4 epochs, loss is 0.15385949611663818 and val_loss is 4.082423210144043\n",
      "for 5 epochs, loss is 0.15004535019397736 and val_loss is 4.086770534515381\n",
      "for 6 epochs, loss is 0.1463465690612793 and val_loss is 4.0914154052734375\n",
      "for 7 epochs, loss is 0.1427602469921112 and val_loss is 4.096340656280518\n",
      "for 8 epochs, loss is 0.13928352296352386 and val_loss is 4.101528644561768\n",
      "for 9 epochs, loss is 0.13591329753398895 and val_loss is 4.106961250305176\n",
      "for 10 epochs, loss is 0.13264641165733337 and val_loss is 4.112619876861572\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.779), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9277), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.7796), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9263), \"['a', 'x', 'a', 'x']\": np.float64(-7.7167), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9277), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1268), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9278), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1268), \"['a', 'x', 'a', 'y']\": np.float64(1.056), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.7799), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9273), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.7803), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9263), \"['a', 'x', 'b', 'x']\": np.float64(-7.7173), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9264), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1266), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9264), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1266), \"['a', 'x', 'b', 'y']\": np.float64(1.0545), \"['a', 'x']\": np.float64(-8.784), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9273), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1269), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9274), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1267), \"['a', 'y', 'a', 'x']\": np.float64(-1.0556), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1268), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0173), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1268), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0173), \"['a', 'y', 'a', 'y']\": np.float64(0.1443), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9274), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1268), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9275), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1267), \"['a', 'y', 'b', 'x']\": np.float64(-1.0556), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1267), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0173), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1267), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0173), \"['a', 'y', 'b', 'y']\": np.float64(0.1442), \"['a', 'y']\": np.float64(1.2015), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-6.7787), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.9276), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-6.7793), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.9263), \"['b', 'x', 'a', 'x']\": np.float64(-7.7163), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.9272), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.1267), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.9273), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.1267), \"['b', 'x', 'a', 'y']\": np.float64(1.0554), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-6.7793), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.9273), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-6.7797), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.9263), \"['b', 'x', 'b', 'x']\": np.float64(-7.7167), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.9262), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.1266), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.9263), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.1266), \"['b', 'x', 'b', 'y']\": np.float64(1.0543), \"['b', 'x']\": np.float64(-8.7831), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.9274), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.1269), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.9275), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.1267), \"['b', 'y', 'a', 'x']\": np.float64(-1.0556), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.1267), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0173), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.1267), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0173), \"['b', 'y', 'a', 'y']\": np.float64(0.1443), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.9275), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.1269), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.9275), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.1267), \"['b', 'y', 'b', 'x']\": np.float64(-1.0557), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.1267), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0173), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.1267), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0173), \"['b', 'y', 'b', 'y']\": np.float64(0.1442), \"['b', 'y']\": np.float64(1.2016)}\n",
      "It s train loss bro [0.2872255742549896, 0.16601838171482086, 0.16184400022029877, 0.1577916145324707, 0.15385949611663818, 0.15004535019397736, 0.1463465690612793, 0.1427602469921112, 0.13928352296352386, 0.13591329753398895, 0.13264641165733337]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 253 action \u001b[0;31m b \u001b[0;35m feedback x \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 0.18058520555496216 and val_loss is 4.119144439697266\n",
      "for 1 epochs, loss is 0.12636782228946686 and val_loss is 4.125786304473877\n",
      "for 2 epochs, loss is 0.12335442751646042 and val_loss is 4.132530212402344\n",
      "for 3 epochs, loss is 0.12043645232915878 and val_loss is 4.139360427856445\n",
      "for 4 epochs, loss is 0.117610402405262 and val_loss is 4.146263599395752\n",
      "for 5 epochs, loss is 0.1148725375533104 and val_loss is 4.153223991394043\n",
      "for 6 epochs, loss is 0.11221995949745178 and val_loss is 4.1602277755737305\n",
      "for 7 epochs, loss is 0.10964911431074142 and val_loss is 4.167260646820068\n",
      "for 8 epochs, loss is 0.10715720802545547 and val_loss is 4.174311637878418\n",
      "for 9 epochs, loss is 0.10474099218845367 and val_loss is 4.1813645362854\n",
      "for 10 epochs, loss is 0.10239748656749725 and val_loss is 4.188411235809326\n",
      "for seq tensor([[1, 2, 0, 3, 0, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1]],\n",
      "       device='cuda:0') all exceptive valance {\"['a', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.4019), \"['a', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7721), \"['a', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.4035), \"['a', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7695), \"['a', 'x', 'a', 'x']\": np.float64(-8.1825), \"['a', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7721), \"['a', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0804), \"['a', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7722), \"['a', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0803), \"['a', 'x', 'a', 'y']\": np.float64(0.8535), \"['a', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.4041), \"['a', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7715), \"['a', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.4052), \"['a', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7697), \"['a', 'x', 'b', 'x']\": np.float64(-8.1844), \"['a', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7696), \"['a', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0801), \"['a', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7696), \"['a', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.08), \"['a', 'x', 'b', 'y']\": np.float64(0.8506), \"['a', 'x']\": np.float64(-9.0456), \"['a', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7711), \"['a', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0804), \"['a', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7713), \"['a', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0802), \"['a', 'y', 'a', 'x']\": np.float64(-0.8524), \"['a', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0803), \"['a', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0084), \"['a', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0803), \"['a', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0083), \"['a', 'y', 'a', 'y']\": np.float64(0.0887), \"['a', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7712), \"['a', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0804), \"['a', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7714), \"['a', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0802), \"['a', 'y', 'b', 'x']\": np.float64(-0.8525), \"['a', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0802), \"['a', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0083), \"['a', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0802), \"['a', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0083), \"['a', 'y', 'b', 'y']\": np.float64(0.0886), \"['a', 'y']\": np.float64(0.9422), \"['b', 'x', 'a', 'x', 'a', 'x']\": np.float64(-7.4024), \"['b', 'x', 'a', 'x', 'a', 'y']\": np.float64(0.7721), \"['b', 'x', 'a', 'x', 'b', 'x']\": np.float64(-7.4041), \"['b', 'x', 'a', 'x', 'b', 'y']\": np.float64(0.7695), \"['b', 'x', 'a', 'x']\": np.float64(-8.1832), \"['b', 'x', 'a', 'y', 'a', 'x']\": np.float64(-0.7714), \"['b', 'x', 'a', 'y', 'a', 'y']\": np.float64(0.0803), \"['b', 'x', 'a', 'y', 'b', 'x']\": np.float64(-0.7716), \"['b', 'x', 'a', 'y', 'b', 'y']\": np.float64(0.0802), \"['b', 'x', 'a', 'y']\": np.float64(0.8527), \"['b', 'x', 'b', 'x', 'a', 'x']\": np.float64(-7.4041), \"['b', 'x', 'b', 'x', 'a', 'y']\": np.float64(0.7715), \"['b', 'x', 'b', 'x', 'b', 'x']\": np.float64(-7.4051), \"['b', 'x', 'b', 'x', 'b', 'y']\": np.float64(0.7697), \"['b', 'x', 'b', 'x']\": np.float64(-8.1844), \"['b', 'x', 'b', 'y', 'a', 'x']\": np.float64(-0.7696), \"['b', 'x', 'b', 'y', 'a', 'y']\": np.float64(0.0801), \"['b', 'x', 'b', 'y', 'b', 'x']\": np.float64(-0.7697), \"['b', 'x', 'b', 'y', 'b', 'y']\": np.float64(0.08), \"['b', 'x', 'b', 'y']\": np.float64(0.8507), \"['b', 'x']\": np.float64(-9.0456), \"['b', 'y', 'a', 'x', 'a', 'x']\": np.float64(-0.7705), \"['b', 'y', 'a', 'x', 'a', 'y']\": np.float64(0.0804), \"['b', 'y', 'a', 'x', 'b', 'x']\": np.float64(-0.7707), \"['b', 'y', 'a', 'x', 'b', 'y']\": np.float64(0.0801), \"['b', 'y', 'a', 'x']\": np.float64(-0.8518), \"['b', 'y', 'a', 'y', 'a', 'x']\": np.float64(-0.0802), \"['b', 'y', 'a', 'y', 'a', 'y']\": np.float64(0.0083), \"['b', 'y', 'a', 'y', 'b', 'x']\": np.float64(-0.0802), \"['b', 'y', 'a', 'y', 'b', 'y']\": np.float64(0.0083), \"['b', 'y', 'a', 'y']\": np.float64(0.0886), \"['b', 'y', 'b', 'x', 'a', 'x']\": np.float64(-0.7707), \"['b', 'y', 'b', 'x', 'a', 'y']\": np.float64(0.0803), \"['b', 'y', 'b', 'x', 'b', 'x']\": np.float64(-0.7708), \"['b', 'y', 'b', 'x', 'b', 'y']\": np.float64(0.0801), \"['b', 'y', 'b', 'x']\": np.float64(-0.8519), \"['b', 'y', 'b', 'y', 'a', 'x']\": np.float64(-0.0801), \"['b', 'y', 'b', 'y', 'a', 'y']\": np.float64(0.0083), \"['b', 'y', 'b', 'y', 'b', 'x']\": np.float64(-0.0801), \"['b', 'y', 'b', 'y', 'b', 'y']\": np.float64(0.0083), \"['b', 'y', 'b', 'y']\": np.float64(0.0885), \"['b', 'y']\": np.float64(0.9415)}\n",
      "It s train loss bro [0.18058520555496216, 0.12636782228946686, 0.12335442751646042, 0.12043645232915878, 0.117610402405262, 0.1148725375533104, 0.11221995949745178, 0.10964911431074142, 0.10715720802545547, 0.10474099218845367, 0.10239748656749725]\n",
      "% good predict : 20\n",
      "\u001b[0;34miteration 254 action \u001b[0;31m a \u001b[0;35m feedback y \u001b[0;31m predict y \u001b[0m\n",
      "for 0 epochs, loss is 2.003445625305176 and val_loss is 4.188620090484619\n",
      "for 1 epochs, loss is 2.362454414367676 and val_loss is 4.1884050369262695\n",
      "for 2 epochs, loss is 2.3610873222351074 and val_loss is 4.187808990478516\n",
      "for 3 epochs, loss is 2.358128309249878 and val_loss is 4.18687105178833\n",
      "for 4 epochs, loss is 2.353754997253418 and val_loss is 4.185626029968262\n",
      "for 5 epochs, loss is 2.348128080368042 and val_loss is 4.184108734130859\n",
      "for 6 epochs, loss is 2.3413963317871094 and val_loss is 4.182344436645508\n",
      "for 7 epochs, loss is 2.3336920738220215 and val_loss is 4.180364608764648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 169\u001b[0m\n\u001b[1;32m    155\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SimpleTokenizerV1(\n\u001b[1;32m    156\u001b[0m     create_all_words_action_outcome_enumerate(env\u001b[38;5;241m.\u001b[39mget_actions(), env\u001b[38;5;241m.\u001b[39mget_outcomes()))\n\u001b[1;32m    158\u001b[0m mymodel \u001b[38;5;241m=\u001b[39m GPTModel({\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39mget_actions()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39mget_outcomes()),\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m51\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device\n\u001b[1;32m    167\u001b[0m     })\n\u001b[0;32m--> 169\u001b[0m evolued_train_loss, evolued_val_loss, good_predicts, finish \u001b[38;5;241m=\u001b[39m \u001b[43mact_in_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmymodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss_act_in_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinish : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinish\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    173\u001b[0m see_evolued_train_loss([finish])\n",
      "Cell \u001b[0;32mIn[70], line 109\u001b[0m, in \u001b[0;36mact_in_env\u001b[0;34m(model, tokenizer, env, valance, iter, path_save, _lr, weight_decay, max_depth)\u001b[0m\n\u001b[1;32m    106\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# train_loss, val_loss = train_simple(model, optimizer, \u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#                         inputs, targets, 50, verbose=True, x_val=x_val, y_val=y_val)\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_for_one_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m save_plot_loss(train_loss, val_loss, path\u001b[38;5;241m=\u001b[39mpath_save, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# We remove 2 word and add 1 word. Beacause compute_expective_valance give us the last word (each action)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[67], line 58\u001b[0m, in \u001b[0;36mtrain_for_one_seq\u001b[0;34m(model, optimizer, inputs, targets, n_iter, x_val, y_val, verbose)\u001b[0m\n\u001b[1;32m     56\u001b[0m all_train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     all_val_loss\u001b[38;5;241m.\u001b[39mappend(val_loss)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn[67], line 6\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, x_val, y_val)\u001b[0m\n\u001b[1;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(x_val, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y_val)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def make_data_set(tokenizer, env, rand_iter:int, context_lenght:int):\n",
    "    \"\"\"\n",
    "    Create a data set from the environment, make action randomly \\\n",
    "    and store (action, feedback) in list of list of tuple for the model\n",
    "    :param tokenizer: the tokenizer to encode the data\n",
    "    :param env: the environment to interact with\n",
    "    :param rand_iter: the number of random iteration\n",
    "    :param context_lenght: the context lenght to use\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    for i in range(rand_iter):\n",
    "        action = np.random.choice(env.get_actions())\n",
    "        feedback = env.outcome(action)\n",
    "        history.append((str(action), str(feedback)))\n",
    "\n",
    "    tmpXfit, tmpYfit = inter_action_and_feedback_size(history, context_lenght)\n",
    "    x_fit = []\n",
    "    for i, one_input in enumerate(tmpXfit):\n",
    "        x_fit.append(tokenizer.encode(one_input))\n",
    "    y_fit = tokenizer.encode(tmpYfit)\n",
    "\n",
    "    return x_fit, y_fit\n",
    "\n",
    "def tempo_recursif_expective_valance(model:nn.Module, env, seq:list,\n",
    "                                    max_depth:int, valance:dict, \n",
    "                                    seuil:float=0.2, proba:float = 1,\n",
    "                                    seq_predi:list = []):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if max_depth == 0:\n",
    "        return {}\n",
    "    max_depth -= 1\n",
    "\n",
    "    if proba < seuil:\n",
    "        return {}\n",
    "    \n",
    "    model.eval()\n",
    "    exceptive_valance = {}\n",
    "    for act in env.get_actions():\n",
    "        new_seq = seq_predi + [act]\n",
    "        seq_to_predict = seq + [tokenizer.encode(act)]\n",
    "        seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.long).to(device)\n",
    "        x = model(seq_to_predict, False)\n",
    "        probs = nn.functional.softmax(x, dim=-1)\n",
    "        # for each outcomes we want proba with act\n",
    "        for out in env.get_outcomes():\n",
    "            tmp_new_seq = new_seq + [out]\n",
    "            tmp_proba = probs[0][tokenizer.encode(out)].item() * proba\n",
    "            if tmp_proba < seuil:\n",
    "                continue\n",
    "            tempo =np.round(valance[(act, out)] * tmp_proba, decimals=4)\n",
    "            # input(f'seq {seq_predi} act {act} out {out} proba {tmp_proba} valance {valance[(act, out)]} tempo {tempo}')\n",
    "\n",
    "            exceptive_valance.update(\n",
    "                tempo_recursif_expective_valance(model, env, \n",
    "                                            seq[2:] + [tokenizer.encode(act), tokenizer.encode(out)],\n",
    "                                            max_depth, valance, seuil, tmp_proba, tmp_new_seq.copy())\n",
    "            )\n",
    "            exceptive_valance[str(tmp_new_seq)] = tempo\n",
    "    return exceptive_valance\n",
    "\n",
    "def compute_expective_valance(model:nn.Module, env,\n",
    "                            seq:list, depth:int, valance:dict):\n",
    "    \"\"\" \n",
    "    Not implemented\n",
    "    \"\"\"\n",
    "    raise Exception(\"Not implemented\")\n",
    "\n",
    "def act_in_env(model: nn.Module, \n",
    "            tokenizer:SimpleTokenizerV1,\n",
    "            env, valance:dict, iter:int,\n",
    "            path_save = \"env\", _lr=1e-2, weight_decay=1e-2, max_depth=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    context_lenght = model.cfg[\"context_length\"]\n",
    "    device = model.cfg[\"device\"]\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=_lr, weight_decay=weight_decay)\n",
    "    evolued_train_loss = []\n",
    "    finish_evolued_train_loss = []\n",
    "    evolued_val_loss = []\n",
    "    good_predicts:list[bool] = []\n",
    "\n",
    "    # TODO delete validation\n",
    "    x_val, y_val = make_data_set(env=env,\n",
    "              rand_iter=1000,\n",
    "              tokenizer=tokenizer,\n",
    "              context_lenght=context_lenght\n",
    "              )\n",
    "\n",
    "    x_val = torch.tensor(x_val, dtype=torch.long).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "    x_train, y_train = make_data_set(env=env,\n",
    "                rand_iter=context_lenght,\n",
    "                tokenizer=tokenizer,\n",
    "                context_lenght=context_lenght\n",
    "                )\n",
    "    assert len(x_train) == 1 and len(y_train) == 1, \"We need to have only one sequence to first train the model\"\n",
    "    assert len(x_train[0]) == context_lenght, \"The context lenght is not correct\"\n",
    "\n",
    "    inputs= torch.tensor(x_train, dtype=torch.long).to(device)\n",
    "    targets = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(0, iter):\n",
    "        # train model\n",
    "        model.train()\n",
    "        # train_loss, val_loss = train_simple(model, optimizer, \n",
    "        #                         inputs, targets, 50, verbose=True, x_val=x_val, y_val=y_val)\n",
    "        train_loss, val_loss = train_for_one_seq(model, optimizer, \n",
    "                                inputs, targets, 11, verbose=True, x_val=x_val, y_val=y_val)\n",
    "        save_plot_loss(train_loss, val_loss, path=path_save, title=f'loss_{i}')\n",
    "        # We remove 2 word and add 1 word. Beacause compute_expective_valance give us the last word (each action)\n",
    "        seq_to_predict = x_train[0][2:] + [targets[-1]]\n",
    "        exceptive_valance = tempo_recursif_expective_valance(model, env, seq_to_predict, max_depth, valance, seuil=0.0001)\n",
    "        print(f'for seq {seq} all exceptive valance {exceptive_valance}')\n",
    "        try:\n",
    "            max_val_pred = eval(max(exceptive_valance, key=exceptive_valance.get))\n",
    "            act = max_val_pred[0]\n",
    "            predi = max_val_pred[1]\n",
    "        except Exception as e:\n",
    "            print(f'\\033[0;31m iterationerror dont find max val pred {e}')\n",
    "            print(\"act chose randomly\\033[0m iteration\")\n",
    "            act = np.random.choice(env.get_actions())\n",
    "\n",
    "        # input(f'for seq {seq} the next action is {act} and the next predi is {predi} in max val pred {max_val_pred}')\n",
    "\n",
    "        # act, predi = compute_expective_valance(model, seq=seq_to_predict, env=env,\n",
    "        #                                        depth=max_depth, valance=valance)\n",
    "        fb = env.outcome(act)\n",
    "        good_predicts.append(predi == fb)\n",
    "        x_train = [x_train[0][2:] + [tokenizer.encode(act)]]\n",
    "        y_train = [tokenizer.encode(fb)]\n",
    "        inputs= torch.tensor(x_train, dtype=torch.long).to(device)\n",
    "        targets= torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        evolued_train_loss.append(train_loss)\n",
    "        evolued_val_loss.append(val_loss)\n",
    "        print(f'It s train loss {train_loss}')\n",
    "        finish_evolued_train_loss.append(train_loss[-1])\n",
    "\n",
    "        print(f'% good predict : {sum(good_predicts[-10:]) * 10 if len(good_predicts) >= 10 else -1}')\n",
    "        print(f'\\033[0;34miteration {i} action \\033[0;31m {act} \\033[0;35m feedback {fb} \\033[0;31m predict {predi} \\033[0m')\n",
    "\n",
    "\n",
    "    return evolued_train_loss, evolued_val_loss, good_predicts, finish_evolued_train_loss\n",
    "\n",
    "\n",
    "valence = {\n",
    "    ('a', 'x') : -10,\n",
    "    ('a', 'y') : 10,\n",
    "    ('b', 'x') : -10,\n",
    "    ('b', 'y') : 10\n",
    "}\n",
    "env = env3Str()\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(\n",
    "    create_all_words_action_outcome_enumerate(env.get_actions(), env.get_outcomes()))\n",
    "\n",
    "mymodel = GPTModel({\n",
    "        \"vocab_size\": len(env.get_actions()) + len(env.get_outcomes()),\n",
    "        \"context_length\": 51,\n",
    "        \"emb_dim\": 16 *8,\n",
    "        \"n_heads\": 1, # 4\n",
    "        \"n_leayers\": 1, # 4\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": device\n",
    "    })\n",
    "\n",
    "evolued_train_loss, evolued_val_loss, good_predicts, finish = act_in_env(\n",
    "    mymodel, tokenizer, env, valence, 300, \"loss_act_in_env\", \n",
    "    _lr=1e-3, weight_decay=1e-3, max_depth=3)\n",
    "print(f'finish : {finish}')\n",
    "see_evolued_train_loss([finish])\n",
    "\n",
    "see_evolued_train_loss(evolued_train_loss)\n",
    "\n",
    "see_evolued_train_loss(evolued_val_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
