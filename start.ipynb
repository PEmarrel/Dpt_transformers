{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following file was inspired by the following tutorial:\n",
    "# https://colab.research.google.com/drive/1SiF0KZJp75rUeetKOWqpsA8clmHP6jMg?usp=sharing#scrollTo=d7utFz27cO9q\n",
    "\n",
    "# And use this cource for explanation:\n",
    "# https://bruno-yun.notion.site/The-Transformer-Model-for-NLG-c4413bd5a8044325a7658cb8ff5535f2\n",
    "# https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro, bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporaire\n",
    "VOCAB_SIZE = 2 # For now we are only using binary data (0, 1) later can be modify by 12\n",
    "CONTEXT_LENGHT = 3 # And we are using a context of 3 bits (may be changed later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads,\n",
    "                 qkv_bias = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        assert( d_out % num_heads == 0), \"d_out should be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        # self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        # self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        # self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias) \n",
    "        # Don't use this because we want to have only one projection to optimize\n",
    "        # To have only one projection we use the following line\n",
    "        self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias) \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer('mask', \n",
    "                            torch.triu(torch.ones(context_length,context_length), \n",
    "                                                diagonal=1))\n",
    "        \n",
    "    def forward(self,x: torch.Tensor):\n",
    "        queries: torch.Tensor\n",
    "        keys: torch.Tensor\n",
    "        values: torch.Tensor\n",
    "        b, num_tokens, d_in = x.shape # b, num_token, d_in\n",
    "\n",
    "        # self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Use one projection to get queries, keys and values\n",
    "        # self.W_qkv(x) -> b, num_token, 3*d_out (is a tensor)\n",
    "        # chunk(3, dim=-1) -> b, num_token, d_out we split the tensor in 3 parts\n",
    "        queries, keys, values= self.W_qkv(x).chunk(3, dim=-1)\n",
    "        \n",
    "        # b, num_token, numheads, head_dim\n",
    "        queries = queries.reshape(b,\n",
    "                                num_tokens,\n",
    "                                self.num_heads,\n",
    "                                self.head_dim\n",
    "                            ).transpose(1, 2)\n",
    "        keys = keys.reshape(b,\n",
    "                            num_tokens,\n",
    "                            self.num_heads,\n",
    "                            self.head_dim\n",
    "                        ).transpose(1, 2)\n",
    "        values = values.reshape(b,\n",
    "                                num_tokens,\n",
    "                                self.num_heads,\n",
    "                                self.head_dim\n",
    "                            ).transpose(1, 2)\n",
    "        \n",
    "        # b, num_heads, num_token, num_token\n",
    "        attn_scores = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "        \n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask[:num_tokens, :num_tokens].unsqueeze(0).unsqueeze(0).bool() == 1, \n",
    "            float('-inf')\n",
    "        )\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.einsum('bhqk, bhkd -> bhqd', \n",
    "                               attn_weights, \n",
    "                               values\n",
    "                            ).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        \n",
    "        \n",
    "        return self.out_proj(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain of class MultiHeadAttention\n",
    "In transformers model, the **MultiHeadAttention** class is used to apply attention mechanisms to input sequences. Understanding how this class works requires first exploring **single-head** attention before generalizing to **multi-head attention**.\n",
    "\n",
    "## Single-Head Attention\n",
    "Attention in Transformers relies on measuring the similarity between tokens in a sequence. This measurement is performed using three distinct transformations:\n",
    "\n",
    "- Query (Q): Represents the current token’s focus when comparing it to all other tokens.\n",
    "- Key (K): Represents a token being compared to the query.\n",
    "- Value (V): Represents the actual information to be aggregated based on attention scores.\n",
    "\n",
    "These three representations are obtained by applying linear transformations to the input sequence. The transformed matrices allow us to compute the attention scores using the scaled dot-product attention formula:\n",
    "\n",
    "$ \n",
    "score(x_i, x_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}} \n",
    "$\n",
    "\n",
    "Where $q_i$ is the query representation of token $i$, $k_j$ is the key representation of token $j$, and $d_k$ is the dimension of the key representation. \n",
    "\n",
    "In forward function is :\n",
    "```python\n",
    "attn_scores = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "```\n",
    "\n",
    "To prevent a token from attending to future tokens (as in autoregressive models like GPT), we apply a mask to the attention scores. This ensures that each token only attends to itself and previous tokens:\n",
    "\n",
    "$\n",
    "a_i = \\sum_{j \\leq i} \\alpha_{ij}v_j\n",
    "$\n",
    "\n",
    "In forward function is :\n",
    "```python\n",
    "attn_scores = attn_scores.masked_fill(self.mask[:num_tokens, :num_tokens] == 0,\n",
    "                                float('-inf'))\n",
    "```\n",
    "\n",
    "Once the attention scores are computed and masked, they are normalized using the softmax function to produce attention weights:\n",
    "\n",
    "$\n",
    "\\alpha_{ij} = \\frac{\\exp(score(x_i,x_j))}{\\sum_j \\exp(score(x_i,x_j))}\n",
    "$\n",
    "\n",
    "In forward function is :\n",
    "```python\n",
    "attn_scores = nn.fonctional.softmax(attn_scores, dim=-1)\n",
    "```\n",
    "\n",
    "To prevent overfitting, a dropout layer is applied to the attention weights (self.dropout) before computing the context vector.\n",
    "\n",
    "To compute the context vector, we multiply the attention weights by the Value (V) representations to obtain the context vector.\n",
    "\n",
    "In forward function is :\n",
    "\n",
    "```python\n",
    "context = torch.einsum('bhqk, bhkd -> bhqd', attn_weights, values)\n",
    "```\n",
    "\n",
    "To resume we have done this :\n",
    "\n",
    "![explain one head](./img/ExplainOneHeadAttention.png)\n",
    "*Source: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/9.pdf)*\n",
    "\n",
    "## Multi-Head Attention\n",
    "Single-head attention allows each token to focus on others, but it has limitations: it can only capture one type of relationship at a time. Multi-head attention improves this by using multiple attention heads in parallel.\n",
    "Instead of computing attention once, we split the input into multiple heads, each with a different representation of the sequence. This allows the model to capture diverse patterns.\n",
    "All heads are concatenated and linearly transformed to produce the final output. This process is implemented in the **MultiHeadAttention**, is why we use self.head_dim or 'd' in code.\n",
    "\n",
    "This figure resume compute with matrix :\n",
    "\n",
    "![Compute](./img/ExplainCompute.png)\n",
    "*Source: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/9.pdf)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The FeedForward in transformer\n",
    "The **FeedForward** class is used to apply a feedforward neural network to the output of the multi-head attention layer. The feedforward network consists of two linear transformations with a GELU (or RELU) activation function in between.\n",
    "\n",
    "## GELU and RELU\n",
    "GELU and RELU are activation functions used in neural networks after linear transformations. GELU is a smoother version of RELU that has been shown to improve performance in transformer models. \n",
    "\n",
    "The GELU function is defined as:\n",
    "\n",
    "$\n",
    "GELU(x) = 0.5x(1 + tanh(\\sqrt{2/\\pi}(x + 0.044715x^3)))\n",
    "$\n",
    "\n",
    "RELU is a simpler activation function that sets all negative values to zero:\n",
    "\n",
    "$\n",
    "RELU(x) = max(0, x)\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm2 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # print(\"we are in one transformer block\")\n",
    "        # print(x.shape)\n",
    "        # print(x)\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        # print(\"after att\", x.shape)\n",
    "        # print(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x+ shortcut\n",
    "        # print(x.shape)\n",
    "        # print(x)\n",
    "        # print('end trs block')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain of class TransformerBlock\n",
    "The **TransformerBlock** class is the core building block of the transformer model. It consists of three main components: the **MultiHeadAttention**, the **FeedForward** layers and **LayerNorm**. The **MultiHeadAttention** layer is used to capture the relationships between tokens, while the **FeedForward** layer is used to apply non-linear transformations to the output of the attention layer. We use **LayerNorm** to normalize X and for normalize the output of the **FeedForward** layer.\n",
    "\n",
    "![image.png](./img/ExplainArchiTrans.png)\n",
    "*Source: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/9.pdf)*\n",
    "\n",
    "In this figure we can see the Residual Stream. In the forward funciton it's name shortcut. It's a way to avoid the vanishing gradient problem. The output of the **FeedForward** layer is added to the input of the **MultiHeadAttention** layer. This allows the model to learn the difference between the input and output of the block, which helps to improve performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Embeddings\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.trf_blocks = nn.ModuleList(\n",
    "                [TransformerBlock(cfg) for _ in range(cfg[\"n_leayers\"])]\n",
    "            )\n",
    "        \n",
    "        # Normalization & Output layer\n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "        # Weight tying: Share weights between embedding and output projection\n",
    "        self.tok_emb.weight = self.out_head.weight\n",
    "        \n",
    "        # Initialize weights (If we need)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # Special initialization for transformer projection layers\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p,\n",
    "                                    mean=0.0,\n",
    "                                    std=0.02 / math.sqrt(2 * self.cfg[\"n_layers\"]))\n",
    "                \n",
    "    def forward(self, x: torch.Tensor, return_full_sequence=True):\n",
    "        b, seq_len = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # print(\"in GPT, forward \",x)\n",
    "        \n",
    "        # Token & Positional Embeddings\n",
    "        # (b, seq_len, emb_dim)\n",
    "        tok_embeds = self.tok_emb(x)\n",
    "        # print(\"tok_embeds \",tok_embeds)\n",
    "        # (seq_len, emb_dim)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=device))\n",
    "        # print(\"pos_embeds \",pos_embeds)\n",
    "        \n",
    "        x = tok_embeds + pos_embeds\n",
    "        # print(\"tok_embeds + pos_embeds \",x)\n",
    "        x = self.drop_emb(x)\n",
    "        # print(\"self.drop_emb(x) \",x)\n",
    "        \n",
    "        # Pass through Transformer Blocks\n",
    "        deleteme = 0\n",
    "        for block in self.trf_blocks:\n",
    "            deleteme += 1\n",
    "            x = block(x)\n",
    "            # print(f\"block(x) n°{deleteme} : {x}\")\n",
    "        \n",
    "        # Final normalization & output projection\n",
    "        x = self.final_norm(x)\n",
    "        # print(\"self.final_norm(x) \",x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        # print(\"logits \",logits)\n",
    "        \n",
    "        if not return_full_sequence:\n",
    "            logits = logits[:, -1, :]  # (b, vocab_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M_small = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_leayers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "OUR_CONFIG = {\n",
    "    \"vocab_size\": VOCAB_SIZE,\n",
    "    \"context_length\": CONTEXT_LENGHT,\n",
    "    \"emb_dim\": 16,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_leayers\": 4,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0],\n",
       " [0, 0, 1],\n",
       " [0, 1, 0],\n",
       " [0, 1, 1],\n",
       " [1, 0, 0],\n",
       " [1, 0, 1],\n",
       " [1, 1, 0],\n",
       " [1, 1, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def all_possible(n, k):\n",
    "    # return all possible lists of k elements, each in range of [0,n)\n",
    "    if k == 0:\n",
    "        yield []\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            for c in all_possible(n, k - 1):\n",
    "                yield [i] + c\n",
    "                \n",
    "list(all_possible(VOCAB_SIZE, CONTEXT_LENGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [0, 0, 0] ---> [0.5524600744247437, 0.44753989577293396]\n",
      "input [0, 0, 1] ---> [0.47954434156417847, 0.5204555988311768]\n",
      "input [0, 1, 0] ---> [0.5379564166069031, 0.46204355359077454]\n",
      "input [0, 1, 1] ---> [0.4603937268257141, 0.5396062731742859]\n",
      "input [1, 0, 0] ---> [0.5444108247756958, 0.4555891156196594]\n",
      "input [1, 0, 1] ---> [0.4618186056613922, 0.5381813645362854]\n",
      "input [1, 1, 0] ---> [0.5288079977035522, 0.47119200229644775]\n",
      "input [1, 1, 1] ---> [0.4674224555492401, 0.5325775146484375]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"423pt\" height=\"373pt\"\n",
       " viewBox=\"0.00 0.00 422.66 372.99\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 368.99)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-368.99 418.66,-368.99 418.66,4 -4,4\"/>\n",
       "<!-- 000 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>000</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"316.06\" cy=\"-66.18\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"316.06\" y=\"-62.48\" font-family=\"Times,serif\" font-size=\"14.00\">000</text>\n",
       "</g>\n",
       "<!-- 000&#45;&gt;000 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>000&#45;&gt;000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M342.87,-72.89C353.63,-73.27 362.66,-71.03 362.66,-66.18 362.66,-63 358.77,-60.94 353.06,-60.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"353.04,-56.5 342.87,-59.47 352.68,-63.49 353.04,-56.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"388.66\" y=\"-62.48\" font-family=\"Times,serif\" font-size=\"14.00\">0(55%)</text>\n",
       "</g>\n",
       "<!-- 001 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>001</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"83.43\" cy=\"-66.18\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"83.43\" y=\"-62.48\" font-family=\"Times,serif\" font-size=\"14.00\">001</text>\n",
       "</g>\n",
       "<!-- 000&#45;&gt;001 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>000&#45;&gt;001</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287.07,-66.18C245.48,-66.18 168.81,-66.18 122.33,-66.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.05,-62.68 112.05,-66.18 122.05,-69.68 122.05,-62.68\"/>\n",
       "<text text-anchor=\"middle\" x=\"178.7\" y=\"-69.98\" font-family=\"Times,serif\" font-size=\"14.00\">1(45%)</text>\n",
       "</g>\n",
       "<!-- 010 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>010</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"35.25\" cy=\"-182.5\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.25\" y=\"-178.8\" font-family=\"Times,serif\" font-size=\"14.00\">010</text>\n",
       "</g>\n",
       "<!-- 001&#45;&gt;010 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>001&#45;&gt;010</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M76.11,-83.85C68.21,-102.91 55.53,-133.51 46.39,-155.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"43.07,-154.47 42.47,-165.05 49.53,-157.15 43.07,-154.47\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.25\" y=\"-123.51\" font-family=\"Times,serif\" font-size=\"14.00\">0(48%)</text>\n",
       "</g>\n",
       "<!-- 011 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>011</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"199.74\" cy=\"-346.99\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"199.74\" y=\"-343.29\" font-family=\"Times,serif\" font-size=\"14.00\">011</text>\n",
       "</g>\n",
       "<!-- 001&#45;&gt;011 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>001&#45;&gt;011</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M90.7,-83.75C110.41,-131.32 164.91,-262.91 188.56,-319.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185.41,-321.53 192.47,-329.43 191.87,-318.85 185.41,-321.53\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.63\" y=\"-205.67\" font-family=\"Times,serif\" font-size=\"14.00\">1(52%)</text>\n",
       "</g>\n",
       "<!-- 100 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>100</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"199.74\" cy=\"-18\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"199.74\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">100</text>\n",
       "</g>\n",
       "<!-- 010&#45;&gt;100 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>010&#45;&gt;100</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.72,-167.02C79.87,-137.87 143.02,-74.72 177.01,-40.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"179.59,-43.1 184.19,-33.55 174.64,-38.15 179.59,-43.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"87.86\" y=\"-107.68\" font-family=\"Times,serif\" font-size=\"14.00\">0(54%)</text>\n",
       "</g>\n",
       "<!-- 101 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>101</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"83.43\" cy=\"-298.81\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"83.43\" y=\"-295.11\" font-family=\"Times,serif\" font-size=\"14.00\">101</text>\n",
       "</g>\n",
       "<!-- 010&#45;&gt;101 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>010&#45;&gt;101</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M37.47,-200.52C43.24,-220.05 55.81,-251.29 66.53,-273.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.5,-275.08 71.14,-282.42 69.75,-271.92 63.5,-275.08\"/>\n",
       "<text text-anchor=\"middle\" x=\"26\" y=\"-240.72\" font-family=\"Times,serif\" font-size=\"14.00\">1(46%)</text>\n",
       "</g>\n",
       "<!-- 110 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>110</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"364.24\" cy=\"-182.5\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"364.24\" y=\"-178.8\" font-family=\"Times,serif\" font-size=\"14.00\">110</text>\n",
       "</g>\n",
       "<!-- 011&#45;&gt;110 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>011&#45;&gt;110</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.22,-331.52C244.36,-302.37 307.52,-239.21 341.5,-205.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"344.09,-207.6 348.68,-198.05 339.14,-202.65 344.09,-207.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"252.36\" y=\"-272.17\" font-family=\"Times,serif\" font-size=\"14.00\">0(46%)</text>\n",
       "</g>\n",
       "<!-- 111 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>111</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"316.06\" cy=\"-298.81\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"316.06\" y=\"-295.11\" font-family=\"Times,serif\" font-size=\"14.00\">111</text>\n",
       "</g>\n",
       "<!-- 011&#45;&gt;111 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>011&#45;&gt;111</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M223.84,-337.01C240.84,-329.97 263.84,-320.44 282.61,-312.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"284.09,-315.84 291.99,-308.78 281.41,-309.38 284.09,-315.84\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.23\" y=\"-313.64\" font-family=\"Times,serif\" font-size=\"14.00\">1(54%)</text>\n",
       "</g>\n",
       "<!-- 100&#45;&gt;000 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>100&#45;&gt;000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M223.84,-27.98C240.84,-35.02 263.84,-44.55 282.61,-52.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281.41,-55.62 291.99,-56.21 284.09,-49.15 281.41,-55.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.23\" y=\"-43.95\" font-family=\"Times,serif\" font-size=\"14.00\">0(54%)</text>\n",
       "</g>\n",
       "<!-- 100&#45;&gt;001 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>100&#45;&gt;001</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M175.64,-27.98C158.64,-35.02 135.65,-44.55 116.87,-52.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.39,-49.15 107.5,-56.21 118.07,-55.62 115.39,-49.15\"/>\n",
       "<text text-anchor=\"middle\" x=\"120.26\" y=\"-28.95\" font-family=\"Times,serif\" font-size=\"14.00\">1(46%)</text>\n",
       "</g>\n",
       "<!-- 101&#45;&gt;010 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>101&#45;&gt;010</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M81.2,-280.79C75.43,-261.26 62.86,-230.02 52.14,-207.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"55.17,-206.23 47.53,-198.89 48.92,-209.39 55.17,-206.23\"/>\n",
       "<text text-anchor=\"middle\" x=\"92.67\" y=\"-248.19\" font-family=\"Times,serif\" font-size=\"14.00\">0(46%)</text>\n",
       "</g>\n",
       "<!-- 101&#45;&gt;011 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>101&#45;&gt;011</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M107.53,-308.8C124.53,-315.84 147.52,-325.36 166.29,-333.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.09,-336.43 175.67,-337.02 167.77,-329.96 165.09,-336.43\"/>\n",
       "<text text-anchor=\"middle\" x=\"110.91\" y=\"-324.77\" font-family=\"Times,serif\" font-size=\"14.00\">1(54%)</text>\n",
       "</g>\n",
       "<!-- 110&#45;&gt;100 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>110&#45;&gt;100</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M348.76,-167.02C319.62,-137.87 256.46,-74.72 222.48,-40.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"224.84,-38.15 215.3,-33.55 219.89,-43.1 224.84,-38.15\"/>\n",
       "<text text-anchor=\"middle\" x=\"259.62\" y=\"-107.68\" font-family=\"Times,serif\" font-size=\"14.00\">0(53%)</text>\n",
       "</g>\n",
       "<!-- 110&#45;&gt;101 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>110&#45;&gt;101</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M340.3,-192.41C290.27,-213.13 174.55,-261.07 117.07,-284.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.6,-281.69 107.71,-288.76 118.28,-288.16 115.6,-281.69\"/>\n",
       "<text text-anchor=\"middle\" x=\"202.68\" y=\"-242.44\" font-family=\"Times,serif\" font-size=\"14.00\">1(47%)</text>\n",
       "</g>\n",
       "<!-- 111&#45;&gt;110 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>111&#45;&gt;110</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M323.38,-281.15C331.27,-262.08 343.95,-231.48 353.09,-209.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"356.42,-210.53 357.01,-199.95 349.95,-207.85 356.42,-210.53\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.23\" y=\"-249.08\" font-family=\"Times,serif\" font-size=\"14.00\">0(47%)</text>\n",
       "</g>\n",
       "<!-- 111&#45;&gt;111 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>111&#45;&gt;111</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M342.87,-305.52C353.63,-305.9 362.66,-303.66 362.66,-298.81 362.66,-295.63 358.77,-293.57 353.06,-292.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"353.04,-289.13 342.87,-292.1 352.68,-296.12 353.04,-289.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"388.66\" y=\"-295.11\" font-family=\"Times,serif\" font-size=\"14.00\">1(53%)</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f77d81a7dd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "gpt = GPTModel(OUR_CONFIG)\n",
    "def plot_model():\n",
    "    dot = Digraph(comment='Baby GPT', engine='circo')\n",
    "\n",
    "    for xi in all_possible(VOCAB_SIZE, CONTEXT_LENGHT):\n",
    "        \n",
    "        # forward the GPT and get probabilities for next token\n",
    "        x = torch.tensor(xi, dtype=torch.long)[None, ...]\n",
    "        # turn the list into a torch tensor and add a batch dimension\n",
    "        logits = gpt(x, False) # forward the gpt neural net\n",
    "        # print('logits :', logits)\n",
    "        probs = nn.functional.softmax(logits, dim=-1) # get the probabilities\n",
    "        y = probs[0].tolist() # remove the batch dimension and unpack the tensor into simple list\n",
    "        print(f\"input {xi} ---> {y}\")\n",
    "\n",
    "        # also build up the transition graph for plotting later\n",
    "        current_node_signature = \"\".join(str(d) for d in xi)\n",
    "        dot.node(current_node_signature)\n",
    "        # input(\"Press Enter to continue...\")\n",
    "\n",
    "        for t in range(VOCAB_SIZE):\n",
    "            next_node = xi[1:] + [t] # crop the context and append the next character\n",
    "            next_node_signature = \"\".join(str(d) for d in next_node)\n",
    "            p = y[t]\n",
    "\n",
    "            label=f\"{t}({p*100:.0f}%)\"\n",
    "            dot.edge(current_node_signature, next_node_signature, label=label)\n",
    "    \n",
    "    return dot\n",
    "\n",
    "plot_model()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0],\n",
       "         [0],\n",
       "         [0]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(OUR_CONFIG)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.randint(0, VOCAB_SIZE, (1, CONTEXT_LENGHT)))\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) \n",
    "output = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
