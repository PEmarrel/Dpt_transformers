{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Pour torch si vous avez un GPU\n",
    "# device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "device = \"cpu\" # Pour forcer l'utilisation du CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environement potentielement testé\n",
    "from environnement.environnement import Environnement as env # mother class\n",
    "from environnement.environnement1 import Environnement1 as env1\n",
    "from environnement.environnement2Str import Environnement2 as env2Str\n",
    "from environnement.environnement3Str import Environnement3 as env3Str\n",
    "from environnement.environnement6Str import Environnement6 as env6Str\n",
    "from environnement.small_loop import small_loop\n",
    "from environnement.small_loop import WorldDisplay\n",
    "from ipywidgets import Output\n",
    "\n",
    "# model machine learning\n",
    "from model.DeepNN import *\n",
    "from model.Tokenizer import *\n",
    "from model.RNN import *\n",
    "from model.CustomDataSet import CustomDataSet, CustomDataSetRNN\n",
    "from outil import *\n",
    "from inter.interactions import Interaction\n",
    "from inter.simpleInteraction import simpleInteraction as inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentLSTM:\n",
    "    def __init__(self, model, all_outcomes, all_actions, tokenizer, valence:dict,\n",
    "                optimizer, gap_train=11, gap_test=11, loss_func=None):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self.optimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._tokenizer:SimpleTokenizerV1 = tokenizer\n",
    "        self._all_outcomes = all_outcomes\n",
    "        self._all_actions = all_actions\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "        self._valence = valence\n",
    "        self._gap_train = gap_train\n",
    "        self._gap_test = gap_test\n",
    "\n",
    "    def fit(self, actions:list, outcomes:list, nb_epoch:int=25, context_lenght=5, validate_loader=None):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent \n",
    "        Avec data set custom, le model prends en inputs plusieurs données\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(actions) + len(outcomes) < context_lenght:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            data_loarder = CustomDataSetRNN(actions=actions, outcomes=outcomes,\n",
    "                    context_lenght=context_lenght, dim_out=2, tokenizer=self._tokenizer)\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                data_loarder, batch_size=32, shuffle=True)\n",
    "            for e in range(nb_epoch):\n",
    "                for x,t in data_loader:\n",
    "                    bs = t.shape[0]\n",
    "                    h = torch.zeros(self._model.num_layers, bs, self._model.hidden_size, device=device)\n",
    "                    cell = torch.zeros(self._model.num_layers, bs, self._model.hidden_size, device=device)\n",
    "\n",
    "                    pred, h, cell = self._model(x, h, cell)\n",
    "\n",
    "                    loss = self._loss_func(pred[:, -1, :], t)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "        else: # Si le model n'est pas un model pytorch\n",
    "            raise Exception('Not implemented')\n",
    "            self._model.fit(action, outcome)\n",
    "            pass\n",
    "\n",
    "    def predict(self, action, gap=5):\n",
    "        if len(self._history_act) + len(self._history_fb) < gap:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        action = self._tokenizer.encode(x)\n",
    "        print(f\"Action: {action}\")\n",
    "\n",
    "        action = torch.tensor([action], dtype=torch.int).to(device)\n",
    "\n",
    "        h = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "        cell = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "        \n",
    "        with torch.no_grad():  # Pas de calcul de gradients en mode prédiction\n",
    "            pred, _, _ = self._model(action, h, cell)\n",
    "\n",
    "        pred_feedback = torch.argmax(pred[:, -1, :]).item()\n",
    "        pred_feedback = self._tokenizer.decode(pred_feedback)\n",
    "        \n",
    "        return pred_feedback\n",
    "    \n",
    "    def recursif_expective_valance(self, seq:list, max_depth:int, seuil:float=0.2, proba:float = 1,\n",
    "                                    seq_predi:list = []):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if max_depth == 0:\n",
    "            return {}\n",
    "        max_depth -= 1\n",
    "\n",
    "        if proba < seuil:\n",
    "            return {}\n",
    "        \n",
    "        self._model.eval()\n",
    "        exceptive_valance = {}\n",
    "        for act in self._all_actions:\n",
    "            new_seq = seq_predi + [act]\n",
    "            seq_to_predict = seq + [self._tokenizer.encode(act)]\n",
    "            seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.int).to(device)\n",
    "\n",
    "            hidden = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "            memory = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "\n",
    "            x, _, _ = self._model(seq_to_predict, hidden, memory)\n",
    "            x = x[0, -1, :]\n",
    "            # Transforme x into list proba\n",
    "            probs = torch.nn.functional.softmax(x, dim=0).tolist()\n",
    "            # for each outcomes we want proba with act\n",
    "            for i, out in enumerate(self._all_outcomes):\n",
    "                tmp_new_seq = new_seq + [out]\n",
    "                tmp_proba = probs[i] * proba\n",
    "                if tmp_proba < seuil:\n",
    "                    continue\n",
    "                tempo = float(np.round(self._valence[inter(act, out)] * tmp_proba, decimals=4))\n",
    "                # input(f'seq {seq_predi} act {act} out {out} proba {tmp_proba} valance {valance[(act, out)]} tempo {tempo}')\n",
    "                exceptive_valance.update(\n",
    "                    self.recursif_expective_valance(seq=seq[2:] + [self._tokenizer.encode(act), self._tokenizer.encode(out)],\n",
    "                                                max_depth=max_depth, seuil=seuil, \n",
    "                                                proba=tmp_proba, seq_predi=tmp_new_seq.copy())\n",
    "                )\n",
    "                exceptive_valance[str(tmp_new_seq)] = tempo\n",
    "        return exceptive_valance\n",
    "        \n",
    "    def decide(self):\n",
    "        x = []\n",
    "        for i in range(len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        seq = self._tokenizer.encode(x)\n",
    "        res = self.recursif_expective_valance(seq=seq, max_depth=3, seuil=0.4)\n",
    "        top_5 = sorted(res.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        # print(f\"Top 5 of sequences with the best expected valance for {x}\")\n",
    "        # for top in top_5:\n",
    "        #     print(f\"Sequence: {top[0]} Expected valance: {top[1]}\")\n",
    "        \n",
    "        # print(f\"Action choisie : {eval(top_5[0][0])[0]}\")\n",
    "        return eval(top_5[0][0])[0]\n",
    "\n",
    "    # Modifier\n",
    "    def action(self, outcome, decide=True, validate_loader=None):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \" \n",
    "                  f\"\\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\")\n",
    "            \n",
    "            if len(self._history_act) + len(self._history_fb) > self._gap_train:\n",
    "                if self._predicted_outcome != outcome:\n",
    "                    self.fit(self._history_act, self._history_fb, validate_loader=validate_loader,\n",
    "                            nb_epoch=25, context_lenght=self._gap_train)\n",
    "                else:\n",
    "                    self._action = self._all_actions[0]\n",
    "\n",
    "            if decide:\n",
    "                self._action = self.decide()\n",
    "            else :\n",
    "                self._action = str(np.random.choice(self._all_actions))\n",
    "            if len(self._history_act) + len(self._history_fb) > self._gap_test:\n",
    "                self._predicted_outcome = self.predict(self._action, gap=self._gap_test)\n",
    "            self._history_act.append(self._action)\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)            \n",
    "            # print(f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\")\n",
    "        \n",
    "        return self._action, self._predicted_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\u001b[0;32m iteration 0 \u001b[0m=======================\n",
      "action forward predi None outcome wall\n",
      "Action choisie : forward \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 1 \u001b[0m=======================\n",
      "Action: forward, Prediction: None, Outcome: wall, \u001b[0;31m Satisfaction: False \u001b[0m\n",
      "action feel_front predi None outcome wall\n",
      "Action choisie : feel_front \u001b[0;34m0 \u001b[0m\n",
      "\n",
      "\n",
      "=======================\u001b[0;32m iteration 2 \u001b[0m=======================\n",
      "Action: feel_front, Prediction: None, Outcome: wall, \u001b[0;31m Satisfaction: False \u001b[0m\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Not enough data to train model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m150\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======================\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0;32m iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m=======================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     action, predi \u001b[38;5;241m=\u001b[39m \u001b[43magent_test2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     outcome \u001b[38;5;241m=\u001b[39m env_test2\u001b[38;5;241m.\u001b[39moutcome(action)\n\u001b[1;32m     38\u001b[0m     history_good\u001b[38;5;241m.\u001b[39mappend(outcome \u001b[38;5;241m==\u001b[39m predi)\n",
      "Cell \u001b[0;32mIn[3], line 155\u001b[0m, in \u001b[0;36mAgentLSTM.action\u001b[0;34m(self, outcome, decide, validate_loader)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history_act) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history_fb) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gap_train:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicted_outcome \u001b[38;5;241m!=\u001b[39m outcome:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_history_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_history_fb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnb_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_actions[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mAgentLSTM.fit\u001b[0;34m(self, actions, outcomes, nb_epoch, context_lenght, validate_loader)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mFonction d'entrainement de l'agent \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mAvec data set custom, le model prends en inputs plusieurs données\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(actions) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(outcomes) \u001b[38;5;241m<\u001b[39m context_lenght:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough data to train model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     34\u001b[0m     data_loarder \u001b[38;5;241m=\u001b[39m CustomDataSetRNN(actions\u001b[38;5;241m=\u001b[39mactions, outcomes\u001b[38;5;241m=\u001b[39moutcomes,\n\u001b[1;32m     35\u001b[0m             context_lenght\u001b[38;5;241m=\u001b[39mcontext_lenght, dim_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer)\n",
      "\u001b[0;31mException\u001b[0m: Not enough data to train model"
     ]
    }
   ],
   "source": [
    "env_test2 = small_loop(x=1, y=1, theta=0)\n",
    "\n",
    "model_ML = LSTM(hidden_size=128, num_emb=len(env_test2.get_outcomes() + env_test2.get_actions()),\n",
    "                num_layers=2, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 5,\n",
    "    inter('forward', 'wall') : -60,\n",
    "    inter('turn_left', 'empty') : -7,\n",
    "    inter('turn_left', 'wall') : -7,\n",
    "    inter('turn_right', 'empty') : -7,\n",
    "    inter('turn_right', 'wall') : -7,\n",
    "    inter('feel_front', 'wall') : -5,\n",
    "    inter('feel_front', 'empty') : -5,\n",
    "}\n",
    "\n",
    "agent_test2 = AgentLSTM(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    valence=valence,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func,\n",
    "    gap_train=3,\n",
    "    gap_test=3)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "for i in range(150):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, decide=True)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    # print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    # print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    # print(\"\\n\")\n",
    "    # sleep(0.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnvDisplay = WorldDisplay()\n",
    "out = Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: turn_right, Prediction: empty, Outcome: empty, \u001b[0;31m Satisfaction: True \u001b[0m\n",
      "Action: [4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "EnvDisplay.show(env_test2.get_world(), env_test2.get_robot(), out)\n",
    "\n",
    "action, predi = agent_test2.action(outcome, decide=True)\n",
    "outcome = env_test2.outcome(action)\n",
    "history_good.append(outcome == predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porba si on avance\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 2 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrédiction de la séquence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_seg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m :  probabilité \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, decode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeocde\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mporba si on avance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mshow_proba_from_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mempty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m show_proba_from_seq([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m show_proba_from_seq([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mturn_left\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mempty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mshow_proba_from_seq\u001b[0;34m(_seg)\u001b[0m\n\u001b[1;32m      6\u001b[0m predi, _, _ \u001b[38;5;241m=\u001b[39m  agent_test2\u001b[38;5;241m.\u001b[39m_model(seq, h, cell)\n\u001b[1;32m      7\u001b[0m prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(predi, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m deocde \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrédiction de la séquence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_seg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m :  probabilité \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, decode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeocde\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 2 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "def show_proba_from_seq(_seg):\n",
    "    seq = tokenizer.encode(_seg)\n",
    "    seq = torch.tensor([seq], dtype=torch.int).to(device)\n",
    "    h = torch.zeros(agent_test2._model.num_layers, 1, agent_test2._model.hidden_size, device=device)\n",
    "    cell = torch.zeros(agent_test2._model.num_layers, 1, agent_test2._model.hidden_size, device=device)\n",
    "    predi, _, _ =  agent_test2._model(seq, h, cell)\n",
    "    prob = torch.nn.functional.softmax(predi, dim=1)\n",
    "    deocde = tokenizer.decode(torch.argmax(predi, dim=1).item())\n",
    "    print(f\"Prédiction de la séquence {_seg} :  probabilité {prob.tolist()}, decode {deocde}\")\n",
    "\n",
    "print(\"porba si on avance\")\n",
    "show_proba_from_seq(['forward', 'empty', 'forward'])\n",
    "show_proba_from_seq(['forward', 'wall', 'forward'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'forward'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'forward'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'forward'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'forward'])\n",
    "\n",
    "print(\"porba si on turn left\")\n",
    "show_proba_from_seq(['forward', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['forward', 'wall', 'turn_left'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'turn_left'])\n",
    "\n",
    "print(\"porba si on turn right\")\n",
    "show_proba_from_seq(['forward', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['forward', 'wall', 'turn_right'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'turn_right'])\n",
    "\n",
    "print(\"porba si on feel front\")\n",
    "show_proba_from_seq(['forward', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['forward', 'wall', 'feel_front'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'feel_front'])\n",
    "\n",
    "def count_pattern(lst, pattern):\n",
    "    return sum(1 for i in range(len(lst) - len(pattern) + 1) if lst[i:i+len(pattern)] == pattern)\n",
    "\n",
    "list_act_out = []\n",
    "for act, out in zip(agent_test2._history_act, agent_test2._history_fb):\n",
    "    list_act_out.append(act)\n",
    "    list_act_out.append(out)\n",
    "\n",
    "print(\"count si on avance\")\n",
    "print(f\"pattern 'forward', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['forward', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['forward', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'forward', 'empty' présent : {count_pattern(list_act_out, ['forward', 'wall', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'forward', 'wall' présent : {count_pattern(list_act_out, ['forward', 'wall', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'forward', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'forward', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'forward', 'wall'])}\")\n",
    "\n",
    "print(\"count si on feel_front\")\n",
    "print(f\"pattern 'forward', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['forward', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['forward', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['forward', 'wall', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['forward', 'wall', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'feel_front', 'wall'])}\")\n",
    "\n",
    "print(\"count si on turn_left turn_right\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'turn_left', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'turn_left', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'turn_left', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'turn_left', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'turn_right', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'turn_right', 'empty'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'turn_right', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'turn_right', 'empty'])}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
