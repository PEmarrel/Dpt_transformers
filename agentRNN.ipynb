{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from ipywidgets import Output\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Pour torch si vous avez un GPU\n",
    "# device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "device = \"cpu\" # Pour forcer l'utilisation du CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environement potentielement testé\n",
    "from environnement.environnement import Environnement as env # mother class\n",
    "from environnement.environnement1 import Environnement1 as env1\n",
    "from environnement.environnement2Str import Environnement2 as env2Str\n",
    "from environnement.environnement3Str import Environnement3 as env3Str\n",
    "from environnement.environnement6Str import Environnement6 as env6Str\n",
    "from environnement.small_loop import small_loop\n",
    "\n",
    "# model machine learning\n",
    "from model.DeepNN import *\n",
    "from model.Tokenizer import *\n",
    "from model.RNN import *\n",
    "from model.CustomDataSet import CustomDataSet, CustomDataSetRNN\n",
    "from outil import *\n",
    "from inter.interactions import Interaction\n",
    "from inter.simpleInteraction import simpleInteraction as inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentLSTM:\n",
    "    def __init__(self, model, all_outcomes, all_actions, tokenizer, valence:dict,\n",
    "                optimizer, gap_train=11, gap_test=11, loss_func=None):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self.optimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._tokenizer:SimpleTokenizerV1 = tokenizer\n",
    "        self._all_outcomes = all_outcomes\n",
    "        self._all_actions = all_actions\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "        self._valence = valence\n",
    "        self._gap_train = gap_train\n",
    "        self._gap_test = gap_test\n",
    "        self._boredom = 0\n",
    "\n",
    "    def fit(self, actions:list, outcomes:list, nb_epoch:int=25, context_lenght=5, validate_loader=None):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent \n",
    "        Avec data set custom, le model prends en inputs plusieurs données\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(actions) + len(outcomes) < context_lenght:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            data_loarder = CustomDataSetRNN(actions=actions, outcomes=outcomes,\n",
    "                    context_lenght=context_lenght, dim_out=2, tokenizer=self._tokenizer)\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                data_loarder, batch_size=32, shuffle=True)\n",
    "            for e in range(nb_epoch):\n",
    "                for x,t in data_loader:\n",
    "                    bs = t.shape[0]\n",
    "                    h = torch.zeros(self._model.num_layers, bs, self._model.hidden_size, device=device)\n",
    "                    cell = torch.zeros(self._model.num_layers, bs, self._model.hidden_size, device=device)\n",
    "\n",
    "                    pred, h, cell = self._model(x, h, cell)\n",
    "\n",
    "                    loss = self._loss_func(pred[:, -1, :], t)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "        else: # Si le model n'est pas un model pytorch\n",
    "            raise Exception('Not implemented')\n",
    "            self._model.fit(action, outcome)\n",
    "            pass\n",
    "\n",
    "    def predict(self, action, gap=5):\n",
    "        if len(self._history_act) + len(self._history_fb) < gap:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        action = self._tokenizer.encode(x)\n",
    "\n",
    "        action = torch.tensor([action], dtype=torch.int).to(device)\n",
    "\n",
    "        h = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "        cell = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "        \n",
    "        with torch.no_grad():  # Pas de calcul de gradients en mode prédiction\n",
    "            pred, _, _ = self._model(action, h, cell)\n",
    "\n",
    "        pred_feedback = torch.argmax(pred[:, -1, :]).item()\n",
    "        pred_feedback = self._tokenizer.decode(pred_feedback)\n",
    "        \n",
    "        return pred_feedback\n",
    "    \n",
    "    def recursif_expective_valance(self, seq:list, max_depth:int, seuil:float=0.2, proba:float = 1,\n",
    "                                    seq_predi:list = []):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if max_depth == 0:\n",
    "            return {}\n",
    "        max_depth -= 1\n",
    "\n",
    "        if proba < seuil:\n",
    "            return {}\n",
    "        \n",
    "        self._model.eval()\n",
    "        exceptive_valance = {}\n",
    "        for act in self._all_actions:\n",
    "            new_seq = seq_predi + [act]\n",
    "            seq_to_predict = seq + [self._tokenizer.encode(act)]\n",
    "            seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.int).to(device)\n",
    "\n",
    "            hidden = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "            memory = torch.zeros(self._model.num_layers, 1, self._model.hidden_size, device=device)\n",
    "\n",
    "            x, _, _ = self._model(seq_to_predict, hidden, memory)\n",
    "            x = x[0, -1, :]\n",
    "            # Transforme x into list proba\n",
    "            probs = torch.nn.functional.softmax(x, dim=0).tolist()\n",
    "            # for each outcomes we want proba with act\n",
    "            for i, out in enumerate(self._all_outcomes):\n",
    "                tmp_new_seq = new_seq + [out]\n",
    "                tmp_proba = probs[i] * proba\n",
    "                if tmp_proba < seuil:\n",
    "                    continue\n",
    "                tempo = float(np.round(self._valence[inter(act, out)] * tmp_proba, decimals=4))\n",
    "                # input(f'seq {seq_predi} act {act} out {out} proba {tmp_proba} valance {valance[(act, out)]} tempo {tempo}')\n",
    "                exceptive_valance.update(\n",
    "                    self.recursif_expective_valance(seq=seq[2:] + [self._tokenizer.encode(act), self._tokenizer.encode(out)],\n",
    "                                                max_depth=max_depth, seuil=seuil, \n",
    "                                                proba=tmp_proba, seq_predi=tmp_new_seq.copy())\n",
    "                )\n",
    "                exceptive_valance[str(tmp_new_seq)] = tempo\n",
    "        return exceptive_valance\n",
    "        \n",
    "    def decide(self):\n",
    "        x = []\n",
    "        for i in range(-self._gap_test//2, 0, 1):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        seq = self._tokenizer.encode(x)\n",
    "        res = self.recursif_expective_valance(seq=seq, max_depth=3, seuil=0.3)\n",
    "        top_5 = sorted(res.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        # print(f\"Top 5 of sequences with the best expected valance for {x}\")\n",
    "        # for top in top_5:\n",
    "        #     print(f\"Sequence: {top[0]} Expected valance: {top[1]}\")\n",
    "        \n",
    "        # print(f\"Action choisie : {eval(top_5[0][0])[0]}\")\n",
    "        return eval(top_5[0][0])[0]\n",
    "\n",
    "    # Modifier\n",
    "    def action(self, outcome, fit=True, decide=True, validate_loader=None, force_fit=False):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        self._boredom += 1\n",
    "        description = None\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            description = f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\"\n",
    "            if len(self._history_act) + len(self._history_fb) > self._gap_train:\n",
    "                # and self._predicted_outcome != outcome\n",
    "                if fit and self._predicted_outcome != outcome:\n",
    "                    self.fit(self._history_act, self._history_fb, validate_loader=validate_loader,\n",
    "                            nb_epoch=50, context_lenght=self._gap_train)\n",
    "                    self._boredom = 0\n",
    "                \n",
    "            if decide and len(self._history_act) + len(self._history_fb) > self._gap_test:\n",
    "                self._action = self.decide()\n",
    "            else :\n",
    "                self._action = str(np.random.choice(self._all_actions))\n",
    "                \n",
    "            if len(self._history_act) + len(self._history_fb) > self._gap_test:\n",
    "                self._predicted_outcome = self.predict(self._action, gap=self._gap_test)\n",
    "            \n",
    "            self._history_act.append(self._action)\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)            \n",
    "            description = f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\"\n",
    "        \n",
    "        return self._action, self._predicted_outcome, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256c2683e1594e7aad9c15d9e6836779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7ea02252354c7fa9de10db7596c9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_test2 = small_loop(x=1, y=1, theta=0, world=np.array([\n",
    "                [1, 1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 0, 1],\n",
    "                [1, 0, 0, 0, 0, 1],\n",
    "                [1, 0, 0, 0, 0, 1],\n",
    "                [1, 0, 0, 0, 0, 1],\n",
    "                [1, 1, 1, 1, 1, 1],\n",
    "            ]))\n",
    "\n",
    "model_ML = LSTM(hidden_size=128, num_emb=len(env_test2.get_outcomes() + env_test2.get_actions()),\n",
    "                num_layers=2, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 1,\n",
    "    inter('forward', 'wall') : -10,\n",
    "    inter('turn_left', 'empty') : -7,\n",
    "    inter('turn_left', 'wall') : -80,\n",
    "    inter('turn_right', 'empty') : -7,\n",
    "    inter('turn_right', 'wall') : -80,\n",
    "    inter('feel_front', 'wall') : -5,\n",
    "    inter('feel_front', 'empty') : -5,\n",
    "}\n",
    "\n",
    "agent_test2 = AgentLSTM(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    valence=valence,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func,\n",
    "    gap_train=15,\n",
    "    gap_test=15)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "\n",
    "out = Output()\n",
    "display(out)\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    # start_time = time.time()\n",
    "    action, predi, description = agent_test2.action(outcome, fit=False, decide=False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    # env_test2.display_world(out)\n",
    "    env_test2.save_world()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3e46d2ddd94a19bf100a7aa72e6b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pe/Documents/stage/Dpt_transformers/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:823: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "action, predi, description = agent_test2.action(outcome, fit=True, decide=True, force_fit=True)\n",
    "outcome = env_test2.outcome(action)\n",
    "history_good.append(outcome == predi)\n",
    "pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "env_test2.save_world()\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    # print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    start_time = time.time()\n",
    "    action, predi, description = agent_test2.action(outcome, fit=True, decide=True)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    # env_test2.display_world(out)\n",
    "    env_test2.save_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Fin de l'execution",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFin de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecution\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m out \u001b[38;5;241m=\u001b[39m Output()\n\u001b[1;32m      3\u001b[0m out_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Fin de l'execution"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Fin de l'execution\")\n",
    "out = Output()\n",
    "out_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a529aacd74f64ed9904dfb108a0c57f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env_test2 = small_loop(x=1, y=1, world=np.array([\n",
    "#                 [1, 1, 1, 1, 1],\n",
    "#                 [1, 0, 0, 0, 1],\n",
    "#                 [1, 1, 1, 0, 1],\n",
    "#                 [1, 0, 1, 0, 1],\n",
    "#                 [1, 0, 0, 0, 1],\n",
    "#                 [1, 1, 1, 1, 1],\n",
    "#             ]))\n",
    "env_test2 = small_loop(x=1, y=1, theta=0, world=np.array([\n",
    "                [1, 1, 1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 0, 0, 1],\n",
    "                [1, 1, 1, 1, 1, 1, 1],\n",
    "            ]))\n",
    "\n",
    "\n",
    "agent_test2._history_act = []\n",
    "agent_test2._history_fb = []\n",
    "agent_test2._action = None\n",
    "\n",
    "# for i in tqdm(range(50)):\n",
    "#     action, predi, description = agent_test2.action(outcome, fit=True, decide=False)\n",
    "#     outcome = env_test2.outcome(action)\n",
    "#     env_test2.save_world()\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    action, predi, description = agent_test2.action(outcome, fit=True, decide=True)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    env_test2.save_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = env_test2.outcome('forward')\n",
    "env_test2.display_world(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = env_test2.outcome('turn_left')\n",
    "env_test2.display_world(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = env_test2.outcome('turn_right')\n",
    "env_test2.display_world(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = env_test2.outcome('feel_front')\n",
    "env_test2.display_world(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Fin de l'execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    env_test2.display_world(out)\n",
    "    action, predi = agent_test2.action(outcome, decide=True)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_proba_from_seq(_seg):\n",
    "    seq = tokenizer.encode(_seg)\n",
    "    seq = torch.tensor([seq], dtype=torch.int).to(device)\n",
    "    h = torch.zeros(agent_test2._model.num_layers, 1, agent_test2._model.hidden_size, device=device)\n",
    "    cell = torch.zeros(agent_test2._model.num_layers, 1, agent_test2._model.hidden_size, device=device)\n",
    "    predi, _, _ =  agent_test2._model(seq, h, cell)\n",
    "    predi = predi[0, -1, :]\n",
    "    prob = torch.nn.functional.softmax(predi, dim=0)\n",
    "    deocde = tokenizer.decode(torch.argmax(predi, dim=0).item())\n",
    "    print(f\"Prédiction de la séquence {_seg} :  probabilité {prob.tolist()}, decode {deocde}\")\n",
    "\n",
    "print(\"porba si on avance\")\n",
    "show_proba_from_seq(['forward', 'empty', 'forward'])\n",
    "show_proba_from_seq(['forward', 'wall', 'forward'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'forward'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'forward'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'forward'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'forward'])\n",
    "\n",
    "print(\"porba si on turn left\")\n",
    "show_proba_from_seq(['forward', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['forward', 'wall', 'turn_left'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'turn_left'])\n",
    "\n",
    "print(\"porba si on turn right\")\n",
    "show_proba_from_seq(['forward', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['forward', 'wall', 'turn_right'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'turn_right'])\n",
    "\n",
    "print(\"porba si on feel front\")\n",
    "show_proba_from_seq(['forward', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['forward', 'wall', 'feel_front'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'feel_front'])\n",
    "\n",
    "def count_pattern(lst, pattern):\n",
    "    return sum(1 for i in range(len(lst) - len(pattern) + 1) if lst[i:i+len(pattern)] == pattern)\n",
    "\n",
    "list_act_out = []\n",
    "for act, out in zip(agent_test2._history_act, agent_test2._history_fb):\n",
    "    list_act_out.append(act)\n",
    "    list_act_out.append(out)\n",
    "\n",
    "print(\"count si on avance\")\n",
    "print(f\"pattern 'forward', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['forward', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['forward', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'forward', 'empty' présent : {count_pattern(list_act_out, ['forward', 'wall', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'forward', 'wall' présent : {count_pattern(list_act_out, ['forward', 'wall', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'forward', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'forward', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'forward', 'wall'])}\")\n",
    "\n",
    "print(\"count si on feel_front\")\n",
    "print(f\"pattern 'forward', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['forward', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['forward', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['forward', 'wall', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['forward', 'wall', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'feel_front', 'wall'])}\")\n",
    "\n",
    "print(\"count si on turn_left turn_right\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'turn_left', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'turn_left', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'turn_left', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'turn_left', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'turn_right', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'turn_right', 'empty'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'turn_right', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'turn_right', 'empty'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See decision\n",
    "x = []\n",
    "for i in range(agent_test2._gap_test):\n",
    "    x.append(agent_test2._history_act[-(agent_test2._gap_test -i)])\n",
    "    x.append(agent_test2._history_fb[-(agent_test2._gap_test -i)])\n",
    "    print(-(agent_test2._gap_test -i), i)\n",
    "seq = agent_test2._tokenizer.encode(x)\n",
    "res = agent_test2.recursif_expective_valance(seq=seq, max_depth=3, seuil=0.4)\n",
    "top_5 = sorted(res.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(f\"Top 5 of sequences with the best expected valance for {x}\")\n",
    "for top in top_5:\n",
    "    print(f\"Sequence: {top[0]} Expected valance: {top[1]}\")\n",
    "\n",
    "print(f\"Action choisie : {eval(top_5[0][0])[0]}\")\n",
    "\n",
    "print(f'size of history act {len(agent_test2._history_act)}')\n",
    "print(f'size of history fb {len(agent_test2._history_fb)}')\n",
    "print(f'gap {agent_test2._gap_test}')\n",
    "\n",
    "print([i for i in zip(agent_test2._history_act, agent_test2._history_fb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_test2._history_act.append('feel_front')\n",
    "agent_test2._history_fb.append('empty')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
