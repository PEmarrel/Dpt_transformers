{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "# Pour torch si vous avez un GPU\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "# device = \"cpu\" # Pour forcer l'utilisation du CPU\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environnement.environnement import Environnement as env # mother class\n",
    "from environnement.small_loop import small_loop\n",
    "from environnement.gridWorld import gridWorld\n",
    "# from environnement.gridWorldMoreFB import \n",
    "\n",
    "# model machine learning\n",
    "from model.Tokenizer import *\n",
    "from model.RNN import *\n",
    "from model.CustomDataSet import CustomDataSet, CustomDataSetRNN\n",
    "from outil import *\n",
    "from inter.simpleInteraction import simpleInteraction as inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_agent_evolued(by_10_bad_inter, by_10_good_inter, pourcent_by_10, mean_val):\n",
    "    plt.clf()\n",
    "    plt.plot(by_10_bad_inter, label='bad inter', color='red')\n",
    "    plt.plot(by_10_good_inter, label='good inter', color='green')\n",
    "    plt.plot(pourcent_by_10, label='pr√©cision', color='blue')\n",
    "    plt.plot(mean_val, label='mean valence', color='black')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"tmp.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'agent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentLSTM:\n",
    "    def __init__(self, valence:dict[inter], model:nn.Module, maxDepth:int,\n",
    "                seuil:float, optimizer, loss_fn, gap:int=11, nb_epochs:int=50,\n",
    "                explo:int=3, data_val:tuple=None, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Create an agent with a LSTM model and spesific decision making.\n",
    "        data_val is composed by list of all actions and outcomes. And \n",
    "        is not useful to train the model. It's just to have a monitoring\n",
    "        of the model. \\n\n",
    "        valence: dict of interactions, is use to know what is a good \n",
    "        comportment \\n\n",
    "        model: the model to train, this agent was create for LSTM model \n",
    "        \\n\n",
    "        \n",
    "        \"\"\"\n",
    "        self.model:nn.Module = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.valence:dict[inter] = valence\n",
    "        self.maxDepth:int = maxDepth\n",
    "        self.seuil:float = seuil\n",
    "        self.gap:int = gap\n",
    "        self.nbEpochs:int = nb_epochs\n",
    "        self.force_fit:bool = False\n",
    "        self.device = device\n",
    "        self.last_train = 0\n",
    "        self.proba = 0\n",
    "        self.explo = explo\n",
    "        \n",
    "        self.seq_to_exe = [] # Sequence choice by Decide\n",
    "        self.compute_seq = {}\n",
    "        self.history_act = [] # History of all actions\n",
    "        self.history_fb = [] # History of all feedback\n",
    "        self.history_inter = [] # History of all interactions\n",
    "        \n",
    "        self.all_outcomes = set()\n",
    "        self.all_act = set()\n",
    "        key:inter = None\n",
    "        for key in valence.keys():\n",
    "            self.all_outcomes.add(key.getOutcome())\n",
    "            self.all_act.add(key.getAction())\n",
    "            \n",
    "        self.all_outcomes = list(self.all_outcomes)\n",
    "        self.all_act = list(self.all_act)\n",
    "        \n",
    "        self.tokenizer = SimpleTokenizerV1(\n",
    "            vocab={key: i for i, key in enumerate(self.all_outcomes + self.all_act)})\n",
    "        \n",
    "        self.seq_explo = []\n",
    "        self.valence_explo = -np.inf\n",
    "        \n",
    "        self.action_choice = self.all_act[0] # Default action, because developpemental start with action\n",
    "        self.history_act.append(self.action_choice)\n",
    "        self.outcome_prediction = None\n",
    "        \n",
    "        self.memory = {}\n",
    "        \n",
    "        # Variable to monitor the model\n",
    "        self.loss_train:list = []\n",
    "        self.acc_train:list = []\n",
    "        self.loss_test:list = [0]\n",
    "        self.acc_test:list = []\n",
    "        self.time_train:list = []\n",
    "        self.time_expected_val:list = []\n",
    "        self.time_train:list = []\n",
    "        self.time_expected_val:list = []\n",
    "        self.predictExplor:list = []\n",
    "        if data_val is not None:\n",
    "            dataset_test = CustomDataSetRNN(actions=data_val[0], outcomes=data_val[1], context_lenght=self.gap, \n",
    "                                    dim_out=len(self.all_outcomes), tokenizer=self.tokenizer)\n",
    "            self.data_loader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)\n",
    "        else:\n",
    "            self.data_loader_test = None\n",
    "        number_patern = 2000000\n",
    "        self.prealloc_df = pd.DataFrame(np.empty((number_patern, 5)), \n",
    "                                    columns=[\"proposition\", \"valence\", \"action\", \"probability\", \"val_sucess\"])\n",
    "        self.prealloc_df = self.prealloc_df.astype({\"proposition\": \"U20\", \"valence\": float, \"action\": \"U20\", \"probability\": float, \"val_sucess\": float})\n",
    "        \n",
    "        self.current_index = 0\n",
    "        self.visu_explo = pd.DataFrame(np.empty((number_patern, 2)), columns=[\"seqence\", \"valence\"])\n",
    "        self.visu_explo = self.visu_explo.astype({\"seqence\": \"U20\", \"valence\": float})\n",
    "        self.current_index_explo = 0\n",
    "        \n",
    "        if data_val is not None:\n",
    "            self.visu_val = pd.DataFrame(np.empty((len(data_val[0]), 3)), \n",
    "                                        columns=[\"seqence\", \"probablility\", \"good\"])\n",
    "            self.visu_val = self.visu_val.astype({\"seqence\": \"U20\", \"probablility\": float, \"good\": bool})\n",
    "            self.current_index_val = 0\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        train model\n",
    "        \"\"\"\n",
    "        dataset = CustomDataSetRNN(actions=self.history_act, outcomes=self.history_fb, \n",
    "                                 context_lenght=self.gap, dim_out=len(self.all_outcomes),\n",
    "                                 tokenizer=self.tokenizer)\n",
    "        \n",
    "        data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        time_train = time.time()\n",
    "        acc_in_epoch = []\n",
    "        acc_in_epoch_test = []\n",
    "        for i in range(self.nbEpochs):\n",
    "            self.model.train()\n",
    "            steps = 0\n",
    "            train_acc = 0\n",
    "            training_loss = []\n",
    "            for tmp, (x,t) in enumerate(data_loader):\n",
    "                x = x.to(self.device)\n",
    "                t = t.to(self.device)\n",
    "                bs = t.shape[0]\n",
    "                h = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=self.device)\n",
    "                cell = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=self.device)\n",
    "\n",
    "                pred, h, cell = self.model(x, h, cell)\n",
    "\n",
    "                loss = self.loss_fn(pred[:, -1, :], t)\n",
    "                training_loss.append(loss.item())\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_acc += sum((pred[:, -1, :].argmax(1) == t).cpu().numpy())\n",
    "                steps += bs\n",
    "            \n",
    "            acc_in_epoch.append(train_acc / steps)\n",
    "            if self.data_loader_test is not None:\n",
    "                self.model.eval()\n",
    "                steps = 0\n",
    "                test_acc = 0\n",
    "                loss_test = []\n",
    "                \n",
    "                for text, label in self.data_loader_test:\n",
    "                    text = text.to(self.device)\n",
    "                    label = label.to(self.device)\n",
    "                    bs = label.shape[0]\n",
    "\n",
    "                    # Initialize hidden and memory states\n",
    "                    hidden = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=self.device)\n",
    "                    memory = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=self.device)\n",
    "                    \n",
    "                    # Forward pass through the model\n",
    "                    pred, hidden, memory = self.model(text, hidden, memory)\n",
    "                    \n",
    "                    for i in range(bs):\n",
    "                        self.visu_val.iloc[steps + i] = [str(self.tokenizer.decode(text[i].cpu().tolist())), \n",
    "                                                         float(torch.nn.functional.softmax(pred[i, -1, :], dim=-1).max().item()), \n",
    "                                                         int(pred[i, -1, :].argmax().item() == label[i])]\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    \n",
    "                    loss = self.loss_fn(pred[:, -1, :], label)\n",
    "                    loss_test.append(loss.item())\n",
    "\n",
    "                    # Calculate test accuracy\n",
    "                    test_acc +=  sum((pred[:, -1, :].argmax(1) == label).cpu().numpy())\n",
    "                    steps += bs\n",
    "                    \n",
    "                loss_test.append(loss_test)\n",
    "                acc_in_epoch_test.append(test_acc / steps)\n",
    "                # print(f\"Validation time: {time.time() - time_val_epoch}\")    \n",
    "            self.loss_train.append(training_loss)\n",
    "            # If acc is 100% we stop the training\n",
    "            if acc_in_epoch[-1] >= 0.99:\n",
    "                self.acc_train.append(acc_in_epoch)\n",
    "                self.acc_test.append(acc_in_epoch_test)\n",
    "                for _ in range(i, self.nbEpochs):\n",
    "                    self.acc_train[-1].append(self.acc_train[-1][-1])\n",
    "                    if self.data_loader_test is not None:\n",
    "                        # self.loss_test.append([self.loss_test[-1][-1]])\n",
    "                        self.acc_test[-1].append(self.acc_test[-1][-1])\n",
    "                break\n",
    "            \n",
    "        # print(f\"Training time: {time.time() - time_train}\")\n",
    "        self.acc_train.append(acc_in_epoch)\n",
    "        self.acc_test.append(acc_in_epoch_test)\n",
    "        self.time_train.append(time.time() - time_train)\n",
    "        \n",
    "    def predict(self, action):\n",
    "        \"\"\"\n",
    "        Predict the feedback of the action, use the last gap actions/outcomes\n",
    "        \"\"\"        \n",
    "        x = []\n",
    "        for i in range(-(self.gap - 1) // 2, 0, 1):\n",
    "            x.append(self.history_act[i])\n",
    "            x.append(self.history_fb[i])\n",
    "        x.append(action)\n",
    "        seq_to_pred = self.tokenizer.encode(x)\n",
    "        # On simule un batch de taille 1\n",
    "        seq_to_pred = torch.tensor([seq_to_pred], device=self.device)\n",
    "        h = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=self.device)\n",
    "        cell = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=self.device)\n",
    "        probs, _, _ = self.model(seq_to_pred, h, cell)\n",
    "        \n",
    "        pred_feedback = torch.argmax(probs[:, -1, :]).item()\n",
    "        pred_feedback = self.tokenizer.decode(pred_feedback)\n",
    "        \n",
    "        return pred_feedback\n",
    "            \n",
    "    def verify_explo(self, seq_to_pred, action, max_depth:int, previous_proba:float):\n",
    "        sub_list = subfinder(self.history_inter, seq_to_pred[-self.explo:])        \n",
    "        dict_explo = None\n",
    "        if sub_list == []:\n",
    "            inter_with_act = [inter(action, out) for out in self.all_outcomes]\n",
    "            best_inter_act = None\n",
    "            tmp_valence = -np.inf\n",
    "            for tmp_inter in inter_with_act:\n",
    "                if self.valence[tmp_inter] > tmp_valence:\n",
    "                    best_inter_act = tmp_inter\n",
    "                    tmp_valence = self.valence[tmp_inter]\n",
    "            proba = []\n",
    "            dict_explo = {\n",
    "                \"action\" : action,\n",
    "                \"expected_valence\" : tmp_valence,\n",
    "                \"outcome\" : {\"proba\": None},\n",
    "            }\n",
    "            for out in self.all_outcomes:\n",
    "                if out == best_inter_act.getOutcome():\n",
    "                    proba.append((1, out))\n",
    "                    if max_depth > 0:\n",
    "                        new_context = seq_to_pred + self.tokenizer.encode([action, out])\n",
    "                        children = self.recursif_expective_valance(context=new_context[2:], \n",
    "                                                                    max_depth=max_depth, \n",
    "                                                                    seuil=self.seuil,\n",
    "                                                                    previous_proba=previous_proba)\n",
    "                    \n",
    "                        best_child = max(children, key=lambda x: x[\"expected_valence\"])\n",
    "                        dict_explo[\"outcome\"] [out] =  best_child\n",
    "                        dict_explo[\"expected_valence\"] += best_child[\"expected_valence\"]\n",
    "                    else:\n",
    "                        dict_explo[\"outcome\"] [out] = {}\n",
    "                else:\n",
    "                    proba.append((-1, out))\n",
    "                    dict_explo[\"outcome\"] [out] = {}\n",
    "                    \n",
    "            dict_explo[\"outcome\"] [\"proba\"] = proba\n",
    "            \n",
    "        return dict_explo\n",
    "    \n",
    "    def recursif_expective_valance(self, context:list, max_depth:int, seuil:float=0.5, previous_proba:float = 1):\n",
    "        \"\"\"\n",
    "        Create the list of proposed sequences\n",
    "        \"\"\"\n",
    "        max_depth -= 1\n",
    "        self.model.eval()\n",
    "        # Compute the expected valence of each action\n",
    "        list_action = []\n",
    "        for act in self.all_act:\n",
    "            dict_seq = {\n",
    "                    \"action\" : act,\n",
    "                    \"expected_valence\" :0,\n",
    "                }\n",
    "            seq_to_predict = context + [self.tokenizer.encode(act)]\n",
    "            explo = self.verify_explo(seq_to_predict, act, max_depth, previous_proba)\n",
    "            if explo is not None:\n",
    "                list_action.append(explo)\n",
    "                continue\n",
    "            \n",
    "            seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.int).to(device)\n",
    "            hidden = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=device)\n",
    "            memory = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=device)\n",
    "            x, _, _ = self.model(seq_to_predict, hidden, memory)\n",
    "            x = x[0, -1, :]\n",
    "            probs = torch.nn.functional.softmax(x, dim=0).tolist()\n",
    "            proba = []\n",
    "            dict_seq[\"outcome\"] = {}\n",
    "            for i, out in enumerate(self.all_outcomes):\n",
    "                tmp_proba = probs[i] * previous_proba\n",
    "                proba.append((probs[i], out))\n",
    "                \n",
    "                if tmp_proba > seuil:\n",
    "                    if max_depth > 0:\n",
    "                        new_context = context + self.tokenizer.encode([act, out])\n",
    "                        children = self.recursif_expective_valance(context=new_context[2:], \n",
    "                                                                   max_depth=max_depth, \n",
    "                                                                   seuil=seuil,\n",
    "                                                                   previous_proba=tmp_proba)\n",
    "                    \n",
    "                        best_child = max(children, key=lambda x: x[\"expected_valence\"])\n",
    "                        dict_seq[\"outcome\"] [out] =  best_child\n",
    "                        dict_seq[\"expected_valence\"] += best_child[\"expected_valence\"] * probs[i] + self.valence[inter(act, out)] * probs[i]\n",
    "                    else:\n",
    "                        dict_seq[\"outcome\"] [out] = {}\n",
    "                        dict_seq[\"expected_valence\"] += self.valence[inter(act, out)] * probs[i]\n",
    "                else:\n",
    "                    dict_seq[\"outcome\"] [out] = {}\n",
    "                    dict_seq[\"expected_valence\"] += self.valence[inter(act, out)] * probs[i]\n",
    "            dict_seq[\"outcome\"] [\"proba\"] = proba\n",
    "            list_action.append(dict_seq)\n",
    "                   \n",
    "        return list_action\n",
    "\n",
    "    \n",
    "    def expective_valance(self):\n",
    "        \"\"\"\n",
    "        Permet de calculer l'expective valance d'une s√©quence d'interaction\n",
    "\n",
    "        Args:\n",
    "            max_depth (int): _description_\n",
    "            seuil (float, optional): _description_. Defaults to 0.2.\n",
    "            verbose (bool, optional): _description_. Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = []\n",
    "        for i in range(-(self.gap - 1) // 2, 0, 1):\n",
    "            x.append(self.history_act[i])\n",
    "            x.append(self.history_fb[i])\n",
    "        seq_to_pred = self.tokenizer.encode(x)\n",
    "        if str(x) in self.memory.keys():\n",
    "            if self.memory[str(x)] [\"iteraction_update\"] >= self.last_fit:\n",
    "                self.compute_seq = self.memory[str(x)] [\"prealloc_df\"]\n",
    "                return\n",
    "    \n",
    "        tmp = self.recursif_expective_valance(context=seq_to_pred,\n",
    "                            max_depth=self.maxDepth,\n",
    "                            previous_proba=1,\n",
    "                            seuil=self.seuil)\n",
    "        self.compute_seq = max(tmp, key=lambda x: x[\"expected_valence\"])\n",
    "        self.memory[str(x)] = {\n",
    "            \"prealloc_df\" : self.compute_seq,\n",
    "            \"iteraction_update\" : len(self.history_act)\n",
    "        }\n",
    "        \n",
    "    def decide(self):\n",
    "        \"\"\"\n",
    "        Permet de choisir une action en fonction de l'expective valance pr√©dite\n",
    "        \"\"\"\n",
    "        if self.compute_seq != {}:\n",
    "            # print(\"i follow : \\n\", self.compute_seq)\n",
    "            self.compute_seq = self.compute_seq[self.history_fb[-1]]\n",
    "            if self.compute_seq == {}:\n",
    "                # self.force_fit = True\n",
    "                time_compute_expective_val = time.time()\n",
    "                self.expective_valance()\n",
    "                print(f\"Time to compute expective valance: {time.time() - time_compute_expective_val}\")\n",
    "                self.time_expected_val.append(time.time() - time_compute_expective_val)\n",
    "            else :\n",
    "                print(\"act choice\")\n",
    "        else :\n",
    "            time_compute_expective_val = time.time()\n",
    "            self.expective_valance()\n",
    "            print(f\"Time to compute expective valance: {time.time() - time_compute_expective_val}\")\n",
    "            self.time_expected_val.append(time.time() - time_compute_expective_val)\n",
    "\n",
    "        if self.compute_seq == []:\n",
    "            print(\"i have no compute seq\")\n",
    "        print(f\"expected valence : {self.compute_seq['expected_valence']:.2f} model predict : {self.compute_seq}\")\n",
    "        action = self.compute_seq[\"action\"]\n",
    "\n",
    "        self.compute_seq = self.compute_seq[\"outcome\"]\n",
    "        return action\n",
    "        \n",
    "    def action(self, real_outcome, verbose=False):\n",
    "        \"\"\"\n",
    "        La fonction action permet √† l'agent de choisir une action en fonction de l'outcome r√©el.\n",
    "        Cette fonction entraine le mod√®le a pr√©voir les outcomes futurs en fonction des actions pass√©es.\n",
    "\n",
    "        Args:\n",
    "            real_outcome : L'outcome r√©el suite √† l'action de l'agent\n",
    "            verbose : Affiche les informations sur l'entrainement ou non\n",
    "        \"\"\"\n",
    "        # La premi√®re √©tape est de sauvegarder l'outcome r√©el\n",
    "        self.history_fb.append(real_outcome)\n",
    "        self.history_inter.append(self.tokenizer.encode(real_outcome))\n",
    "        good_pred:bool = self.outcome_prediction == real_outcome\n",
    "        if verbose :\n",
    "            print(f\"Iteration {len(self.history_act)} \\033[0;31m Action: {self.action_choice} \\033[0m, Prediction: {self.outcome_prediction}, Outcome: {real_outcome}, \\033[0;31m Satisfaction: {good_pred} \\033[0m\")\n",
    "        \n",
    "        # Ensuite nous regardons si nous devons entrainer le mod√®le\n",
    "        # not(explore) and \n",
    "        if (not(good_pred) or self.force_fit) and (len(self.history_fb) + len(self.history_fb) > self.gap) and (self.proba > 0.6 or self.proba < 0.4):\n",
    "            print(\"fit because probability is \", self.proba)\n",
    "            self.fit()\n",
    "            self.last_fit = len(self.history_act)\n",
    "            self.force_fit = False\n",
    "            \n",
    "        elif len(self.history_fb) + len(self.history_fb) > self.gap:\n",
    "            for _ in range(self.nbEpochs):\n",
    "                self.acc_train.append(self.acc_train[-1])\n",
    "                if self.data_loader_test is not None:\n",
    "                    pass\n",
    "                    # self.loss_test.append([self.loss_test[-1][-1]])\n",
    "\n",
    "        # Nous devons maintenant choisir une action\n",
    "        if len(self.history_fb) + len(self.history_fb) > self.gap:\n",
    "            self.action_choice = self.decide()\n",
    "            self.outcome_prediction = self.predict(self.action_choice)\n",
    "        else:\n",
    "            inter_max, value = max(self.valence.items(), key=lambda y: y[1])\n",
    "            self.action_choice = inter_max.getAction()\n",
    "        # self.action_choice = np.random.choice(self.all_act)\n",
    "        self.history_act.append(self.action_choice)\n",
    "        self.history_inter.append(self.tokenizer.encode(self.action_choice))\n",
    "        \n",
    "        return self.action_choice, self.outcome_prediction\n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environenment = small_loop(x=1, y=1, theta=0, world=np.array([\n",
    "#                 [1, 1, 1, 1, 1],\n",
    "#                 [1, 0, 0, 0, 1],\n",
    "#                 [1, 0, 1, 0, 1],\n",
    "#                 [1, 0, 0, 0, 1],\n",
    "#                 [1, 1, 1, 1, 1],\n",
    "#             ]))\n",
    "\n",
    "# environenment = small_loop(x=1, y=1, theta=0, world=np.array([\n",
    "#                 [1, 1, 1, 1, 1, 1],\n",
    "#                 [1, 0, 0, 0, 1, 1],\n",
    "#                 [1, 0, 1, 0, 0, 1],\n",
    "#                 [1, 0, 1, 1, 0, 1],\n",
    "#                 [1, 0, 0, 0, 0, 1],\n",
    "#                 [1, 1, 1, 1, 1, 1],\n",
    "#             ]))\n",
    "\n",
    "environenment = small_loop(x=1, y=1, theta=0, world=np.array([\n",
    "                [1, 1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 0, 1],\n",
    "                [1, 0, 0, 0, 0, 1],\n",
    "                [1, 0, 0, 0, 0, 1],\n",
    "                [1, 0, 0, 0, 0, 1],\n",
    "                [1, 1, 1, 1, 1, 1],\n",
    "            ]))\n",
    "\n",
    "environenment = small_loop(x=1, y=1, theta=0, world=np.array([\n",
    "                [1, 1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 0, 1],\n",
    "                [1, 1, 1, 1, 1, 1],\n",
    "            ]))\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 1,\n",
    "    inter('forward', 'wall') : -100,\n",
    "    inter('turn_left', 'empty') : -20,\n",
    "    inter('turn_left', 'wall') : -100,\n",
    "    inter('turn_right', 'empty') : -20,\n",
    "    inter('turn_right', 'wall') : -100,\n",
    "    inter('feel_front', 'wall') : -15,\n",
    "    inter('feel_front', 'empty') : -12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environenment = gridWorld(x=1, y=1, theta=0, world=np.array([\n",
    "#                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "#                 [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
    "#                 [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],\n",
    "#                 [1, 0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
    "#                 [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "#                 [1, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "#                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "#             ]))\n",
    "environenment.save_world(\"imgToGif2\")\n",
    "\n",
    "environenment = gridWorld(x=1, y=1, theta=0, world=np.array([\n",
    "                [1, 1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 1, 1],\n",
    "                [1, 0, 1, 0, 0, 1],\n",
    "                [1, 0, 1, 1, 0, 1],\n",
    "                [1, 0, 0, 0, 0, 1],\n",
    "                [1, 1, 1, 1, 1, 1],\n",
    "            ]))\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 50,\n",
    "    inter('forward', 'wall') : -200,\n",
    "    inter('turn_left', 'empty') : -35,\n",
    "    inter('turn_left', 'wall') : -100,\n",
    "    inter('turn_right', 'empty') : -35,\n",
    "    inter('turn_right', 'wall') : -100,\n",
    "    inter('feel_front', 'wall') : -20,\n",
    "    inter('feel_front', 'empty') : -20,\n",
    "    inter('feel_right', 'wall') : -20,\n",
    "    inter('feel_right', 'empty') : -20,\n",
    "    inter('feel_left', 'wall') : -20,\n",
    "    inter('feel_left', 'empty') : -20   \n",
    "}\n",
    "\n",
    "# Pour √©valuler la performance du mod√®le\n",
    "act_val, fb_val = [], []\n",
    "for i in trange(1000):\n",
    "    action = np.random.choice(environenment.get_actions())\n",
    "    outcome = environenment.outcome(action)\n",
    "    act_val.append(action)\n",
    "    fb_val.append(outcome)\n",
    "    \n",
    "data_val = (act_val, fb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "len_vocab = len(environenment.get_outcomes() + environenment.get_actions())\n",
    "\n",
    "# Create the LSTM classifier model\n",
    "lstm_classifier = LSTM_Classifeur(num_emb=len_vocab, output_size=len(environenment.get_outcomes()), \n",
    "                       num_layers=num_layers, hidden_size=hidden_size, dropout=0.5).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_classifier.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(environenment.get_outcomes() + environenment.get_actions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour √©valuler la performance du mod√®le\n",
    "# act_val, fb_val = [], []\n",
    "# for i in trange(1000):\n",
    "#     action = np.random.choice(environenment.get_actions())\n",
    "#     outcome = environenment.outcome(action)\n",
    "#     act_val.append(action)\n",
    "#     fb_val.append(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentLSTM(valence=valence, model=lstm_classifier, optimizer=optimizer, loss_fn=loss_func, explo=3,\n",
    "    data_val=data_val, device=device, gap=10, nb_epochs=50, maxDepth=5, seuil=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_good = []\n",
    "history_good_inter = []\n",
    "history_bad_inter = []\n",
    "hisrory_val = []\n",
    "pourcent_by_10  = []\n",
    "by_10_good_inter  = []\n",
    "by_10_bad_inter  = []\n",
    "mean_val = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "\n",
    "outcome = environenment.outcome(agent.action_choice)\n",
    "environenment.save_world(path=\"imgToGif3\")\n",
    "\n",
    "print(\"Start ...\")\n",
    "print(agent.action_choice)\n",
    "print(outcome)\n",
    "\n",
    "for i in tqdm(range(500)):\n",
    "    if i % 50 == 0:\n",
    "        agent.force_fit = True\n",
    "    if i % 100 == 0:\n",
    "        see_agent_evolued(by_10_bad_inter, by_10_good_inter, pourcent_by_10, mean_val)\n",
    "    action, predi = agent.action(outcome, verbose=True)\n",
    "    print(agent.compute_seq)\n",
    "    df2 = agent.prealloc_df\n",
    "    df_explo2 = agent.visu_explo\n",
    "    #  save_evolued_acc(agent.acc_train[-agent.nb_epoch:], path=f\"plot3/acc_train_{i}.png\")\n",
    "    outcome = environenment.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    history_good_inter.append((action == 'forward' and outcome == 'empty'))\n",
    "    history_bad_inter.append((action == 'forward' and outcome == 'wall'))\n",
    "    hisrory_val.append(valence[inter(action, outcome)])\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) if len(history_good) >= 10 else 0)\n",
    "    by_10_good_inter.append(sum(history_good_inter[-10:]) if len(history_good_inter) >= 10 else 0)\n",
    "    by_10_bad_inter.append(sum(history_bad_inter[-10:]) if len(history_bad_inter) >= 10 else 0)\n",
    "    mean_val.append(np.mean(hisrory_val[-10:]) / 10 if len(hisrory_val) >= 10 else 0)\n",
    "    environenment.save_world(path=\"imgToGif3\")\n",
    "\n",
    "agent.prealloc_df.to_csv(\"prealloc_df.csv\")\n",
    "agent.visu_explo.to_csv(\"df_explo.csv\")\n",
    "\n",
    "pourcent_by_10 = pourcent_by_10[10:]\n",
    "by_10_good_inter = by_10_good_inter[10:]\n",
    "by_10_bad_inter = by_10_bad_inter[10:]\n",
    "mean_val = mean_val[10:]\n",
    "raise Exception(\"End ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tmp exe 6 + 10 + \n",
    "for i in tqdm(range(500)):\n",
    "    if i % 30 == 0:\n",
    "        agent.force_fit = True\n",
    "    action, predi = agent.action(outcome, verbose=True)\n",
    "    df2 = agent.prealloc_df\n",
    "    df_explo2 = agent.visu_explo\n",
    "    outcome = environenment.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    history_good_inter.append((action == 'forward' and outcome == 'empty'))\n",
    "    history_bad_inter.append((action == 'forward' and outcome == 'wall'))\n",
    "    hisrory_val.append(valence[inter(action, outcome)])\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) if len(history_good) >= 10 else 0)\n",
    "    by_10_good_inter.append(sum(history_good_inter[-10:]) if len(history_good_inter) >= 10 else 0)\n",
    "    by_10_bad_inter.append(sum(history_bad_inter[-10:]) if len(history_bad_inter) >= 10 else 0)\n",
    "    mean_val.append(np.mean(hisrory_val[-10:]) / 10 if len(hisrory_val) >= 10 else 0)\n",
    "    environenment.save_world(\"imgToGif3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map integers to colors\n",
    "def int_to_color(value):\n",
    "    if value == 0:\n",
    "        return \"yellow\"\n",
    "    return \"gray\"\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#1 / (len(by_10_bad_inter) -10)\n",
    "for i, value in enumerate(agent.predictExplor):\n",
    "    ax.add_patch(plt.Rectangle((i, -10), 0.1, 10, color=int_to_color(value), alpha=0.4))\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(by_10_bad_inter, label='Bad inter', color='red')\n",
    "plt.plot(by_10_good_inter, label='Good inter', color='green')\n",
    "plt.plot(pourcent_by_10, label='Good pred', color='blue')\n",
    "# Transparent\n",
    "# plt.plot(mean_val, label='mean valence', color='black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"tmp.pdf\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save interaction and action and outcome\n",
    "np.save(\"action.npy\", agent.history_act)\n",
    "np.save(\"feedback.npy\", agent.history_fb)\n",
    "np.save(\"inter.npy\", agent.history_inter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(accs:list[list]):\n",
    "    new_list = []\n",
    "    for acc in accs:\n",
    "        new_list.append(acc[-1])\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Training accuracy')\n",
    "    plt.plot(new_list, label='Training accuracy', color='blue')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(agent.acc_train)\n",
    "print(len(agent.acc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_evolued_loss(agent.loss_train)\n",
    "see_evolued_acc(agent.acc_train)\n",
    "see_evolued_acc(agent.acc_test)\n",
    "\n",
    "see_evolued_acc(agent.time_train)\n",
    "see_evolued_acc(agent.time_expected_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More feel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environenment = gridWorldMoreFB(x=1, y=1, theta=0, range_feel=3, world=np.array([\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
    "                [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],\n",
    "                [1, 0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
    "                [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "                [1, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            ]))\n",
    "\n",
    "# environenment = gridWorldMoreFB(x=1, y=1, theta=0, range_feel=3, world=np.array([\n",
    "#                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "#                 [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#                 [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#                 [1, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "#                 [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#                 [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "#             ]))\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty_1') : 10,\n",
    "    inter('forward', 'empty_2') : 10,\n",
    "    inter('forward', 'empty_3') : 10,\n",
    "    inter('forward', 'wall') : -20,\n",
    "    inter('turn_left', 'empty_1') : -6,\n",
    "    inter('turn_left', 'empty_2') : -6,\n",
    "    inter('turn_left', 'empty_3') : -6,\n",
    "    inter('turn_left', 'wall') : -6,\n",
    "    inter('turn_right', 'empty_1') : -6,\n",
    "    inter('turn_right', 'empty_2') : -6,\n",
    "    inter('turn_right', 'empty_3') : -6,\n",
    "    inter('turn_right', 'wall') : -6,\n",
    "    inter('feel_front', 'empty_1') : -2,\n",
    "    inter('feel_front', 'empty_2') : -2,\n",
    "    inter('feel_front', 'empty_3') : -2,\n",
    "    inter('feel_front', 'wall') : -2,\n",
    "    inter('feel_right', 'empty_1') : -2,\n",
    "    inter('feel_right', 'empty_2') : -2,\n",
    "    inter('feel_right', 'empty_3') : -2,\n",
    "    inter('feel_right', 'wall') : -2,\n",
    "    inter('feel_left', 'empty_1') : -2,\n",
    "    inter('feel_left', 'empty_2') : -2,\n",
    "    inter('feel_left', 'empty_3') : -2,\n",
    "    inter('feel_left', 'wall') : -2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "num_layers = 1\n",
    "len_vocab = len(environenment.get_outcomes() + environenment.get_actions())\n",
    "\n",
    "lstm_classifier = LSTM_Classifeur(num_emb=len_vocab, output_size=len(environenment.get_outcomes()), \n",
    "                       num_layers=num_layers, hidden_size=hidden_size, dropout=0.5).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_classifier.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(environenment.get_outcomes() + environenment.get_actions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentLSTM(valence=valence, model=lstm_classifier, optimizer=optimizer, loss_fn=loss_func, explo=3,\n",
    "    data_val=None, device=device, gap=7, nb_epochs=50, maxDepth=5, seuil=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_good = []\n",
    "history_good_inter = []\n",
    "history_bad_inter = []\n",
    "hisrory_val = []\n",
    "pourcent_by_10  = []\n",
    "by_10_good_inter  = []\n",
    "by_10_bad_inter  = []\n",
    "pourcent_by_100  = []\n",
    "by_100_good_inter  = []\n",
    "by_100_bad_inter  = []\n",
    "mean_val = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "\n",
    "outcome = environenment.outcome(agent.action_choice)\n",
    "environenment.save_world(path=\"imgToGif3\")\n",
    "\n",
    "print(\"Start ...\")\n",
    "print(agent.action_choice)\n",
    "print(outcome)\n",
    "plot_output = widgets.Output()\n",
    "for i in tqdm(range(500)):\n",
    "    if i % 50 == 0:\n",
    "        agent.force_fit = True\n",
    "    if i % 100 == 0:\n",
    "        see_agent_evolued(by_10_bad_inter, by_10_good_inter, pourcent_by_10, mean_val, plot_output)\n",
    "    action, predi = agent.action(outcome, verbose=True)\n",
    "    print(agent.compute_seq)\n",
    "    df2 = agent.prealloc_df\n",
    "    df_explo2 = agent.visu_explo\n",
    "    #  save_evolued_acc(agent.acc_train[-agent.nb_epoch:], path=f\"plot3/acc_train_{i}.png\")\n",
    "    outcome = environenment.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    history_good_inter.append((action == 'forward' and outcome == 'empty_1'))\n",
    "    history_bad_inter.append((action == 'forward' and outcome == 'wall'))\n",
    "    hisrory_val.append(valence[inter(action, outcome)])\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) if len(history_good) >= 10 else 0)\n",
    "    by_10_good_inter.append(sum(history_good_inter[-10:]) if len(history_good_inter) >= 10 else 0)\n",
    "    by_10_bad_inter.append(sum(history_bad_inter[-10:]) if len(history_bad_inter) >= 10 else 0)\n",
    "    by_100_good_inter.append(sum(history_good_inter[-100:]) if len(history_good_inter) >= 100 else 0)\n",
    "    by_100_bad_inter.append(sum(history_bad_inter[-100:]) if len(history_bad_inter) >= 100 else 0)\n",
    "    mean_val.append(np.mean(hisrory_val[-10:]) / 10 if len(hisrory_val) >= 10 else 0)\n",
    "    environenment.save_world(path=\"imgToGif3\")\n",
    "\n",
    "agent.prealloc_df.to_csv(\"prealloc_df.csv\")\n",
    "agent.visu_explo.to_csv(\"df_explo.csv\")\n",
    "\n",
    "pourcent_by_10 = pourcent_by_10[10:]\n",
    "by_10_good_inter = by_10_good_inter[10:]\n",
    "by_10_bad_inter = by_10_bad_inter[10:]\n",
    "by_100_good_inter = by_100_good_inter[100:]\n",
    "by_100_bad_inter = by_100_bad_inter[100:]\n",
    "mean_val = mean_val[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environenment = gridWorldMoreFB(x=1, y=1, theta=0, range_feel=3, world=np.array([\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
    "                [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],\n",
    "                [1, 0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
    "                [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "                [1, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            ]))\n",
    "\n",
    "agent.history_act = [agent.history_act[-1]]\n",
    "agent.history_fb = [agent.history_fb[-1]]\n",
    "agent.history_inter = [agent.history_inter[-2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    if i % 30 == 0:\n",
    "        agent.force_fit = True\n",
    "    if i % 100 == 0:\n",
    "        see_agent_evolued(by_10_bad_inter, by_10_good_inter, pourcent_by_10, mean_val, plot_output)\n",
    "    action, predi = agent.action(outcome, verbose=True)\n",
    "    df2 = agent.prealloc_df\n",
    "    df_explo2 = agent.visu_explo\n",
    "    outcome = environenment.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    history_good_inter.append((action == 'forward' and outcome == 'empty_1'))\n",
    "    history_bad_inter.append((action == 'forward' and outcome == 'wall'))\n",
    "    hisrory_val.append(valence[inter(action, outcome)])\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) if len(history_good) >= 10 else 0)\n",
    "    by_10_good_inter.append(sum(history_good_inter[-10:]) if len(history_good_inter) >= 10 else 0)\n",
    "    by_10_bad_inter.append(sum(history_bad_inter[-10:]) if len(history_bad_inter) >= 10 else 0)\n",
    "    mean_val.append(np.mean(hisrory_val[-10:]) / 10 if len(hisrory_val) >= 10 else 0)\n",
    "    environenment.save_world(\"imgToGif3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_evolued_loss(agent.loss_train)\n",
    "see_evolued_acc(agent.acc_train)\n",
    "see_evolued_acc(agent.acc_test)\n",
    "\n",
    "see_evolued_acc(agent.time_train)\n",
    "see_evolued_acc(agent.time_expected_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map integers to colors\n",
    "def int_to_color(value):\n",
    "    if value == 0:\n",
    "        return \"yellow\"\n",
    "    return \"gray\"\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#1 / (len(by_10_bad_inter) -10)\n",
    "for i, value in enumerate(agent.predictExplor):\n",
    "    ax.add_patch(plt.Rectangle((i, -10), 0.1, 10, color=int_to_color(value), alpha=0.4))\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(by_10_bad_inter, label='bad inter', color='red')\n",
    "plt.plot(by_10_good_inter, label='good inter', color='green')\n",
    "plt.plot(pourcent_by_10, label='pr√©cision', color='blue')\n",
    "# Transparent\n",
    "# plt.plot(mean_val, label='mean valence', color='black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map integers to colors\n",
    "def int_to_color(value):\n",
    "    if value == 0:\n",
    "        return \"yellow\"\n",
    "    return \"gray\"\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#1 / (len(by_10_bad_inter) -10)\n",
    "for i, value in enumerate(agent.predictExplor):\n",
    "    ax.add_patch(plt.Rectangle((i, -10), 0.1, 10, color=int_to_color(value), alpha=0.4))\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(by_100_bad_inter, label='bad inter', color='red')\n",
    "plt.plot(by_100_good_inter, label='good inter', color='green')\n",
    "# plt.plot(pourcent_by_10, label='pr√©cision', color='blue')\n",
    "# Transparent\n",
    "# plt.plot(mean_val, label='mean valence', color='black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
