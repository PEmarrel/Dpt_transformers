{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Pour torch si vous avez un GPU\n",
    "# device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "device = \"cpu\" # Pour forcer l'utilisation du CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent qui utilise les LSTM\n",
    "Dans se notebook nous allons implémenter un agent qui utilise les LSTM pour prédire le prochain feedback. À la suite de cette prédiction nous allons implémenter un mécanisme capable de décider la meilleur action a faire.\n",
    "\n",
    "- model : LSTM\n",
    "- environement : grille 2D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environnement.environnement import Environnement as env # mother class\n",
    "from environnement.small_loop import small_loop\n",
    "\n",
    "# model machine learning\n",
    "from model.Tokenizer import *\n",
    "from model.RNN import *\n",
    "from model.CustomDataSet import CustomDataSet, CustomDataSetRNN\n",
    "from outil import *\n",
    "from inter.simpleInteraction import simpleInteraction as inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'agent :\n",
    "Nous devons passer à l'agent une valence qui doit contenir chaque action et outcome de l'environement. Cette valence permet à l'agent de connaitre toutes les actions et feedback possible. Elle aussi util au moment de la décision, c'est elle qui définie ce qu'es un bon comportement. \n",
    "\n",
    "Nous devons aussi lui passé un model de type RNN dans notre cas un LSTM (non entrainer), qui devra prédire les feedback possible. Pour que le modèl s'entraine nous avons besoin d'un optimizer et d'une fonction de perte. Pour obtenir de meilleur performance, nous allons passé à l'agent un nombre. Ce nombre corespond aux nombre d'action et d'outcome, que le modèl doit utiliser pour s'entrainer et prédire. Ce nombre sera appeler `gap_train` pour l'entrainement et `gap_predi` pour la prédiction. Pour obtenir de meilleur performance il faut que ce nombre soit le même pour les deux cas.\n",
    "\n",
    "Comme pour les agents développemental d'Olivier Georgeon, l'agent doit pouvoir sélectionner une séquence d'action à réaliser (tant que tout ce passe bien)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentLSTM:\n",
    "    def __init__(self, valence:dict[inter, float], model:nn.Module, max_depth:int, seuil:float,\n",
    "                optimizer, loss_fn, gap_train:int=11, gap_predi:int=11, nb_epoch:int=100, data_validate=None):\n",
    "        self.model = model\n",
    "        self.valence = valence\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.gap_train = gap_train\n",
    "        self.gap_predi = gap_predi\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.seq_to_exe = [] # Séquence d'actions et d'outcome choisit par la décision\n",
    "        self.history_act = [] # Historique des actions\n",
    "        self.history_fb = [] # Historique des feedbacks\n",
    "        self.data_validate = data_validate\n",
    "        self.max_depth:int = max_depth\n",
    "        self.seuil:float = seuil\n",
    "        \n",
    "        self.all_outcomes = set()\n",
    "        self.all_act = set()\n",
    "        key:inter = None\n",
    "        for key in valence.keys():\n",
    "            self.all_outcomes.add(key.getOutcome())\n",
    "            self.all_act.add(key.getAction())\n",
    "        self.all_outcomes = list(self.all_outcomes)\n",
    "        self.all_act = list(self.all_act)\n",
    "        \n",
    "        self.action_choice = self.all_act[0] # De base nous choisissons la première action\n",
    "        self.history_act.append(self.action_choice)\n",
    "        self.outcome_prediction = None # De base le modèl ne prédi rien\n",
    "        \n",
    "        # number_patern = np.sum([(len(self.all_act) * len(self.all_outcomes)) **i for i in range(1, self.max_depth + 1)])\n",
    "        number_patern = 20000\n",
    "        self.prealloc_df = pd.DataFrame(np.empty((number_patern, 4)), columns=[\"proposition\", \"valence\", \"action\", \"probability\"])\n",
    "        # self.prealloc_df = self.prealloc_df.astype({\"proposition\": \"U20\", \"valence\": float, \"action\": int, \"probability\": float})\n",
    "        self.current_index = 0\n",
    "        \n",
    "        # Nous avons besoin d'un tokenizer pour transformer les actions et outcomes en entiers\n",
    "        # Pour des questions de simplicité, nous voulons que les outcomes soient passé en premier\n",
    "        self.tokenizer = SimpleTokenizerV1(\n",
    "            vocab={key: i for i, key in enumerate(self.all_outcomes + self.all_act)})\n",
    "        \n",
    "        # Variable moniteur\n",
    "        self.loss_train = [] # Contient toutes les listes des pertes d'entrainement\n",
    "        self.loss_val = [] # Contient toutes les listes des pertes de validation\n",
    "        self.acc_train = []\n",
    "        self.acc_val = []\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Fonction de l'agent pour entrainer le modèle\n",
    "        \"\"\"        \n",
    "        dataset = CustomDataSetRNN(actions=self.history_act, outcomes=self.history_fb, \n",
    "                                 context_lenght=self.gap_train, dim_out=len(self.all_outcomes),\n",
    "                                 tokenizer=self.tokenizer)\n",
    "        \n",
    "        data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "        \n",
    "        if self.data_validate is not None:\n",
    "            dataset_test = CustomDataSetRNN(actions=self.data_validate[0], outcomes=self.data_validate[1], \n",
    "                                 context_lenght=self.gap_train, dim_out=len(self.all_outcomes),\n",
    "                                 tokenizer=self.tokenizer)\n",
    "            data_loader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)\n",
    "            loss_test = []\n",
    "        \n",
    "        for _ in range(self.nb_epoch):\n",
    "            self.model.train()\n",
    "            steps = 0\n",
    "            train_acc = 0\n",
    "            training_loss = []\n",
    "            for x,t in data_loader:\n",
    "                bs = t.shape[0]\n",
    "                h = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=device)\n",
    "                cell = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=device)\n",
    "\n",
    "                pred, h, cell = self.model(x, h, cell)\n",
    "\n",
    "                loss = self.loss_fn(pred[:, -1, :], t)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                training_loss.append(loss.item())\n",
    "                \n",
    "                train_acc += (pred[:, -1, :].argmax(1) == t).sum()\n",
    "                steps += bs\n",
    "                \n",
    "            self.loss_train.append(training_loss)\n",
    "            self.acc_train.append(train_acc)\n",
    "            \n",
    "            if self.data_validate is not None:\n",
    "                self.model.eval()\n",
    "                steps = 0\n",
    "                test_acc = 0\n",
    "                loss_test = []\n",
    "                \n",
    "                for text, label in data_loader_test:\n",
    "                    text = text.to(device)\n",
    "                    label = label.to(device)\n",
    "                    bs = label.shape[0]\n",
    "\n",
    "                    # Initialize hidden and memory states\n",
    "                    hidden = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=device)\n",
    "                    memory = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=device)\n",
    "                    \n",
    "                    # Forward pass through the model\n",
    "                    pred, hidden, memory = self.model(text, hidden, memory)\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    loss = self.loss_fn(pred[:, -1, :], label)\n",
    "                    loss_test.append(loss.item())\n",
    "\n",
    "                    # Calculate test accuracy\n",
    "                    test_acc += (pred[:, -1, :].argmax(1) == label).sum()\n",
    "                    steps += bs\n",
    "                    \n",
    "                self.loss_val.append(loss_test)\n",
    "                self.acc_val.append(test_acc)\n",
    "                \n",
    "    def predict(self, action):\n",
    "        \"\"\"\n",
    "        Fonction de l'agent pour prédire l'outcome en fonction de l'action \\\n",
    "        utilise l'historique des actions et outcomes comme contexte\n",
    "\n",
    "        Args:\n",
    "            action : L'action dont on prédit l'outcome \n",
    "\n",
    "        Raises:\n",
    "            Exception: Si l'historique des actions et outcomes est insuffisant\n",
    "\n",
    "        Returns:\n",
    "            out : L'outcome prédit\n",
    "        \"\"\"        \n",
    "        # Nous devons recupérer les gap dernières actions/outcomes\n",
    "        x = []\n",
    "        for i in range(-(self.gap_predi - 1) // 2, 0, 1):\n",
    "            x.append(self.history_act[i])\n",
    "            x.append(self.history_fb[i])\n",
    "        x.append(action)\n",
    "        seq_to_pred = self.tokenizer.encode(x)\n",
    "        # On simule un batch de taille 1\n",
    "        seq_to_pred = torch.tensor([seq_to_pred], device=device)\n",
    "        h = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=device)\n",
    "        cell = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=device)\n",
    "        probs, _, _ = self.model(seq_to_pred, h, cell)\n",
    "        \n",
    "        pred_feedback = torch.argmax(probs[:, -1, :]).item()\n",
    "        pred_feedback = self.tokenizer.decode(pred_feedback)\n",
    "        \n",
    "        return pred_feedback\n",
    "    \n",
    "    # def recursive_expective_valance(self, max_depth:int, proba:int, seuil:float, h, cell, verbose:bool=False):\n",
    "    #     \"\"\"\n",
    "    #     Teste toutes les actions possibles pour leur calculer l'expective valance\n",
    "        \n",
    "    #     Args:\n",
    "    #         max_depth (int): Maximum d'action à tester\n",
    "    #         seuil (float): Ne continuer que si le seuil n'est pas dépassé\n",
    "    #         h : La couche cachée du modèle\n",
    "    #         cell : La mémoire du modèle\n",
    "    #         verbose (bool, optional): Defaults to False.\n",
    "    #     \"\"\"\n",
    "    #     if max_depth <= 0 or proba < seuil:\n",
    "    #         return {}\n",
    "        \n",
    "    #     dict_seq = {}\n",
    "    #     for act in self.all_act:\n",
    "    #         act_to_pred = self.tokenizer.encode(act)\n",
    "    #         act_to_pred = torch.tensor([act_to_pred], device=device).unsqueeze(0)\n",
    "    #         # print(\"Hidden and Cell\")\n",
    "    #         # print(h)\n",
    "    #         # print(cell)\n",
    "            \n",
    "    #         probs, h_new, cell_new = self.model(act_to_pred, h, cell)\n",
    "    #         probs = torch.nn.functional.softmax(probs[0, -1, :], dim=0).tolist()\n",
    "    #         for fb in self.all_outcomes:\n",
    "    #             # print(f'probability \\033[0;32m proba {probs} \\033[0m')\n",
    "    #             fb_encode = self.tokenizer.encode(fb)\n",
    "    #             # print(fb_encode)\n",
    "    #             prob = probs[fb_encode]\n",
    "                \n",
    "    #             dict_seq[str([act, fb])] = prob * self.valence[inter(act, fb)]\n",
    "    #             if verbose:\n",
    "    #                 # print(f\"Depth {max_depth}, Action: {act}, Outcome: {fb}, Valence: {dict_seq[act]}\")\n",
    "    #                 pass\n",
    "                    \n",
    "    #             x = torch.tensor([fb_encode], device=device).unsqueeze(0)                    \n",
    "    #             _, h_new_to_pass, cell_new_to_pass = self.model(x, h_new, cell_new)\n",
    "    #             following = self.recursive_expective_valance(max_depth=max_depth - 1, \n",
    "    #                                                         proba=proba * prob,\n",
    "    #                                                         seuil=seuil, \n",
    "    #                                                         h=h_new_to_pass, \n",
    "    #                                                         cell=cell_new_to_pass, \n",
    "    #                                                         verbose=verbose)\n",
    "                \n",
    "    #             for key, value in following.items():\n",
    "    #                 new_key = [act, fb]\n",
    "    #                 new_key += eval(key)\n",
    "    #                 dict_seq[str(new_key)] = value + dict_seq[str([act, fb])]\n",
    "                    \n",
    "    #     return dict_seq\n",
    "    \n",
    "    def recursif_expective_valance(self, context:list, max_depth:int, seuil:float=0.2, proba:float = 1, seq_predi:list = [], valence_precedente:float = 0):\n",
    "        \"\"\"\n",
    "        Create the list of proposed sequences\n",
    "        \"\"\"\n",
    "        max_depth -= 1\n",
    "        self.model.eval()\n",
    "        \n",
    "        # print('debug recu val :')\n",
    "        # print(\"probabilite\", proba)\n",
    "        # print(\"context\", context)\n",
    "        # print(\"max_depth\", max_depth)\n",
    "        # print(\"seuil\", seuil)\n",
    "        # print(\"seq_predi\", seq_predi)\n",
    "        \n",
    "        # Compute the expected valence of each action\n",
    "        for act in self.all_act:\n",
    "            new_seq = seq_predi + [act]\n",
    "            seq_to_predict = context + [self.tokenizer.encode(act)]\n",
    "            seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.int).to(device)\n",
    "\n",
    "            hidden = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=device)\n",
    "            memory = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=device)\n",
    "\n",
    "            x, _, _ = self.model(seq_to_predict, hidden, memory)\n",
    "            x = x[0, -1, :]\n",
    "\n",
    "            # Transforme x into list proba\n",
    "            probs = torch.nn.functional.softmax(x, dim=0).tolist()\n",
    "            \n",
    "            # for each outcome, record the expected valence\n",
    "            for i, out in enumerate(self.all_outcomes):\n",
    "                tmp_new_seq = new_seq + [out]\n",
    "                tmp_proba = probs[i] * proba\n",
    "                # If the probability is above a threshold\n",
    "                if tmp_proba > seuil:\n",
    "                    # Record the proposed sequence with its expected valence\n",
    "                    expected_valence = float(np.round(self.valence[inter(act, out)] * tmp_proba, decimals=4)) + valence_precedente\n",
    "                    self.prealloc_df.iloc[self.current_index] = [str(tmp_new_seq), expected_valence, tmp_new_seq[0], tmp_proba]\n",
    "                    self.current_index += 1\n",
    "                    # If the max_depth is not reached \n",
    "                    if max_depth > 0: \n",
    "                        # Recursively look for longer sequences\n",
    "                        new_context = context + self.tokenizer.encode([act, out])\n",
    "                        self.recursif_expective_valance(context=new_context[2:], max_depth=max_depth, seuil=seuil, \n",
    "                            proba=tmp_proba, seq_predi=tmp_new_seq.copy(), valence_precedente=expected_valence)\n",
    "    \n",
    "    def expective_valance(self, verbose:bool=False):\n",
    "        \"\"\"\n",
    "        Permet de calculer l'expective valance d'une séquence d'interaction\n",
    "\n",
    "        Args:\n",
    "            max_depth (int): _description_\n",
    "            seuil (float, optional): _description_. Defaults to 0.2.\n",
    "            verbose (bool, optional): _description_. Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = []\n",
    "        for i in range(-(self.gap_predi - 1) // 2, 0, 1):\n",
    "            x.append(self.history_act[i])\n",
    "            x.append(self.history_fb[i])\n",
    "        seq_to_pred = self.tokenizer.encode(x)\n",
    "        self.prealloc_df[:] = np.nan\n",
    "        self.prealloc_df[\"valence\"] = -np.inf\n",
    "        self.current_index = 0\n",
    "        return self.recursif_expective_valance(context=seq_to_pred,\n",
    "                                            max_depth=self.max_depth,\n",
    "                                            proba=1, seq_predi=[],\n",
    "                                            seuil=self.seuil)\n",
    "    def decide(self):\n",
    "        if self.seq_to_exe and len(self.seq_to_exe) > 1:\n",
    "            out = self.seq_to_exe.pop(0)\n",
    "            if out == self.history_fb[-1]:\n",
    "                act = self.seq_to_exe.pop(0)\n",
    "                return act\n",
    "            else:\n",
    "                self.seq_to_exe = []\n",
    "        self.seq_to_exe = []        \n",
    "        \n",
    "        self.expective_valance()\n",
    "        self.seq_to_exe = self.prealloc_df.sort_values(by=\"valence\", ascending=False).iloc[0].proposition\n",
    "        self.seq_to_exe = eval(self.seq_to_exe)\n",
    "        print(\"after compute ...\")\n",
    "        print(self.seq_to_exe)\n",
    "        act = self.seq_to_exe.pop(0)\n",
    "        return act\n",
    "        \n",
    "    def action(self, real_outcome, verbose=False, explore:bool=False):\n",
    "        \"\"\"\n",
    "        La fonction action permet à l'agent de choisir une action en fonction de l'outcome réel.\n",
    "        Cette fonction entraine le modèle a prévoir les outcomes futurs en fonction des actions passées.\n",
    "\n",
    "        Args:\n",
    "            real_outcome : L'outcome réel suite à l'action de l'agent\n",
    "            verbose : Affiche les informations sur l'entrainement ou non\n",
    "        \"\"\"\n",
    "        # La première étape est de sauvegarder l'outcome réel\n",
    "        self.history_fb.append(real_outcome)\n",
    "        good_pred:bool = self.outcome_prediction == real_outcome\n",
    "        if verbose :\n",
    "            print(f\"Action: {self.action_choice}, Prediction: {self.outcome_prediction}, Outcome: {real_outcome}, \\033[0;31m Satisfaction: {good_pred} \\033[0m\")\n",
    "        \n",
    "        # Ensuite nous regardons si nous devons entrainer le modèle\n",
    "        # not(explore) and \n",
    "        if not(good_pred) and len(self.history_fb) + len(self.history_fb) > self.gap_train:\n",
    "            self.fit()\n",
    "        # Nous devons maintenant choisir une action\n",
    "        if len(self.history_fb) + len(self.history_fb) > self.gap_predi:\n",
    "            self.action_choice = self.decide()\n",
    "            self.outcome_prediction = self.predict(self.action_choice)\n",
    "        else:\n",
    "            self.action_choice = np.random.choice(self.all_act)\n",
    "        # self.action_choice = np.random.choice(self.all_act)\n",
    "        self.history_act.append(self.action_choice)\n",
    "        \n",
    "        return self.action_choice, self.outcome_prediction\n",
    "        \n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environement\n",
    "Nous pouvons entrainer sur divers environement. Ici nous allons tester sur une grille 2D avec un robot qui peut faire ces actions :\n",
    "- forward\n",
    "- turn left\n",
    "- turn right\n",
    "- feel_front\n",
    "\n",
    "L'enviroment renvoie simplement :\n",
    "- Empty\n",
    "- Wall\n",
    "\n",
    "# Valence\n",
    "Pour donner un bon comportement a notre robot nous allons essayer avec cette valence :\n",
    "- forward, empty : 1\n",
    "- forward, wall : -30  `Nous voulons que le robot évite les murs`\n",
    "- turn_left, empty : -7\n",
    "- turn_left, wall : -7 `Cas impossible, mais nous devons le noter`\n",
    "- turn_right, empty : -7\n",
    "- turn_right, wall : -7 `Cas impossible, mais nous devons le noter`\n",
    "- feel_front, wall : -5\n",
    "- feel_front, empty : -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environenment = small_loop(x=1, y=1, theta=0, world=np.array([\n",
    "                [1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 1],\n",
    "                [1, 0, 1, 0, 1],\n",
    "                [1, 0, 0, 0, 1],\n",
    "                [1, 1, 1, 1, 1],\n",
    "            ]))\n",
    "\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 7,\n",
    "    inter('forward', 'wall') : -30,\n",
    "    inter('turn_left', 'empty') : -7,\n",
    "    inter('turn_left', 'wall') : -7,\n",
    "    inter('turn_right', 'empty') : -7,\n",
    "    inter('turn_right', 'wall') : -7,\n",
    "    inter('feel_front', 'wall') : -7,\n",
    "    inter('feel_front', 'empty') : -7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "len_vocab = len(environenment.get_outcomes() + environenment.get_actions())\n",
    "\n",
    "# Create the LSTM classifier model\n",
    "lstm_classifier = LSTM(num_emb=len_vocab, output_size=2, \n",
    "                       num_layers=num_layers, hidden_size=hidden_size).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_classifier.parameters(), lr=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(environenment.get_outcomes() + environenment.get_actions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour évaluler la performance du modèle\n",
    "act_val, fb_val = [], []\n",
    "for i in trange(1000):\n",
    "    action = np.random.choice(environenment.get_actions())\n",
    "    outcome = environenment.outcome(action)\n",
    "    act_val.append(action)\n",
    "    fb_val.append(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# Définition de l'agent\n",
    "agent = AgentLSTM(valence=valence, model=lstm_classifier, optimizer=optimizer, loss_fn=loss_func,\n",
    "    gap_predi=5, gap_train=5, max_depth=4, seuil=0.5, nb_epoch=100,\n",
    "    data_validate=(act_val, fb_val))\n",
    "history_good = []\n",
    "pourcent_by_10  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "outcome = environenment.outcome(agent.action_choice)\n",
    "\n",
    "for i in tqdm(range(80)):\n",
    "    action, predi = agent.action(outcome, verbose=True, explore=True)\n",
    "    df = agent.prealloc_df\n",
    "    outcome = environenment.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    # env_test2.display_world(out)\n",
    "    environenment.save_world()\n",
    "\n",
    "print(agent.loss_train)\n",
    "see_evolued_acc(agent.loss_train)\n",
    "    \n",
    "# for i in tqdm(range(40)):\n",
    "#     action, predi = agent.action(outcome, verbose=True, explore=False)\n",
    "#     outcome = environenment.outcome(action)\n",
    "#     history_good.append(outcome == predi)\n",
    "#     pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "#     # env_test2.display_world(out)\n",
    "#     environenment.save_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.loss_train)\n",
    "see_evolued_acc(agent.loss_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
