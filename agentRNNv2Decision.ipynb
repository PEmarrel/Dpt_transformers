{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pandas as pd\n",
    "# Pour torch si vous avez un GPU\n",
    "# device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "device = \"cpu\" # Pour forcer l'utilisation du CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environnement.environnement import Environnement as env # mother class\n",
    "from environnement.small_loop import small_loop\n",
    "\n",
    "# model machine learning\n",
    "from model.Tokenizer import *\n",
    "from model.RNN import *\n",
    "from model.CustomDataSet import CustomDataSet, CustomDataSetRNN\n",
    "from outil import *\n",
    "from inter.simpleInteraction import simpleInteraction as inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'agent :\n",
    "L'idée de se fichier est de mettre en place une exploration guidé. Le principe est de repérer les séquences pour lequel le model de prédicition n'est pas sûr, et de choisir celle qui aurait la meilleur valence. Cette idée ne peut fonctionner que si les séquences sont déterministe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentLSTM:\n",
    "    def __init__(self, valence:dict[inter, float], model:nn.Module, max_depth:int, seuil:float,\n",
    "                optimizer, loss_fn, gap_train:int=11, gap_predi:int=11, nb_epoch:int=100, data_validate=None):\n",
    "        self.model = model\n",
    "        self.valence = valence\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.gap_train = gap_train\n",
    "        self.gap_predi = gap_predi\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.seq_to_exe = [] # Séquence d'actions et d'outcome choisit par la décision\n",
    "        self.history_act = [] # Historique des actions\n",
    "        self.history_fb = [] # Historique des feedbacks\n",
    "        self.history_inter = [] # Historique des interactions\n",
    "        self.data_validate = data_validate\n",
    "        self.max_depth:int = max_depth\n",
    "        self.seuil:float = seuil\n",
    "        self.force_fit = False\n",
    "        \n",
    "        self.all_outcomes = set()\n",
    "        self.all_act = set()\n",
    "        key:inter = None\n",
    "        for key in valence.keys():\n",
    "            self.all_outcomes.add(key.getOutcome())\n",
    "            self.all_act.add(key.getAction())\n",
    "        self.all_outcomes = list(self.all_outcomes)\n",
    "        self.all_act = list(self.all_act)\n",
    "        \n",
    "        self.action_choice = self.all_act[0] # De base nous choisissons la première action\n",
    "        self.history_act.append(self.action_choice)\n",
    "        self.outcome_prediction = None # De base le modèl ne prédi rien\n",
    "        \n",
    "        # number_patern = np.sum([(len(self.all_act) * len(self.all_outcomes)) **i for i in range(1, (self.max_depth // 2) +1)])\n",
    "        number_patern = 2000000\n",
    "        self.prealloc_df = pd.DataFrame(np.empty((number_patern, 5)), columns=[\"proposition\", \"valence\", \"action\", \"probability\", \"val_sucess\"])\n",
    "        # self.prealloc_df = self.prealloc_df.astype({\"proposition\": \"U20\", \"valence\": float, \"action\": int, \"probability\": float})\n",
    "        self.current_index = 0\n",
    "        \n",
    "        self.visu_explo = pd.DataFrame(np.empty((number_patern, 2)), columns=[\"seqence\", \"valence\"])\n",
    "        self.visu_explo = self.visu_explo.astype({\"seqence\": \"U20\", \"valence\": float})\n",
    "        self.current_index_explo = 0\n",
    "        \n",
    "        self.seq_explo = []\n",
    "        self.valence_explo = -np.inf\n",
    "        \n",
    "        self.visu_val = pd.DataFrame(np.empty((len(data_validate[0]), 3)), columns=[\"seqence\", \"probablility\", \"good\"])\n",
    "        self.visu_val = self.visu_val.astype({\"seqence\": \"U20\", \"probablility\": float, \"good\": bool})\n",
    "        \n",
    "        # Nous avons besoin d'un tokenizer pour transformer les actions et outcomes en entiers\n",
    "        # Pour des questions de simplicité, nous voulons que les outcomes soient passé en premier\n",
    "        self.tokenizer = SimpleTokenizerV1(\n",
    "            vocab={key: i for i, key in enumerate(self.all_outcomes + self.all_act)})\n",
    "        \n",
    "        # Variable moniteur\n",
    "        self.loss_train = [] # Contient toutes les listes des pertes d'entrainement\n",
    "        self.loss_val = [] # Contient toutes les listes des pertes de validation\n",
    "        self.acc_train = []\n",
    "        self.acc_val = []\n",
    "        \n",
    "        self.time_train = []\n",
    "        self.time_compute_expected_valence = []\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Fonction de l'agent pour entrainer le modèle\n",
    "        \"\"\"        \n",
    "        dataset = CustomDataSetRNN(actions=self.history_act, outcomes=self.history_fb, \n",
    "                                 context_lenght=self.gap_train, dim_out=len(self.all_outcomes),\n",
    "                                 tokenizer=self.tokenizer)\n",
    "        \n",
    "        data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        if self.data_validate is not None:\n",
    "            dataset_test = CustomDataSetRNN(actions=self.data_validate[0], outcomes=self.data_validate[1], \n",
    "                                 context_lenght=self.gap_train, dim_out=len(self.all_outcomes),\n",
    "                                 tokenizer=self.tokenizer)\n",
    "            data_loader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)\n",
    "            loss_test = []\n",
    "        \n",
    "        time_train = time.time()\n",
    "        for i in range(self.nb_epoch):\n",
    "            self.model.train()\n",
    "            steps = 0\n",
    "            train_acc = 0\n",
    "            training_loss = []\n",
    "            for tmp, (x,t) in enumerate(data_loader):\n",
    "                bs = t.shape[0]\n",
    "                h = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=device)\n",
    "                cell = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=device)\n",
    "\n",
    "                pred, h, cell = self.model(x, h, cell)\n",
    "\n",
    "                loss = self.loss_fn(pred[:, -1, :], t)\n",
    "                training_loss.append(loss.item())\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_acc += sum((pred[:, -1, :].argmax(1) == t).cpu().numpy())\n",
    "                steps += bs\n",
    "                \n",
    "            self.acc_train.append(train_acc / steps)\n",
    "            # time_val_epoch = time.time()\n",
    "            if self.data_validate is not None:\n",
    "                self.model.eval()\n",
    "                steps = 0\n",
    "                test_acc = 0\n",
    "                loss_test = []\n",
    "                \n",
    "                for text, label in data_loader_test:\n",
    "                    text = text.to(device)\n",
    "                    label = label.to(device)\n",
    "                    bs = label.shape[0]\n",
    "\n",
    "                    # Initialize hidden and memory states\n",
    "                    hidden = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=device)\n",
    "                    memory = torch.zeros(self.model.num_layers, bs, self.model.hidden_size, device=device)\n",
    "                    \n",
    "                    # Forward pass through the model\n",
    "                    pred, hidden, memory = self.model(text, hidden, memory)\n",
    "                    \n",
    "                    for i in range(bs):\n",
    "                        self.visu_val.iloc[steps + i] = [str(self.tokenizer.decode(text[i].cpu().tolist())), \n",
    "                                                         float(torch.nn.functional.softmax(pred[i, -1, :], dim=-1).max().item()), \n",
    "                                                         int(pred[i, -1, :].argmax().item() == label[i])]\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    loss = self.loss_fn(pred[:, -1, :], label)\n",
    "                    loss_test.append(loss.item())\n",
    "\n",
    "                    # Calculate test accuracy\n",
    "                    test_acc +=  sum((pred[:, -1, :].argmax(1) == label).cpu().numpy())\n",
    "                    steps += bs\n",
    "                    \n",
    "                loss_test.append(loss_test)\n",
    "                self.acc_val.append(test_acc / steps)\n",
    "                # print(f\"Validation time: {time.time() - time_val_epoch}\")    \n",
    "            self.loss_train.append(training_loss)\n",
    "            # If acc is 100% we stop the training\n",
    "            if self.acc_train[-1] >= 0.99:\n",
    "                for _ in range(i, self.nb_epoch):\n",
    "                    self.acc_train.append(self.acc_train[-1])\n",
    "                    self.acc_val.append(self.acc_val[-1])\n",
    "                break\n",
    "            \n",
    "        print(f\"Training time: {time.time() - time_train}\")\n",
    "        self.time_train.append(time.time() - time_train)\n",
    "                \n",
    "    def predict(self, action):\n",
    "        \"\"\"\n",
    "        Fonction de l'agent pour prédire l'outcome en fonction de l'action \\\n",
    "        utilise l'historique des actions et outcomes comme contexte\n",
    "\n",
    "        Args:\n",
    "            action : L'action dont on prédit l'outcome \n",
    "\n",
    "        Raises:\n",
    "            Exception: Si l'historique des actions et outcomes est insuffisant\n",
    "\n",
    "        Returns:\n",
    "            out : L'outcome prédit\n",
    "        \"\"\"        \n",
    "        # Nous devons recupérer les gap dernières actions/outcomes\n",
    "        x = []\n",
    "        for i in range(-(self.gap_predi - 1) // 2, 0, 1):\n",
    "            x.append(self.history_act[i])\n",
    "            x.append(self.history_fb[i])\n",
    "        x.append(action)\n",
    "        seq_to_pred = self.tokenizer.encode(x)\n",
    "        # On simule un batch de taille 1\n",
    "        seq_to_pred = torch.tensor([seq_to_pred], device=device)\n",
    "        h = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=device)\n",
    "        cell = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=device)\n",
    "        probs, _, _ = self.model(seq_to_pred, h, cell)\n",
    "        \n",
    "        pred_feedback = torch.argmax(probs[:, -1, :]).item()\n",
    "        pred_feedback = self.tokenizer.decode(pred_feedback)\n",
    "        \n",
    "        return pred_feedback\n",
    "    \n",
    "    def recursif_valance_explo(self, max_depth:int, seq_predi:list = [], valence_succes_pred:float = 0):\n",
    "        max_depth -= 1\n",
    "        inter_max, value = max(self.valence.items(), key=lambda y: y[1])\n",
    "        for _ in range(max_depth):\n",
    "            seq_predi += [inter_max.getAction(), inter_max.getOutcome()]\n",
    "            valence_succes_pred += value\n",
    "        self.visu_explo.iloc[self.current_index_explo] = [str(seq_predi), valence_succes_pred]\n",
    "        self.current_index_explo += 1\n",
    "        if valence_succes_pred > self.valence_explo:\n",
    "            self.seq_explo = seq_predi\n",
    "            self.valence_explo = valence_succes_pred                    \n",
    "    \n",
    "    def recursif_expective_valance(self, context:list, max_depth:int, seuil:float=0.5, proba:float = 1, seq_predi:list = [], valence_pred:float = 0, valence_succes_pred:float = 0):\n",
    "        \"\"\"\n",
    "        Create the list of proposed sequences\n",
    "        \"\"\"\n",
    "        max_depth -= 1\n",
    "        \n",
    "        self.model.eval()\n",
    "        # Compute the expected valence of each action\n",
    "        for act in self.all_act:\n",
    "            new_seq = seq_predi + [act]\n",
    "            seq_to_predict = context + [self.tokenizer.encode(act)]\n",
    "            \n",
    "            # print('find seq')\n",
    "            # print(new_seq)\n",
    "            # print([i for i in zip(self.history_act, self.history_fb)])\n",
    "            sub_list = subfinder(self.history_inter, seq_to_predict)\n",
    "            # print(sub_list)\n",
    "            # print(f'for act : {act} and context : {seq_to_predict}')\n",
    "            if sub_list == []:\n",
    "                # print('i want explore')\n",
    "                # Get max valence and outcome associate by act\n",
    "                inter_max, value = max([(inter(act, out), self.valence[inter(act, out)]) for out in self.all_outcomes], key=lambda y: y[1])\n",
    "                # print(\"debug\")\n",
    "                # print('act :', act)\n",
    "                # print(inter_max)\n",
    "                # print(value)\n",
    "                # print('valence_succes_pred :', valence_succes_pred)\n",
    "                # print('goal :', self.valence_explo)\n",
    "                new_seq += [inter_max.getOutcome()]\n",
    "                tmp_value = valence_succes_pred + value\n",
    "                self.visu_explo.iloc[self.current_index_explo] = [str(new_seq), valence_succes_pred]\n",
    "                self.current_index_explo += 1\n",
    "                if tmp_value > self.valence_explo:\n",
    "                    # print('start new sequence')\n",
    "                    # print(new_seq)\n",
    "                    # print('valence')\n",
    "                    # print(valence_succes_pred)\n",
    "                    self.seq_explo = new_seq\n",
    "                    self.valence_explo = valence_succes_pred\n",
    "                # Nous n'avons jamais vue la séquence, nous choisisont d'imaginer le meilleur sénario\n",
    "                # print('sequence not seen')\n",
    "                self.recursif_valance_explo(max_depth=max_depth, seq_predi=new_seq , \n",
    "                                            valence_succes_pred=tmp_value)\n",
    "                continue\n",
    "            # print('sequence already seen')\n",
    "            \n",
    "            seq_to_predict = torch.tensor([seq_to_predict], dtype=torch.int).to(device)\n",
    "\n",
    "            hidden = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=device)\n",
    "            memory = torch.zeros(self.model.num_layers, 1, self.model.hidden_size, device=device)\n",
    "\n",
    "            x, _, _ = self.model(seq_to_predict, hidden, memory)\n",
    "            x = x[0, -1, :]\n",
    "            # Transforme x into list proba\n",
    "            probs = torch.nn.functional.softmax(x, dim=0).tolist()\n",
    "            # for each outcome, record the expected valence\n",
    "            for i, out in enumerate(self.all_outcomes):\n",
    "                tmp_new_seq = new_seq + [out]\n",
    "                tmp_proba = probs[i] * proba\n",
    "                # If the probability is above a threshold\n",
    "                sucess_valence = self.valence[inter(act, out)] + valence_succes_pred\n",
    "                expected_valence = None\n",
    "                if tmp_proba > seuil:\n",
    "                    expected_valence = float(np.round(self.valence[inter(act, out)] * tmp_proba, decimals=4)) + valence_pred\n",
    "                    # If the max_depth is not reached \n",
    "                    if max_depth > 0: \n",
    "                        # Recursively look for longer sequences\n",
    "                        new_context = context + self.tokenizer.encode([act, out])\n",
    "                        self.recursif_expective_valance(context=new_context[2:], max_depth=max_depth, seuil=seuil, \n",
    "                            proba=tmp_proba, seq_predi=tmp_new_seq.copy(), valence_pred=expected_valence, valence_succes_pred=sucess_valence)\n",
    "                self.prealloc_df.iloc[self.current_index] = [str(tmp_new_seq), expected_valence, tmp_new_seq[0], tmp_proba, sucess_valence]\n",
    "                self.current_index += 1\n",
    "    \n",
    "    def expective_valance(self, verbose:bool=False):\n",
    "        \"\"\"\n",
    "        Permet de calculer l'expective valance d'une séquence d'interaction\n",
    "\n",
    "        Args:\n",
    "            max_depth (int): _description_\n",
    "            seuil (float, optional): _description_. Defaults to 0.2.\n",
    "            verbose (bool, optional): _description_. Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = []\n",
    "        for i in range(-(self.gap_predi - 1) // 2, 0, 1):\n",
    "            x.append(self.history_act[i])\n",
    "            x.append(self.history_fb[i])\n",
    "        seq_to_pred = self.tokenizer.encode(x)\n",
    "        self.prealloc_df[:] = np.empty((len(self.prealloc_df), 5))\n",
    "        self.prealloc_df[\"valence\"] = -np.inf\n",
    "        self.current_index = 0\n",
    "        self.seq_explo = []\n",
    "        self.valence_explo = -np.inf\n",
    "        self.visu_explo[:] = np.empty((len(self.visu_explo), 2))\n",
    "        self.current_index_explo = 0\n",
    "        return self.recursif_expective_valance(context=seq_to_pred,\n",
    "                                            max_depth=self.max_depth,\n",
    "                                            proba=1, seq_predi=[],\n",
    "                                            seuil=self.seuil)\n",
    "    def decide(self):\n",
    "        if self.seq_to_exe and len(self.seq_to_exe) > 1:\n",
    "            out = self.seq_to_exe.pop(0)\n",
    "            if out == self.history_fb[-1]:\n",
    "                act = self.seq_to_exe.pop(0)\n",
    "                return act\n",
    "            else:\n",
    "                self.force_fit = True\n",
    "        self.seq_to_exe = []        \n",
    "        \n",
    "        time_compute_expective_val = time.time()\n",
    "        self.expective_valance()\n",
    "        print(f\"Time to compute expective valance: {time.time() - time_compute_expective_val}\")\n",
    "        self.time_compute_expected_valence.append(time.time() - time_compute_expective_val)\n",
    "        # Keep row with probability between 0.4 and 0.6\n",
    "        # compute_df = self.prealloc_df[(self.prealloc_df[\"probability\"] > 0.3) & (self.prealloc_df[\"probability\"] < 0.7)]\n",
    "        # if len(compute_df) == 0:\n",
    "        self.seq_to_exe = self.prealloc_df.sort_values(by=\"valence\", ascending=False).iloc[0].proposition\n",
    "        expected_val = self.prealloc_df.sort_values(by=\"valence\", ascending=False).iloc[0].valence\n",
    "        # else:\n",
    "            # print(\"renforce ...\")\n",
    "            # self.seq_to_exe = compute_df.sort_values(by=\"val_sucess\", ascending=False).iloc[0].proposition\n",
    "            # self.force_fit = True\n",
    "        print(f\"before compute ... model predict : {self.seq_to_exe}, explo want : {self.seq_explo}\")\n",
    "        # tempo\n",
    "        print(f\"expected valence : {expected_val}\")\n",
    "        print(f\"valence explo : {self.valence_explo}\")\n",
    "        if self.seq_to_exe is not None and expected_val > self.valence_explo:\n",
    "            self.seq_to_exe = eval(self.seq_to_exe)\n",
    "            print(\"after compute ...\", self.seq_to_exe)\n",
    "        else:\n",
    "            print(\"explo ...\", self.seq_explo)\n",
    "            self.seq_to_exe = self.seq_explo\n",
    "            self.force_fit = True\n",
    "        act = self.seq_to_exe.pop(0)\n",
    "        return act\n",
    "        \n",
    "    def action(self, real_outcome, verbose=False, explore:bool=False):\n",
    "        \"\"\"\n",
    "        La fonction action permet à l'agent de choisir une action en fonction de l'outcome réel.\n",
    "        Cette fonction entraine le modèle a prévoir les outcomes futurs en fonction des actions passées.\n",
    "\n",
    "        Args:\n",
    "            real_outcome : L'outcome réel suite à l'action de l'agent\n",
    "            verbose : Affiche les informations sur l'entrainement ou non\n",
    "        \"\"\"\n",
    "        # La première étape est de sauvegarder l'outcome réel\n",
    "        self.history_fb.append(real_outcome)\n",
    "        self.history_inter.append(self.tokenizer.encode(real_outcome))\n",
    "        good_pred:bool = self.outcome_prediction == real_outcome\n",
    "        if verbose :\n",
    "            print(f\"\\033[0;31m Action: {self.action_choice} \\033[0m, Prediction: {self.outcome_prediction}, Outcome: {real_outcome}, \\033[0;31m Satisfaction: {good_pred} \\033[0m\")\n",
    "        \n",
    "        # Ensuite nous regardons si nous devons entrainer le modèle\n",
    "        # not(explore) and \n",
    "        if (not(good_pred) or self.force_fit)and len(self.history_fb) + len(self.history_fb) > self.gap_train:\n",
    "            self.fit()\n",
    "            self.force_fit = False\n",
    "            \n",
    "        elif len(self.history_fb) + len(self.history_fb) > self.gap_train:\n",
    "            for _ in range(self.nb_epoch):\n",
    "                self.acc_train.append(self.acc_train[-1])\n",
    "                self.acc_val.append(self.acc_val[-1])\n",
    "\n",
    "        # Nous devons maintenant choisir une action\n",
    "        if len(self.history_fb) + len(self.history_fb) > self.gap_predi:\n",
    "            self.action_choice = self.decide()\n",
    "            self.outcome_prediction = self.predict(self.action_choice)\n",
    "        else:\n",
    "            inter_max, value = max(self.valence.items(), key=lambda y: y[1])\n",
    "            self.action_choice = inter_max.getAction()\n",
    "        # self.action_choice = np.random.choice(self.all_act)\n",
    "        self.history_act.append(self.action_choice)\n",
    "        self.history_inter.append(self.tokenizer.encode(self.action_choice))\n",
    "        \n",
    "        return self.action_choice, self.outcome_prediction\n",
    "        \n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environenment = small_loop(x=1, y=1, theta=0, world=np.array([\n",
    "                [1, 1, 1, 1, 1],\n",
    "                [1, 0, 0, 0, 1],\n",
    "                [1, 0, 1, 0, 1],\n",
    "                [1, 0, 0, 0, 1],\n",
    "                [1, 1, 1, 1, 1],\n",
    "            ]))\n",
    "\n",
    "# environenment = small_loop(x=1, y=1, theta=0, world=np.array([\n",
    "#                 [1, 1, 1, 1, 1, 1],\n",
    "#                 [1, 0, 0, 0, 1, 1],\n",
    "#                 [1, 0, 1, 0, 0, 1],\n",
    "#                 [1, 0, 1, 1, 0, 1],\n",
    "#                 [1, 0, 0, 0, 0, 1],\n",
    "#                 [1, 1, 1, 1, 1, 1],\n",
    "#             ]))\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 5,\n",
    "    inter('forward', 'wall') : -100,\n",
    "    inter('turn_left', 'empty') : -21,\n",
    "    inter('turn_left', 'wall') : -100,\n",
    "    inter('turn_right', 'empty') : -21,\n",
    "    inter('turn_right', 'wall') : -100,\n",
    "    inter('feel_front', 'wall') : -20,\n",
    "    inter('feel_front', 'empty') : -15,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "hidden_size = 16\n",
    "num_layers = 1\n",
    "len_vocab = len(environenment.get_outcomes() + environenment.get_actions())\n",
    "\n",
    "# Create the LSTM classifier model\n",
    "lstm_classifier = LSTM(num_emb=len_vocab, output_size=2, \n",
    "                       num_layers=num_layers, hidden_size=hidden_size, dropout=0.5).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm_classifier.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(environenment.get_outcomes() + environenment.get_actions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour évaluler la performance du modèle\n",
    "act_val, fb_val = [], []\n",
    "for i in trange(1000):\n",
    "    action = np.random.choice(environenment.get_actions())\n",
    "    outcome = environenment.outcome(action)\n",
    "    act_val.append(action)\n",
    "    fb_val.append(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentLSTM(valence=valence, model=lstm_classifier, optimizer=optimizer, loss_fn=loss_func,\n",
    "    gap_predi=3, gap_train=3, max_depth=5, seuil=0.3, nb_epoch=20,\n",
    "    data_validate=(act_val, fb_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_good = []\n",
    "history_good_inter = []\n",
    "history_bad_inter = []\n",
    "hisrory_val = []\n",
    "pourcent_by_10  = []\n",
    "by_10_good_inter  = []\n",
    "by_10_bad_inter  = []\n",
    "mean_val = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "\n",
    "outcome = environenment.outcome(agent.action_choice)\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    action, predi = agent.action(outcome, verbose=True)\n",
    "    df = agent.prealloc_df\n",
    "    df_val = agent.visu_val\n",
    "    df_explo = agent.visu_explo\n",
    "    outcome = environenment.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    history_good_inter.append((action == 'forward' and outcome == 'empty'))\n",
    "    history_bad_inter.append((action == 'forward' and outcome == 'wall'))\n",
    "    hisrory_val.append(valence[inter(action, outcome)])\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    by_10_good_inter.append(sum(history_good_inter[-10:]) if len(history_good_inter) >= 10 else 0)\n",
    "    by_10_bad_inter.append(sum(history_bad_inter[-10:]) if len(history_bad_inter) >= 10 else 0)\n",
    "    mean_val.append(np.mean(hisrory_val[-10:]) if len(hisrory_val) >= 10 else 0)\n",
    "    environenment.save_world()\n",
    "    \n",
    "pourcent_by_10 = pourcent_by_10[10:]\n",
    "by_10_good_inter = by_10_good_inter[10:]\n",
    "by_10_bad_inter = by_10_bad_inter[10:]\n",
    "mean_val = mean_val[10:]\n",
    "\n",
    "    \n",
    "# for i in tqdm(range(40)):\n",
    "#     action, predi = agent.action(outcome, verbose=True, explore=False)\n",
    "#     outcome = environenment.outcome(action)\n",
    "#     history_good.append(outcome == predi)\n",
    "#     pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "#     # env_test2.display_world(out)\n",
    "#     environenment.save_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(50)):\n",
    "    action, predi = agent.action(outcome, verbose=True)\n",
    "    df2 = agent.prealloc_df\n",
    "    df_val2 = agent.visu_val\n",
    "    df_explo2 = agent.visu_explo\n",
    "    outcome = environenment.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    history_good_inter.append((action == 'forward' and outcome == 'empty'))\n",
    "    history_bad_inter.append((action == 'forward' and outcome == 'wall'))\n",
    "    hisrory_val.append(valence[inter(action, outcome)])\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    by_10_good_inter.append(sum(history_good_inter[-10:]) if len(history_good_inter) >= 10 else 0)\n",
    "    by_10_bad_inter.append(sum(history_bad_inter[-10:]) if len(history_bad_inter) >= 10 else 0)\n",
    "    mean_val.append(np.mean(hisrory_val[-10:]) if len(hisrory_val) >= 10 else 0)\n",
    "    environenment.save_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subfinder(mylist, pattern):\n",
    "    matches = []\n",
    "    for i in range(len(mylist)):\n",
    "        if mylist[i] == pattern[0] and mylist[i:i+len(pattern)] == pattern:\n",
    "            matches.append(pattern)\n",
    "    return matches\n",
    "\n",
    "subfinder([('turn_left', 'empty'), ('turn_left', 'empty'), ('turn_right', 'empty'), ('turn_right', 'empty')],\n",
    "          [('turn_left', 'empty'), ('turn_right', 'empty')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([str(i) for i in zip(agent.history_act, agent.history_fb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_evolued_loss(agent.loss_train)\n",
    "see_evolued_loss(agent.loss_val)\n",
    "see_evolued_acc(agent.acc_train)\n",
    "see_evolued_acc(agent.acc_val)\n",
    "\n",
    "see_evolued_acc(agent.time_train)\n",
    "see_evolued_acc(agent.time_compute_expected_valence)\n",
    "\n",
    "pourcent_by_10 = [i / 10 for i in pourcent_by_10]\n",
    "plt.plot(by_10_bad_inter, label=f'bad inter', color='red')\n",
    "plt.plot(by_10_good_inter, label=f'good inter', color='green')\n",
    "plt.plot(pourcent_by_10, label=f'global', color='blue')\n",
    "plt.plot(mean_val, label=f'mean valence', color='black')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "\n",
    "# Get weights\n",
    "embds = agent.model.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "# Plotting function\n",
    "def plot_words(data, start, stop, step):\n",
    "    trace = go.Scatter(\n",
    "        x = data[start:stop:step, 0], \n",
    "        y = data[start:stop:step, 1],\n",
    "        mode = 'markers',\n",
    "        text = [tokenizer.decode(i) for i in range(start, stop, step)]\n",
    "    )\n",
    "    layout = dict(title= 't-SNE 1 vs t-SNE 2',\n",
    "                  yaxis = dict(title='t-SNE 2'),\n",
    "                  xaxis = dict(title='t-SNE 1'),\n",
    "                  hovermode= 'closest')\n",
    "    fig = dict(data = [trace], layout= layout)\n",
    "    py.iplot(fig)\n",
    "\n",
    "# Visualize words in two dimensions \n",
    "# Set perplexity to a value less than the number of samples\n",
    "perplexity_value = min(30, len(embds) - 1)  # Ensure perplexity is less than the number of samples\n",
    "conv_tsne_embds = TSNE(n_components=2, perplexity=5).fit_transform(embds)\n",
    "plot_words(conv_tsne_embds, 0, len(embds), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
