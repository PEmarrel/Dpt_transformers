{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Pour torch si vous avez un GPU\n",
    "# device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "device = \"cpu\" # Pour forcer l'utilisation du CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environement potentielement testé\n",
    "from environnement.environnement import Environnement as env # mother class\n",
    "from environnement.environnement1 import Environnement1 as env1\n",
    "from environnement.environnement2Str import Environnement2 as env2Str\n",
    "from environnement.environnement3Str import Environnement3 as env3Str\n",
    "from environnement.environnement6Str import Environnement6 as env6Str\n",
    "from environnement.small_loop import small_loop\n",
    "from ipywidgets import Output\n",
    "\n",
    "# model machine learning\n",
    "from model.DeepNN import *\n",
    "from model.Tokenizer import *\n",
    "from model.CustomDataSet import CustomDataSet\n",
    "from outil import *\n",
    "from inter.interactions import Interaction\n",
    "from inter.simpleInteraction import simpleInteraction as inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'agent qui devine les meilleurs patterns\n",
    "Je vais maintenant implémenter un mécanisme qui va tenter de trouver les meilleurs pattern.\n",
    "## Critere d'un pattern :\n",
    "Un bon pattern est un pattern pour lequel Prob * Valance est élever. Nous voulons que l'agent sout capable de dire : cette action est bonne car après je vais pouvoir avoir une bonne valence. Donc il faut prédire sur du long terme.\n",
    "## L'idée\n",
    "L'idée est de regarder les probabilités que donne le modèl pour certainne action. Créer des enchainements d'actions, en découlera des prédictions de pattern. Nous pourrons alors éxecuter des bonnes séquence d'action.\n",
    "\n",
    "## Valence\n",
    "La valence est primordiale pour notre. C'est elle qui va faire emerger un comportement à notre agent. \n",
    "\n",
    "### Exemple small loop\n",
    "Prennons un environement un peu complexe. On peut imaginer un monde en 2 dimensions, sur une grille de 5x5 avec les bords et la case du centre comme obstacle. Nous voulons que notre agent apprenne a ce balader sans se prendre d'obstacle. Graphiquement cela donne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_env = small_loop(x=1, y=1, theta=0)\n",
    "# demo_env.display_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginons que nous avons un modèle prédictif parfait. Il faut choisir la meilleur action. Pour simplifier disons que nous pouvons prédire que sur 3 interactions futur, sans connaitre la position absolut du robot la meilleur solution est la suivante :\n",
    "- **Feel front** regarder si devans nous il y a un obstacle. Nous voulons que la prédiction soit égal à 50% wall 50% Empty. En tout cas une probabilité qui s'en rapproche.\n",
    "- **move** Nous voulons que le robot agisse en fonction, soit avancé soit touner.\n",
    "\n",
    "Si nous somme dans une intéraction ou nous avons `feel front` alors nous voulons directement faire l'action qui correspond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentPE:\n",
    "    def __init__(self, model, all_outcomes, all_actions, tokenizer, optimizer=None, loss_func=None):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self._otimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._tokenizer:SimpleTokenizerV1 = tokenizer\n",
    "        self._all_outcomes = all_outcomes\n",
    "        self._all_actions = all_actions\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "\n",
    "\n",
    "    def fit(self, actions:list, outcomes:list, nb_epoch:int= 5, validate_loader=None):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent \n",
    "        Avec data set custom, le model prends en inputs plusieurs données\n",
    "        \"\"\"\n",
    "        context_lenght = self._model.input_size\n",
    "        if len(actions) + len(outcomes) < context_lenght:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "\n",
    "        actions = [self._tokenizer.encode(act) for act in actions]\n",
    "        outcomes = self._tokenizer.encode(outcomes)\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.train()\n",
    "            # actions = torch.tensor(actions, dtype=torch.float).to(device) # On passe toutes les actions que l'agent a fais\n",
    "            # outcomes = torch.tensor(outcomes, dtype=torch.long).to(device) # On passe toutes oputcomes qu'il a \n",
    "\n",
    "            data_loarder = CustomDataSet(actions=actions, outcomes=outcomes, # On va créer un dataset\n",
    "                         context_lenght=context_lenght, dim_out=len(self._all_outcomes))\n",
    "            \n",
    "            # A la place de donnée x = [act1] y = [out1]\n",
    "            # Nous voulons donné : x = [act1, out1, act2] y = [out2]\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader( # On utilise torch pour charger les données\n",
    "                data_loarder,batch_size=32, shuffle=True)\n",
    "\n",
    "            train_with_batch(model=self._model, \n",
    "                    train_loader=data_loader,\n",
    "                    optimizer=self._otimizer,\n",
    "                    loss_func=self._loss_func,\n",
    "                    nb_epochs=nb_epoch,\n",
    "                    validate_loader=validate_loader,\n",
    "                    print_=True)\n",
    "        else: # Si le model n'est pas un model pytorch\n",
    "            raise Exception('Not implemented')\n",
    "            self._model.fit(action, outcome)\n",
    "            pass\n",
    "\n",
    "    def get_prediction(self, action):\n",
    "        gap = (self._model.input_size - 1) // 2\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        action = self._tokenizer.encode(x)\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor(action, dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.nn.functional.softmax(x, dim=0)\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return x\n",
    "       # def check_all_actions(self):\n",
    "    #     act_to_test = None\n",
    "    #     for act in self._all_actions:\n",
    "    #         if act not in self._history_act:\n",
    "    #             act_to_test = act\n",
    "    #             break\n",
    "    #     return act_to_test\n",
    "    \n",
    "    # def decide(self):\n",
    "    #     \"\"\"\n",
    "    #     Fonction qui choisit l'action a faire en fonction des prédictions \\\n",
    "    #     du modèles entrainné. Nous renforçons choisisons les actions que \\\n",
    "    #     ou le modèle n'est pas sûr.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     act_test = self.check_all_actions()\n",
    "    #     if act_test:\n",
    "    #         print(\"i don't know\", act_test)\n",
    "    #         self._action = act_test\n",
    "    #         return act_test\n",
    "\n",
    "    #     best_act = self._all_actions[0]\n",
    "    #     best_expected_val = -np.inf\n",
    "\n",
    "    #     # On vérifie que l'on a vue assez d'interaction pour faire des prédictions\n",
    "    #     if len(self._history_act) + len(self._history_fb) < self._model.input_size:\n",
    "    #         print(\"Not enough data to make a decision\")\n",
    "    #         return best_act\n",
    "\n",
    "    #     # Vérifie si le modèles est sur de sa prédiction\n",
    "    #     for act in self._all_actions:\n",
    "    #         probs:torch.Tensor = self.get_prediction(act)\n",
    "    #         max_prob = torch.max(probs).item()\n",
    "    #         # Formule utiliser : 1 / n + 0.5 / n\n",
    "    #         print(f'for action {act} probs {probs} max_prob {max_prob}')\n",
    "    #         if max_prob < 1 / probs.size(dim=0) + 0.5 / probs.size(dim=0):\n",
    "    #             print(\"je ne suis pas sur de \", act)\n",
    "    #             if len(self._all_actions) + len(self._all_outcomes) > self._model.input_size:\n",
    "    #                 self.fit(self._history_act, self._history_fb, validate_loader=None)\n",
    "\n",
    "    #         probs:torch.Tensor = self.get_prediction(act)\n",
    "    #         if max_prob < 1 / probs.size(dim=0) + 0.5 / probs.size(dim=0):\n",
    "    #             print(\"je n'arrive pas à être sur de \", act)\n",
    "    #             return act\n",
    "            \n",
    "    #         # Si le modèle as une prédiction sur, on regarde sa valance\n",
    "    #         predi = self._tokenizer.decode(torch.argmax(probs, dim=0).item())\n",
    "    #         expected_val = self._valance[inter(act, predi)]\n",
    "    #         if expected_val > best_expected_val:\n",
    "    #             best_act = act\n",
    "    #             best_expected_val = expected_val\n",
    "    #             print(f\"Action: {act}, Expected valance: {expected_val}\")\n",
    "    #     self._action = best_act\n",
    "    #     return best_act\n",
    "\n",
    "    # Modifier\n",
    "    def predict(self, action):\n",
    "        \"\"\"\n",
    "        Funciton de prédiction\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self._history_act) + len(self._history_fb) + 1 < self._model.input_size:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        \n",
    "        gap = (self._model.input_size - 1) // 2\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        action = self._tokenizer.encode(x)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor(action, dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.argmax(x, dim=0).item()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return self._tokenizer.decode(x)\n",
    "\n",
    "    # Modofier\n",
    "    def action(self, outcome, fit=True, validate_loader=None):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \" \n",
    "                  f\"\\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\")\n",
    "            if self._predicted_outcome != outcome:\n",
    "                if len(self._history_act) + len(self._history_fb) > self._model.input_size:\n",
    "                    self.fit(self._history_act, self._history_fb, validate_loader=validate_loader)\n",
    "                else:\n",
    "                    self._action = self._all_actions[0]\n",
    "\n",
    "            # Maintenant nous choisissons la prochaine action en fonction de la valance\n",
    "            self._action = str(np.random.choice(self._all_actions))\n",
    "            if len(self._history_act) + len(self._history_fb) + 1 > self._model.input_size:\n",
    "                self._predicted_outcome = self.predict(self._action)\n",
    "            self._history_act.append(self._action)\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)            \n",
    "            print(f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\")\n",
    "        \n",
    "        return self._action, self._predicted_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test2 = small_loop(x=1, y=1, theta=0)\n",
    "\n",
    "model_ML = DeepNetwork(hidden_size=[16], input_size=3, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=0.01, weight_decay=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "agent_test2 = AgentPE(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "for i in range(200):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On teste certaines sequences de prédiction\n",
    "print(\"Test de prédiction\")\n",
    "seq = tokenizer.encode(['forward', 'wall', 'forward'])\n",
    "seq = torch.tensor(seq, dtype=torch.float).to(device)\n",
    "predi =  agent_test2._model(seq)\n",
    "prob = torch.nn.functional.softmax(predi, dim=0)\n",
    "deocde = tokenizer.decode(torch.argmax(predi, dim=0).item())\n",
    "print(f\"Prédiction de la séquence 'forward', 'wall', 'forward', probabilité {prob}, decode {deocde}\")\n",
    "\n",
    "seq = tokenizer.encode(['feel_front', 'empty', 'forward'])\n",
    "seq = torch.tensor(seq, dtype=torch.float).to(device)\n",
    "predi =  agent_test2._model(seq)\n",
    "prob = torch.nn.functional.softmax(predi, dim=0)\n",
    "print(prob)\n",
    "deocde = tokenizer.decode(torch.argmax(predi, dim=0).item())\n",
    "print(f\"Prédiction de la séquence 'feel_front', 'empty', 'forward' probabilité {prob} decode {deocde}\")\n",
    "\n",
    "seq = tokenizer.encode(['feel_front', 'wall', 'forward'])\n",
    "seq = torch.tensor(seq, dtype=torch.float).to(device)\n",
    "predi =  agent_test2._model(seq)\n",
    "prob = torch.nn.functional.softmax(predi, dim=0)\n",
    "deocde = tokenizer.decode(torch.argmax(predi, dim=0).item())\n",
    "print(f\"Prédiction de la séquence 'feel_front', 'wall', 'forward', probabilité {prob} decode {deocde}\")\n",
    "\n",
    "# Cas non prédictible correctement :\n",
    "seq = tokenizer.encode(['forward', 'empty', 'forward'])\n",
    "seq = torch.tensor(seq, dtype=torch.float).to(device)\n",
    "predi =  agent_test2._model(seq)\n",
    "prob = torch.nn.functional.softmax(predi, dim=0)\n",
    "deocde = tokenizer.decode(torch.argmax(predi, dim=0).item())\n",
    "print(f\"Prédiction de la séquence forward', 'empty', 'forward' :  probabilité {prob}, decode {deocde}\")\n",
    "\n",
    "seq = tokenizer.encode(['turn_left', 'empty', 'forward'])\n",
    "seq = torch.tensor(seq, dtype=torch.float).to(device)\n",
    "predi =  agent_test2._model(seq)\n",
    "prob = torch.nn.functional.softmax(predi, dim=0)\n",
    "deocde = tokenizer.decode(torch.argmax(predi, dim=0).item())\n",
    "print(f\"Prédiction de la séquence 'turn_left', 'empty', 'forward'  probabilité {prob}, decode {deocde}\")\n",
    "\n",
    "seq = tokenizer.encode(['turn_right', 'empty', 'forward'])\n",
    "seq = torch.tensor(seq, dtype=torch.float).to(device)\n",
    "predi =  agent_test2._model(seq)\n",
    "prob = torch.nn.functional.softmax(predi, dim=0)\n",
    "deocde = tokenizer.decode(torch.argmax(predi, dim=0).item())\n",
    "print(f\"Prédiction de la séquence 'turn_right', 'empty', 'forward', probabilité {prob}, decode {deocde}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1er version de la fonction qui détermine l'expected valence :\n",
    "\n",
    "def tempo_recursif_expective_valance(model:nn.Module, env, seq:list,\n",
    "                                    max_depth:int, valance:dict, tokenizer:SimpleTokenizerV1,\n",
    "                                    seuil:float=0.2, proba:float = 1,\n",
    "                                    seq_predi:list = []):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if max_depth == 0:\n",
    "        return {}\n",
    "    max_depth -= 1\n",
    "\n",
    "    if proba < seuil:\n",
    "        return {}\n",
    "    \n",
    "    model.eval()\n",
    "    exceptive_valance = {}\n",
    "    for act in env.get_actions():\n",
    "        new_seq = seq_predi + [act]\n",
    "        seq_to_predict = seq + [tokenizer.encode(act)]\n",
    "        seq_to_predict = torch.tensor(seq_to_predict, dtype=torch.float).to(device)\n",
    "        x = model(seq_to_predict)\n",
    "        # Transforme x into list proba\n",
    "        probs = torch.nn.functional.softmax(x, dim=0).tolist()\n",
    "        # for each outcomes we want proba with act\n",
    "        for i, out in enumerate(env.get_outcomes()):\n",
    "            tmp_new_seq = new_seq + [out]\n",
    "            tmp_proba = probs[i] * proba\n",
    "            if tmp_proba < seuil:\n",
    "                continue\n",
    "            tempo = float(np.round(valance[inter(act, out)] * tmp_proba, decimals=4))\n",
    "            # input(f'seq {seq_predi} act {act} out {out} proba {tmp_proba} valance {valance[(act, out)]} tempo {tempo}')\n",
    "\n",
    "            exceptive_valance.update(\n",
    "                tempo_recursif_expective_valance(model=model, env=env, seq=seq[2:] + [tokenizer.encode(act), tokenizer.encode(out)],\n",
    "                                            max_depth=max_depth, valance=valance, seuil=seuil, \n",
    "                                            proba=tmp_proba, seq_predi=tmp_new_seq.copy(), tokenizer=tokenizer)\n",
    "            )\n",
    "            exceptive_valance[str(tmp_new_seq)] = tempo\n",
    "    return exceptive_valance\n",
    "\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 10,\n",
    "    inter('forward', 'wall') : -30,\n",
    "    inter('turn_left', 'empty') : -3,\n",
    "    inter('turn_left', 'wall') : -3,\n",
    "    inter('turn_right', 'empty') : -3,\n",
    "    inter('turn_right', 'wall') : -3,\n",
    "    inter('feel_front', 'wall') : -5,\n",
    "    inter('feel_front', 'empty') : -5,\n",
    "}\n",
    "\n",
    "print(env_test2.get_actions())\n",
    "print(env_test2.get_outcomes())\n",
    "\n",
    "\n",
    "res = tempo_recursif_expective_valance(model=model_ML, env=env_test2,\n",
    "        seq=tokenizer.encode(['feel_front', 'wall']), max_depth=3, valance=valence, tokenizer=tokenizer, seuil=0.2)\n",
    "\n",
    "print(res)\n",
    "\n",
    "# Top 5 of sequences with the best expected valance\n",
    "top_5 = sorted(res.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for top in top_5:\n",
    "    print(f\"Sequence: {top[0]} Expected valance: {top[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"probalité\")\n",
    "seq = tokenizer.encode(['forward', 'wall', 'forward'])\n",
    "seq = torch.tensor(seq, dtype=torch.float).to(device)\n",
    "predi =  agent_test2._model(seq)\n",
    "prob = torch.nn.functional.softmax(predi, dim=0)\n",
    "deocde = tokenizer.decode(torch.argmax(predi, dim=0).item())\n",
    "print(f\"Prédiction de la séquence 'turn_right', 'empty', 'forward', probabilité {prob}, decode {deocde}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentPEBis:\n",
    "    def __init__(self, model, all_outcomes, all_actions, tokenizer, valence:dict,\n",
    "                optimizer=None, loss_func=None):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self._otimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._tokenizer:SimpleTokenizerV1 = tokenizer\n",
    "        self._all_outcomes = all_outcomes\n",
    "        self._all_actions = all_actions\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "        self._valence = valence\n",
    "\n",
    "\n",
    "    def fit(self, actions:list, outcomes:list, nb_epoch:int= 20, validate_loader=None):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent \n",
    "        Avec data set custom, le model prends en inputs plusieurs données\n",
    "        \"\"\"\n",
    "        context_lenght = self._model.input_size\n",
    "        if len(actions) + len(outcomes) < context_lenght:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "\n",
    "        actions = [self._tokenizer.encode(act) for act in actions]\n",
    "        outcomes = self._tokenizer.encode(outcomes)\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.train()\n",
    "            data_loarder = CustomDataSet(actions=actions, outcomes=outcomes,\n",
    "                         context_lenght=context_lenght, dim_out=len(self._all_outcomes))\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                data_loarder,batch_size=32, shuffle=True)\n",
    "\n",
    "            train_with_batch(model=self._model, \n",
    "                    train_loader=data_loader,\n",
    "                    optimizer=self._otimizer,\n",
    "                    loss_func=self._loss_func,\n",
    "                    nb_epochs=nb_epoch,\n",
    "                    validate_loader=validate_loader,\n",
    "                    print_=True)\n",
    "        else: # Si le model n'est pas un model pytorch\n",
    "            raise Exception('Not implemented')\n",
    "            self._model.fit(action, outcome)\n",
    "            pass\n",
    "\n",
    "    def get_prediction(self, action):\n",
    "        gap = (self._model.input_size - 1) // 2\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        action = self._tokenizer.encode(x)\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor(action, dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.nn.functional.softmax(x, dim=0)\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def predict(self, action):\n",
    "        \"\"\"\n",
    "        Funciton de prédiction\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self._history_act) + len(self._history_fb) + 1 < self._model.input_size:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        \n",
    "        gap = (self._model.input_size - 1) // 2\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        action = self._tokenizer.encode(x)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor(action, dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.argmax(x, dim=0).item()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return self._tokenizer.decode(x)\n",
    "    \n",
    "    def recursif_expective_valance(self, seq:list, max_depth:int, seuil:float=0.2, proba:float = 1,\n",
    "                                    seq_predi:list = []):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if max_depth == 0:\n",
    "            return {}\n",
    "        max_depth -= 1\n",
    "\n",
    "        if proba < seuil:\n",
    "            return {}\n",
    "        \n",
    "        self._model.eval()\n",
    "        exceptive_valance = {}\n",
    "        for act in self._all_actions:\n",
    "            new_seq = seq_predi + [act]\n",
    "            seq_to_predict = seq + [tokenizer.encode(act)]\n",
    "            seq_to_predict = torch.tensor(seq_to_predict, dtype=torch.float).to(device)\n",
    "            x = self._model(seq_to_predict)\n",
    "            # Transforme x into list proba\n",
    "            probs = torch.nn.functional.softmax(x, dim=0).tolist()\n",
    "            # for each outcomes we want proba with act\n",
    "            for i, out in enumerate(self._all_outcomes):\n",
    "                tmp_new_seq = new_seq + [out]\n",
    "                tmp_proba = probs[i] * proba\n",
    "                if tmp_proba < seuil:\n",
    "                    continue\n",
    "                tempo = float(np.round(self._valence[inter(act, out)] * tmp_proba, decimals=4))\n",
    "                # input(f'seq {seq_predi} act {act} out {out} proba {tmp_proba} valance {valance[(act, out)]} tempo {tempo}')\n",
    "                exceptive_valance.update(\n",
    "                    self.recursif_expective_valance(seq=seq[2:] + [tokenizer.encode(act), tokenizer.encode(out)],\n",
    "                                                max_depth=max_depth, seuil=seuil, \n",
    "                                                proba=tmp_proba, seq_predi=tmp_new_seq.copy())\n",
    "                )\n",
    "                exceptive_valance[str(tmp_new_seq)] = tempo\n",
    "        return exceptive_valance\n",
    "        \n",
    "\n",
    "    def decide(self):\n",
    "        gap = (self._model.input_size - 1) // 2\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        seq = self._tokenizer.encode(x)\n",
    "        res = self.recursif_expective_valance(seq=seq, max_depth=3, seuil=0.2)\n",
    "        top_5 = sorted(res.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(f\"Top 5 of sequences with the best expected valance for {x}\")\n",
    "        for top in top_5:\n",
    "            print(f\"Sequence: {top[0]} Expected valance: {top[1]}\")\n",
    "        \n",
    "        print(f\"Action choisie : {eval(top_5[0][0])[0]}\")\n",
    "        return eval(top_5[0][0])[0]\n",
    "\n",
    "\n",
    "    # Modifier\n",
    "    def action(self, outcome, decide=True, validate_loader=None):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \" \n",
    "                  f\"\\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\")\n",
    "            if self._predicted_outcome != outcome:\n",
    "                if len(self._history_act) + len(self._history_fb) > self._model.input_size:\n",
    "                    self.fit(self._history_act, self._history_fb, validate_loader=validate_loader)\n",
    "                else:\n",
    "                    self._action = self._all_actions[0]\n",
    "\n",
    "            # Maintenant nous choisissons la prochaine action en fonction de la valance\n",
    "            if decide:\n",
    "                self._action = self.decide()\n",
    "            else :\n",
    "                self._action = str(np.random.choice(self._all_actions))\n",
    "            if len(self._history_act) + len(self._history_fb) + 1 > self._model.input_size:\n",
    "                self._predicted_outcome = self.predict(self._action)\n",
    "            self._history_act.append(self._action)\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)            \n",
    "            print(f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\")\n",
    "        \n",
    "        return self._action, self._predicted_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test2 = small_loop(x=1, y=1, theta=0)\n",
    "\n",
    "model_ML = DeepNetwork(hidden_size=[16], input_size=3, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=0.01, weight_decay=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 1,\n",
    "    inter('forward', 'wall') : -10,\n",
    "    inter('turn_left', 'empty') : -3,\n",
    "    inter('turn_left', 'wall') : -3,\n",
    "    inter('turn_right', 'empty') : -3,\n",
    "    inter('turn_right', 'wall') : -3,\n",
    "    inter('feel_front', 'wall') : -5,\n",
    "    inter('feel_front', 'empty') : -5,\n",
    "}\n",
    "\n",
    "agent_test2 = AgentPEBis(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    valence=valence,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "for i in range(10):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# On l'entraine 200 fois\n",
    "\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, decide=True)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "    # sleep(0.5)\n",
    "    # EnvDisplay.show(env_test2.get_world(), env_test2.get_robot(), out)\n",
    "    # display(out)\n",
    "    # time.sleep(0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultat\n",
    "Nous voyons un problème apparaître, le modèl prédit à 49% 'forward', 'empty' parce qu'il ne s'est pas assez entrainner. Donc en espected valence le choix forward aurais bcp d'importance alors qu'il y a peu de possibilité.\n",
    "\n",
    "Il y a un autre problème, le modèle ne fais pas de différence entre `turn left` et `turn right`, nous avons vue que le modèle avec le mécanisme de décision pouvais choisire de faire l'un puis l'autre avant de nouveau faire `forward`. Ce qui n'a pas de sens.\n",
    "\n",
    "## Solution\n",
    "Nous pouvons données plus d'epoch lors du fit. Mais le problème risque toujours de se poser si l'environement change ou si un pattern d'interaction est sous exploité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agentbored:\n",
    "    def __init__(self, model, all_outcomes, all_actions, tokenizer, valence:dict,\n",
    "                optimizer=None, loss_func=None):\n",
    "        \"\"\" \n",
    "        Création de l'agent.\n",
    "        \n",
    "        - self._action : action précédente\n",
    "        - self._predicted_outcome : prédiction de l'outcome précédent\n",
    "        \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "        self._model = model\n",
    "        self._otimizer = optimizer\n",
    "        self._loss_func = loss_func\n",
    "        self._tokenizer:SimpleTokenizerV1 = tokenizer\n",
    "        self._all_outcomes = all_outcomes\n",
    "        self._all_actions = all_actions\n",
    "        self._history_act = []\n",
    "        self._history_fb = []\n",
    "        self._valence = valence\n",
    "        self._bored = 0\n",
    "\n",
    "    def fit(self, actions:list, outcomes:list, nb_epoch:int= 5, validate_loader=None):\n",
    "        \"\"\"\n",
    "        Fonction d'entrainement de l'agent \n",
    "        Avec data set custom, le model prends en inputs plusieurs données\n",
    "        \"\"\"\n",
    "        context_lenght = self._model.input_size\n",
    "        if len(actions) + len(outcomes) < context_lenght:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "\n",
    "        actions = [self._tokenizer.encode(act) for act in actions]\n",
    "        outcomes = self._tokenizer.encode(outcomes)\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.train()\n",
    "            data_loarder = CustomDataSet(actions=actions, outcomes=outcomes,\n",
    "                         context_lenght=context_lenght, dim_out=len(self._all_outcomes))\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                data_loarder,batch_size=32, shuffle=True)\n",
    "\n",
    "            train_with_batch(model=self._model, \n",
    "                    train_loader=data_loader,\n",
    "                    optimizer=self._otimizer,\n",
    "                    loss_func=self._loss_func,\n",
    "                    nb_epochs=nb_epoch,\n",
    "                    validate_loader=validate_loader,\n",
    "                    print_=False)\n",
    "        else: # Si le model n'est pas un model pytorch\n",
    "            raise Exception('Not implemented')\n",
    "            self._model.fit(action, outcome)\n",
    "            pass\n",
    "\n",
    "    def get_prediction(self, action):\n",
    "        gap = (self._model.input_size - 1) // 2\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        action = self._tokenizer.encode(x)\n",
    "        \n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor(action, dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.nn.functional.softmax(x, dim=0)\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def predict(self, action):\n",
    "        \"\"\"\n",
    "        Funciton de prédiction\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self._history_act) + len(self._history_fb) + 1 < self._model.input_size:\n",
    "            raise Exception(\"Not enough data to train model\")\n",
    "        \n",
    "        gap = (self._model.input_size - 1) // 2\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        x.append(action)\n",
    "        action = self._tokenizer.encode(x)\n",
    "        if isinstance(self._model, torch.nn.Module):\n",
    "            self._model.eval() \n",
    "            action = torch.tensor(action, dtype=torch.float).to(device)\n",
    "            x = self._model(action)\n",
    "            x = torch.argmax(x, dim=0).item()\n",
    "\n",
    "        else:\n",
    "            raise Exception('Not implemented')\n",
    "            x=self._model.predict(action)\n",
    "        \n",
    "        return self._tokenizer.decode(x)\n",
    "    \n",
    "    def recursif_expective_valance(self, seq:list, max_depth:int, seuil:float=0.2, proba:float = 1,\n",
    "                                    seq_predi:list = []):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if max_depth == 0:\n",
    "            return {}\n",
    "        max_depth -= 1\n",
    "\n",
    "        if proba < seuil:\n",
    "            return {}\n",
    "        \n",
    "        self._model.eval()\n",
    "        exceptive_valance = {}\n",
    "        for act in self._all_actions:\n",
    "            new_seq = seq_predi + [act]\n",
    "            seq_to_predict = seq + [self._tokenizer.encode(act)]\n",
    "            seq_to_predict = torch.tensor(seq_to_predict, dtype=torch.float).to(device)\n",
    "            x = self._model(seq_to_predict)\n",
    "            # Transforme x into list proba\n",
    "            probs = torch.nn.functional.softmax(x, dim=0).tolist()\n",
    "            # for each outcomes we want proba with act\n",
    "            for i, out in enumerate(self._all_outcomes):\n",
    "                tmp_new_seq = new_seq + [out]\n",
    "                tmp_proba = probs[i] * proba\n",
    "                if tmp_proba < seuil:\n",
    "                    continue\n",
    "                tempo = float(np.round(self._valence[inter(act, out)] * tmp_proba, decimals=4))\n",
    "                # input(f'seq {seq_predi} act {act} out {out} proba {tmp_proba} valance {valance[(act, out)]} tempo {tempo}')\n",
    "                exceptive_valance.update(\n",
    "                    self.recursif_expective_valance(seq=seq[2:] + [tokenizer.encode(act), tokenizer.encode(out)],\n",
    "                                                max_depth=max_depth, seuil=seuil, \n",
    "                                                proba=tmp_proba, seq_predi=tmp_new_seq.copy())\n",
    "                )\n",
    "                exceptive_valance[str(tmp_new_seq)] = tempo\n",
    "        return exceptive_valance\n",
    "        \n",
    "    def decide(self):\n",
    "        gap = (self._model.input_size - 1) // 2\n",
    "        x = []\n",
    "        for i in range(len(self._history_act) - gap, len(self._history_act)):\n",
    "            x.append(self._history_act[i])\n",
    "            x.append(self._history_fb[i])\n",
    "        seq = self._tokenizer.encode(x)\n",
    "        res = self.recursif_expective_valance(seq=seq, max_depth=3, seuil=0.4)\n",
    "        top_5 = sorted(res.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        # print(f\"Top 5 of sequences with the best expected valance for {x}\")\n",
    "        # for top in top_5:\n",
    "        #     print(f\"Sequence: {top[0]} Expected valance: {top[1]}\")\n",
    "        \n",
    "        # print(f\"Action choisie : {eval(top_5[0][0])[0]}\")\n",
    "        return eval(top_5[0][0])[0]\n",
    "\n",
    "    # Modifier\n",
    "    def action(self, outcome, decide=True, validate_loader=None):\n",
    "        \"\"\" \n",
    "        Fonction qui choisit l'action a faire en fonction de la dernière \\\n",
    "        intéraction avec l'environnement. \\n\n",
    "        C'est ici que nous allons implémenter un mécanisme de ML \\\n",
    "        pour choisir la prochaine action.\n",
    "\n",
    "        :param: **outcome** feedback de la dernière intéraction avec l'environnement\n",
    "\n",
    "        :return: **action** action à effectuer\n",
    "        \"\"\"\n",
    "        if self._action is not None:\n",
    "            self._history_fb.append(outcome)\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {outcome}, \" \n",
    "                  f\"\\033[0;31m Satisfaction: {self._predicted_outcome == outcome} \\033[0m\")\n",
    "            if len(self._history_act) + len(self._history_fb) > self._model.input_size:\n",
    "                # print(\"j'ai assez de données pour entrainer\")\n",
    "                if self._predicted_outcome != outcome:\n",
    "                    self.fit(self._history_act, self._history_fb, validate_loader=validate_loader)\n",
    "                    self._bored = 0\n",
    "                else:\n",
    "                    self._action = self._all_actions[0]\n",
    "                if self._bored >= 10: # Ajout de bored qui permet de relancer l'entrainement\n",
    "                    self._bored = 0\n",
    "                    self.fit(self._history_act, self._history_fb, validate_loader=validate_loader)\n",
    "                    decide = False\n",
    "            if decide:\n",
    "                self._action = self.decide()\n",
    "            else :\n",
    "                self._action = str(np.random.choice(self._all_actions))\n",
    "            if len(self._history_act) + len(self._history_fb) + 1 > self._model.input_size:\n",
    "                self._predicted_outcome = self.predict(self._action)\n",
    "            self._history_act.append(self._action)\n",
    "            self._bored += 1\n",
    "        else:\n",
    "            self._action = self._all_actions[0]\n",
    "            self._history_act.append(self._action)            \n",
    "            # print(f\"Action de base : {self._action} Prediction: {self._predicted_outcome}\")\n",
    "        \n",
    "        return self._action, self._predicted_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test2 = small_loop(x=1, y=1, theta=0, world=np.array([\n",
    "                [1, 1, 1, 1,1,1,1,1, 1],\n",
    "                [1, 0, 0, 0,0,0,0,0, 1],\n",
    "                [1, 1, 1, 1,1,1,1,1, 1],\n",
    "            ]))\n",
    "\n",
    "model_ML = DeepNetwork(hidden_size=[16], input_size=3, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 5,\n",
    "    inter('forward', 'wall') : -90,\n",
    "    inter('turn_left', 'empty') : -7,\n",
    "    inter('turn_left', 'wall') : -7,\n",
    "    inter('turn_right', 'empty') : -7,\n",
    "    inter('turn_right', 'wall') : -7,\n",
    "    inter('feel_front', 'wall') : -1,\n",
    "    inter('feel_front', 'empty') : -1,\n",
    "}\n",
    "\n",
    "agent_test2 = Agentbored(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    valence=valence,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "for i in range(200):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, decide=True)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    # print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    # print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    # print(\"\\n\")\n",
    "    # sleep(0.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test2.display_world(out)\n",
    "display(out)\n",
    "\n",
    "action, predi = agent_test2.action(outcome, decide=True)\n",
    "outcome = env_test2.outcome(action)\n",
    "history_good.append(outcome == predi)\n",
    "# print(f'action {action} predi {predi} outcome {outcome}')\n",
    "# print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "# print(\"\\n\")\n",
    "# sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_proba_from_seq(_seg):\n",
    "    seq = tokenizer.encode(_seg)\n",
    "    seq = torch.tensor(seq, dtype=torch.float).to(device)\n",
    "    predi =  agent_test2._model(seq)\n",
    "    prob = torch.nn.functional.softmax(predi, dim=0)\n",
    "    deocde = tokenizer.decode(torch.argmax(predi, dim=0).item())\n",
    "    print(f\"Prédiction de la séquence {_seg} :  probabilité {prob.tolist()}, decode {deocde}\")\n",
    "\n",
    "print(\"porba si on avance\")\n",
    "show_proba_from_seq(['forward', 'empty', 'forward'])\n",
    "show_proba_from_seq(['forward', 'wall', 'forward'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'forward'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'forward'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'forward'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'forward'])\n",
    "\n",
    "print(\"porba si on turn left\")\n",
    "show_proba_from_seq(['forward', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['forward', 'wall', 'turn_left'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'turn_left'])\n",
    "\n",
    "print(\"porba si on turn right\")\n",
    "show_proba_from_seq(['forward', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['forward', 'wall', 'turn_right'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'turn_right'])\n",
    "\n",
    "print(\"porba si on feel front\")\n",
    "show_proba_from_seq(['forward', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['forward', 'wall', 'feel_front'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'feel_front'])\n",
    "\n",
    "def count_pattern(lst, pattern):\n",
    "    return sum(1 for i in range(len(lst) - len(pattern) + 1) if lst[i:i+len(pattern)] == pattern)\n",
    "\n",
    "list_act_out = []\n",
    "for act, out in zip(agent_test2._history_act, agent_test2._history_fb):\n",
    "    list_act_out.append(act)\n",
    "    list_act_out.append(out)\n",
    "\n",
    "print(\"count si on avance\")\n",
    "print(f\"pattern 'forward', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['forward', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['forward', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'forward', 'empty' présent : {count_pattern(list_act_out, ['forward', 'wall', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'forward', 'wall' présent : {count_pattern(list_act_out, ['forward', 'wall', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'forward', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'forward', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'forward', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'forward', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'forward', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'forward', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'forward', 'wall'])}\")\n",
    "\n",
    "print(\"count si on feel_front\")\n",
    "print(f\"pattern 'forward', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['forward', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['forward', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['forward', 'wall', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'forward', 'wall', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['forward', 'wall', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'empty', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'empty', 'feel_front', 'wall'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'feel_front', 'empty' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'feel_front', 'empty'])}\")\n",
    "print(f\"pattern 'feel_front', 'wall', 'feel_front', 'wall' présent : {count_pattern(list_act_out, ['feel_front', 'wall', 'feel_front', 'wall'])}\")\n",
    "\n",
    "print(\"count si on turn_left turn_right\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'turn_left', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'turn_left', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'turn_left', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'turn_left', 'empty'])}\")\n",
    "print(f\"pattern 'turn_right', 'empty', 'turn_right', 'empty' présent : {count_pattern(list_act_out, ['turn_right', 'empty', 'turn_right', 'empty'])}\")\n",
    "print(f\"pattern 'turn_left', 'empty', 'turn_right', 'empty' présent : {count_pattern(list_act_out, ['turn_left', 'empty', 'turn_right', 'empty'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Fin \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randome + decision\n",
    "count si on avance\n",
    "- pattern 'forward', 'empty', 'forward', 'empty' présent : 4\n",
    "- pattern 'forward', 'empty', 'forward', 'wall' présent : 5\n",
    "- pattern 'forward', 'wall', 'forward', 'empty' présent : 0\n",
    "- pattern 'forward', 'wall', 'forward', 'wall' présent : 48\n",
    "- pattern 'turn_left', 'empty', 'forward', 'empty' présent : 5\n",
    "- pattern 'turn_left', 'empty', 'forward', 'wall' présent : 1\n",
    "- pattern 'turn_right', 'empty', 'forward', 'empty' présent : 7\n",
    "- pattern 'turn_right', 'empty', 'forward', 'wall' présent : 5\n",
    "- pattern 'feel_front', 'empty', 'forward', 'empty' présent : 4\n",
    "- pattern 'feel_front', 'empty', 'forward', 'wall' présent : 0\n",
    "- pattern 'feel_front', 'wall', 'forward', 'empty' présent : 0\n",
    "- pattern 'feel_front', 'wall', 'forward', 'wall' présent : 15\n",
    "\n",
    "count si on feel_front\n",
    "- pattern 'forward', 'empty', 'feel_front', 'empty' présent : 1\n",
    "- pattern 'forward', 'empty', 'feel_front', 'wall' présent : 2\n",
    "- pattern 'forward', 'wall', 'feel_front', 'empty' présent : 0\n",
    "- pattern 'forward', 'wall', 'feel_front', 'wall' présent : 17\n",
    "- pattern 'turn_left', 'empty', 'feel_front', 'empty' présent : 5\n",
    "- pattern 'turn_left', 'empty', 'feel_front', 'wall' présent : 6\n",
    "- pattern 'turn_right', 'empty', 'feel_front', 'empty' présent : 3\n",
    "- pattern 'turn_right', 'empty', 'feel_front', 'wall' présent : 3\n",
    "- pattern 'feel_front', 'empty', 'feel_front', 'empty' présent : 0\n",
    "- pattern 'feel_front', 'empty', 'feel_front', 'wall' présent : 0\n",
    "- pattern 'feel_front', 'wall', 'feel_front', 'empty' présent : 0\n",
    "- pattern 'feel_front', 'wall', 'feel_front', 'wall' présent : 1\n",
    "\n",
    "count si on turn_left turn_right\n",
    "- pattern 'turn_left', 'empty', 'turn_left', 'empty' présent : 3\n",
    "- pattern 'turn_right', 'empty', 'turn_left', 'empty' présent : 10\n",
    "- pattern 'turn_right', 'empty', 'turn_right', 'empty' présent : 11\n",
    "- pattern 'turn_left', 'empty', 'turn_right', 'empty' présent : 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juste randome\n",
    "\n",
    "count si on avance\n",
    "- pattern 'forward', 'empty', 'forward', 'empty' présent : 2\n",
    "- pattern 'forward', 'empty', 'forward', 'wall' présent : 0\n",
    "- pattern 'forward', 'wall', 'forward', 'empty' présent : 0\n",
    "- pattern 'forward', 'wall', 'forward', 'wall' présent : 3\n",
    "- pattern 'turn_left', 'empty', 'forward', 'empty' présent : 3\n",
    "- pattern 'turn_left', 'empty', 'forward', 'wall' présent : 2\n",
    "- pattern 'turn_right', 'empty', 'forward', 'empty' présent : 3\n",
    "- pattern 'turn_right', 'empty', 'forward', 'wall' présent : 6\n",
    "- pattern 'feel_front', 'empty', 'forward', 'empty' présent : 2\n",
    "- pattern 'feel_front', 'empty', 'forward', 'wall' présent : 0\n",
    "- pattern 'feel_front', 'wall', 'forward', 'empty' présent : 0\n",
    "- pattern 'feel_front', 'wall', 'forward', 'wall' présent : 4\n",
    "\n",
    "count si on feel_front\n",
    "- pattern 'forward', 'empty', 'feel_front', 'empty' présent : 1\n",
    "- pattern 'forward', 'empty', 'feel_front', 'wall' présent : 2\n",
    "- pattern 'forward', 'wall', 'feel_front', 'empty' présent : 0\n",
    "- pattern 'forward', 'wall', 'feel_front', 'wall' présent : 4\n",
    "- pattern 'turn_left', 'empty', 'feel_front', 'empty' présent : 0\n",
    "- pattern 'turn_left', 'empty', 'feel_front', 'wall' présent : 2\n",
    "- pattern 'turn_right', 'empty', 'feel_front', 'empty' présent : 3\n",
    "- pattern 'turn_right', 'empty', 'feel_front', 'wall' présent : 4\n",
    "- pattern 'feel_front', 'empty', 'feel_front', 'empty' présent : 0\n",
    "- pattern 'feel_front', 'empty', 'feel_front', 'wall' présent : 0\n",
    "- pattern 'feel_front', 'wall', 'feel_front', 'empty' présent : 0\n",
    "- pattern 'feel_front', 'wall', 'feel_front', 'wall' présent : 8\n",
    "\n",
    "count si on turn_left turn_right\n",
    "- pattern 'turn_left', 'empty', 'turn_left', 'empty' présent : 8\n",
    "- pattern 'turn_right', 'empty', 'turn_left', 'empty' présent : 4\n",
    "- pattern 'turn_right', 'empty', 'turn_right', 'empty' présent : 5\n",
    "- pattern 'turn_left', 'empty', 'turn_right', 'empty' présent : 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilité obtenut après 500 itération :\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 1,\n",
    "    inter('forward', 'wall') : -30,\n",
    "    inter('turn_left', 'empty') : -5,\n",
    "    inter('turn_left', 'wall') : -5,\n",
    "    inter('turn_right', 'empty') : -5,\n",
    "    inter('turn_right', 'wall') : -5,\n",
    "    inter('feel_front', 'wall') : -5,\n",
    "    inter('feel_front', 'empty') : -5,\n",
    "}\n",
    "## porba si on avance\n",
    "- Prédiction de la séquence ['forward', 'empty', 'forward'] :  probabilité [0.49284496903419495, 0.5071550607681274], decode empty  \n",
    "    Sur cette séquence, il est normal que le modèl ne soit pas sur. Mais a cause de du mécansime de décision on peut s'attendre a ce que cette probabilité soit très variable\n",
    "- Prédiction de la séquence ['forward', 'wall', 'forward'] :  probabilité [0.9998795986175537, 0.00012042034359183162], decode wall  \n",
    "    Ici la prédiciton est logique et sur\n",
    "- Prédiction de la séquence ['turn_left', 'empty', 'forward'] :  probabilité [0.2555951178073883, 0.7444049119949341], decode empty  \n",
    "    Pour cette sequence on voit l'effet négatif du mécanisme de décision. Vue que le robot ne fais plus d'action aléatoire certain pattern deviennent sous représenté et d'autre au contraire sont trop présent (je reviens sur ce phénomen après) \n",
    "- Prédiction de la séquence ['turn_right', 'empty', 'forward'] :  probabilité [0.10819040238857269, 0.8918095827102661], decode empty  \n",
    "    Cette séquence prouve ce que je raconte au dessus, l'action tourné a droite a plus souvent aboutit sur empty.\n",
    "- Prédiction de la séquence ['feel_front', 'empty', 'forward'] :  probabilité [0.04110181704163551, 0.9588981866836548], decode empty  \n",
    "    Séquence parfaite et pertinent pour l'agent.\n",
    "- Prédiction de la séquence ['feel_front', 'wall', 'forward'] :  probabilité [0.9806479811668396, 0.019352057948708534], decode wall\n",
    "\n",
    "## porba si on turn left\n",
    "- Prédiction de la séquence ['forward', 'empty', 'turn_left'] :  probabilité [0.2387831062078476, 0.7612168788909912], decode empty  \n",
    "    étonnant, le modèl n'a pas encore compris que `turn left` (ou `turn right`) renvois tout le temps empty \n",
    "- Prédiction de la séquence ['forward', 'wall', 'turn_left'] :  probabilité [0.9999569654464722, 4.300793443690054e-05], decode wall  \n",
    "    Abérent\n",
    "- Prédiction de la séquence ['turn_left', 'empty', 'turn_left'] :  probabilité [0.023873118683695793, 0.9761269092559814], decode empty  \n",
    "- Prédiction de la séquence ['turn_right', 'empty', 'turn_left'] :  probabilité [0.000307118782075122, 0.9996929168701172], decode empty  \n",
    "- Prédiction de la séquence ['feel_front', 'empty', 'turn_left'] :  probabilité [0.00010853375715669245, 0.9998914003372192], decode empty  \n",
    "- Prédiction de la séquence ['feel_front', 'wall', 'turn_left'] :  probabilité [0.11550579220056534, 0.8844942450523376], decode empty  \n",
    "\n",
    "\n",
    "## porba si on turn right\n",
    "- Prédiction de la séquence ['forward', 'empty', 'turn_right'] :  probabilité [0.42596185207366943, 0.5740381479263306], decode empty  \n",
    "- Prédiction de la séquence ['forward', 'wall', 'turn_right'] :  probabilité [0.9999817609786987, 1.8181275663664564e-05], decode wall  \n",
    "    Abérent\n",
    "- Prédiction de la séquence ['turn_left', 'empty', 'turn_right'] :  probabilité [0.1661546677350998, 0.8338453769683838], decode empty  \n",
    "- Prédiction de la séquence ['turn_right', 'empty', 'turn_right'] :  probabilité [0.0050218072719872, 0.994978129863739], decode empty  \n",
    "- Prédiction de la séquence ['feel_front', 'empty', 'turn_right'] :  probabilité [3.543415732565336e-05, 0.9999645948410034], decode empty  \n",
    "- Prédiction de la séquence ['feel_front', 'wall', 'turn_right'] :  probabilité [0.7933242321014404, 0.20667579770088196], decode wall  \n",
    "\n",
    "## porba si on feel front\n",
    "- Prédiction de la séquence ['forward', 'empty', 'feel_front'] :  probabilité [0.6370701789855957, 0.3629298508167267], decode wall  \n",
    "    Pourquoi pas, mais toujours sensible aux interactions faites avant.\n",
    "- Prédiction de la séquence ['forward', 'wall', 'feel_front'] :  probabilité [0.9999923706054688, 7.685893251618836e-06], decode wall  \n",
    "    C'est ce qu'on veut\n",
    "- Prédiction de la séquence ['turn_left', 'empty', 'feel_front'] :  probabilité [0.3203613758087158, 0.679638683795929], decode empty  \n",
    "    Pourquoi pas, mais toujours sensible aux interactions faites avant.\n",
    "- Prédiction de la séquence ['turn_right', 'empty', 'feel_front'] :  probabilité [0.11235635727643967, 0.8876436352729797], decode empty  \n",
    "    Pourquoi pas, mais toujours sensible aux interactions faites avant.\n",
    "- Prédiction de la séquence ['feel_front', 'empty', 'feel_front'] :  probabilité [0.0010404880158603191, 0.9989595413208008], decode empty  \n",
    "    C'est ce qu'on veut\n",
    "- Prédiction de la séquence ['feel_front', 'wall', 'feel_front'] :  probabilité [0.9912147521972656, 0.008785244077444077], decode wall  \n",
    "    C'est ce qu'on veut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test2 = small_loop(x=1, y=1, theta=0)\n",
    "\n",
    "model_ML = DeepNetwork(hidden_size=[16], input_size=3, output_size=2)\n",
    "optimizer = torch.optim.Adam(model_ML.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tokenizer = SimpleTokenizerV1(create_dico_numerate_word(env_test2.get_outcomes() + env_test2.get_actions()))\n",
    "\n",
    "valence = {\n",
    "    inter('forward', 'empty') : 1,\n",
    "    inter('forward', 'wall') : -30,\n",
    "    inter('turn_left', 'empty') : -10,\n",
    "    inter('turn_left', 'wall') : -10,\n",
    "    inter('turn_right', 'empty') : -10,\n",
    "    inter('turn_right', 'wall') : -10,\n",
    "    inter('feel_front', 'wall') : -5,\n",
    "    inter('feel_front', 'empty') : -5,\n",
    "}\n",
    "\n",
    "agent_test2 = Agentbored(\n",
    "    model=model_ML,\n",
    "    all_outcomes= env_test2.get_outcomes(),\n",
    "    all_actions= env_test2.get_actions(),\n",
    "    valence=valence,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func)\n",
    "\n",
    "history_good = []\n",
    "pourcent_by_10 = []\n",
    "outcome = None\n",
    "for i in range(10):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, False)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "for i in range(500):\n",
    "    print(f\"=======================\\033[0;32m iteration {i} \\033[0m=======================\")\n",
    "    action, predi = agent_test2.action(outcome, decide=True)\n",
    "    outcome = env_test2.outcome(action)\n",
    "    history_good.append(outcome == predi)\n",
    "    pourcent_by_10.append(sum(history_good[-10:]) * 10 if len(history_good) >= 10 else 0)\n",
    "    print(f'action {action} predi {predi} outcome {outcome}')\n",
    "    print(f\"Action choisie : {action} \\033[0;34m{pourcent_by_10[-1]} \\033[0m\")\n",
    "    print(\"\\n\")\n",
    "    # sleep(0.5)\n",
    "    # EnvDisplay.show(env_test2.get_world(), env_test2.get_robot(), out)\n",
    "    # display(out)\n",
    "    # time.sleep(0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_proba_from_seq(_seg):\n",
    "    seq = tokenizer.encode(_seg)\n",
    "    seq = torch.tensor(seq, dtype=torch.float).to(device)\n",
    "    predi =  agent_test2._model(seq)\n",
    "    prob = torch.nn.functional.softmax(predi, dim=0)\n",
    "    deocde = tokenizer.decode(torch.argmax(predi, dim=0).item())\n",
    "    print(f\"Prédiction de la séquence {_seg} :  probabilité {prob.tolist()}, decode {deocde}\")\n",
    "\n",
    "print(\"porba si on avance\")\n",
    "show_proba_from_seq(['forward', 'empty', 'forward'])\n",
    "show_proba_from_seq(['forward', 'wall', 'forward'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'forward'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'forward'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'forward'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'forward'])\n",
    "\n",
    "print(\"porba si on turn left\")\n",
    "show_proba_from_seq(['forward', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['forward', 'wall', 'turn_left'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'turn_left'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'turn_left'])\n",
    "\n",
    "print(\"porba si on turn right\")\n",
    "show_proba_from_seq(['forward', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['forward', 'wall', 'turn_right'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'turn_right'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'turn_right'])\n",
    "\n",
    "print(\"porba si on feel front\")\n",
    "show_proba_from_seq(['forward', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['forward', 'wall', 'feel_front'])\n",
    "show_proba_from_seq(['turn_left', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['turn_right', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['feel_front', 'empty', 'feel_front'])\n",
    "show_proba_from_seq(['feel_front', 'wall', 'feel_front'])\n",
    "\n",
    "# Save in file txt agent._histoty_act and agent._history_fb\n",
    "\n",
    "with open('history_agent.txt', 'w') as f:\n",
    "    list_to_save = zip(agent_test2._history_act, agent_test2._history_fb)\n",
    "    for x, y in list_to_save:\n",
    "        f.write(str(f\"{x}, {y}; \")) \n",
    "    f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# def show_decision_from_seq(_seg):\n",
    "#     gap = (agent_test2._model.input_size - 1) // 2\n",
    "#     x = []\n",
    "#     for i in range(len(agent_test2._history_act) - gap, len(agent_test2._history_act)):\n",
    "#         x.append(self._history_act[i])\n",
    "#         x.append(agent_test2._history_fb[i])\n",
    "#     seq = self._tokenizer.encode(x)\n",
    "#     res = self.recursif_expective_valance(seq=seq, max_depth=3, seuil=0.2)\n",
    "#     top_5 = sorted(res.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "#     print(f\"Top 5 of sequences with the best expected valance for {x}\")\n",
    "#     for top in top_5:\n",
    "#         print(f\"Sequence: {top[0]} Expected valance: {top[1]}\")\n",
    "    \n",
    "#     print(f\"Action choisie : {eval(top_5[0][0])[0]}\")\n",
    "#     return eval(top_5[0][0])[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test time training\n",
    "\n",
    "multi op memory literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stage (torch)",
   "language": "python",
   "name": "dpt_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
